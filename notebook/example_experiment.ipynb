{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================\n",
      "++++++++++++++++\n",
      "TRIAL NUMBER 0\n",
      "++++++++++++++++\n",
      "================\n",
      "  > Rereading graph data...\n",
      "  > Initializing multiround object...\n",
      "11758 1049758\n",
      "\n",
      "=========================\n",
      "STARTING ADVER EXPERIMENT\n",
      "=========================\n",
      "MAIN CONFIG: [('device', 'cuda:0'), ('exp_type', 'ADVER'), ('full_oracle', False), ('node_expiration_time', 1), ('round_budget_neg', 0), ('round_budget_pos', 0), ('round_new_neg', 459), ('round_new_pos', 128), ('round_num', 3), ('save_df', True), ('save_embedding', False), ('save_temp_sne', False), ('setting_type', 'INDUCTIVE'), ('task_type', 'NODE'), ('verbose', 4)]\n",
      "MODEL CONFIG: [('act_name', 'LeakyReLU'), ('addon_internal_agg', 'OR'), ('addon_name', 'NONE'), ('addon_perc', 0.05), ('addon_round_window', 7), ('alpha', 0), ('att_heads', 2), ('beta', 0), ('boost_agg_backbone', <class 'models.benchmarks_supervised.booster.GIN_noparam'>), ('boost_metric', <function average_precision_score at 0x000001C8520A5760>), ('boost_predictor', <class 'xgboost.sklearn.XGBClassifier'>), ('device', 'cuda:0'), ('drop_rate', 0), ('dropout_rate', 0.1), ('early_stopping', 75), ('embed_type', 'temporal'), ('etypes', ['none']), ('gamma', 1), ('h_feats', 64), ('k', 1), ('lambda1', 1), ('lambda2', 1), ('lambda_', 1), ('loss_sample', True), ('loss_sample_ratio', 0.1), ('loss_type', 'ndist'), ('mlp_feats', 64), ('mlp_layers', 2), ('model_name', 'XGB-SP'), ('n_estimator', 500), ('norm_name', 'layer'), ('num_epoch', 100), ('num_layers', 2), ('num_round_epoch', 50), ('round_window', 7), ('temporal_agg', 'mean_final'), ('tloss_type', 'normal'), ('training_type', 'round'), ('verbose', 4)]\n",
      "STRAT CONFIG: [('augment_name', 'REAGE'), ('augment_neg_ratio', 0.09772070079945569), ('augment_pos_ratio', 0.02727929920054431), ('augment_prob', 0.5), ('augment_round_split', 8), ('device', 'cuda:0'), ('pseudo_name', 'NONE'), ('pseudo_new_pos_only', False), ('pseudo_skip_initial', False), ('verbose', 4)]\n",
      "TRAIN CONFIG: [('device', 'cuda:0'), ('dset_name', 'tolokers_bid'), ('early_stopping', 75), ('learning_rate', 0.01), ('loss', <function cross_entropy at 0x000001C84AF6F880>), ('loss_gamma', 5), ('num_epoch', 100), ('num_round_epoch', 50), ('optimizer', <class 'torch.optim.adam.Adam'>), ('random_state', 7777777), ('ratio_initial', 0.5), ('ratio_test', 0.2), ('ratio_train', 0.6), ('ratio_val', 0.2), ('round_reset_model', False), ('round_train_list', []), ('stuck_stopping', 50), ('stuck_threshold', 0), ('train_max_retry', 10), ('verbose', 4)]\n",
      "ADVER CONFIG: [('adver_choose_name', 'GREEDY'), ('adver_conn_coef', 0.1), ('adver_feat_coef', 1), ('adver_mod_name', 'REPLAY'), ('device', 'cuda:0'), ('verbose', 4)]\n",
      "\n",
      "INITIAL GRAPH: 11758 {1: 2566, 0: 9192} Nodes, 1049758 Edges\n",
      "=========================\n",
      "        \n",
      ">> Initializing graph...\n",
      "MAIN GRAPH: 11758 {1: 2566, 0: 9192} Nodes, 1049758 Edges\n",
      "\n",
      ">> Initializing experiment type-specific modules:\n",
      "  >> Initializing adversarial modules...\n",
      ">> Initializing task-specific modules:\n",
      ">> Initializing classifier model...\n",
      ">> Initializing other strategies...\n",
      "\n",
      "FINISH INITIALIZATION\n",
      "=====================\n",
      "        \n",
      "  > Starting round 0...\n",
      "\n",
      "+++++++++++++++++\n",
      "STARTING ROUND 0!\n",
      "+++++++++++++++++\n",
      "        \n",
      "ASSIGNED GRAPH FOR TRAINING: 5879 {0: 4594, 1: 1285} Nodes, 266971 Edges\n",
      "\n",
      ">> Splitting to train-validation set...\n",
      "Alotting train-val-test split for round 0\n",
      "Initial pool: 0, Prediction pool: 0, Budget pool: 0, Full pool: 5879\n",
      "Full graph size: 5879, Training: 3527, Val: 2352, Test: 5879\n",
      "    >> INITIAL DATA SPLIT: 3527 train rows ({0: 2756, 1: 771}) | 2352 val rows ({0: 1838, 1: 514}) | 0 test rows ({})\n",
      "    >> AUGMENTED DATA SPLIT: 3527 train rows ({0: 2756, 1: 771}) | 2352 val rows ({0: 1838, 1: 514}) | 0 test rows ({})\n",
      "    >> Updated cross-entropy weight to 3.574578469520104...\n",
      "\n",
      "-----------------------\n",
      "STARTING MODEL TRAININGps_l\n",
      "-----------------------\n",
      "\n",
      "TRAINING NODE EMBEDDER\n",
      "Detailed Loss: recon 0.68634450-0.68634450 | disen 0.69920760-0.00000000 | temporal 0.65196383-0.00000000 | total 0.68634450\n",
      "Epoch 1, loss: 0.68634450-(best 0.68634450)\n",
      "\n",
      "Detailed Loss: recon 0.46272284-0.46272284 | disen 0.72593647-0.00000000 | temporal 0.66736954-0.00000000 | total 0.46272284\n",
      "Epoch 2, loss: 0.46272284-(best 0.46272284)\n",
      "\n",
      "Detailed Loss: recon 0.44085264-0.44085264 | disen 0.73215073-0.00000000 | temporal 0.67321414-0.00000000 | total 0.44085264\n",
      "Epoch 3, loss: 0.44085264-(best 0.44085264)\n",
      "\n",
      "Detailed Loss: recon 0.44742024-0.44742024 | disen 0.73600066-0.00000000 | temporal 0.67615211-0.00000000 | total 0.44742024\n",
      "Epoch 4, loss: 0.44742024-(best 0.44085264)\n",
      "\n",
      "Detailed Loss: recon 0.43350586-0.43350586 | disen 0.73771548-0.00000000 | temporal 0.67757934-0.00000000 | total 0.43350586\n",
      "Epoch 5, loss: 0.43350586-(best 0.43350586)\n",
      "\n",
      "Detailed Loss: recon 0.42430109-0.42430109 | disen 0.74029863-0.00000000 | temporal 0.67736095-0.00000000 | total 0.42430109\n",
      "Epoch 6, loss: 0.42430109-(best 0.42430109)\n",
      "\n",
      "Detailed Loss: recon 0.39644727-0.39644727 | disen 0.73729134-0.00000000 | temporal 0.67670482-0.00000000 | total 0.39644727\n",
      "Epoch 7, loss: 0.39644727-(best 0.39644727)\n",
      "\n",
      "Detailed Loss: recon 0.38459343-0.38459343 | disen 0.73606163-0.00000000 | temporal 0.67596608-0.00000000 | total 0.38459343\n",
      "Epoch 8, loss: 0.38459343-(best 0.38459343)\n",
      "\n",
      "Detailed Loss: recon 0.37914547-0.37914547 | disen 0.73673463-0.00000000 | temporal 0.67489100-0.00000000 | total 0.37914547\n",
      "Epoch 9, loss: 0.37914547-(best 0.37914547)\n",
      "\n",
      "Detailed Loss: recon 0.37379488-0.37379488 | disen 0.73892820-0.00000000 | temporal 0.67369050-0.00000000 | total 0.37379488\n",
      "Epoch 10, loss: 0.37379488-(best 0.37379488)\n",
      "\n",
      "Detailed Loss: recon 0.37027967-0.37027967 | disen 0.73540235-0.00000000 | temporal 0.67293596-0.00000000 | total 0.37027967\n",
      "Epoch 11, loss: 0.37027967-(best 0.37027967)\n",
      "\n",
      "Detailed Loss: recon 0.36676526-0.36676526 | disen 0.73700130-0.00000000 | temporal 0.67268544-0.00000000 | total 0.36676526\n",
      "Epoch 12, loss: 0.36676526-(best 0.36676526)\n",
      "\n",
      "Detailed Loss: recon 0.36200076-0.36200076 | disen 0.73808753-0.00000000 | temporal 0.67305458-0.00000000 | total 0.36200076\n",
      "Epoch 13, loss: 0.36200076-(best 0.36200076)\n",
      "\n",
      "Detailed Loss: recon 0.35881218-0.35881218 | disen 0.73268425-0.00000000 | temporal 0.67383534-0.00000000 | total 0.35881218\n",
      "Epoch 14, loss: 0.35881218-(best 0.35881218)\n",
      "\n",
      "Detailed Loss: recon 0.34006453-0.34006453 | disen 0.73393637-0.00000000 | temporal 0.67439729-0.00000000 | total 0.34006453\n",
      "Epoch 15, loss: 0.34006453-(best 0.34006453)\n",
      "\n",
      "Detailed Loss: recon 0.34634441-0.34634441 | disen 0.73333192-0.00000000 | temporal 0.67528486-0.00000000 | total 0.34634441\n",
      "Epoch 16, loss: 0.34634441-(best 0.34006453)\n",
      "\n",
      "Detailed Loss: recon 0.32720357-0.32720357 | disen 0.73854738-0.00000000 | temporal 0.67627031-0.00000000 | total 0.32720357\n",
      "Epoch 17, loss: 0.32720357-(best 0.32720357)\n",
      "\n",
      "Detailed Loss: recon 0.32822114-0.32822114 | disen 0.73499393-0.00000000 | temporal 0.67654157-0.00000000 | total 0.32822114\n",
      "Epoch 18, loss: 0.32822114-(best 0.32720357)\n",
      "\n",
      "Detailed Loss: recon 0.32629615-0.32629615 | disen 0.73389912-0.00000000 | temporal 0.67669839-0.00000000 | total 0.32629615\n",
      "Epoch 19, loss: 0.32629615-(best 0.32629615)\n",
      "\n",
      "Detailed Loss: recon 0.32745436-0.32745436 | disen 0.73239088-0.00000000 | temporal 0.67691755-0.00000000 | total 0.32745436\n",
      "Epoch 20, loss: 0.32745436-(best 0.32629615)\n",
      "\n",
      "Detailed Loss: recon 0.31639278-0.31639278 | disen 0.73140156-0.00000000 | temporal 0.67680979-0.00000000 | total 0.31639278\n",
      "Epoch 21, loss: 0.31639278-(best 0.31639278)\n",
      "\n",
      "Detailed Loss: recon 0.31408289-0.31408289 | disen 0.73433012-0.00000000 | temporal 0.67668217-0.00000000 | total 0.31408289\n",
      "Epoch 22, loss: 0.31408289-(best 0.31408289)\n",
      "\n",
      "Detailed Loss: recon 0.31007165-0.31007165 | disen 0.73359406-0.00000000 | temporal 0.67665368-0.00000000 | total 0.31007165\n",
      "Epoch 23, loss: 0.31007165-(best 0.31007165)\n",
      "\n",
      "Detailed Loss: recon 0.30632344-0.30632344 | disen 0.73040169-0.00000000 | temporal 0.67666709-0.00000000 | total 0.30632344\n",
      "Epoch 24, loss: 0.30632344-(best 0.30632344)\n",
      "\n",
      "Detailed Loss: recon 0.30459970-0.30459970 | disen 0.73211801-0.00000000 | temporal 0.67670250-0.00000000 | total 0.30459970\n",
      "Epoch 25, loss: 0.30459970-(best 0.30459970)\n",
      "\n",
      "Detailed Loss: recon 0.30945593-0.30945593 | disen 0.73015666-0.00000000 | temporal 0.67679632-0.00000000 | total 0.30945593\n",
      "Epoch 26, loss: 0.30945593-(best 0.30459970)\n",
      "\n",
      "Detailed Loss: recon 0.29679090-0.29679090 | disen 0.73148644-0.00000000 | temporal 0.67687201-0.00000000 | total 0.29679090\n",
      "Epoch 27, loss: 0.29679090-(best 0.29679090)\n",
      "\n",
      "Detailed Loss: recon 0.29652321-0.29652321 | disen 0.73066044-0.00000000 | temporal 0.67684889-0.00000000 | total 0.29652321\n",
      "Epoch 28, loss: 0.29652321-(best 0.29652321)\n",
      "\n",
      "Detailed Loss: recon 0.29435849-0.29435849 | disen 0.73205388-0.00000000 | temporal 0.67689550-0.00000000 | total 0.29435849\n",
      "Epoch 29, loss: 0.29435849-(best 0.29435849)\n",
      "\n",
      "Detailed Loss: recon 0.29539347-0.29539347 | disen 0.72845197-0.00000000 | temporal 0.67668623-0.00000000 | total 0.29539347\n",
      "Epoch 30, loss: 0.29539347-(best 0.29435849)\n",
      "\n",
      "Detailed Loss: recon 0.29286787-0.29286787 | disen 0.72824293-0.00000000 | temporal 0.67661202-0.00000000 | total 0.29286787\n",
      "Epoch 31, loss: 0.29286787-(best 0.29286787)\n",
      "\n",
      "Detailed Loss: recon 0.29239023-0.29239023 | disen 0.73092908-0.00000000 | temporal 0.67665267-0.00000000 | total 0.29239023\n",
      "Epoch 32, loss: 0.29239023-(best 0.29239023)\n",
      "\n",
      "Detailed Loss: recon 0.28815258-0.28815258 | disen 0.73065835-0.00000000 | temporal 0.67687017-0.00000000 | total 0.28815258\n",
      "Epoch 33, loss: 0.28815258-(best 0.28815258)\n",
      "\n",
      "Detailed Loss: recon 0.28913903-0.28913903 | disen 0.73168510-0.00000000 | temporal 0.67721748-0.00000000 | total 0.28913903\n",
      "Epoch 34, loss: 0.28913903-(best 0.28815258)\n",
      "\n",
      "Detailed Loss: recon 0.28769907-0.28769907 | disen 0.73060262-0.00000000 | temporal 0.67712027-0.00000000 | total 0.28769907\n",
      "Epoch 35, loss: 0.28769907-(best 0.28769907)\n",
      "\n",
      "Detailed Loss: recon 0.28652647-0.28652647 | disen 0.72944534-0.00000000 | temporal 0.67718524-0.00000000 | total 0.28652647\n",
      "Epoch 36, loss: 0.28652647-(best 0.28652647)\n",
      "\n",
      "Detailed Loss: recon 0.28518283-0.28518283 | disen 0.73023164-0.00000000 | temporal 0.67739135-0.00000000 | total 0.28518283\n",
      "Epoch 37, loss: 0.28518283-(best 0.28518283)\n",
      "\n",
      "Detailed Loss: recon 0.27989227-0.27989227 | disen 0.72948241-0.00000000 | temporal 0.67735827-0.00000000 | total 0.27989227\n",
      "Epoch 38, loss: 0.27989227-(best 0.27989227)\n",
      "\n",
      "Detailed Loss: recon 0.27852669-0.27852669 | disen 0.72993088-0.00000000 | temporal 0.67750192-0.00000000 | total 0.27852669\n",
      "Epoch 39, loss: 0.27852669-(best 0.27852669)\n",
      "\n",
      "Detailed Loss: recon 0.27818227-0.27818227 | disen 0.72993374-0.00000000 | temporal 0.67769778-0.00000000 | total 0.27818227\n",
      "Epoch 40, loss: 0.27818227-(best 0.27818227)\n",
      "\n",
      "Detailed Loss: recon 0.27563003-0.27563003 | disen 0.73091310-0.00000000 | temporal 0.67762285-0.00000000 | total 0.27563003\n",
      "Epoch 41, loss: 0.27563003-(best 0.27563003)\n",
      "\n",
      "Detailed Loss: recon 0.27496821-0.27496821 | disen 0.73269904-0.00000000 | temporal 0.67739564-0.00000000 | total 0.27496821\n",
      "Epoch 42, loss: 0.27496821-(best 0.27496821)\n",
      "\n",
      "Detailed Loss: recon 0.27651173-0.27651173 | disen 0.73111969-0.00000000 | temporal 0.67758405-0.00000000 | total 0.27651173\n",
      "Epoch 43, loss: 0.27651173-(best 0.27496821)\n",
      "\n",
      "Detailed Loss: recon 0.27406779-0.27406779 | disen 0.73267806-0.00000000 | temporal 0.67764562-0.00000000 | total 0.27406779\n",
      "Epoch 44, loss: 0.27406779-(best 0.27406779)\n",
      "\n",
      "Detailed Loss: recon 0.27220750-0.27220750 | disen 0.73141956-0.00000000 | temporal 0.67763340-0.00000000 | total 0.27220750\n",
      "Epoch 45, loss: 0.27220750-(best 0.27220750)\n",
      "\n",
      "Detailed Loss: recon 0.26970035-0.26970035 | disen 0.73040307-0.00000000 | temporal 0.67789984-0.00000000 | total 0.26970035\n",
      "Epoch 46, loss: 0.26970035-(best 0.26970035)\n",
      "\n",
      "Detailed Loss: recon 0.26948637-0.26948637 | disen 0.73098207-0.00000000 | temporal 0.67778981-0.00000000 | total 0.26948637\n",
      "Epoch 47, loss: 0.26948637-(best 0.26948637)\n",
      "\n",
      "Detailed Loss: recon 0.26794791-0.26794791 | disen 0.73214912-0.00000000 | temporal 0.67781460-0.00000000 | total 0.26794791\n",
      "Epoch 48, loss: 0.26794791-(best 0.26794791)\n",
      "\n",
      "Detailed Loss: recon 0.26606971-0.26606971 | disen 0.73188841-0.00000000 | temporal 0.67812043-0.00000000 | total 0.26606971\n",
      "Epoch 49, loss: 0.26606971-(best 0.26606971)\n",
      "\n",
      "Detailed Loss: recon 0.26467827-0.26467827 | disen 0.73239368-0.00000000 | temporal 0.67810136-0.00000000 | total 0.26467827\n",
      "Epoch 50, loss: 0.26467827-(best 0.26467827)\n",
      "\n",
      "Detailed Loss: recon 0.26465866-0.26465866 | disen 0.73270941-0.00000000 | temporal 0.67822701-0.00000000 | total 0.26465866\n",
      "Epoch 51, loss: 0.26465866-(best 0.26465866)\n",
      "\n",
      "Detailed Loss: recon 0.26602620-0.26602620 | disen 0.73227781-0.00000000 | temporal 0.67807209-0.00000000 | total 0.26602620\n",
      "Epoch 52, loss: 0.26602620-(best 0.26465866)\n",
      "\n",
      "Detailed Loss: recon 0.26649785-0.26649785 | disen 0.72976255-0.00000000 | temporal 0.67780131-0.00000000 | total 0.26649785\n",
      "Epoch 53, loss: 0.26649785-(best 0.26465866)\n",
      "\n",
      "Detailed Loss: recon 0.26268521-0.26268521 | disen 0.73293924-0.00000000 | temporal 0.67813653-0.00000000 | total 0.26268521\n",
      "Epoch 54, loss: 0.26268521-(best 0.26268521)\n",
      "\n",
      "Detailed Loss: recon 0.26332265-0.26332265 | disen 0.73339278-0.00000000 | temporal 0.67818594-0.00000000 | total 0.26332265\n",
      "Epoch 55, loss: 0.26332265-(best 0.26268521)\n",
      "\n",
      "Detailed Loss: recon 0.26050085-0.26050085 | disen 0.73010808-0.00000000 | temporal 0.67789835-0.00000000 | total 0.26050085\n",
      "Epoch 56, loss: 0.26050085-(best 0.26050085)\n",
      "\n",
      "Detailed Loss: recon 0.26275441-0.26275441 | disen 0.73097175-0.00000000 | temporal 0.67764169-0.00000000 | total 0.26275441\n",
      "Epoch 57, loss: 0.26275441-(best 0.26050085)\n",
      "\n",
      "Detailed Loss: recon 0.26121002-0.26121002 | disen 0.73356521-0.00000000 | temporal 0.67820144-0.00000000 | total 0.26121002\n",
      "Epoch 58, loss: 0.26121002-(best 0.26050085)\n",
      "\n",
      "Detailed Loss: recon 0.25887862-0.25887862 | disen 0.73478204-0.00000000 | temporal 0.67820442-0.00000000 | total 0.25887862\n",
      "Epoch 59, loss: 0.25887862-(best 0.25887862)\n",
      "\n",
      "Detailed Loss: recon 0.25757360-0.25757360 | disen 0.73221242-0.00000000 | temporal 0.67798543-0.00000000 | total 0.25757360\n",
      "Epoch 60, loss: 0.25757360-(best 0.25757360)\n",
      "\n",
      "Detailed Loss: recon 0.25962368-0.25962368 | disen 0.73329914-0.00000000 | temporal 0.67773128-0.00000000 | total 0.25962368\n",
      "Epoch 61, loss: 0.25962368-(best 0.25757360)\n",
      "\n",
      "Detailed Loss: recon 0.25881466-0.25881466 | disen 0.73194778-0.00000000 | temporal 0.67793155-0.00000000 | total 0.25881466\n",
      "Epoch 62, loss: 0.25881466-(best 0.25757360)\n",
      "\n",
      "Detailed Loss: recon 0.25921467-0.25921467 | disen 0.73549342-0.00000000 | temporal 0.67843240-0.00000000 | total 0.25921467\n",
      "Epoch 63, loss: 0.25921467-(best 0.25757360)\n",
      "\n",
      "Detailed Loss: recon 0.25814086-0.25814086 | disen 0.73430479-0.00000000 | temporal 0.67799044-0.00000000 | total 0.25814086\n",
      "Epoch 64, loss: 0.25814086-(best 0.25757360)\n",
      "\n",
      "Detailed Loss: recon 0.25798285-0.25798285 | disen 0.73375893-0.00000000 | temporal 0.67771131-0.00000000 | total 0.25798285\n",
      "Epoch 65, loss: 0.25798285-(best 0.25757360)\n",
      "\n",
      "Detailed Loss: recon 0.25627401-0.25627401 | disen 0.73340821-0.00000000 | temporal 0.67812181-0.00000000 | total 0.25627401\n",
      "Epoch 66, loss: 0.25627401-(best 0.25627401)\n",
      "\n",
      "Detailed Loss: recon 0.25746769-0.25746769 | disen 0.73359442-0.00000000 | temporal 0.67811090-0.00000000 | total 0.25746769\n",
      "Epoch 67, loss: 0.25746769-(best 0.25627401)\n",
      "\n",
      "Detailed Loss: recon 0.25673348-0.25673348 | disen 0.73236537-0.00000000 | temporal 0.67798990-0.00000000 | total 0.25673348\n",
      "Epoch 68, loss: 0.25673348-(best 0.25627401)\n",
      "\n",
      "Detailed Loss: recon 0.25477743-0.25477743 | disen 0.73004287-0.00000000 | temporal 0.67782086-0.00000000 | total 0.25477743\n",
      "Epoch 69, loss: 0.25477743-(best 0.25477743)\n",
      "\n",
      "Detailed Loss: recon 0.25625774-0.25625774 | disen 0.73169756-0.00000000 | temporal 0.67786199-0.00000000 | total 0.25625774\n",
      "Epoch 70, loss: 0.25625774-(best 0.25477743)\n",
      "\n",
      "Detailed Loss: recon 0.25521410-0.25521410 | disen 0.73250365-0.00000000 | temporal 0.67803299-0.00000000 | total 0.25521410\n",
      "Epoch 71, loss: 0.25521410-(best 0.25477743)\n",
      "\n",
      "Detailed Loss: recon 0.25632781-0.25632781 | disen 0.73126757-0.00000000 | temporal 0.67790914-0.00000000 | total 0.25632781\n",
      "Epoch 72, loss: 0.25632781-(best 0.25477743)\n",
      "\n",
      "Detailed Loss: recon 0.25289249-0.25289249 | disen 0.73086017-0.00000000 | temporal 0.67772400-0.00000000 | total 0.25289249\n",
      "Epoch 73, loss: 0.25289249-(best 0.25289249)\n",
      "\n",
      "Detailed Loss: recon 0.25258839-0.25258839 | disen 0.73195094-0.00000000 | temporal 0.67778349-0.00000000 | total 0.25258839\n",
      "Epoch 74, loss: 0.25258839-(best 0.25258839)\n",
      "\n",
      "Detailed Loss: recon 0.25375530-0.25375530 | disen 0.73128885-0.00000000 | temporal 0.67772257-0.00000000 | total 0.25375530\n",
      "Epoch 75, loss: 0.25375530-(best 0.25258839)\n",
      "\n",
      "Detailed Loss: recon 0.25495356-0.25495356 | disen 0.73288143-0.00000000 | temporal 0.67734122-0.00000000 | total 0.25495356\n",
      "Epoch 76, loss: 0.25495356-(best 0.25258839)\n",
      "\n",
      "Detailed Loss: recon 0.25235069-0.25235069 | disen 0.73171574-0.00000000 | temporal 0.67754012-0.00000000 | total 0.25235069\n",
      "Epoch 77, loss: 0.25235069-(best 0.25235069)\n",
      "\n",
      "Detailed Loss: recon 0.25207037-0.25207037 | disen 0.73326647-0.00000000 | temporal 0.67757767-0.00000000 | total 0.25207037\n",
      "Epoch 78, loss: 0.25207037-(best 0.25207037)\n",
      "\n",
      "Detailed Loss: recon 0.25301319-0.25301319 | disen 0.73399127-0.00000000 | temporal 0.67770994-0.00000000 | total 0.25301319\n",
      "Epoch 79, loss: 0.25301319-(best 0.25207037)\n",
      "\n",
      "Detailed Loss: recon 0.25233263-0.25233263 | disen 0.73474634-0.00000000 | temporal 0.67744839-0.00000000 | total 0.25233263\n",
      "Epoch 80, loss: 0.25233263-(best 0.25207037)\n",
      "\n",
      "Detailed Loss: recon 0.25080848-0.25080848 | disen 0.73397547-0.00000000 | temporal 0.67753941-0.00000000 | total 0.25080848\n",
      "Epoch 81, loss: 0.25080848-(best 0.25080848)\n",
      "\n",
      "Detailed Loss: recon 0.25109169-0.25109169 | disen 0.73369825-0.00000000 | temporal 0.67753512-0.00000000 | total 0.25109169\n",
      "Epoch 82, loss: 0.25109169-(best 0.25080848)\n",
      "\n",
      "Detailed Loss: recon 0.25110558-0.25110558 | disen 0.73189330-0.00000000 | temporal 0.67739952-0.00000000 | total 0.25110558\n",
      "Epoch 83, loss: 0.25110558-(best 0.25080848)\n",
      "\n",
      "Detailed Loss: recon 0.25108379-0.25108379 | disen 0.73164058-0.00000000 | temporal 0.67758793-0.00000000 | total 0.25108379\n",
      "Epoch 84, loss: 0.25108379-(best 0.25080848)\n",
      "\n",
      "Detailed Loss: recon 0.25054309-0.25054309 | disen 0.73382646-0.00000000 | temporal 0.67754692-0.00000000 | total 0.25054309\n",
      "Epoch 85, loss: 0.25054309-(best 0.25054309)\n",
      "\n",
      "Detailed Loss: recon 0.25160357-0.25160357 | disen 0.73335105-0.00000000 | temporal 0.67761064-0.00000000 | total 0.25160357\n",
      "Epoch 86, loss: 0.25160357-(best 0.25054309)\n",
      "\n",
      "Detailed Loss: recon 0.24978766-0.24978766 | disen 0.73317701-0.00000000 | temporal 0.67758417-0.00000000 | total 0.24978766\n",
      "Epoch 87, loss: 0.24978766-(best 0.24978766)\n",
      "\n",
      "Detailed Loss: recon 0.25044107-0.25044107 | disen 0.73531473-0.00000000 | temporal 0.67752254-0.00000000 | total 0.25044107\n",
      "Epoch 88, loss: 0.25044107-(best 0.24978766)\n",
      "\n",
      "Detailed Loss: recon 0.24946877-0.24946877 | disen 0.73232496-0.00000000 | temporal 0.67732936-0.00000000 | total 0.24946877\n",
      "Epoch 89, loss: 0.24946877-(best 0.24946877)\n",
      "\n",
      "Detailed Loss: recon 0.24931072-0.24931072 | disen 0.73340142-0.00000000 | temporal 0.67726600-0.00000000 | total 0.24931072\n",
      "Epoch 90, loss: 0.24931072-(best 0.24931072)\n",
      "\n",
      "Detailed Loss: recon 0.25143597-0.25143597 | disen 0.73364639-0.00000000 | temporal 0.67765278-0.00000000 | total 0.25143597\n",
      "Epoch 91, loss: 0.25143597-(best 0.24931072)\n",
      "\n",
      "Detailed Loss: recon 0.24840182-0.24840182 | disen 0.73315084-0.00000000 | temporal 0.67727220-0.00000000 | total 0.24840182\n",
      "Epoch 92, loss: 0.24840182-(best 0.24840182)\n",
      "\n",
      "Detailed Loss: recon 0.24929512-0.24929512 | disen 0.73346293-0.00000000 | temporal 0.67730665-0.00000000 | total 0.24929512\n",
      "Epoch 93, loss: 0.24929512-(best 0.24840182)\n",
      "\n",
      "Detailed Loss: recon 0.24993680-0.24993680 | disen 0.73529869-0.00000000 | temporal 0.67759937-0.00000000 | total 0.24993680\n",
      "Epoch 94, loss: 0.24993680-(best 0.24840182)\n",
      "\n",
      "Detailed Loss: recon 0.24962489-0.24962489 | disen 0.73178315-0.00000000 | temporal 0.67752767-0.00000000 | total 0.24962489\n",
      "Epoch 95, loss: 0.24962489-(best 0.24840182)\n",
      "\n",
      "Detailed Loss: recon 0.24925749-0.24925749 | disen 0.73292649-0.00000000 | temporal 0.67724723-0.00000000 | total 0.24925749\n",
      "Epoch 96, loss: 0.24925749-(best 0.24840182)\n",
      "\n",
      "Detailed Loss: recon 0.24891475-0.24891475 | disen 0.73551965-0.00000000 | temporal 0.67742127-0.00000000 | total 0.24891475\n",
      "Epoch 97, loss: 0.24891475-(best 0.24840182)\n",
      "\n",
      "Detailed Loss: recon 0.24858555-0.24858555 | disen 0.73372364-0.00000000 | temporal 0.67775083-0.00000000 | total 0.24858555\n",
      "Epoch 98, loss: 0.24858555-(best 0.24840182)\n",
      "\n",
      "Detailed Loss: recon 0.24979341-0.24979341 | disen 0.73567432-0.00000000 | temporal 0.67778260-0.00000000 | total 0.24979341\n",
      "Epoch 99, loss: 0.24979341-(best 0.24840182)\n",
      "\n",
      "Detailed Loss: recon 0.25102448-0.25102448 | disen 0.73423594-0.00000000 | temporal 0.67766154-0.00000000 | total 0.25102448\n",
      "Epoch 100, loss: 0.25102448-(best 0.24840182)\n",
      "\n",
      ">> Reached final epoch. Loading best model...\n",
      "TRAINING XGB PREDICTOR\n",
      ">> Ending training!\n",
      "\n",
      "-----------------------\n",
      "TIME COST: 6.961515665054321 s\n",
      "Best Val: REC 62.45 PRE 43.32 MF1 66.69 AUC 79.33 TP 321 FP 420 TN 1418 FN 193\n",
      "-----------------------\n",
      "\n",
      "ASSIGNED GRAPH FOR EVALUATION: 11758 {1: 2566, 0: 9192} Nodes, 1049758 Edges\n",
      "\n",
      "-------------------\n",
      "STARTING EVALUATION\n",
      "-------------------\n",
      "\n",
      "Dataset - Overall: REC 49.96 PRE 51.49 MF1 68.61 AUC 81.24 TP 1282 FP 1208 TN 7984 FN 1284 | 11758 {1: 2566, 0: 9192}\n",
      "Dataset - Train: REC 54.99 PRE 54.99 MF1 71.20 AUC 84.57 TP 424 FP 347 TN 2409 FN 347 | 3527 {0: 2756, 1: 771}\n",
      "Dataset - Val: REC 49.22 PRE 48.47 MF1 67.20 AUC 79.98 TP 253 FP 269 TN 1569 FN 261 | 2352 {0: 1838, 1: 514}\n",
      "Dataset - Test: REC 47.23 PRE 50.54 MF1 67.58 AUC 79.68 TP 605 FP 592 TN 4006 FN 676 | 5879 {1: 1281, 0: 4598}\n",
      "PREDICTION STATUS - {'1-True': 1890, '0-True': 5186, '0-False': 4006, '1-False': 676}\n",
      "    >> 676 positive nodes left unpredicted...\n",
      "    >> 676 OG positive nodes left unpredicted...\n",
      "Selecting data as budgeted ground truth for next round...\n",
      "  > Starting round 1...\n",
      "\n",
      "+++++++++++++++++\n",
      "STARTING ROUND 1!\n",
      "+++++++++++++++++\n",
      "        \n",
      "ASSIGNED GRAPH FOR TRAINING: 11758 {1: 2566, 0: 9192} Nodes, 1049758 Edges\n",
      "\n",
      ">> Splitting to train-validation set...\n",
      "Alotting train-val-test split for round 1\n",
      "Initial pool: 5879, Prediction pool: 2490, Budget pool: 0, Full pool: 7076\n",
      "Full graph size: 11758, Training: 4245, Val: 2831, Test: 11758\n",
      "    >> INITIAL DATA SPLIT: 4245 train rows ({1: 1134, 0: 3111}) | 2831 val rows ({0: 2075, 1: 756}) | 4682 test rows ({0: 4006, 1: 676})\n",
      "    >> AUGMENTED DATA SPLIT: 4245 train rows ({1: 1134, 0: 3111}) | 2831 val rows ({0: 2075, 1: 756}) | 4682 test rows ({0: 4006, 1: 676})\n",
      "    >> Updated cross-entropy weight to 2.7433862433862433...\n",
      "\n",
      "-----------------------\n",
      "STARTING MODEL TRAININGps_l\n",
      "-----------------------\n",
      "\n",
      "TRAINING NODE EMBEDDER\n",
      "Detailed Loss: recon 0.22767310-0.22767310 | disen 0.60213041-0.00000000 | temporal 0.56732082-0.00000000 | total 0.22767310\n",
      "Epoch 1, loss: 0.22767310-(best 0.22767310)\n",
      "\n",
      "Detailed Loss: recon 0.44793159-0.44793159 | disen 0.60812271-0.00000000 | temporal 0.56849927-0.00000000 | total 0.44793159\n",
      "Epoch 2, loss: 0.44793159-(best 0.22767310)\n",
      "\n",
      "Detailed Loss: recon 0.24269068-0.24269068 | disen 0.60525393-0.00000000 | temporal 0.56760091-0.00000000 | total 0.24269068\n",
      "Epoch 3, loss: 0.24269068-(best 0.22767310)\n",
      "\n",
      "Detailed Loss: recon 0.27039915-0.27039915 | disen 0.60229337-0.00000000 | temporal 0.56670213-0.00000000 | total 0.27039915\n",
      "Epoch 4, loss: 0.27039915-(best 0.22767310)\n",
      "\n",
      "Detailed Loss: recon 0.29523045-0.29523045 | disen 0.60022753-0.00000000 | temporal 0.56652945-0.00000000 | total 0.29523045\n",
      "Epoch 5, loss: 0.29523045-(best 0.22767310)\n",
      "\n",
      "Detailed Loss: recon 0.30094108-0.30094108 | disen 0.59914017-0.00000000 | temporal 0.56670195-0.00000000 | total 0.30094108\n",
      "Epoch 6, loss: 0.30094108-(best 0.22767310)\n",
      "\n",
      "Detailed Loss: recon 0.27659148-0.27659148 | disen 0.59974259-0.00000000 | temporal 0.56707847-0.00000000 | total 0.27659148\n",
      "Epoch 7, loss: 0.27659148-(best 0.22767310)\n",
      "\n",
      "Detailed Loss: recon 0.26574212-0.26574212 | disen 0.60017818-0.00000000 | temporal 0.56752735-0.00000000 | total 0.26574212\n",
      "Epoch 8, loss: 0.26574212-(best 0.22767310)\n",
      "\n",
      "Detailed Loss: recon 0.26874846-0.26874846 | disen 0.59914398-0.00000000 | temporal 0.56787455-0.00000000 | total 0.26874846\n",
      "Epoch 9, loss: 0.26874846-(best 0.22767310)\n",
      "\n",
      "Detailed Loss: recon 0.27117714-0.27117714 | disen 0.60037184-0.00000000 | temporal 0.56809503-0.00000000 | total 0.27117714\n",
      "Epoch 10, loss: 0.27117714-(best 0.22767310)\n",
      "\n",
      "Detailed Loss: recon 0.27790481-0.27790481 | disen 0.59955966-0.00000000 | temporal 0.56814474-0.00000000 | total 0.27790481\n",
      "Epoch 11, loss: 0.27790481-(best 0.22767310)\n",
      "\n",
      "Detailed Loss: recon 0.27483705-0.27483705 | disen 0.59784842-0.00000000 | temporal 0.56805396-0.00000000 | total 0.27483705\n",
      "Epoch 12, loss: 0.27483705-(best 0.22767310)\n",
      "\n",
      "Detailed Loss: recon 0.26660681-0.26660681 | disen 0.59792459-0.00000000 | temporal 0.56787312-0.00000000 | total 0.26660681\n",
      "Epoch 13, loss: 0.26660681-(best 0.22767310)\n",
      "\n",
      "Detailed Loss: recon 0.25836408-0.25836408 | disen 0.59676766-0.00000000 | temporal 0.56759876-0.00000000 | total 0.25836408\n",
      "Epoch 14, loss: 0.25836408-(best 0.22767310)\n",
      "\n",
      "Detailed Loss: recon 0.26045275-0.26045275 | disen 0.59605432-0.00000000 | temporal 0.56736642-0.00000000 | total 0.26045275\n",
      "Epoch 15, loss: 0.26045275-(best 0.22767310)\n",
      "\n",
      "Detailed Loss: recon 0.26019141-0.26019141 | disen 0.59689146-0.00000000 | temporal 0.56731462-0.00000000 | total 0.26019141\n",
      "Epoch 16, loss: 0.26019141-(best 0.22767310)\n",
      "\n",
      "Detailed Loss: recon 0.25810379-0.25810379 | disen 0.59610260-0.00000000 | temporal 0.56727487-0.00000000 | total 0.25810379\n",
      "Epoch 17, loss: 0.25810379-(best 0.22767310)\n",
      "\n",
      "Detailed Loss: recon 0.26004624-0.26004624 | disen 0.59478611-0.00000000 | temporal 0.56730598-0.00000000 | total 0.26004624\n",
      "Epoch 18, loss: 0.26004624-(best 0.22767310)\n",
      "\n",
      "Detailed Loss: recon 0.26111227-0.26111227 | disen 0.59471077-0.00000000 | temporal 0.56742603-0.00000000 | total 0.26111227\n",
      "Epoch 19, loss: 0.26111227-(best 0.22767310)\n",
      "\n",
      "Detailed Loss: recon 0.25554785-0.25554785 | disen 0.59616446-0.00000000 | temporal 0.56756300-0.00000000 | total 0.25554785\n",
      "Epoch 20, loss: 0.25554785-(best 0.22767310)\n",
      "\n",
      "Detailed Loss: recon 0.25709027-0.25709027 | disen 0.59623289-0.00000000 | temporal 0.56769484-0.00000000 | total 0.25709027\n",
      "Epoch 21, loss: 0.25709027-(best 0.22767310)\n",
      "\n",
      "Detailed Loss: recon 0.25443399-0.25443399 | disen 0.59468842-0.00000000 | temporal 0.56776243-0.00000000 | total 0.25443399\n",
      "Epoch 22, loss: 0.25443399-(best 0.22767310)\n",
      "\n",
      "Detailed Loss: recon 0.25412947-0.25412947 | disen 0.59533602-0.00000000 | temporal 0.56782919-0.00000000 | total 0.25412947\n",
      "Epoch 23, loss: 0.25412947-(best 0.22767310)\n",
      "\n",
      "Detailed Loss: recon 0.25277296-0.25277296 | disen 0.59456861-0.00000000 | temporal 0.56785023-0.00000000 | total 0.25277296\n",
      "Epoch 24, loss: 0.25277296-(best 0.22767310)\n",
      "\n",
      "Detailed Loss: recon 0.24822645-0.24822645 | disen 0.59530914-0.00000000 | temporal 0.56784827-0.00000000 | total 0.24822645\n",
      "Epoch 25, loss: 0.24822645-(best 0.22767310)\n",
      "\n",
      "Detailed Loss: recon 0.24682759-0.24682759 | disen 0.59514630-0.00000000 | temporal 0.56777769-0.00000000 | total 0.24682759\n",
      "Epoch 26, loss: 0.24682759-(best 0.22767310)\n",
      "\n",
      "Detailed Loss: recon 0.24834369-0.24834369 | disen 0.59495759-0.00000000 | temporal 0.56770617-0.00000000 | total 0.24834369\n",
      "Epoch 27, loss: 0.24834369-(best 0.22767310)\n",
      "\n",
      "Detailed Loss: recon 0.24974273-0.24974273 | disen 0.59545207-0.00000000 | temporal 0.56769466-0.00000000 | total 0.24974273\n",
      "Epoch 28, loss: 0.24974273-(best 0.22767310)\n",
      "\n",
      "Detailed Loss: recon 0.24505286-0.24505286 | disen 0.59501034-0.00000000 | temporal 0.56769282-0.00000000 | total 0.24505286\n",
      "Epoch 29, loss: 0.24505286-(best 0.22767310)\n",
      "\n",
      "Detailed Loss: recon 0.24345636-0.24345636 | disen 0.59524482-0.00000000 | temporal 0.56773496-0.00000000 | total 0.24345636\n",
      "Epoch 30, loss: 0.24345636-(best 0.22767310)\n",
      "\n",
      "Detailed Loss: recon 0.24349377-0.24349377 | disen 0.59520245-0.00000000 | temporal 0.56778753-0.00000000 | total 0.24349377\n",
      "Epoch 31, loss: 0.24349377-(best 0.22767310)\n",
      "\n",
      "Detailed Loss: recon 0.24064772-0.24064772 | disen 0.59599519-0.00000000 | temporal 0.56786966-0.00000000 | total 0.24064772\n",
      "Epoch 32, loss: 0.24064772-(best 0.22767310)\n",
      "\n",
      "Detailed Loss: recon 0.23984402-0.23984402 | disen 0.59526932-0.00000000 | temporal 0.56790304-0.00000000 | total 0.23984402\n",
      "Epoch 33, loss: 0.23984402-(best 0.22767310)\n",
      "\n",
      "Detailed Loss: recon 0.24211311-0.24211311 | disen 0.59608364-0.00000000 | temporal 0.56793618-0.00000000 | total 0.24211311\n",
      "Epoch 34, loss: 0.24211311-(best 0.22767310)\n",
      "\n",
      "Detailed Loss: recon 0.23983186-0.23983186 | disen 0.59569812-0.00000000 | temporal 0.56794268-0.00000000 | total 0.23983186\n",
      "Epoch 35, loss: 0.23983186-(best 0.22767310)\n",
      "\n",
      "Detailed Loss: recon 0.23812801-0.23812801 | disen 0.59593695-0.00000000 | temporal 0.56797051-0.00000000 | total 0.23812801\n",
      "Epoch 36, loss: 0.23812801-(best 0.22767310)\n",
      "\n",
      "Detailed Loss: recon 0.23695952-0.23695952 | disen 0.59573770-0.00000000 | temporal 0.56797194-0.00000000 | total 0.23695952\n",
      "Epoch 37, loss: 0.23695952-(best 0.22767310)\n",
      "\n",
      "Detailed Loss: recon 0.23767209-0.23767209 | disen 0.59579402-0.00000000 | temporal 0.56802940-0.00000000 | total 0.23767209\n",
      "Epoch 38, loss: 0.23767209-(best 0.22767310)\n",
      "\n",
      "Detailed Loss: recon 0.24014693-0.24014693 | disen 0.59598100-0.00000000 | temporal 0.56811434-0.00000000 | total 0.24014693\n",
      "Epoch 39, loss: 0.24014693-(best 0.22767310)\n",
      "\n",
      "Detailed Loss: recon 0.23596832-0.23596832 | disen 0.59627855-0.00000000 | temporal 0.56819725-0.00000000 | total 0.23596832\n",
      "Epoch 40, loss: 0.23596832-(best 0.22767310)\n",
      "\n",
      "Detailed Loss: recon 0.23729227-0.23729227 | disen 0.59679294-0.00000000 | temporal 0.56825471-0.00000000 | total 0.23729227\n",
      "Epoch 41, loss: 0.23729227-(best 0.22767310)\n",
      "\n",
      "Detailed Loss: recon 0.23578495-0.23578495 | disen 0.59691393-0.00000000 | temporal 0.56836468-0.00000000 | total 0.23578495\n",
      "Epoch 42, loss: 0.23578495-(best 0.22767310)\n",
      "\n",
      "Detailed Loss: recon 0.23531985-0.23531985 | disen 0.59758782-0.00000000 | temporal 0.56839770-0.00000000 | total 0.23531985\n",
      "Epoch 43, loss: 0.23531985-(best 0.22767310)\n",
      "\n",
      "Detailed Loss: recon 0.23311017-0.23311017 | disen 0.59675694-0.00000000 | temporal 0.56845468-0.00000000 | total 0.23311017\n",
      "Epoch 44, loss: 0.23311017-(best 0.22767310)\n",
      "\n",
      "Detailed Loss: recon 0.23631060-0.23631060 | disen 0.59748876-0.00000000 | temporal 0.56849033-0.00000000 | total 0.23631060\n",
      "Epoch 45, loss: 0.23631060-(best 0.22767310)\n",
      "\n",
      "Detailed Loss: recon 0.23562983-0.23562983 | disen 0.59738910-0.00000000 | temporal 0.56852549-0.00000000 | total 0.23562983\n",
      "Epoch 46, loss: 0.23562983-(best 0.22767310)\n",
      "\n",
      "Detailed Loss: recon 0.23490803-0.23490803 | disen 0.59846568-0.00000000 | temporal 0.56856334-0.00000000 | total 0.23490803\n",
      "Epoch 47, loss: 0.23490803-(best 0.22767310)\n",
      "\n",
      "Detailed Loss: recon 0.23375809-0.23375809 | disen 0.59799027-0.00000000 | temporal 0.56860095-0.00000000 | total 0.23375809\n",
      "Epoch 48, loss: 0.23375809-(best 0.22767310)\n",
      "\n",
      "Detailed Loss: recon 0.23139876-0.23139876 | disen 0.59818017-0.00000000 | temporal 0.56863987-0.00000000 | total 0.23139876\n",
      "Epoch 49, loss: 0.23139876-(best 0.22767310)\n",
      "\n",
      "Detailed Loss: recon 0.23153335-0.23153335 | disen 0.59870607-0.00000000 | temporal 0.56867391-0.00000000 | total 0.23153335\n",
      "Epoch 50, loss: 0.23153335-(best 0.22767310)\n",
      "\n",
      ">> Reached final epoch. Loading best model...\n",
      "TRAINING XGB PREDICTOR\n",
      ">> Ending training!\n",
      "\n",
      "-----------------------\n",
      "TIME COST: 5.8683552742004395 s\n",
      "Best Val: REC 68.25 PRE 48.59 MF1 68.16 AUC 78.90 TP 516 FP 546 TN 1529 FN 240\n",
      "-----------------------\n",
      "Generating additional positive data from adversary...\n",
      "Generating additional negative data by duplicating random nodes...\n",
      "\n",
      "ASSIGNED GRAPH FOR EVALUATION: 12345 {1: 2694, 0: 9651} Nodes, 1146430 Edges\n",
      "\n",
      "-------------------\n",
      "STARTING EVALUATION\n",
      "-------------------\n",
      "\n",
      "Dataset - Overall: REC 58.50 PRE 51.19 MF1 70.37 AUC 81.23 TP 1576 FP 1503 TN 8148 FN 1118 | 12345 {1: 2694, 0: 9651}\n",
      "Dataset - Train: REC 77.69 PRE 64.40 MF1 79.03 AUC 88.39 TP 881 FP 487 TN 2624 FN 253 | 4245 {1: 1134, 0: 3111}\n",
      "Dataset - Val: REC 56.88 PRE 48.75 MF1 66.58 AUC 76.71 TP 430 FP 452 TN 1623 FN 326 | 2831 {0: 2075, 1: 756}\n",
      "Dataset - Test: REC 32.96 PRE 31.97 MF1 60.03 AUC 74.29 TP 265 FP 564 TN 3901 FN 539 | 5269 {0: 4465, 1: 804}\n",
      "PREDICTION STATUS - {'1-True': 2155, '0-True': 5750, '0-False': 3901, '1-False': 539}\n",
      "    >> 539 positive nodes left unpredicted...\n",
      "    >> 445 OG positive nodes left unpredicted...\n",
      "-----------------------\n",
      "PREDICTION RESULT - ROUNDS\n",
      "Dataset - Round 0: REC 53.94 PRE 48.46 MF1 68.21 AUC 79.02 TP 691 FP 735 TN 3863 FN 590 | 5879 {1: 1281, 0: 4598}\n",
      "Dataset - Round 1: REC 26.56 PRE 39.08 MF1 58.15 AUC 71.72 TP 34 FP 53 TN 406 FN 94 | 587 {1: 128, 0: 459}\n",
      "-----------------------\n",
      "PREDICTION RESULT - SEEDS\n",
      "Seeds - Current: REC 28.91 PRE 33.04 MF1 56.53 AUC 70.45 TP 37 FP 75 TN 384 FN 91 | 587 {1: 128, 0: 459}\n",
      "Seeds - Prev: REC 0.00 PRE 0.00 MF1 39.86 AUC 70.45 TP 0 FP 70 TN 389 FN 128 | 587 {1: 128, 0: 459}\n",
      "Selecting data as budgeted ground truth for next round...\n",
      "  > Starting round 2...\n",
      "\n",
      "+++++++++++++++++\n",
      "STARTING ROUND 2!\n",
      "+++++++++++++++++\n",
      "        \n",
      "ASSIGNED GRAPH FOR TRAINING: 12345 {1: 2694, 0: 9651} Nodes, 1146430 Edges\n",
      "\n",
      ">> Splitting to train-validation set...\n",
      "Alotting train-val-test split for round 2\n",
      "Initial pool: 5879, Prediction pool: 5569, Budget pool: 0, Full pool: 7905\n",
      "Full graph size: 12345, Training: 4743, Val: 3162, Test: 12345\n",
      "    >> INITIAL DATA SPLIT: 4743 train rows ({1: 1293, 0: 3450}) | 3162 val rows ({0: 2300, 1: 862}) | 4440 test rows ({0: 3901, 1: 539})\n",
      "    >> AUGMENTED DATA SPLIT: 4743 train rows ({1: 1293, 0: 3450}) | 3162 val rows ({0: 2300, 1: 862}) | 4440 test rows ({0: 3901, 1: 539})\n",
      "    >> Updated cross-entropy weight to 2.668213457076566...\n",
      "\n",
      "-----------------------\n",
      "STARTING MODEL TRAININGps_l\n",
      "-----------------------\n",
      "\n",
      "TRAINING NODE EMBEDDER\n",
      "Detailed Loss: recon 0.45592484-0.45592484 | disen 0.60896528-0.00000000 | temporal 0.59240752-0.00000000 | total 0.45592484\n",
      "Epoch 1, loss: 0.45592484-(best 0.45592484)\n",
      "\n",
      "Detailed Loss: recon 0.24234316-0.24234316 | disen 0.60277617-0.00000000 | temporal 0.59089255-0.00000000 | total 0.24234316\n",
      "Epoch 2, loss: 0.24234316-(best 0.24234316)\n",
      "\n",
      "Detailed Loss: recon 0.30207315-0.30207315 | disen 0.59910858-0.00000000 | temporal 0.59028065-0.00000000 | total 0.30207315\n",
      "Epoch 3, loss: 0.30207315-(best 0.24234316)\n",
      "\n",
      "Detailed Loss: recon 0.29950571-0.29950571 | disen 0.59680122-0.00000000 | temporal 0.59041905-0.00000000 | total 0.29950571\n",
      "Epoch 4, loss: 0.29950571-(best 0.24234316)\n",
      "\n",
      "Detailed Loss: recon 0.27641091-0.27641091 | disen 0.59585834-0.00000000 | temporal 0.59090734-0.00000000 | total 0.27641091\n",
      "Epoch 5, loss: 0.27641091-(best 0.24234316)\n",
      "\n",
      "Detailed Loss: recon 0.26862860-0.26862860 | disen 0.59733391-0.00000000 | temporal 0.59149981-0.00000000 | total 0.26862860\n",
      "Epoch 6, loss: 0.26862860-(best 0.24234316)\n",
      "\n",
      "Detailed Loss: recon 0.28033578-0.28033578 | disen 0.59749192-0.00000000 | temporal 0.59191936-0.00000000 | total 0.28033578\n",
      "Epoch 7, loss: 0.28033578-(best 0.24234316)\n",
      "\n",
      "Detailed Loss: recon 0.28211612-0.28211612 | disen 0.59615523-0.00000000 | temporal 0.59211940-0.00000000 | total 0.28211612\n",
      "Epoch 8, loss: 0.28211612-(best 0.24234316)\n",
      "\n",
      "Detailed Loss: recon 0.28156817-0.28156817 | disen 0.59513807-0.00000000 | temporal 0.59221524-0.00000000 | total 0.28156817\n",
      "Epoch 9, loss: 0.28156817-(best 0.24234316)\n",
      "\n",
      "Detailed Loss: recon 0.27544665-0.27544665 | disen 0.59425855-0.00000000 | temporal 0.59213525-0.00000000 | total 0.27544665\n",
      "Epoch 10, loss: 0.27544665-(best 0.24234316)\n",
      "\n",
      "Detailed Loss: recon 0.26735598-0.26735598 | disen 0.59380579-0.00000000 | temporal 0.59199667-0.00000000 | total 0.26735598\n",
      "Epoch 11, loss: 0.26735598-(best 0.24234316)\n",
      "\n",
      "Detailed Loss: recon 0.26449096-0.26449096 | disen 0.59324229-0.00000000 | temporal 0.59175068-0.00000000 | total 0.26449096\n",
      "Epoch 12, loss: 0.26449096-(best 0.24234316)\n",
      "\n",
      "Detailed Loss: recon 0.26171023-0.26171023 | disen 0.59298491-0.00000000 | temporal 0.59159279-0.00000000 | total 0.26171023\n",
      "Epoch 13, loss: 0.26171023-(best 0.24234316)\n",
      "\n",
      "Detailed Loss: recon 0.26319313-0.26319313 | disen 0.59228510-0.00000000 | temporal 0.59142828-0.00000000 | total 0.26319313\n",
      "Epoch 14, loss: 0.26319313-(best 0.24234316)\n",
      "\n",
      "Detailed Loss: recon 0.26709348-0.26709348 | disen 0.59189332-0.00000000 | temporal 0.59132737-0.00000000 | total 0.26709348\n",
      "Epoch 15, loss: 0.26709348-(best 0.24234316)\n",
      "\n",
      "Detailed Loss: recon 0.25965497-0.25965497 | disen 0.59106922-0.00000000 | temporal 0.59128588-0.00000000 | total 0.25965497\n",
      "Epoch 16, loss: 0.25965497-(best 0.24234316)\n",
      "\n",
      "Detailed Loss: recon 0.25541666-0.25541666 | disen 0.59203857-0.00000000 | temporal 0.59132510-0.00000000 | total 0.25541666\n",
      "Epoch 17, loss: 0.25541666-(best 0.24234316)\n",
      "\n",
      "Detailed Loss: recon 0.25893405-0.25893405 | disen 0.59189188-0.00000000 | temporal 0.59136242-0.00000000 | total 0.25893405\n",
      "Epoch 18, loss: 0.25893405-(best 0.24234316)\n",
      "\n",
      "Detailed Loss: recon 0.25885719-0.25885719 | disen 0.59256959-0.00000000 | temporal 0.59145457-0.00000000 | total 0.25885719\n",
      "Epoch 19, loss: 0.25885719-(best 0.24234316)\n",
      "\n",
      "Detailed Loss: recon 0.25260565-0.25260565 | disen 0.59046769-0.00000000 | temporal 0.59154695-0.00000000 | total 0.25260565\n",
      "Epoch 20, loss: 0.25260565-(best 0.24234316)\n",
      "\n",
      "Detailed Loss: recon 0.25622827-0.25622827 | disen 0.59136301-0.00000000 | temporal 0.59151709-0.00000000 | total 0.25622827\n",
      "Epoch 21, loss: 0.25622827-(best 0.24234316)\n",
      "\n",
      "Detailed Loss: recon 0.25437975-0.25437975 | disen 0.59140015-0.00000000 | temporal 0.59150326-0.00000000 | total 0.25437975\n",
      "Epoch 22, loss: 0.25437975-(best 0.24234316)\n",
      "\n",
      "Detailed Loss: recon 0.25245780-0.25245780 | disen 0.59188390-0.00000000 | temporal 0.59145409-0.00000000 | total 0.25245780\n",
      "Epoch 23, loss: 0.25245780-(best 0.24234316)\n",
      "\n",
      "Detailed Loss: recon 0.25103807-0.25103807 | disen 0.59112942-0.00000000 | temporal 0.59148234-0.00000000 | total 0.25103807\n",
      "Epoch 24, loss: 0.25103807-(best 0.24234316)\n",
      "\n",
      "Detailed Loss: recon 0.25039750-0.25039750 | disen 0.59059453-0.00000000 | temporal 0.59148645-0.00000000 | total 0.25039750\n",
      "Epoch 25, loss: 0.25039750-(best 0.24234316)\n",
      "\n",
      "Detailed Loss: recon 0.24870333-0.24870333 | disen 0.59125572-0.00000000 | temporal 0.59144497-0.00000000 | total 0.24870333\n",
      "Epoch 26, loss: 0.24870333-(best 0.24234316)\n",
      "\n",
      "Detailed Loss: recon 0.24710065-0.24710065 | disen 0.59157640-0.00000000 | temporal 0.59146780-0.00000000 | total 0.24710065\n",
      "Epoch 27, loss: 0.24710065-(best 0.24234316)\n",
      "\n",
      "Detailed Loss: recon 0.24903455-0.24903455 | disen 0.59136915-0.00000000 | temporal 0.59150910-0.00000000 | total 0.24903455\n",
      "Epoch 28, loss: 0.24903455-(best 0.24234316)\n",
      "\n",
      "Detailed Loss: recon 0.24978840-0.24978840 | disen 0.59211349-0.00000000 | temporal 0.59156513-0.00000000 | total 0.24978840\n",
      "Epoch 29, loss: 0.24978840-(best 0.24234316)\n",
      "\n",
      "Detailed Loss: recon 0.24754070-0.24754070 | disen 0.59168488-0.00000000 | temporal 0.59159666-0.00000000 | total 0.24754070\n",
      "Epoch 30, loss: 0.24754070-(best 0.24234316)\n",
      "\n",
      "Detailed Loss: recon 0.24461107-0.24461107 | disen 0.59269172-0.00000000 | temporal 0.59167516-0.00000000 | total 0.24461107\n",
      "Epoch 31, loss: 0.24461107-(best 0.24234316)\n",
      "\n",
      "Detailed Loss: recon 0.24326482-0.24326482 | disen 0.59238029-0.00000000 | temporal 0.59171951-0.00000000 | total 0.24326482\n",
      "Epoch 32, loss: 0.24326482-(best 0.24234316)\n",
      "\n",
      "Detailed Loss: recon 0.24527389-0.24527389 | disen 0.59283841-0.00000000 | temporal 0.59174967-0.00000000 | total 0.24527389\n",
      "Epoch 33, loss: 0.24527389-(best 0.24234316)\n",
      "\n",
      "Detailed Loss: recon 0.24402466-0.24402466 | disen 0.59273958-0.00000000 | temporal 0.59169155-0.00000000 | total 0.24402466\n",
      "Epoch 34, loss: 0.24402466-(best 0.24234316)\n",
      "\n",
      "Detailed Loss: recon 0.24148248-0.24148248 | disen 0.59361225-0.00000000 | temporal 0.59165180-0.00000000 | total 0.24148248\n",
      "Epoch 35, loss: 0.24148248-(best 0.24148248)\n",
      "\n",
      "Detailed Loss: recon 0.24273899-0.24273899 | disen 0.59266746-0.00000000 | temporal 0.59163755-0.00000000 | total 0.24273899\n",
      "Epoch 36, loss: 0.24273899-(best 0.24148248)\n",
      "\n",
      "Detailed Loss: recon 0.23985788-0.23985788 | disen 0.59282386-0.00000000 | temporal 0.59167504-0.00000000 | total 0.23985788\n",
      "Epoch 37, loss: 0.23985788-(best 0.23985788)\n",
      "\n",
      "Detailed Loss: recon 0.24082696-0.24082696 | disen 0.59280908-0.00000000 | temporal 0.59179860-0.00000000 | total 0.24082696\n",
      "Epoch 38, loss: 0.24082696-(best 0.23985788)\n",
      "\n",
      "Detailed Loss: recon 0.23838994-0.23838994 | disen 0.59279269-0.00000000 | temporal 0.59183276-0.00000000 | total 0.23838994\n",
      "Epoch 39, loss: 0.23838994-(best 0.23838994)\n",
      "\n",
      "Detailed Loss: recon 0.23734066-0.23734066 | disen 0.59349728-0.00000000 | temporal 0.59189534-0.00000000 | total 0.23734066\n",
      "Epoch 40, loss: 0.23734066-(best 0.23734066)\n",
      "\n",
      "Detailed Loss: recon 0.23741768-0.23741768 | disen 0.59347630-0.00000000 | temporal 0.59190053-0.00000000 | total 0.23741768\n",
      "Epoch 41, loss: 0.23741768-(best 0.23734066)\n",
      "\n",
      "Detailed Loss: recon 0.23779458-0.23779458 | disen 0.59307039-0.00000000 | temporal 0.59193087-0.00000000 | total 0.23779458\n",
      "Epoch 42, loss: 0.23779458-(best 0.23734066)\n",
      "\n",
      "Detailed Loss: recon 0.23751430-0.23751430 | disen 0.59352350-0.00000000 | temporal 0.59186673-0.00000000 | total 0.23751430\n",
      "Epoch 43, loss: 0.23751430-(best 0.23734066)\n",
      "\n",
      "Detailed Loss: recon 0.23694268-0.23694268 | disen 0.59386587-0.00000000 | temporal 0.59186286-0.00000000 | total 0.23694268\n",
      "Epoch 44, loss: 0.23694268-(best 0.23694268)\n",
      "\n",
      "Detailed Loss: recon 0.23615366-0.23615366 | disen 0.59349549-0.00000000 | temporal 0.59187132-0.00000000 | total 0.23615366\n",
      "Epoch 45, loss: 0.23615366-(best 0.23615366)\n",
      "\n",
      "Detailed Loss: recon 0.23574023-0.23574023 | disen 0.59362918-0.00000000 | temporal 0.59191042-0.00000000 | total 0.23574023\n",
      "Epoch 46, loss: 0.23574023-(best 0.23574023)\n",
      "\n",
      "Detailed Loss: recon 0.23403785-0.23403785 | disen 0.59402770-0.00000000 | temporal 0.59192824-0.00000000 | total 0.23403785\n",
      "Epoch 47, loss: 0.23403785-(best 0.23403785)\n",
      "\n",
      "Detailed Loss: recon 0.23273329-0.23273329 | disen 0.59494460-0.00000000 | temporal 0.59192210-0.00000000 | total 0.23273329\n",
      "Epoch 48, loss: 0.23273329-(best 0.23273329)\n",
      "\n",
      "Detailed Loss: recon 0.23366937-0.23366937 | disen 0.59426951-0.00000000 | temporal 0.59195495-0.00000000 | total 0.23366937\n",
      "Epoch 49, loss: 0.23366937-(best 0.23273329)\n",
      "\n",
      "Detailed Loss: recon 0.23358481-0.23358481 | disen 0.59471166-0.00000000 | temporal 0.59193552-0.00000000 | total 0.23358481\n",
      "Epoch 50, loss: 0.23358481-(best 0.23273329)\n",
      "\n",
      ">> Reached final epoch. Loading best model...\n",
      "TRAINING XGB PREDICTOR\n",
      ">> Ending training!\n",
      "\n",
      "-----------------------\n",
      "TIME COST: 7.092024803161621 s\n",
      "Best Val: REC 59.05 PRE 52.15 MF1 68.55 AUC 77.66 TP 509 FP 467 TN 1833 FN 353\n",
      "-----------------------\n",
      "Generating additional positive data from adversary...\n",
      "Generating additional negative data by duplicating random nodes...\n",
      "\n",
      "ASSIGNED GRAPH FOR EVALUATION: 12932 {1: 2822, 0: 10110} Nodes, 1254078 Edges\n",
      "\n",
      "-------------------\n",
      "STARTING EVALUATION\n",
      "-------------------\n",
      "\n",
      "Dataset - Overall: REC 59.50 PRE 48.53 MF1 69.26 AUC 80.51 TP 1679 FP 1781 TN 8329 FN 1143 | 12932 {1: 2822, 0: 10110}\n",
      "Dataset - Train: REC 72.00 PRE 57.75 MF1 74.12 AUC 84.59 TP 931 FP 681 TN 2769 FN 362 | 4743 {1: 1293, 0: 3450}\n",
      "Dataset - Val: REC 58.00 PRE 48.64 MF1 66.42 AUC 76.05 TP 500 FP 528 TN 1772 FN 362 | 3162 {0: 2300, 1: 862}\n",
      "Dataset - Test: REC 37.18 PRE 30.24 MF1 60.89 AUC 74.87 TP 248 FP 572 TN 3788 FN 419 | 5027 {0: 4360, 1: 667}\n",
      "PREDICTION STATUS - {'1-True': 2403, '0-True': 6322, '0-False': 3788, '1-False': 419}\n",
      "    >> 419 positive nodes left unpredicted...\n",
      "    >> 287 OG positive nodes left unpredicted...\n",
      "-----------------------\n",
      "PREDICTION RESULT - ROUNDS\n",
      "Dataset - Round 0: REC 61.51 PRE 46.79 MF1 68.68 AUC 79.72 TP 788 FP 896 TN 3702 FN 493 | 5879 {1: 1281, 0: 4598}\n",
      "Dataset - Round 1: REC 49.22 PRE 43.45 MF1 64.92 AUC 76.73 TP 63 FP 82 TN 377 FN 65 | 587 {1: 128, 0: 459}\n",
      "Dataset - Round 2: REC 38.28 PRE 37.98 MF1 60.40 AUC 70.67 TP 49 FP 80 TN 379 FN 79 | 587 {1: 128, 0: 459}\n",
      "-----------------------\n",
      "PREDICTION RESULT - SEEDS\n",
      "Seeds - Current: REC 39.84 PRE 35.42 MF1 59.33 AUC 70.32 TP 51 FP 93 TN 366 FN 77 | 587 {1: 128, 0: 459}\n",
      "Seeds - Prev: REC 0.00 PRE 0.00 MF1 39.61 AUC 70.32 TP 0 FP 74 TN 385 FN 128 | 587 {1: 128, 0: 459}\n",
      "Selecting data as budgeted ground truth for next round...\n",
      "The file ../checkpoint/working_model_file_1730091606.1735857_epoch.py does not exist\n",
      "The file ../checkpoint/working_model_file_1730091606.1735857_round-M.json does not exist\n",
      "The file ../checkpoint/working_model_file_1730091606.1735857_round-T.json does not exist\n",
      "The file ../checkpoint/working_model_file_1730091606.1735857_round.pt does not exist\n",
      "================\n",
      "++++++++++++++++\n",
      "TRIAL NUMBER 1\n",
      "++++++++++++++++\n",
      "================\n",
      "  > Rereading graph data...\n",
      "  > Initializing multiround object...\n",
      "11758 1049758\n",
      "\n",
      "=========================\n",
      "STARTING ADVER EXPERIMENT\n",
      "=========================\n",
      "MAIN CONFIG: [('device', 'cuda:0'), ('exp_type', 'ADVER'), ('full_oracle', False), ('node_expiration_time', 1), ('round_budget_neg', 0), ('round_budget_pos', 0), ('round_new_neg', 459), ('round_new_pos', 128), ('round_num', 3), ('save_df', True), ('save_embedding', False), ('save_temp_sne', False), ('setting_type', 'INDUCTIVE'), ('task_type', 'NODE'), ('verbose', 4)]\n",
      "MODEL CONFIG: [('act_name', 'LeakyReLU'), ('addon_internal_agg', 'OR'), ('addon_name', 'NONE'), ('addon_perc', 0.05), ('addon_round_window', 7), ('alpha', 0), ('att_heads', 2), ('beta', 0), ('boost_agg_backbone', <class 'models.benchmarks_supervised.booster.GIN_noparam'>), ('boost_metric', <function average_precision_score at 0x000001C8520A5760>), ('boost_predictor', <class 'xgboost.sklearn.XGBClassifier'>), ('device', 'cuda:0'), ('drop_rate', 0), ('dropout_rate', 0.1), ('early_stopping', 75), ('embed_type', 'temporal'), ('etypes', ['none']), ('gamma', 1), ('h_feats', 64), ('k', 1), ('lambda1', 1), ('lambda2', 1), ('lambda_', 1), ('loss_sample', True), ('loss_sample_ratio', 0.1), ('loss_type', 'ndist'), ('mlp_feats', 64), ('mlp_layers', 2), ('model_name', 'XGB-SP'), ('n_estimator', 500), ('norm_name', 'layer'), ('num_epoch', 100), ('num_layers', 2), ('num_round_epoch', 50), ('round_window', 7), ('temporal_agg', 'mean_final'), ('tloss_type', 'normal'), ('training_type', 'round'), ('verbose', 4)]\n",
      "STRAT CONFIG: [('augment_name', 'REAGE'), ('augment_neg_ratio', 0.09772070079945569), ('augment_pos_ratio', 0.02727929920054431), ('augment_prob', 0.5), ('augment_round_split', 8), ('device', 'cuda:0'), ('pseudo_name', 'NONE'), ('pseudo_new_pos_only', False), ('pseudo_skip_initial', False), ('verbose', 4)]\n",
      "TRAIN CONFIG: [('ce_weight', 2.668213457076566), ('device', 'cuda:0'), ('dset_name', 'tolokers_bid'), ('early_stopping', 75), ('learning_rate', 0.01), ('loss', <function cross_entropy at 0x000001C84AF6F880>), ('loss_gamma', 5), ('num_epoch', 100), ('num_round_epoch', 50), ('optimizer', <class 'torch.optim.adam.Adam'>), ('random_state', 7777777), ('ratio_initial', 0.5), ('ratio_test', 0.2), ('ratio_train', 0.6), ('ratio_val', 0.2), ('round_reset_model', False), ('round_train_list', []), ('stuck_stopping', 50), ('stuck_threshold', 0), ('train_max_retry', 10), ('verbose', 4)]\n",
      "ADVER CONFIG: [('adver_choose_name', 'GREEDY'), ('adver_conn_coef', 0.1), ('adver_feat_coef', 1), ('adver_mod_name', 'REPLAY'), ('device', 'cuda:0'), ('verbose', 4)]\n",
      "\n",
      "INITIAL GRAPH: 11758 {1: 2566, 0: 9192} Nodes, 1049758 Edges\n",
      "=========================\n",
      "        \n",
      ">> Initializing graph...\n",
      "MAIN GRAPH: 11758 {1: 2566, 0: 9192} Nodes, 1049758 Edges\n",
      "\n",
      ">> Initializing experiment type-specific modules:\n",
      "  >> Initializing adversarial modules...\n",
      ">> Initializing task-specific modules:\n",
      ">> Initializing classifier model...\n",
      ">> Initializing other strategies...\n",
      "\n",
      "FINISH INITIALIZATION\n",
      "=====================\n",
      "        \n",
      "  > Starting round 0...\n",
      "\n",
      "+++++++++++++++++\n",
      "STARTING ROUND 0!\n",
      "+++++++++++++++++\n",
      "        \n",
      "ASSIGNED GRAPH FOR TRAINING: 5879 {1: 1301, 0: 4578} Nodes, 265551 Edges\n",
      "\n",
      ">> Splitting to train-validation set...\n",
      "Alotting train-val-test split for round 0\n",
      "Initial pool: 0, Prediction pool: 0, Budget pool: 0, Full pool: 5879\n",
      "Full graph size: 5879, Training: 3527, Val: 2352, Test: 5879\n",
      "    >> INITIAL DATA SPLIT: 3527 train rows ({0: 2746, 1: 781}) | 2352 val rows ({1: 520, 0: 1832}) | 0 test rows ({})\n",
      "    >> AUGMENTED DATA SPLIT: 3527 train rows ({0: 2746, 1: 781}) | 2352 val rows ({1: 520, 0: 1832}) | 0 test rows ({})\n",
      "    >> Updated cross-entropy weight to 3.5160051216389245...\n",
      "\n",
      "-----------------------\n",
      "STARTING MODEL TRAININGps_l\n",
      "-----------------------\n",
      "\n",
      "TRAINING NODE EMBEDDER\n",
      "Detailed Loss: recon 0.79564929-0.79564929 | disen 0.69829142-0.00000000 | temporal 0.64149231-0.00000000 | total 0.79564929\n",
      "Epoch 1, loss: 0.79564929-(best 0.79564929)\n",
      "\n",
      "Detailed Loss: recon 0.45161796-0.45161796 | disen 0.72229147-0.00000000 | temporal 0.65808916-0.00000000 | total 0.45161796\n",
      "Epoch 2, loss: 0.45161796-(best 0.45161796)\n",
      "\n",
      "Detailed Loss: recon 0.39517236-0.39517236 | disen 0.72931850-0.00000000 | temporal 0.66556287-0.00000000 | total 0.39517236\n",
      "Epoch 3, loss: 0.39517236-(best 0.39517236)\n",
      "\n",
      "Detailed Loss: recon 0.38128942-0.38128942 | disen 0.73320568-0.00000000 | temporal 0.66927546-0.00000000 | total 0.38128942\n",
      "Epoch 4, loss: 0.38128942-(best 0.38128942)\n",
      "\n",
      "Detailed Loss: recon 0.37932786-0.37932786 | disen 0.73440719-0.00000000 | temporal 0.67135811-0.00000000 | total 0.37932786\n",
      "Epoch 5, loss: 0.37932786-(best 0.37932786)\n",
      "\n",
      "Detailed Loss: recon 0.35462567-0.35462567 | disen 0.73818308-0.00000000 | temporal 0.67177868-0.00000000 | total 0.35462567\n",
      "Epoch 6, loss: 0.35462567-(best 0.35462567)\n",
      "\n",
      "Detailed Loss: recon 0.34972882-0.34972882 | disen 0.73813534-0.00000000 | temporal 0.67142838-0.00000000 | total 0.34972882\n",
      "Epoch 7, loss: 0.34972882-(best 0.34972882)\n",
      "\n",
      "Detailed Loss: recon 0.33113927-0.33113927 | disen 0.73981333-0.00000000 | temporal 0.67086101-0.00000000 | total 0.33113927\n",
      "Epoch 8, loss: 0.33113927-(best 0.33113927)\n",
      "\n",
      "Detailed Loss: recon 0.31913608-0.31913608 | disen 0.73825932-0.00000000 | temporal 0.66999906-0.00000000 | total 0.31913608\n",
      "Epoch 9, loss: 0.31913608-(best 0.31913608)\n",
      "\n",
      "Detailed Loss: recon 0.30631730-0.30631730 | disen 0.73843396-0.00000000 | temporal 0.66900307-0.00000000 | total 0.30631730\n",
      "Epoch 10, loss: 0.30631730-(best 0.30631730)\n",
      "\n",
      "Detailed Loss: recon 0.30952251-0.30952251 | disen 0.73962760-0.00000000 | temporal 0.66818148-0.00000000 | total 0.30952251\n",
      "Epoch 11, loss: 0.30952251-(best 0.30631730)\n",
      "\n",
      "Detailed Loss: recon 0.30617368-0.30617368 | disen 0.73640347-0.00000000 | temporal 0.66765082-0.00000000 | total 0.30617368\n",
      "Epoch 12, loss: 0.30617368-(best 0.30617368)\n",
      "\n",
      "Detailed Loss: recon 0.30387640-0.30387640 | disen 0.73698270-0.00000000 | temporal 0.66791576-0.00000000 | total 0.30387640\n",
      "Epoch 13, loss: 0.30387640-(best 0.30387640)\n",
      "\n",
      "Detailed Loss: recon 0.30459946-0.30459946 | disen 0.73872948-0.00000000 | temporal 0.66796982-0.00000000 | total 0.30459946\n",
      "Epoch 14, loss: 0.30459946-(best 0.30387640)\n",
      "\n",
      "Detailed Loss: recon 0.29241964-0.29241964 | disen 0.73659086-0.00000000 | temporal 0.66879165-0.00000000 | total 0.29241964\n",
      "Epoch 15, loss: 0.29241964-(best 0.29241964)\n",
      "\n",
      "Detailed Loss: recon 0.28919786-0.28919786 | disen 0.73841441-0.00000000 | temporal 0.66946745-0.00000000 | total 0.28919786\n",
      "Epoch 16, loss: 0.28919786-(best 0.28919786)\n",
      "\n",
      "Detailed Loss: recon 0.28553927-0.28553927 | disen 0.73625112-0.00000000 | temporal 0.67006117-0.00000000 | total 0.28553927\n",
      "Epoch 17, loss: 0.28553927-(best 0.28553927)\n",
      "\n",
      "Detailed Loss: recon 0.28206968-0.28206968 | disen 0.73874462-0.00000000 | temporal 0.67058563-0.00000000 | total 0.28206968\n",
      "Epoch 18, loss: 0.28206968-(best 0.28206968)\n",
      "\n",
      "Detailed Loss: recon 0.28145859-0.28145859 | disen 0.73572838-0.00000000 | temporal 0.67124438-0.00000000 | total 0.28145859\n",
      "Epoch 19, loss: 0.28145859-(best 0.28145859)\n",
      "\n",
      "Detailed Loss: recon 0.28169093-0.28169093 | disen 0.73656136-0.00000000 | temporal 0.67137408-0.00000000 | total 0.28169093\n",
      "Epoch 20, loss: 0.28169093-(best 0.28145859)\n",
      "\n",
      "Detailed Loss: recon 0.27751911-0.27751911 | disen 0.73725867-0.00000000 | temporal 0.67151999-0.00000000 | total 0.27751911\n",
      "Epoch 21, loss: 0.27751911-(best 0.27751911)\n",
      "\n",
      "Detailed Loss: recon 0.27753851-0.27753851 | disen 0.73551416-0.00000000 | temporal 0.67161936-0.00000000 | total 0.27753851\n",
      "Epoch 22, loss: 0.27753851-(best 0.27751911)\n",
      "\n",
      "Detailed Loss: recon 0.27083716-0.27083716 | disen 0.73933733-0.00000000 | temporal 0.67133409-0.00000000 | total 0.27083716\n",
      "Epoch 23, loss: 0.27083716-(best 0.27083716)\n",
      "\n",
      "Detailed Loss: recon 0.27128944-0.27128944 | disen 0.73555732-0.00000000 | temporal 0.67117345-0.00000000 | total 0.27128944\n",
      "Epoch 24, loss: 0.27128944-(best 0.27083716)\n",
      "\n",
      "Detailed Loss: recon 0.27132976-0.27132976 | disen 0.73999643-0.00000000 | temporal 0.67118388-0.00000000 | total 0.27132976\n",
      "Epoch 25, loss: 0.27132976-(best 0.27083716)\n",
      "\n",
      "Detailed Loss: recon 0.26967341-0.26967341 | disen 0.73785353-0.00000000 | temporal 0.67107213-0.00000000 | total 0.26967341\n",
      "Epoch 26, loss: 0.26967341-(best 0.26967341)\n",
      "\n",
      "Detailed Loss: recon 0.26513970-0.26513970 | disen 0.73489428-0.00000000 | temporal 0.67121160-0.00000000 | total 0.26513970\n",
      "Epoch 27, loss: 0.26513970-(best 0.26513970)\n",
      "\n",
      "Detailed Loss: recon 0.25971240-0.25971240 | disen 0.73574889-0.00000000 | temporal 0.67171985-0.00000000 | total 0.25971240\n",
      "Epoch 28, loss: 0.25971240-(best 0.25971240)\n",
      "\n",
      "Detailed Loss: recon 0.25437099-0.25437099 | disen 0.73928243-0.00000000 | temporal 0.67185372-0.00000000 | total 0.25437099\n",
      "Epoch 29, loss: 0.25437099-(best 0.25437099)\n",
      "\n",
      "Detailed Loss: recon 0.25952506-0.25952506 | disen 0.73852503-0.00000000 | temporal 0.67184788-0.00000000 | total 0.25952506\n",
      "Epoch 30, loss: 0.25952506-(best 0.25437099)\n",
      "\n",
      "Detailed Loss: recon 0.25538060-0.25538060 | disen 0.73670328-0.00000000 | temporal 0.67201328-0.00000000 | total 0.25538060\n",
      "Epoch 31, loss: 0.25538060-(best 0.25437099)\n",
      "\n",
      "Detailed Loss: recon 0.25100845-0.25100845 | disen 0.73790824-0.00000000 | temporal 0.67182523-0.00000000 | total 0.25100845\n",
      "Epoch 32, loss: 0.25100845-(best 0.25100845)\n",
      "\n",
      "Detailed Loss: recon 0.25379330-0.25379330 | disen 0.73859078-0.00000000 | temporal 0.67191893-0.00000000 | total 0.25379330\n",
      "Epoch 33, loss: 0.25379330-(best 0.25100845)\n",
      "\n",
      "Detailed Loss: recon 0.24858567-0.24858567 | disen 0.73546207-0.00000000 | temporal 0.67166847-0.00000000 | total 0.24858567\n",
      "Epoch 34, loss: 0.24858567-(best 0.24858567)\n",
      "\n",
      "Detailed Loss: recon 0.25223207-0.25223207 | disen 0.73834014-0.00000000 | temporal 0.67164165-0.00000000 | total 0.25223207\n",
      "Epoch 35, loss: 0.25223207-(best 0.24858567)\n",
      "\n",
      "Detailed Loss: recon 0.24659346-0.24659346 | disen 0.73788851-0.00000000 | temporal 0.67188817-0.00000000 | total 0.24659346\n",
      "Epoch 36, loss: 0.24659346-(best 0.24659346)\n",
      "\n",
      "Detailed Loss: recon 0.24875501-0.24875501 | disen 0.73821932-0.00000000 | temporal 0.67176598-0.00000000 | total 0.24875501\n",
      "Epoch 37, loss: 0.24875501-(best 0.24659346)\n",
      "\n",
      "Detailed Loss: recon 0.24408285-0.24408285 | disen 0.73909360-0.00000000 | temporal 0.67176306-0.00000000 | total 0.24408285\n",
      "Epoch 38, loss: 0.24408285-(best 0.24408285)\n",
      "\n",
      "Detailed Loss: recon 0.24524948-0.24524948 | disen 0.73602128-0.00000000 | temporal 0.67186403-0.00000000 | total 0.24524948\n",
      "Epoch 39, loss: 0.24524948-(best 0.24408285)\n",
      "\n",
      "Detailed Loss: recon 0.24191087-0.24191087 | disen 0.73767215-0.00000000 | temporal 0.67162681-0.00000000 | total 0.24191087\n",
      "Epoch 40, loss: 0.24191087-(best 0.24191087)\n",
      "\n",
      "Detailed Loss: recon 0.24012917-0.24012917 | disen 0.74020380-0.00000000 | temporal 0.67140639-0.00000000 | total 0.24012917\n",
      "Epoch 41, loss: 0.24012917-(best 0.24012917)\n",
      "\n",
      "Detailed Loss: recon 0.24236833-0.24236833 | disen 0.73410237-0.00000000 | temporal 0.67145658-0.00000000 | total 0.24236833\n",
      "Epoch 42, loss: 0.24236833-(best 0.24012917)\n",
      "\n",
      "Detailed Loss: recon 0.24182704-0.24182704 | disen 0.73890424-0.00000000 | temporal 0.67147136-0.00000000 | total 0.24182704\n",
      "Epoch 43, loss: 0.24182704-(best 0.24012917)\n",
      "\n",
      "Detailed Loss: recon 0.24199331-0.24199331 | disen 0.73682332-0.00000000 | temporal 0.67161655-0.00000000 | total 0.24199331\n",
      "Epoch 44, loss: 0.24199331-(best 0.24012917)\n",
      "\n",
      "Detailed Loss: recon 0.24359654-0.24359654 | disen 0.73931110-0.00000000 | temporal 0.67163008-0.00000000 | total 0.24359654\n",
      "Epoch 45, loss: 0.24359654-(best 0.24012917)\n",
      "\n",
      "Detailed Loss: recon 0.23586850-0.23586850 | disen 0.73721385-0.00000000 | temporal 0.67180490-0.00000000 | total 0.23586850\n",
      "Epoch 46, loss: 0.23586850-(best 0.23586850)\n",
      "\n",
      "Detailed Loss: recon 0.23769355-0.23769355 | disen 0.73703289-0.00000000 | temporal 0.67155904-0.00000000 | total 0.23769355\n",
      "Epoch 47, loss: 0.23769355-(best 0.23586850)\n",
      "\n",
      "Detailed Loss: recon 0.23380941-0.23380941 | disen 0.73723197-0.00000000 | temporal 0.67147797-0.00000000 | total 0.23380941\n",
      "Epoch 48, loss: 0.23380941-(best 0.23380941)\n",
      "\n",
      "Detailed Loss: recon 0.23722117-0.23722117 | disen 0.74069417-0.00000000 | temporal 0.67136210-0.00000000 | total 0.23722117\n",
      "Epoch 49, loss: 0.23722117-(best 0.23380941)\n",
      "\n",
      "Detailed Loss: recon 0.23450649-0.23450649 | disen 0.73633677-0.00000000 | temporal 0.67162967-0.00000000 | total 0.23450649\n",
      "Epoch 50, loss: 0.23450649-(best 0.23380941)\n",
      "\n",
      "Detailed Loss: recon 0.23611161-0.23611161 | disen 0.73700106-0.00000000 | temporal 0.67168528-0.00000000 | total 0.23611161\n",
      "Epoch 51, loss: 0.23611161-(best 0.23380941)\n",
      "\n",
      "Detailed Loss: recon 0.23237579-0.23237579 | disen 0.73650789-0.00000000 | temporal 0.67166400-0.00000000 | total 0.23237579\n",
      "Epoch 52, loss: 0.23237579-(best 0.23237579)\n",
      "\n",
      "Detailed Loss: recon 0.23499262-0.23499262 | disen 0.73870730-0.00000000 | temporal 0.67147273-0.00000000 | total 0.23499262\n",
      "Epoch 53, loss: 0.23499262-(best 0.23237579)\n",
      "\n",
      "Detailed Loss: recon 0.23149514-0.23149514 | disen 0.73497057-0.00000000 | temporal 0.67140365-0.00000000 | total 0.23149514\n",
      "Epoch 54, loss: 0.23149514-(best 0.23149514)\n",
      "\n",
      "Detailed Loss: recon 0.23078963-0.23078963 | disen 0.73846436-0.00000000 | temporal 0.67154890-0.00000000 | total 0.23078963\n",
      "Epoch 55, loss: 0.23078963-(best 0.23078963)\n",
      "\n",
      "Detailed Loss: recon 0.22884357-0.22884357 | disen 0.73684669-0.00000000 | temporal 0.67169094-0.00000000 | total 0.22884357\n",
      "Epoch 56, loss: 0.22884357-(best 0.22884357)\n",
      "\n",
      "Detailed Loss: recon 0.22977380-0.22977380 | disen 0.73468167-0.00000000 | temporal 0.67182642-0.00000000 | total 0.22977380\n",
      "Epoch 57, loss: 0.22977380-(best 0.22884357)\n",
      "\n",
      "Detailed Loss: recon 0.22583506-0.22583506 | disen 0.73372459-0.00000000 | temporal 0.67168474-0.00000000 | total 0.22583506\n",
      "Epoch 58, loss: 0.22583506-(best 0.22583506)\n",
      "\n",
      "Detailed Loss: recon 0.23051614-0.23051614 | disen 0.73488146-0.00000000 | temporal 0.67163604-0.00000000 | total 0.23051614\n",
      "Epoch 59, loss: 0.23051614-(best 0.22583506)\n",
      "\n",
      "Detailed Loss: recon 0.22502056-0.22502056 | disen 0.73706305-0.00000000 | temporal 0.67192334-0.00000000 | total 0.22502056\n",
      "Epoch 60, loss: 0.22502056-(best 0.22502056)\n",
      "\n",
      "Detailed Loss: recon 0.22320387-0.22320387 | disen 0.73647463-0.00000000 | temporal 0.67217404-0.00000000 | total 0.22320387\n",
      "Epoch 61, loss: 0.22320387-(best 0.22320387)\n",
      "\n",
      "Detailed Loss: recon 0.22462484-0.22462484 | disen 0.73489916-0.00000000 | temporal 0.67201555-0.00000000 | total 0.22462484\n",
      "Epoch 62, loss: 0.22462484-(best 0.22320387)\n",
      "\n",
      "Detailed Loss: recon 0.22155347-0.22155347 | disen 0.73992634-0.00000000 | temporal 0.67213309-0.00000000 | total 0.22155347\n",
      "Epoch 63, loss: 0.22155347-(best 0.22155347)\n",
      "\n",
      "Detailed Loss: recon 0.22325653-0.22325653 | disen 0.73686194-0.00000000 | temporal 0.67207307-0.00000000 | total 0.22325653\n",
      "Epoch 64, loss: 0.22325653-(best 0.22155347)\n",
      "\n",
      "Detailed Loss: recon 0.22157621-0.22157621 | disen 0.73919857-0.00000000 | temporal 0.67212844-0.00000000 | total 0.22157621\n",
      "Epoch 65, loss: 0.22157621-(best 0.22155347)\n",
      "\n",
      "Detailed Loss: recon 0.22176290-0.22176290 | disen 0.73427200-0.00000000 | temporal 0.67226923-0.00000000 | total 0.22176290\n",
      "Epoch 66, loss: 0.22176290-(best 0.22155347)\n",
      "\n",
      "Detailed Loss: recon 0.22122940-0.22122940 | disen 0.73468673-0.00000000 | temporal 0.67188674-0.00000000 | total 0.22122940\n",
      "Epoch 67, loss: 0.22122940-(best 0.22122940)\n",
      "\n",
      "Detailed Loss: recon 0.21948104-0.21948104 | disen 0.73581505-0.00000000 | temporal 0.67211986-0.00000000 | total 0.21948104\n",
      "Epoch 68, loss: 0.21948104-(best 0.21948104)\n",
      "\n",
      "Detailed Loss: recon 0.21962273-0.21962273 | disen 0.73607528-0.00000000 | temporal 0.67224145-0.00000000 | total 0.21962273\n",
      "Epoch 69, loss: 0.21962273-(best 0.21948104)\n",
      "\n",
      "Detailed Loss: recon 0.21791399-0.21791399 | disen 0.73628151-0.00000000 | temporal 0.67232662-0.00000000 | total 0.21791399\n",
      "Epoch 70, loss: 0.21791399-(best 0.21791399)\n",
      "\n",
      "Detailed Loss: recon 0.21740535-0.21740535 | disen 0.73540890-0.00000000 | temporal 0.67231303-0.00000000 | total 0.21740535\n",
      "Epoch 71, loss: 0.21740535-(best 0.21740535)\n",
      "\n",
      "Detailed Loss: recon 0.21745992-0.21745992 | disen 0.73794937-0.00000000 | temporal 0.67246521-0.00000000 | total 0.21745992\n",
      "Epoch 72, loss: 0.21745992-(best 0.21740535)\n",
      "\n",
      "Detailed Loss: recon 0.21623367-0.21623367 | disen 0.73568273-0.00000000 | temporal 0.67249417-0.00000000 | total 0.21623367\n",
      "Epoch 73, loss: 0.21623367-(best 0.21623367)\n",
      "\n",
      "Detailed Loss: recon 0.21799311-0.21799311 | disen 0.73610997-0.00000000 | temporal 0.67248183-0.00000000 | total 0.21799311\n",
      "Epoch 74, loss: 0.21799311-(best 0.21623367)\n",
      "\n",
      "Detailed Loss: recon 0.21711771-0.21711771 | disen 0.73749095-0.00000000 | temporal 0.67257410-0.00000000 | total 0.21711771\n",
      "Epoch 75, loss: 0.21711771-(best 0.21623367)\n",
      "\n",
      "Detailed Loss: recon 0.21495938-0.21495938 | disen 0.73590612-0.00000000 | temporal 0.67265391-0.00000000 | total 0.21495938\n",
      "Epoch 76, loss: 0.21495938-(best 0.21495938)\n",
      "\n",
      "Detailed Loss: recon 0.21677153-0.21677153 | disen 0.73868459-0.00000000 | temporal 0.67270005-0.00000000 | total 0.21677153\n",
      "Epoch 77, loss: 0.21677153-(best 0.21495938)\n",
      "\n",
      "Detailed Loss: recon 0.21534598-0.21534598 | disen 0.73799151-0.00000000 | temporal 0.67290503-0.00000000 | total 0.21534598\n",
      "Epoch 78, loss: 0.21534598-(best 0.21495938)\n",
      "\n",
      "Detailed Loss: recon 0.21631977-0.21631977 | disen 0.73936230-0.00000000 | temporal 0.67290044-0.00000000 | total 0.21631977\n",
      "Epoch 79, loss: 0.21631977-(best 0.21495938)\n",
      "\n",
      "Detailed Loss: recon 0.21448998-0.21448998 | disen 0.73809296-0.00000000 | temporal 0.67315716-0.00000000 | total 0.21448998\n",
      "Epoch 80, loss: 0.21448998-(best 0.21448998)\n",
      "\n",
      "Detailed Loss: recon 0.21473186-0.21473186 | disen 0.73710561-0.00000000 | temporal 0.67346621-0.00000000 | total 0.21473186\n",
      "Epoch 81, loss: 0.21473186-(best 0.21448998)\n",
      "\n",
      "Detailed Loss: recon 0.21491186-0.21491186 | disen 0.73828900-0.00000000 | temporal 0.67304021-0.00000000 | total 0.21491186\n",
      "Epoch 82, loss: 0.21491186-(best 0.21448998)\n",
      "\n",
      "Detailed Loss: recon 0.21182196-0.21182196 | disen 0.74047279-0.00000000 | temporal 0.67312664-0.00000000 | total 0.21182196\n",
      "Epoch 83, loss: 0.21182196-(best 0.21182196)\n",
      "\n",
      "Detailed Loss: recon 0.21303156-0.21303156 | disen 0.74066609-0.00000000 | temporal 0.67322856-0.00000000 | total 0.21303156\n",
      "Epoch 84, loss: 0.21303156-(best 0.21182196)\n",
      "\n",
      "Detailed Loss: recon 0.21359324-0.21359324 | disen 0.73749280-0.00000000 | temporal 0.67332554-0.00000000 | total 0.21359324\n",
      "Epoch 85, loss: 0.21359324-(best 0.21182196)\n",
      "\n",
      "Detailed Loss: recon 0.21270399-0.21270399 | disen 0.73504066-0.00000000 | temporal 0.67331636-0.00000000 | total 0.21270399\n",
      "Epoch 86, loss: 0.21270399-(best 0.21182196)\n",
      "\n",
      "Detailed Loss: recon 0.21006921-0.21006921 | disen 0.73757172-0.00000000 | temporal 0.67322975-0.00000000 | total 0.21006921\n",
      "Epoch 87, loss: 0.21006921-(best 0.21006921)\n",
      "\n",
      "Detailed Loss: recon 0.21048445-0.21048445 | disen 0.74016732-0.00000000 | temporal 0.67333877-0.00000000 | total 0.21048445\n",
      "Epoch 88, loss: 0.21048445-(best 0.21006921)\n",
      "\n",
      "Detailed Loss: recon 0.21406655-0.21406655 | disen 0.74142432-0.00000000 | temporal 0.67331320-0.00000000 | total 0.21406655\n",
      "Epoch 89, loss: 0.21406655-(best 0.21006921)\n",
      "\n",
      "Detailed Loss: recon 0.21035314-0.21035314 | disen 0.73825753-0.00000000 | temporal 0.67288756-0.00000000 | total 0.21035314\n",
      "Epoch 90, loss: 0.21035314-(best 0.21006921)\n",
      "\n",
      "Detailed Loss: recon 0.21118954-0.21118954 | disen 0.73945081-0.00000000 | temporal 0.67323607-0.00000000 | total 0.21118954\n",
      "Epoch 91, loss: 0.21118954-(best 0.21006921)\n",
      "\n",
      "Detailed Loss: recon 0.20927933-0.20927933 | disen 0.73846090-0.00000000 | temporal 0.67320532-0.00000000 | total 0.20927933\n",
      "Epoch 92, loss: 0.20927933-(best 0.20927933)\n",
      "\n",
      "Detailed Loss: recon 0.21187672-0.21187672 | disen 0.73654580-0.00000000 | temporal 0.67303759-0.00000000 | total 0.21187672\n",
      "Epoch 93, loss: 0.21187672-(best 0.20927933)\n",
      "\n",
      "Detailed Loss: recon 0.21136837-0.21136837 | disen 0.73694605-0.00000000 | temporal 0.67347085-0.00000000 | total 0.21136837\n",
      "Epoch 94, loss: 0.21136837-(best 0.20927933)\n",
      "\n",
      "Detailed Loss: recon 0.20961015-0.20961015 | disen 0.73773730-0.00000000 | temporal 0.67335093-0.00000000 | total 0.20961015\n",
      "Epoch 95, loss: 0.20961015-(best 0.20927933)\n",
      "\n",
      "Detailed Loss: recon 0.21047382-0.21047382 | disen 0.73318565-0.00000000 | temporal 0.67337102-0.00000000 | total 0.21047382\n",
      "Epoch 96, loss: 0.21047382-(best 0.20927933)\n",
      "\n",
      "Detailed Loss: recon 0.20970133-0.20970133 | disen 0.73865300-0.00000000 | temporal 0.67373115-0.00000000 | total 0.20970133\n",
      "Epoch 97, loss: 0.20970133-(best 0.20927933)\n",
      "\n",
      "Detailed Loss: recon 0.20716833-0.20716833 | disen 0.74241269-0.00000000 | temporal 0.67361552-0.00000000 | total 0.20716833\n",
      "Epoch 98, loss: 0.20716833-(best 0.20716833)\n",
      "\n",
      "Detailed Loss: recon 0.21195541-0.21195541 | disen 0.73994005-0.00000000 | temporal 0.67322803-0.00000000 | total 0.21195541\n",
      "Epoch 99, loss: 0.21195541-(best 0.20716833)\n",
      "\n",
      "Detailed Loss: recon 0.20688491-0.20688491 | disen 0.74019575-0.00000000 | temporal 0.67354208-0.00000000 | total 0.20688491\n",
      "Epoch 100, loss: 0.20688491-(best 0.20688491)\n",
      "\n",
      ">> Reached final epoch. Loading best model...\n",
      "TRAINING XGB PREDICTOR\n",
      ">> Ending training!\n",
      "\n",
      "-----------------------\n",
      "TIME COST: 8.601107358932495 s\n",
      "Best Val: REC 52.69 PRE 51.12 MF1 68.98 AUC 80.79 TP 274 FP 262 TN 1570 FN 246\n",
      "-----------------------\n",
      "\n",
      "ASSIGNED GRAPH FOR EVALUATION: 11758 {1: 2566, 0: 9192} Nodes, 1049758 Edges\n",
      "\n",
      "-------------------\n",
      "STARTING EVALUATION\n",
      "-------------------\n",
      "\n",
      "Dataset - Overall: REC 62.24 PRE 46.78 MF1 68.76 AUC 81.41 TP 1597 FP 1817 TN 7375 FN 969 | 11758 {1: 2566, 0: 9192}\n",
      "Dataset - Train: REC 70.68 PRE 51.59 MF1 72.64 AUC 85.54 TP 552 FP 518 TN 2228 FN 229 | 3527 {0: 2746, 1: 781}\n",
      "Dataset - Val: REC 60.58 PRE 46.32 MF1 68.12 AUC 80.27 TP 315 FP 365 TN 1467 FN 205 | 2352 {1: 520, 0: 1832}\n",
      "Dataset - Test: REC 57.71 PRE 43.87 MF1 66.60 AUC 79.43 TP 730 FP 934 TN 3680 FN 535 | 5879 {0: 4614, 1: 1265}\n",
      "PREDICTION STATUS - {'1-True': 2031, '0-False': 3680, '0-True': 5512, '1-False': 535}\n",
      "    >> 535 positive nodes left unpredicted...\n",
      "    >> 535 OG positive nodes left unpredicted...\n",
      "Selecting data as budgeted ground truth for next round...\n",
      "  > Starting round 1...\n",
      "\n",
      "+++++++++++++++++\n",
      "STARTING ROUND 1!\n",
      "+++++++++++++++++\n",
      "        \n",
      "ASSIGNED GRAPH FOR TRAINING: 11758 {1: 2566, 0: 9192} Nodes, 1049758 Edges\n",
      "\n",
      ">> Splitting to train-validation set...\n",
      "Alotting train-val-test split for round 1\n",
      "Initial pool: 5879, Prediction pool: 3414, Budget pool: 0, Full pool: 7543\n",
      "Full graph size: 11758, Training: 4525, Val: 3018, Test: 11758\n",
      "    >> INITIAL DATA SPLIT: 4525 train rows ({1: 1218, 0: 3307}) | 3018 val rows ({1: 813, 0: 2205}) | 4215 test rows ({0: 3680, 1: 535})\n",
      "    >> AUGMENTED DATA SPLIT: 4525 train rows ({1: 1218, 0: 3307}) | 3018 val rows ({1: 813, 0: 2205}) | 4215 test rows ({0: 3680, 1: 535})\n",
      "    >> Updated cross-entropy weight to 2.7151067323481115...\n",
      "\n",
      "-----------------------\n",
      "STARTING MODEL TRAININGps_l\n",
      "-----------------------\n",
      "\n",
      "TRAINING NODE EMBEDDER\n",
      "Detailed Loss: recon 0.19016537-0.19016537 | disen 0.60037124-0.00000000 | temporal 0.56591177-0.00000000 | total 0.19016537\n",
      "Epoch 1, loss: 0.19016537-(best 0.19016537)\n",
      "\n",
      "Detailed Loss: recon 0.31975135-0.31975135 | disen 0.60368037-0.00000000 | temporal 0.56439650-0.00000000 | total 0.31975135\n",
      "Epoch 2, loss: 0.31975135-(best 0.19016537)\n",
      "\n",
      "Detailed Loss: recon 0.19895922-0.19895922 | disen 0.60158455-0.00000000 | temporal 0.56610519-0.00000000 | total 0.19895922\n",
      "Epoch 3, loss: 0.19895922-(best 0.19016537)\n",
      "\n",
      "Detailed Loss: recon 0.29094669-0.29094669 | disen 0.60294032-0.00000000 | temporal 0.56687826-0.00000000 | total 0.29094669\n",
      "Epoch 4, loss: 0.29094669-(best 0.19016537)\n",
      "\n",
      "Detailed Loss: recon 0.21947309-0.21947309 | disen 0.60280192-0.00000000 | temporal 0.56664622-0.00000000 | total 0.21947309\n",
      "Epoch 5, loss: 0.21947309-(best 0.19016537)\n",
      "\n",
      "Detailed Loss: recon 0.20278174-0.20278174 | disen 0.60434127-0.00000000 | temporal 0.56618923-0.00000000 | total 0.20278174\n",
      "Epoch 6, loss: 0.20278174-(best 0.19016537)\n",
      "\n",
      "Detailed Loss: recon 0.21847992-0.21847992 | disen 0.60502219-0.00000000 | temporal 0.56579715-0.00000000 | total 0.21847992\n",
      "Epoch 7, loss: 0.21847992-(best 0.19016537)\n",
      "\n",
      "Detailed Loss: recon 0.22922337-0.22922337 | disen 0.60432905-0.00000000 | temporal 0.56564635-0.00000000 | total 0.22922337\n",
      "Epoch 8, loss: 0.22922337-(best 0.19016537)\n",
      "\n",
      "Detailed Loss: recon 0.23733521-0.23733521 | disen 0.60421872-0.00000000 | temporal 0.56568980-0.00000000 | total 0.23733521\n",
      "Epoch 9, loss: 0.23733521-(best 0.19016537)\n",
      "\n",
      "Detailed Loss: recon 0.22077513-0.22077513 | disen 0.60417789-0.00000000 | temporal 0.56590456-0.00000000 | total 0.22077513\n",
      "Epoch 10, loss: 0.22077513-(best 0.19016537)\n",
      "\n",
      "Detailed Loss: recon 0.21774714-0.21774714 | disen 0.60491621-0.00000000 | temporal 0.56619704-0.00000000 | total 0.21774714\n",
      "Epoch 11, loss: 0.21774714-(best 0.19016537)\n",
      "\n",
      "Detailed Loss: recon 0.21542439-0.21542439 | disen 0.60497141-0.00000000 | temporal 0.56645292-0.00000000 | total 0.21542439\n",
      "Epoch 12, loss: 0.21542439-(best 0.19016537)\n",
      "\n",
      "Detailed Loss: recon 0.21681219-0.21681219 | disen 0.60469729-0.00000000 | temporal 0.56662637-0.00000000 | total 0.21681219\n",
      "Epoch 13, loss: 0.21681219-(best 0.19016537)\n",
      "\n",
      "Detailed Loss: recon 0.21536967-0.21536967 | disen 0.60460979-0.00000000 | temporal 0.56673676-0.00000000 | total 0.21536967\n",
      "Epoch 14, loss: 0.21536967-(best 0.19016537)\n",
      "\n",
      "Detailed Loss: recon 0.21468107-0.21468107 | disen 0.60352749-0.00000000 | temporal 0.56672859-0.00000000 | total 0.21468107\n",
      "Epoch 15, loss: 0.21468107-(best 0.19016537)\n",
      "\n",
      "Detailed Loss: recon 0.21240532-0.21240532 | disen 0.60403746-0.00000000 | temporal 0.56666666-0.00000000 | total 0.21240532\n",
      "Epoch 16, loss: 0.21240532-(best 0.19016537)\n",
      "\n",
      "Detailed Loss: recon 0.20790838-0.20790838 | disen 0.60422409-0.00000000 | temporal 0.56653476-0.00000000 | total 0.20790838\n",
      "Epoch 17, loss: 0.20790838-(best 0.19016537)\n",
      "\n",
      "Detailed Loss: recon 0.20562384-0.20562384 | disen 0.60218436-0.00000000 | temporal 0.56642616-0.00000000 | total 0.20562384\n",
      "Epoch 18, loss: 0.20562384-(best 0.19016537)\n",
      "\n",
      "Detailed Loss: recon 0.20793459-0.20793459 | disen 0.60331553-0.00000000 | temporal 0.56629694-0.00000000 | total 0.20793459\n",
      "Epoch 19, loss: 0.20793459-(best 0.19016537)\n",
      "\n",
      "Detailed Loss: recon 0.20457104-0.20457104 | disen 0.60301977-0.00000000 | temporal 0.56620002-0.00000000 | total 0.20457104\n",
      "Epoch 20, loss: 0.20457104-(best 0.19016537)\n",
      "\n",
      "Detailed Loss: recon 0.20380685-0.20380685 | disen 0.60297269-0.00000000 | temporal 0.56612235-0.00000000 | total 0.20380685\n",
      "Epoch 21, loss: 0.20380685-(best 0.19016537)\n",
      "\n",
      "Detailed Loss: recon 0.20295173-0.20295173 | disen 0.60332829-0.00000000 | temporal 0.56612450-0.00000000 | total 0.20295173\n",
      "Epoch 22, loss: 0.20295173-(best 0.19016537)\n",
      "\n",
      "Detailed Loss: recon 0.20218432-0.20218432 | disen 0.60319340-0.00000000 | temporal 0.56614840-0.00000000 | total 0.20218432\n",
      "Epoch 23, loss: 0.20218432-(best 0.19016537)\n",
      "\n",
      "Detailed Loss: recon 0.19818643-0.19818643 | disen 0.60311580-0.00000000 | temporal 0.56622988-0.00000000 | total 0.19818643\n",
      "Epoch 24, loss: 0.19818643-(best 0.19016537)\n",
      "\n",
      "Detailed Loss: recon 0.20048013-0.20048013 | disen 0.60316002-0.00000000 | temporal 0.56627464-0.00000000 | total 0.20048013\n",
      "Epoch 25, loss: 0.20048013-(best 0.19016537)\n",
      "\n",
      "Detailed Loss: recon 0.19834237-0.19834237 | disen 0.60315895-0.00000000 | temporal 0.56636184-0.00000000 | total 0.19834237\n",
      "Epoch 26, loss: 0.19834237-(best 0.19016537)\n",
      "\n",
      "Detailed Loss: recon 0.20053121-0.20053121 | disen 0.60296082-0.00000000 | temporal 0.56634527-0.00000000 | total 0.20053121\n",
      "Epoch 27, loss: 0.20053121-(best 0.19016537)\n",
      "\n",
      "Detailed Loss: recon 0.19817901-0.19817901 | disen 0.60365748-0.00000000 | temporal 0.56630945-0.00000000 | total 0.19817901\n",
      "Epoch 28, loss: 0.19817901-(best 0.19016537)\n",
      "\n",
      "Detailed Loss: recon 0.19815487-0.19815487 | disen 0.60343629-0.00000000 | temporal 0.56621003-0.00000000 | total 0.19815487\n",
      "Epoch 29, loss: 0.19815487-(best 0.19016537)\n",
      "\n",
      "Detailed Loss: recon 0.19565761-0.19565761 | disen 0.60287684-0.00000000 | temporal 0.56610817-0.00000000 | total 0.19565761\n",
      "Epoch 30, loss: 0.19565761-(best 0.19016537)\n",
      "\n",
      "Detailed Loss: recon 0.19379203-0.19379203 | disen 0.60341275-0.00000000 | temporal 0.56598526-0.00000000 | total 0.19379203\n",
      "Epoch 31, loss: 0.19379203-(best 0.19016537)\n",
      "\n",
      "Detailed Loss: recon 0.19346194-0.19346194 | disen 0.60294127-0.00000000 | temporal 0.56587082-0.00000000 | total 0.19346194\n",
      "Epoch 32, loss: 0.19346194-(best 0.19016537)\n",
      "\n",
      "Detailed Loss: recon 0.19480400-0.19480400 | disen 0.60268545-0.00000000 | temporal 0.56580412-0.00000000 | total 0.19480400\n",
      "Epoch 33, loss: 0.19480400-(best 0.19016537)\n",
      "\n",
      "Detailed Loss: recon 0.19502333-0.19502333 | disen 0.60363019-0.00000000 | temporal 0.56566298-0.00000000 | total 0.19502333\n",
      "Epoch 34, loss: 0.19502333-(best 0.19016537)\n",
      "\n",
      "Detailed Loss: recon 0.19111845-0.19111845 | disen 0.60344589-0.00000000 | temporal 0.56566787-0.00000000 | total 0.19111845\n",
      "Epoch 35, loss: 0.19111845-(best 0.19016537)\n",
      "\n",
      "Detailed Loss: recon 0.19138485-0.19138485 | disen 0.60339803-0.00000000 | temporal 0.56569886-0.00000000 | total 0.19138485\n",
      "Epoch 36, loss: 0.19138485-(best 0.19016537)\n",
      "\n",
      "Detailed Loss: recon 0.19168286-0.19168286 | disen 0.60341400-0.00000000 | temporal 0.56570512-0.00000000 | total 0.19168286\n",
      "Epoch 37, loss: 0.19168286-(best 0.19016537)\n",
      "\n",
      "Detailed Loss: recon 0.18991481-0.18991481 | disen 0.60310477-0.00000000 | temporal 0.56569105-0.00000000 | total 0.18991481\n",
      "Epoch 38, loss: 0.18991481-(best 0.18991481)\n",
      "\n",
      "Detailed Loss: recon 0.18999004-0.18999004 | disen 0.60388380-0.00000000 | temporal 0.56564647-0.00000000 | total 0.18999004\n",
      "Epoch 39, loss: 0.18999004-(best 0.18991481)\n",
      "\n",
      "Detailed Loss: recon 0.19101068-0.19101068 | disen 0.60350740-0.00000000 | temporal 0.56558412-0.00000000 | total 0.19101068\n",
      "Epoch 40, loss: 0.19101068-(best 0.18991481)\n",
      "\n",
      "Detailed Loss: recon 0.18996707-0.18996707 | disen 0.60274351-0.00000000 | temporal 0.56552684-0.00000000 | total 0.18996707\n",
      "Epoch 41, loss: 0.18996707-(best 0.18991481)\n",
      "\n",
      "Detailed Loss: recon 0.19020280-0.19020280 | disen 0.60218561-0.00000000 | temporal 0.56550634-0.00000000 | total 0.19020280\n",
      "Epoch 42, loss: 0.19020280-(best 0.18991481)\n",
      "\n",
      "Detailed Loss: recon 0.18799078-0.18799078 | disen 0.60269976-0.00000000 | temporal 0.56554145-0.00000000 | total 0.18799078\n",
      "Epoch 43, loss: 0.18799078-(best 0.18799078)\n",
      "\n",
      "Detailed Loss: recon 0.18881121-0.18881121 | disen 0.60300016-0.00000000 | temporal 0.56558007-0.00000000 | total 0.18881121\n",
      "Epoch 44, loss: 0.18881121-(best 0.18799078)\n",
      "\n",
      "Detailed Loss: recon 0.18746750-0.18746750 | disen 0.60278702-0.00000000 | temporal 0.56566572-0.00000000 | total 0.18746750\n",
      "Epoch 45, loss: 0.18746750-(best 0.18746750)\n",
      "\n",
      "Detailed Loss: recon 0.18753943-0.18753943 | disen 0.60369647-0.00000000 | temporal 0.56570768-0.00000000 | total 0.18753943\n",
      "Epoch 46, loss: 0.18753943-(best 0.18746750)\n",
      "\n",
      "Detailed Loss: recon 0.18963367-0.18963367 | disen 0.60392839-0.00000000 | temporal 0.56569558-0.00000000 | total 0.18963367\n",
      "Epoch 47, loss: 0.18963367-(best 0.18746750)\n",
      "\n",
      "Detailed Loss: recon 0.18624382-0.18624382 | disen 0.60412955-0.00000000 | temporal 0.56566584-0.00000000 | total 0.18624382\n",
      "Epoch 48, loss: 0.18624382-(best 0.18624382)\n",
      "\n",
      "Detailed Loss: recon 0.18736604-0.18736604 | disen 0.60417926-0.00000000 | temporal 0.56567979-0.00000000 | total 0.18736604\n",
      "Epoch 49, loss: 0.18736604-(best 0.18624382)\n",
      "\n",
      "Detailed Loss: recon 0.18584821-0.18584821 | disen 0.60367829-0.00000000 | temporal 0.56571805-0.00000000 | total 0.18584821\n",
      "Epoch 50, loss: 0.18584821-(best 0.18584821)\n",
      "\n",
      ">> Reached final epoch. Loading best model...\n",
      "TRAINING XGB PREDICTOR\n",
      ">> Ending training!\n",
      "\n",
      "-----------------------\n",
      "TIME COST: 7.056885719299316 s\n",
      "Best Val: REC 60.02 PRE 49.80 MF1 67.59 AUC 78.02 TP 488 FP 492 TN 1713 FN 325\n",
      "-----------------------\n",
      "Generating additional positive data from adversary...\n",
      "Generating additional negative data by duplicating random nodes...\n",
      "\n",
      "ASSIGNED GRAPH FOR EVALUATION: 12345 {1: 2694, 0: 9651} Nodes, 1158882 Edges\n",
      "\n",
      "-------------------\n",
      "STARTING EVALUATION\n",
      "-------------------\n",
      "\n",
      "Dataset - Overall: REC 57.54 PRE 54.46 MF1 71.61 AUC 83.08 TP 1550 FP 1296 TN 8355 FN 1144 | 12345 {1: 2694, 0: 9651}\n",
      "Dataset - Train: REC 75.53 PRE 65.43 MF1 78.96 AUC 89.32 TP 920 FP 486 TN 2821 FN 298 | 4525 {1: 1218, 0: 3307}\n",
      "Dataset - Val: REC 57.20 PRE 50.32 MF1 67.38 AUC 78.00 TP 465 FP 459 TN 1746 FN 348 | 3018 {1: 813, 0: 2205}\n",
      "Dataset - Test: REC 24.89 PRE 31.98 MF1 58.96 AUC 75.98 TP 165 FP 351 TN 3788 FN 498 | 4802 {0: 4139, 1: 663}\n",
      "PREDICTION STATUS - {'1-True': 2196, '0-False': 3788, '0-True': 5863, '1-False': 498}\n",
      "    >> 498 positive nodes left unpredicted...\n",
      "    >> 403 OG positive nodes left unpredicted...\n",
      "-----------------------\n",
      "PREDICTION RESULT - ROUNDS\n",
      "Dataset - Round 0: REC 50.67 PRE 50.31 MF1 68.43 AUC 80.32 TP 641 FP 633 TN 3981 FN 624 | 5879 {0: 4614, 1: 1265}\n",
      "Dataset - Round 1: REC 25.78 PRE 39.29 MF1 57.98 AUC 70.07 TP 33 FP 51 TN 408 FN 95 | 587 {1: 128, 0: 459}\n",
      "-----------------------\n",
      "PREDICTION RESULT - SEEDS\n",
      "Seeds - Current: REC 30.47 PRE 38.24 MF1 58.91 AUC 70.51 TP 39 FP 63 TN 396 FN 89 | 587 {1: 128, 0: 459}\n",
      "Seeds - Prev: REC 0.00 PRE 0.00 MF1 38.60 AUC 70.51 TP 0 FP 90 TN 369 FN 128 | 587 {1: 128, 0: 459}\n",
      "Selecting data as budgeted ground truth for next round...\n",
      "  > Starting round 2...\n",
      "\n",
      "+++++++++++++++++\n",
      "STARTING ROUND 2!\n",
      "+++++++++++++++++\n",
      "        \n",
      "ASSIGNED GRAPH FOR TRAINING: 12345 {1: 2694, 0: 9651} Nodes, 1158882 Edges\n",
      "\n",
      ">> Splitting to train-validation set...\n",
      "Alotting train-val-test split for round 2\n",
      "Initial pool: 5879, Prediction pool: 6260, Budget pool: 0, Full pool: 8059\n",
      "Full graph size: 12345, Training: 4835, Val: 3224, Test: 12345\n",
      "    >> INITIAL DATA SPLIT: 4835 train rows ({1: 1317, 0: 3518}) | 3224 val rows ({0: 2345, 1: 879}) | 4286 test rows ({0: 3788, 1: 498})\n",
      "    >> AUGMENTED DATA SPLIT: 4835 train rows ({1: 1317, 0: 3518}) | 3224 val rows ({0: 2345, 1: 879}) | 4286 test rows ({0: 3788, 1: 498})\n",
      "    >> Updated cross-entropy weight to 2.671222475322703...\n",
      "\n",
      "-----------------------\n",
      "STARTING MODEL TRAININGps_l\n",
      "-----------------------\n",
      "\n",
      "TRAINING NODE EMBEDDER\n",
      "Detailed Loss: recon 0.18328810-0.18328810 | disen 0.60374784-0.00000000 | temporal 0.58788818-0.00000000 | total 0.18328810\n",
      "Epoch 1, loss: 0.18328810-(best 0.18328810)\n",
      "\n",
      "Detailed Loss: recon 0.22217929-0.22217929 | disen 0.60677880-0.00000000 | temporal 0.58728099-0.00000000 | total 0.22217929\n",
      "Epoch 2, loss: 0.22217929-(best 0.18328810)\n",
      "\n",
      "Detailed Loss: recon 0.20965874-0.20965874 | disen 0.60386360-0.00000000 | temporal 0.58849239-0.00000000 | total 0.20965874\n",
      "Epoch 3, loss: 0.20965874-(best 0.18328810)\n",
      "\n",
      "Detailed Loss: recon 0.19986475-0.19986475 | disen 0.60397577-0.00000000 | temporal 0.58846122-0.00000000 | total 0.19986475\n",
      "Epoch 4, loss: 0.19986475-(best 0.18328810)\n",
      "\n",
      "Detailed Loss: recon 0.18709227-0.18709227 | disen 0.60328281-0.00000000 | temporal 0.58810931-0.00000000 | total 0.18709227\n",
      "Epoch 5, loss: 0.18709227-(best 0.18328810)\n",
      "\n",
      "Detailed Loss: recon 0.19355820-0.19355820 | disen 0.60428160-0.00000000 | temporal 0.58780479-0.00000000 | total 0.19355820\n",
      "Epoch 6, loss: 0.19355820-(best 0.18328810)\n",
      "\n",
      "Detailed Loss: recon 0.19281070-0.19281070 | disen 0.60398793-0.00000000 | temporal 0.58775973-0.00000000 | total 0.19281070\n",
      "Epoch 7, loss: 0.19281070-(best 0.18328810)\n",
      "\n",
      "Detailed Loss: recon 0.18711162-0.18711162 | disen 0.60290432-0.00000000 | temporal 0.58791286-0.00000000 | total 0.18711162\n",
      "Epoch 8, loss: 0.18711162-(best 0.18328810)\n",
      "\n",
      "Detailed Loss: recon 0.18797034-0.18797034 | disen 0.60133612-0.00000000 | temporal 0.58820671-0.00000000 | total 0.18797034\n",
      "Epoch 9, loss: 0.18797034-(best 0.18328810)\n",
      "\n",
      "Detailed Loss: recon 0.19128922-0.19128922 | disen 0.60001880-0.00000000 | temporal 0.58827084-0.00000000 | total 0.19128922\n",
      "Epoch 10, loss: 0.19128922-(best 0.18328810)\n",
      "\n",
      "Detailed Loss: recon 0.19016412-0.19016412 | disen 0.59994841-0.00000000 | temporal 0.58819091-0.00000000 | total 0.19016412\n",
      "Epoch 11, loss: 0.19016412-(best 0.18328810)\n",
      "\n",
      "Detailed Loss: recon 0.18618205-0.18618205 | disen 0.60013640-0.00000000 | temporal 0.58800280-0.00000000 | total 0.18618205\n",
      "Epoch 12, loss: 0.18618205-(best 0.18328810)\n",
      "\n",
      "Detailed Loss: recon 0.18705830-0.18705830 | disen 0.60082728-0.00000000 | temporal 0.58782429-0.00000000 | total 0.18705830\n",
      "Epoch 13, loss: 0.18705830-(best 0.18328810)\n",
      "\n",
      "Detailed Loss: recon 0.19072646-0.19072646 | disen 0.59981537-0.00000000 | temporal 0.58770746-0.00000000 | total 0.19072646\n",
      "Epoch 14, loss: 0.19072646-(best 0.18328810)\n",
      "\n",
      "Detailed Loss: recon 0.18856886-0.18856886 | disen 0.60114950-0.00000000 | temporal 0.58769470-0.00000000 | total 0.18856886\n",
      "Epoch 15, loss: 0.18856886-(best 0.18328810)\n",
      "\n",
      "Detailed Loss: recon 0.18624996-0.18624996 | disen 0.59971464-0.00000000 | temporal 0.58782685-0.00000000 | total 0.18624996\n",
      "Epoch 16, loss: 0.18624996-(best 0.18328810)\n",
      "\n",
      "Detailed Loss: recon 0.18557678-0.18557678 | disen 0.59945190-0.00000000 | temporal 0.58791608-0.00000000 | total 0.18557678\n",
      "Epoch 17, loss: 0.18557678-(best 0.18328810)\n",
      "\n",
      "Detailed Loss: recon 0.18510012-0.18510012 | disen 0.59911019-0.00000000 | temporal 0.58800346-0.00000000 | total 0.18510012\n",
      "Epoch 18, loss: 0.18510012-(best 0.18328810)\n",
      "\n",
      "Detailed Loss: recon 0.18506470-0.18506470 | disen 0.59818375-0.00000000 | temporal 0.58797282-0.00000000 | total 0.18506470\n",
      "Epoch 19, loss: 0.18506470-(best 0.18328810)\n",
      "\n",
      "Detailed Loss: recon 0.18382493-0.18382493 | disen 0.59925568-0.00000000 | temporal 0.58787727-0.00000000 | total 0.18382493\n",
      "Epoch 20, loss: 0.18382493-(best 0.18328810)\n",
      "\n",
      "Detailed Loss: recon 0.18321680-0.18321680 | disen 0.59914005-0.00000000 | temporal 0.58776474-0.00000000 | total 0.18321680\n",
      "Epoch 21, loss: 0.18321680-(best 0.18321680)\n",
      "\n",
      "Detailed Loss: recon 0.18479329-0.18479329 | disen 0.59915733-0.00000000 | temporal 0.58772463-0.00000000 | total 0.18479329\n",
      "Epoch 22, loss: 0.18479329-(best 0.18321680)\n",
      "\n",
      "Detailed Loss: recon 0.18445800-0.18445800 | disen 0.59826720-0.00000000 | temporal 0.58780164-0.00000000 | total 0.18445800\n",
      "Epoch 23, loss: 0.18445800-(best 0.18321680)\n",
      "\n",
      "Detailed Loss: recon 0.18309574-0.18309574 | disen 0.59835660-0.00000000 | temporal 0.58789742-0.00000000 | total 0.18309574\n",
      "Epoch 24, loss: 0.18309574-(best 0.18309574)\n",
      "\n",
      "Detailed Loss: recon 0.18171787-0.18171787 | disen 0.59880495-0.00000000 | temporal 0.58795589-0.00000000 | total 0.18171787\n",
      "Epoch 25, loss: 0.18171787-(best 0.18171787)\n",
      "\n",
      "Detailed Loss: recon 0.18317318-0.18317318 | disen 0.59790194-0.00000000 | temporal 0.58809257-0.00000000 | total 0.18317318\n",
      "Epoch 26, loss: 0.18317318-(best 0.18171787)\n",
      "\n",
      "Detailed Loss: recon 0.18088315-0.18088315 | disen 0.59820151-0.00000000 | temporal 0.58804017-0.00000000 | total 0.18088315\n",
      "Epoch 27, loss: 0.18088315-(best 0.18088315)\n",
      "\n",
      "Detailed Loss: recon 0.18136951-0.18136951 | disen 0.59785438-0.00000000 | temporal 0.58797163-0.00000000 | total 0.18136951\n",
      "Epoch 28, loss: 0.18136951-(best 0.18088315)\n",
      "\n",
      "Detailed Loss: recon 0.18249306-0.18249306 | disen 0.59938645-0.00000000 | temporal 0.58791339-0.00000000 | total 0.18249306\n",
      "Epoch 29, loss: 0.18249306-(best 0.18088315)\n",
      "\n",
      "Detailed Loss: recon 0.17982516-0.17982516 | disen 0.59941256-0.00000000 | temporal 0.58787060-0.00000000 | total 0.17982516\n",
      "Epoch 30, loss: 0.17982516-(best 0.17982516)\n",
      "\n",
      "Detailed Loss: recon 0.18116072-0.18116072 | disen 0.59912944-0.00000000 | temporal 0.58794045-0.00000000 | total 0.18116072\n",
      "Epoch 31, loss: 0.18116072-(best 0.17982516)\n",
      "\n",
      "Detailed Loss: recon 0.17977673-0.17977673 | disen 0.59885728-0.00000000 | temporal 0.58799714-0.00000000 | total 0.17977673\n",
      "Epoch 32, loss: 0.17977673-(best 0.17977673)\n",
      "\n",
      "Detailed Loss: recon 0.17979978-0.17979978 | disen 0.59829271-0.00000000 | temporal 0.58802062-0.00000000 | total 0.17979978\n",
      "Epoch 33, loss: 0.17979978-(best 0.17977673)\n",
      "\n",
      "Detailed Loss: recon 0.17900476-0.17900476 | disen 0.59814954-0.00000000 | temporal 0.58801162-0.00000000 | total 0.17900476\n",
      "Epoch 34, loss: 0.17900476-(best 0.17900476)\n",
      "\n",
      "Detailed Loss: recon 0.18017054-0.18017054 | disen 0.59799016-0.00000000 | temporal 0.58798158-0.00000000 | total 0.18017054\n",
      "Epoch 35, loss: 0.18017054-(best 0.17900476)\n",
      "\n",
      "Detailed Loss: recon 0.17938358-0.17938358 | disen 0.59920490-0.00000000 | temporal 0.58788681-0.00000000 | total 0.17938358\n",
      "Epoch 36, loss: 0.17938358-(best 0.17900476)\n",
      "\n",
      "Detailed Loss: recon 0.17852400-0.17852400 | disen 0.59876412-0.00000000 | temporal 0.58790529-0.00000000 | total 0.17852400\n",
      "Epoch 37, loss: 0.17852400-(best 0.17852400)\n",
      "\n",
      "Detailed Loss: recon 0.17857671-0.17857671 | disen 0.59757721-0.00000000 | temporal 0.58793664-0.00000000 | total 0.17857671\n",
      "Epoch 38, loss: 0.17857671-(best 0.17852400)\n",
      "\n",
      "Detailed Loss: recon 0.17862646-0.17862646 | disen 0.59805673-0.00000000 | temporal 0.58794945-0.00000000 | total 0.17862646\n",
      "Epoch 39, loss: 0.17862646-(best 0.17852400)\n",
      "\n",
      "Detailed Loss: recon 0.17822313-0.17822313 | disen 0.59908795-0.00000000 | temporal 0.58791131-0.00000000 | total 0.17822313\n",
      "Epoch 40, loss: 0.17822313-(best 0.17822313)\n",
      "\n",
      "Detailed Loss: recon 0.17727426-0.17727426 | disen 0.59883600-0.00000000 | temporal 0.58792681-0.00000000 | total 0.17727426\n",
      "Epoch 41, loss: 0.17727426-(best 0.17727426)\n",
      "\n",
      "Detailed Loss: recon 0.17846283-0.17846283 | disen 0.59818673-0.00000000 | temporal 0.58793533-0.00000000 | total 0.17846283\n",
      "Epoch 42, loss: 0.17846283-(best 0.17727426)\n",
      "\n",
      "Detailed Loss: recon 0.17778990-0.17778990 | disen 0.59824479-0.00000000 | temporal 0.58799481-0.00000000 | total 0.17778990\n",
      "Epoch 43, loss: 0.17778990-(best 0.17727426)\n",
      "\n",
      "Detailed Loss: recon 0.17829531-0.17829531 | disen 0.59845102-0.00000000 | temporal 0.58805186-0.00000000 | total 0.17829531\n",
      "Epoch 44, loss: 0.17829531-(best 0.17727426)\n",
      "\n",
      "Detailed Loss: recon 0.17834911-0.17834911 | disen 0.59809554-0.00000000 | temporal 0.58805180-0.00000000 | total 0.17834911\n",
      "Epoch 45, loss: 0.17834911-(best 0.17727426)\n",
      "\n",
      "Detailed Loss: recon 0.17658713-0.17658713 | disen 0.59839034-0.00000000 | temporal 0.58800822-0.00000000 | total 0.17658713\n",
      "Epoch 46, loss: 0.17658713-(best 0.17658713)\n",
      "\n",
      "Detailed Loss: recon 0.17673343-0.17673343 | disen 0.59841716-0.00000000 | temporal 0.58800936-0.00000000 | total 0.17673343\n",
      "Epoch 47, loss: 0.17673343-(best 0.17658713)\n",
      "\n",
      "Detailed Loss: recon 0.17671005-0.17671005 | disen 0.59713572-0.00000000 | temporal 0.58803010-0.00000000 | total 0.17671005\n",
      "Epoch 48, loss: 0.17671005-(best 0.17658713)\n",
      "\n",
      "Detailed Loss: recon 0.17511719-0.17511719 | disen 0.59753835-0.00000000 | temporal 0.58806950-0.00000000 | total 0.17511719\n",
      "Epoch 49, loss: 0.17511719-(best 0.17511719)\n",
      "\n",
      "Detailed Loss: recon 0.17608230-0.17608230 | disen 0.59719849-0.00000000 | temporal 0.58802706-0.00000000 | total 0.17608230\n",
      "Epoch 50, loss: 0.17608230-(best 0.17511719)\n",
      "\n",
      ">> Reached final epoch. Loading best model...\n",
      "TRAINING XGB PREDICTOR\n",
      ">> Ending training!\n",
      "\n",
      "-----------------------\n",
      "TIME COST: 7.245419979095459 s\n",
      "Best Val: REC 55.86 PRE 50.20 MF1 66.91 AUC 76.63 TP 491 FP 487 TN 1858 FN 388\n",
      "-----------------------\n",
      "Generating additional positive data from adversary...\n",
      "Generating additional negative data by duplicating random nodes...\n",
      "\n",
      "ASSIGNED GRAPH FOR EVALUATION: 12932 {1: 2822, 0: 10110} Nodes, 1265020 Edges\n",
      "\n",
      "-------------------\n",
      "STARTING EVALUATION\n",
      "-------------------\n",
      "\n",
      "Dataset - Overall: REC 56.70 PRE 50.03 MF1 69.47 AUC 81.07 TP 1600 FP 1598 TN 8512 FN 1222 | 12932 {1: 2822, 0: 10110}\n",
      "Dataset - Train: REC 70.92 PRE 58.56 MF1 74.35 AUC 85.35 TP 934 FP 661 TN 2857 FN 383 | 4835 {1: 1317, 0: 3518}\n",
      "Dataset - Val: REC 59.04 PRE 47.40 MF1 65.83 AUC 76.02 TP 519 FP 576 TN 1769 FN 360 | 3224 {0: 2345, 1: 879}\n",
      "Dataset - Test: REC 23.48 PRE 28.94 MF1 58.09 AUC 74.76 TP 147 FP 361 TN 3886 FN 479 | 4873 {0: 4247, 1: 626}\n",
      "PREDICTION STATUS - {'1-True': 2343, '0-False': 3886, '0-True': 6224, '1-False': 479}\n",
      "    >> 479 positive nodes left unpredicted...\n",
      "    >> 308 OG positive nodes left unpredicted...\n",
      "-----------------------\n",
      "PREDICTION RESULT - ROUNDS\n",
      "Dataset - Round 0: REC 53.91 PRE 50.59 MF1 69.27 AUC 80.29 TP 682 FP 666 TN 3948 FN 583 | 5879 {0: 4614, 1: 1265}\n",
      "Dataset - Round 1: REC 34.38 PRE 39.29 MF1 60.20 AUC 72.09 TP 44 FP 68 TN 391 FN 84 | 587 {1: 128, 0: 459}\n",
      "Dataset - Round 2: REC 23.44 PRE 28.57 MF1 53.68 AUC 64.72 TP 30 FP 75 TN 384 FN 98 | 587 {1: 128, 0: 459}\n",
      "-----------------------\n",
      "PREDICTION RESULT - SEEDS\n",
      "Seeds - Current: REC 23.44 PRE 32.97 MF1 55.37 AUC 61.52 TP 30 FP 61 TN 398 FN 98 | 587 {1: 128, 0: 459}\n",
      "Seeds - Prev: REC 0.00 PRE 0.00 MF1 40.22 AUC 61.52 TP 0 FP 64 TN 395 FN 128 | 587 {1: 128, 0: 459}\n",
      "Selecting data as budgeted ground truth for next round...\n",
      "The file ../checkpoint/working_model_file_1730091629.362217_epoch.py does not exist\n",
      "The file ../checkpoint/working_model_file_1730091629.362217_round-M.json does not exist\n",
      "The file ../checkpoint/working_model_file_1730091629.362217_round-T.json does not exist\n",
      "The file ../checkpoint/working_model_file_1730091629.362217_round.pt does not exist\n",
      "================\n",
      "++++++++++++++++\n",
      "TRIAL NUMBER 2\n",
      "++++++++++++++++\n",
      "================\n",
      "  > Rereading graph data...\n",
      "  > Initializing multiround object...\n",
      "11758 1049758\n",
      "\n",
      "=========================\n",
      "STARTING ADVER EXPERIMENT\n",
      "=========================\n",
      "MAIN CONFIG: [('device', 'cuda:0'), ('exp_type', 'ADVER'), ('full_oracle', False), ('node_expiration_time', 1), ('round_budget_neg', 0), ('round_budget_pos', 0), ('round_new_neg', 459), ('round_new_pos', 128), ('round_num', 3), ('save_df', True), ('save_embedding', False), ('save_temp_sne', False), ('setting_type', 'INDUCTIVE'), ('task_type', 'NODE'), ('verbose', 4)]\n",
      "MODEL CONFIG: [('act_name', 'LeakyReLU'), ('addon_internal_agg', 'OR'), ('addon_name', 'NONE'), ('addon_perc', 0.05), ('addon_round_window', 7), ('alpha', 0), ('att_heads', 2), ('beta', 0), ('boost_agg_backbone', <class 'models.benchmarks_supervised.booster.GIN_noparam'>), ('boost_metric', <function average_precision_score at 0x000001C8520A5760>), ('boost_predictor', <class 'xgboost.sklearn.XGBClassifier'>), ('device', 'cuda:0'), ('drop_rate', 0), ('dropout_rate', 0.1), ('early_stopping', 75), ('embed_type', 'temporal'), ('etypes', ['none']), ('gamma', 1), ('h_feats', 64), ('k', 1), ('lambda1', 1), ('lambda2', 1), ('lambda_', 1), ('loss_sample', True), ('loss_sample_ratio', 0.1), ('loss_type', 'ndist'), ('mlp_feats', 64), ('mlp_layers', 2), ('model_name', 'XGB-SP'), ('n_estimator', 500), ('norm_name', 'layer'), ('num_epoch', 100), ('num_layers', 2), ('num_round_epoch', 50), ('round_window', 7), ('temporal_agg', 'mean_final'), ('tloss_type', 'normal'), ('training_type', 'round'), ('verbose', 4)]\n",
      "STRAT CONFIG: [('augment_name', 'REAGE'), ('augment_neg_ratio', 0.09772070079945569), ('augment_pos_ratio', 0.02727929920054431), ('augment_prob', 0.5), ('augment_round_split', 8), ('device', 'cuda:0'), ('pseudo_name', 'NONE'), ('pseudo_new_pos_only', False), ('pseudo_skip_initial', False), ('verbose', 4)]\n",
      "TRAIN CONFIG: [('ce_weight', 2.671222475322703), ('device', 'cuda:0'), ('dset_name', 'tolokers_bid'), ('early_stopping', 75), ('learning_rate', 0.01), ('loss', <function cross_entropy at 0x000001C84AF6F880>), ('loss_gamma', 5), ('num_epoch', 100), ('num_round_epoch', 50), ('optimizer', <class 'torch.optim.adam.Adam'>), ('random_state', 7777777), ('ratio_initial', 0.5), ('ratio_test', 0.2), ('ratio_train', 0.6), ('ratio_val', 0.2), ('round_reset_model', False), ('round_train_list', []), ('stuck_stopping', 50), ('stuck_threshold', 0), ('train_max_retry', 10), ('verbose', 4)]\n",
      "ADVER CONFIG: [('adver_choose_name', 'GREEDY'), ('adver_conn_coef', 0.1), ('adver_feat_coef', 1), ('adver_mod_name', 'REPLAY'), ('device', 'cuda:0'), ('verbose', 4)]\n",
      "\n",
      "INITIAL GRAPH: 11758 {1: 2566, 0: 9192} Nodes, 1049758 Edges\n",
      "=========================\n",
      "        \n",
      ">> Initializing graph...\n",
      "MAIN GRAPH: 11758 {1: 2566, 0: 9192} Nodes, 1049758 Edges\n",
      "\n",
      ">> Initializing experiment type-specific modules:\n",
      "  >> Initializing adversarial modules...\n",
      ">> Initializing task-specific modules:\n",
      ">> Initializing classifier model...\n",
      ">> Initializing other strategies...\n",
      "\n",
      "FINISH INITIALIZATION\n",
      "=====================\n",
      "        \n",
      "  > Starting round 0...\n",
      "\n",
      "+++++++++++++++++\n",
      "STARTING ROUND 0!\n",
      "+++++++++++++++++\n",
      "        \n",
      "ASSIGNED GRAPH FOR TRAINING: 5879 {1: 1249, 0: 4630} Nodes, 255107 Edges\n",
      "\n",
      ">> Splitting to train-validation set...\n",
      "Alotting train-val-test split for round 0\n",
      "Initial pool: 0, Prediction pool: 0, Budget pool: 0, Full pool: 5879\n",
      "Full graph size: 5879, Training: 3527, Val: 2352, Test: 5879\n",
      "    >> INITIAL DATA SPLIT: 3527 train rows ({0: 2778, 1: 749}) | 2352 val rows ({1: 500, 0: 1852}) | 0 test rows ({})\n",
      "    >> AUGMENTED DATA SPLIT: 3527 train rows ({0: 2778, 1: 749}) | 2352 val rows ({1: 500, 0: 1852}) | 0 test rows ({})\n",
      "    >> Updated cross-entropy weight to 3.7089452603471296...\n",
      "\n",
      "-----------------------\n",
      "STARTING MODEL TRAININGps_l\n",
      "-----------------------\n",
      "\n",
      "TRAINING NODE EMBEDDER\n",
      "Detailed Loss: recon 0.69300836-0.69300836 | disen 0.70582819-0.00000000 | temporal 0.64425004-0.00000000 | total 0.69300836\n",
      "Epoch 1, loss: 0.69300836-(best 0.69300836)\n",
      "\n",
      "Detailed Loss: recon 0.42815813-0.42815813 | disen 0.73120272-0.00000000 | temporal 0.66098440-0.00000000 | total 0.42815813\n",
      "Epoch 2, loss: 0.42815813-(best 0.42815813)\n",
      "\n",
      "Detailed Loss: recon 0.39731750-0.39731750 | disen 0.73477757-0.00000000 | temporal 0.66733265-0.00000000 | total 0.39731750\n",
      "Epoch 3, loss: 0.39731750-(best 0.39731750)\n",
      "\n",
      "Detailed Loss: recon 0.39174867-0.39174867 | disen 0.73042023-0.00000000 | temporal 0.67035061-0.00000000 | total 0.39174867\n",
      "Epoch 4, loss: 0.39174867-(best 0.39174867)\n",
      "\n",
      "Detailed Loss: recon 0.39542875-0.39542875 | disen 0.73714650-0.00000000 | temporal 0.67171663-0.00000000 | total 0.39542875\n",
      "Epoch 5, loss: 0.39542875-(best 0.39174867)\n",
      "\n",
      "Detailed Loss: recon 0.37877116-0.37877116 | disen 0.73796701-0.00000000 | temporal 0.67130673-0.00000000 | total 0.37877116\n",
      "Epoch 6, loss: 0.37877116-(best 0.37877116)\n",
      "\n",
      "Detailed Loss: recon 0.34894142-0.34894142 | disen 0.73640025-0.00000000 | temporal 0.67052805-0.00000000 | total 0.34894142\n",
      "Epoch 7, loss: 0.34894142-(best 0.34894142)\n",
      "\n",
      "Detailed Loss: recon 0.33863801-0.33863801 | disen 0.74195540-0.00000000 | temporal 0.66975284-0.00000000 | total 0.33863801\n",
      "Epoch 8, loss: 0.33863801-(best 0.33863801)\n",
      "\n",
      "Detailed Loss: recon 0.33252358-0.33252358 | disen 0.74247307-0.00000000 | temporal 0.66858149-0.00000000 | total 0.33252358\n",
      "Epoch 9, loss: 0.33252358-(best 0.33252358)\n",
      "\n",
      "Detailed Loss: recon 0.33277971-0.33277971 | disen 0.73766696-0.00000000 | temporal 0.66830391-0.00000000 | total 0.33277971\n",
      "Epoch 10, loss: 0.33277971-(best 0.33252358)\n",
      "\n",
      "Detailed Loss: recon 0.32713407-0.32713407 | disen 0.73980463-0.00000000 | temporal 0.66801792-0.00000000 | total 0.32713407\n",
      "Epoch 11, loss: 0.32713407-(best 0.32713407)\n",
      "\n",
      "Detailed Loss: recon 0.31143856-0.31143856 | disen 0.73951685-0.00000000 | temporal 0.66839248-0.00000000 | total 0.31143856\n",
      "Epoch 12, loss: 0.31143856-(best 0.31143856)\n",
      "\n",
      "Detailed Loss: recon 0.31401306-0.31401306 | disen 0.73658776-0.00000000 | temporal 0.66914982-0.00000000 | total 0.31401306\n",
      "Epoch 13, loss: 0.31401306-(best 0.31143856)\n",
      "\n",
      "Detailed Loss: recon 0.31222075-0.31222075 | disen 0.74004984-0.00000000 | temporal 0.66988719-0.00000000 | total 0.31222075\n",
      "Epoch 14, loss: 0.31222075-(best 0.31143856)\n",
      "\n",
      "Detailed Loss: recon 0.30752981-0.30752981 | disen 0.74119741-0.00000000 | temporal 0.67050481-0.00000000 | total 0.30752981\n",
      "Epoch 15, loss: 0.30752981-(best 0.30752981)\n",
      "\n",
      "Detailed Loss: recon 0.30368340-0.30368340 | disen 0.74031258-0.00000000 | temporal 0.67106736-0.00000000 | total 0.30368340\n",
      "Epoch 16, loss: 0.30368340-(best 0.30368340)\n",
      "\n",
      "Detailed Loss: recon 0.30606818-0.30606818 | disen 0.73911119-0.00000000 | temporal 0.67097282-0.00000000 | total 0.30606818\n",
      "Epoch 17, loss: 0.30606818-(best 0.30368340)\n",
      "\n",
      "Detailed Loss: recon 0.29489404-0.29489404 | disen 0.74065751-0.00000000 | temporal 0.67078209-0.00000000 | total 0.29489404\n",
      "Epoch 18, loss: 0.29489404-(best 0.29489404)\n",
      "\n",
      "Detailed Loss: recon 0.29451704-0.29451704 | disen 0.73838568-0.00000000 | temporal 0.67070293-0.00000000 | total 0.29451704\n",
      "Epoch 19, loss: 0.29451704-(best 0.29451704)\n",
      "\n",
      "Detailed Loss: recon 0.29423106-0.29423106 | disen 0.73841417-0.00000000 | temporal 0.67074925-0.00000000 | total 0.29423106\n",
      "Epoch 20, loss: 0.29423106-(best 0.29423106)\n",
      "\n",
      "Detailed Loss: recon 0.28835440-0.28835440 | disen 0.73841345-0.00000000 | temporal 0.67091471-0.00000000 | total 0.28835440\n",
      "Epoch 21, loss: 0.28835440-(best 0.28835440)\n",
      "\n",
      "Detailed Loss: recon 0.28687030-0.28687030 | disen 0.73370433-0.00000000 | temporal 0.67122281-0.00000000 | total 0.28687030\n",
      "Epoch 22, loss: 0.28687030-(best 0.28687030)\n",
      "\n",
      "Detailed Loss: recon 0.28329980-0.28329980 | disen 0.73799467-0.00000000 | temporal 0.67155647-0.00000000 | total 0.28329980\n",
      "Epoch 23, loss: 0.28329980-(best 0.28329980)\n",
      "\n",
      "Detailed Loss: recon 0.27989969-0.27989969 | disen 0.73719609-0.00000000 | temporal 0.67209274-0.00000000 | total 0.27989969\n",
      "Epoch 24, loss: 0.27989969-(best 0.27989969)\n",
      "\n",
      "Detailed Loss: recon 0.28323430-0.28323430 | disen 0.73758852-0.00000000 | temporal 0.67234528-0.00000000 | total 0.28323430\n",
      "Epoch 25, loss: 0.28323430-(best 0.27989969)\n",
      "\n",
      "Detailed Loss: recon 0.27819675-0.27819675 | disen 0.73683757-0.00000000 | temporal 0.67222893-0.00000000 | total 0.27819675\n",
      "Epoch 26, loss: 0.27819675-(best 0.27819675)\n",
      "\n",
      "Detailed Loss: recon 0.27738822-0.27738822 | disen 0.73718834-0.00000000 | temporal 0.67223042-0.00000000 | total 0.27738822\n",
      "Epoch 27, loss: 0.27738822-(best 0.27738822)\n",
      "\n",
      "Detailed Loss: recon 0.27540356-0.27540356 | disen 0.73870069-0.00000000 | temporal 0.67219800-0.00000000 | total 0.27540356\n",
      "Epoch 28, loss: 0.27540356-(best 0.27540356)\n",
      "\n",
      "Detailed Loss: recon 0.27314869-0.27314869 | disen 0.73883629-0.00000000 | temporal 0.67193711-0.00000000 | total 0.27314869\n",
      "Epoch 29, loss: 0.27314869-(best 0.27314869)\n",
      "\n",
      "Detailed Loss: recon 0.26875457-0.26875457 | disen 0.73774821-0.00000000 | temporal 0.67188436-0.00000000 | total 0.26875457\n",
      "Epoch 30, loss: 0.26875457-(best 0.26875457)\n",
      "\n",
      "Detailed Loss: recon 0.27116147-0.27116147 | disen 0.73928434-0.00000000 | temporal 0.67184150-0.00000000 | total 0.27116147\n",
      "Epoch 31, loss: 0.27116147-(best 0.26875457)\n",
      "\n",
      "Detailed Loss: recon 0.26714715-0.26714715 | disen 0.73712671-0.00000000 | temporal 0.67199796-0.00000000 | total 0.26714715\n",
      "Epoch 32, loss: 0.26714715-(best 0.26714715)\n",
      "\n",
      "Detailed Loss: recon 0.26574776-0.26574776 | disen 0.73572677-0.00000000 | temporal 0.67224705-0.00000000 | total 0.26574776\n",
      "Epoch 33, loss: 0.26574776-(best 0.26574776)\n",
      "\n",
      "Detailed Loss: recon 0.26630932-0.26630932 | disen 0.73790163-0.00000000 | temporal 0.67224431-0.00000000 | total 0.26630932\n",
      "Epoch 34, loss: 0.26630932-(best 0.26574776)\n",
      "\n",
      "Detailed Loss: recon 0.26182103-0.26182103 | disen 0.73850977-0.00000000 | temporal 0.67194390-0.00000000 | total 0.26182103\n",
      "Epoch 35, loss: 0.26182103-(best 0.26182103)\n",
      "\n",
      "Detailed Loss: recon 0.26154923-0.26154923 | disen 0.73640978-0.00000000 | temporal 0.67204541-0.00000000 | total 0.26154923\n",
      "Epoch 36, loss: 0.26154923-(best 0.26154923)\n",
      "\n",
      "Detailed Loss: recon 0.26023141-0.26023141 | disen 0.73625427-0.00000000 | temporal 0.67153043-0.00000000 | total 0.26023141\n",
      "Epoch 37, loss: 0.26023141-(best 0.26023141)\n",
      "\n",
      "Detailed Loss: recon 0.26214856-0.26214856 | disen 0.73428965-0.00000000 | temporal 0.67148685-0.00000000 | total 0.26214856\n",
      "Epoch 38, loss: 0.26214856-(best 0.26023141)\n",
      "\n",
      "Detailed Loss: recon 0.25995380-0.25995380 | disen 0.73480153-0.00000000 | temporal 0.67135972-0.00000000 | total 0.25995380\n",
      "Epoch 39, loss: 0.25995380-(best 0.25995380)\n",
      "\n",
      "Detailed Loss: recon 0.26051274-0.26051274 | disen 0.73332083-0.00000000 | temporal 0.67144972-0.00000000 | total 0.26051274\n",
      "Epoch 40, loss: 0.26051274-(best 0.25995380)\n",
      "\n",
      "Detailed Loss: recon 0.25758699-0.25758699 | disen 0.73279458-0.00000000 | temporal 0.67163318-0.00000000 | total 0.25758699\n",
      "Epoch 41, loss: 0.25758699-(best 0.25758699)\n",
      "\n",
      "Detailed Loss: recon 0.25436002-0.25436002 | disen 0.73312801-0.00000000 | temporal 0.67144370-0.00000000 | total 0.25436002\n",
      "Epoch 42, loss: 0.25436002-(best 0.25436002)\n",
      "\n",
      "Detailed Loss: recon 0.25571838-0.25571838 | disen 0.73353839-0.00000000 | temporal 0.67149234-0.00000000 | total 0.25571838\n",
      "Epoch 43, loss: 0.25571838-(best 0.25436002)\n",
      "\n",
      "Detailed Loss: recon 0.25669998-0.25669998 | disen 0.73423916-0.00000000 | temporal 0.67141992-0.00000000 | total 0.25669998\n",
      "Epoch 44, loss: 0.25669998-(best 0.25436002)\n",
      "\n",
      "Detailed Loss: recon 0.25395328-0.25395328 | disen 0.73276860-0.00000000 | temporal 0.67166686-0.00000000 | total 0.25395328\n",
      "Epoch 45, loss: 0.25395328-(best 0.25395328)\n",
      "\n",
      "Detailed Loss: recon 0.25164965-0.25164965 | disen 0.73141706-0.00000000 | temporal 0.67150855-0.00000000 | total 0.25164965\n",
      "Epoch 46, loss: 0.25164965-(best 0.25164965)\n",
      "\n",
      "Detailed Loss: recon 0.25086421-0.25086421 | disen 0.73028499-0.00000000 | temporal 0.67181134-0.00000000 | total 0.25086421\n",
      "Epoch 47, loss: 0.25086421-(best 0.25086421)\n",
      "\n",
      "Detailed Loss: recon 0.24987178-0.24987178 | disen 0.73370147-0.00000000 | temporal 0.67164099-0.00000000 | total 0.24987178\n",
      "Epoch 48, loss: 0.24987178-(best 0.24987178)\n",
      "\n",
      "Detailed Loss: recon 0.25120842-0.25120842 | disen 0.73139775-0.00000000 | temporal 0.67114878-0.00000000 | total 0.25120842\n",
      "Epoch 49, loss: 0.25120842-(best 0.24987178)\n",
      "\n",
      "Detailed Loss: recon 0.24906266-0.24906266 | disen 0.73186541-0.00000000 | temporal 0.67155010-0.00000000 | total 0.24906266\n",
      "Epoch 50, loss: 0.24906266-(best 0.24906266)\n",
      "\n",
      "Detailed Loss: recon 0.24973643-0.24973643 | disen 0.73240918-0.00000000 | temporal 0.67178851-0.00000000 | total 0.24973643\n",
      "Epoch 51, loss: 0.24973643-(best 0.24906266)\n",
      "\n",
      "Detailed Loss: recon 0.24603844-0.24603844 | disen 0.72913373-0.00000000 | temporal 0.67187732-0.00000000 | total 0.24603844\n",
      "Epoch 52, loss: 0.24603844-(best 0.24603844)\n",
      "\n",
      "Detailed Loss: recon 0.24551260-0.24551260 | disen 0.73056114-0.00000000 | temporal 0.67175293-0.00000000 | total 0.24551260\n",
      "Epoch 53, loss: 0.24551260-(best 0.24551260)\n",
      "\n",
      "Detailed Loss: recon 0.24867374-0.24867374 | disen 0.73039341-0.00000000 | temporal 0.67175853-0.00000000 | total 0.24867374\n",
      "Epoch 54, loss: 0.24867374-(best 0.24551260)\n",
      "\n",
      "Detailed Loss: recon 0.24881706-0.24881706 | disen 0.72791040-0.00000000 | temporal 0.67178291-0.00000000 | total 0.24881706\n",
      "Epoch 55, loss: 0.24881706-(best 0.24551260)\n",
      "\n",
      "Detailed Loss: recon 0.24904603-0.24904603 | disen 0.72779918-0.00000000 | temporal 0.67213142-0.00000000 | total 0.24904603\n",
      "Epoch 56, loss: 0.24904603-(best 0.24551260)\n",
      "\n",
      "Detailed Loss: recon 0.24766681-0.24766681 | disen 0.72946429-0.00000000 | temporal 0.67218202-0.00000000 | total 0.24766681\n",
      "Epoch 57, loss: 0.24766681-(best 0.24551260)\n",
      "\n",
      "Detailed Loss: recon 0.24581006-0.24581006 | disen 0.73175704-0.00000000 | temporal 0.67190069-0.00000000 | total 0.24581006\n",
      "Epoch 58, loss: 0.24581006-(best 0.24551260)\n",
      "\n",
      "Detailed Loss: recon 0.24504368-0.24504368 | disen 0.72969890-0.00000000 | temporal 0.67164528-0.00000000 | total 0.24504368\n",
      "Epoch 59, loss: 0.24504368-(best 0.24504368)\n",
      "\n",
      "Detailed Loss: recon 0.24408075-0.24408075 | disen 0.72807777-0.00000000 | temporal 0.67178762-0.00000000 | total 0.24408075\n",
      "Epoch 60, loss: 0.24408075-(best 0.24408075)\n",
      "\n",
      "Detailed Loss: recon 0.24368946-0.24368946 | disen 0.72802973-0.00000000 | temporal 0.67202127-0.00000000 | total 0.24368946\n",
      "Epoch 61, loss: 0.24368946-(best 0.24368946)\n",
      "\n",
      "Detailed Loss: recon 0.24426959-0.24426959 | disen 0.73020101-0.00000000 | temporal 0.67199624-0.00000000 | total 0.24426959\n",
      "Epoch 62, loss: 0.24426959-(best 0.24368946)\n",
      "\n",
      "Detailed Loss: recon 0.24250059-0.24250059 | disen 0.73093432-0.00000000 | temporal 0.67208982-0.00000000 | total 0.24250059\n",
      "Epoch 63, loss: 0.24250059-(best 0.24250059)\n",
      "\n",
      "Detailed Loss: recon 0.24436687-0.24436687 | disen 0.72907203-0.00000000 | temporal 0.67195457-0.00000000 | total 0.24436687\n",
      "Epoch 64, loss: 0.24436687-(best 0.24250059)\n",
      "\n",
      "Detailed Loss: recon 0.24259697-0.24259697 | disen 0.72959268-0.00000000 | temporal 0.67231047-0.00000000 | total 0.24259697\n",
      "Epoch 65, loss: 0.24259697-(best 0.24250059)\n",
      "\n",
      "Detailed Loss: recon 0.24101228-0.24101228 | disen 0.72816706-0.00000000 | temporal 0.67224067-0.00000000 | total 0.24101228\n",
      "Epoch 66, loss: 0.24101228-(best 0.24101228)\n",
      "\n",
      "Detailed Loss: recon 0.23940913-0.23940913 | disen 0.72997820-0.00000000 | temporal 0.67193842-0.00000000 | total 0.23940913\n",
      "Epoch 67, loss: 0.23940913-(best 0.23940913)\n",
      "\n",
      "Detailed Loss: recon 0.24071166-0.24071166 | disen 0.72702610-0.00000000 | temporal 0.67215395-0.00000000 | total 0.24071166\n",
      "Epoch 68, loss: 0.24071166-(best 0.23940913)\n",
      "\n",
      "Detailed Loss: recon 0.24081151-0.24081151 | disen 0.72664481-0.00000000 | temporal 0.67232591-0.00000000 | total 0.24081151\n",
      "Epoch 69, loss: 0.24081151-(best 0.23940913)\n",
      "\n",
      "Detailed Loss: recon 0.23994112-0.23994112 | disen 0.72830766-0.00000000 | temporal 0.67255491-0.00000000 | total 0.23994112\n",
      "Epoch 70, loss: 0.23994112-(best 0.23940913)\n",
      "\n",
      "Detailed Loss: recon 0.24328260-0.24328260 | disen 0.72551298-0.00000000 | temporal 0.67219275-0.00000000 | total 0.24328260\n",
      "Epoch 71, loss: 0.24328260-(best 0.23940913)\n",
      "\n",
      "Detailed Loss: recon 0.24065672-0.24065672 | disen 0.72634113-0.00000000 | temporal 0.67204255-0.00000000 | total 0.24065672\n",
      "Epoch 72, loss: 0.24065672-(best 0.23940913)\n",
      "\n",
      "Detailed Loss: recon 0.24072836-0.24072836 | disen 0.72947443-0.00000000 | temporal 0.67269146-0.00000000 | total 0.24072836\n",
      "Epoch 73, loss: 0.24072836-(best 0.23940913)\n",
      "\n",
      "Detailed Loss: recon 0.23821108-0.23821108 | disen 0.72783673-0.00000000 | temporal 0.67258263-0.00000000 | total 0.23821108\n",
      "Epoch 74, loss: 0.23821108-(best 0.23821108)\n",
      "\n",
      "Detailed Loss: recon 0.24089740-0.24089740 | disen 0.72449255-0.00000000 | temporal 0.67265862-0.00000000 | total 0.24089740\n",
      "Epoch 75, loss: 0.24089740-(best 0.23821108)\n",
      "\n",
      "Detailed Loss: recon 0.24068084-0.24068084 | disen 0.72696853-0.00000000 | temporal 0.67310309-0.00000000 | total 0.24068084\n",
      "Epoch 76, loss: 0.24068084-(best 0.23821108)\n",
      "\n",
      "Detailed Loss: recon 0.23938523-0.23938523 | disen 0.72280872-0.00000000 | temporal 0.67309195-0.00000000 | total 0.23938523\n",
      "Epoch 77, loss: 0.23938523-(best 0.23821108)\n",
      "\n",
      "Detailed Loss: recon 0.24298285-0.24298285 | disen 0.72574139-0.00000000 | temporal 0.67263228-0.00000000 | total 0.24298285\n",
      "Epoch 78, loss: 0.24298285-(best 0.23821108)\n",
      "\n",
      "Detailed Loss: recon 0.24036616-0.24036616 | disen 0.72922611-0.00000000 | temporal 0.67277718-0.00000000 | total 0.24036616\n",
      "Epoch 79, loss: 0.24036616-(best 0.23821108)\n",
      "\n",
      "Detailed Loss: recon 0.23864077-0.23864077 | disen 0.72732681-0.00000000 | temporal 0.67292643-0.00000000 | total 0.23864077\n",
      "Epoch 80, loss: 0.23864077-(best 0.23821108)\n",
      "\n",
      "Detailed Loss: recon 0.23896644-0.23896644 | disen 0.72870183-0.00000000 | temporal 0.67257804-0.00000000 | total 0.23896644\n",
      "Epoch 81, loss: 0.23896644-(best 0.23821108)\n",
      "\n",
      "Detailed Loss: recon 0.24036853-0.24036853 | disen 0.72580445-0.00000000 | temporal 0.67251015-0.00000000 | total 0.24036853\n",
      "Epoch 82, loss: 0.24036853-(best 0.23821108)\n",
      "\n",
      "Detailed Loss: recon 0.23825806-0.23825806 | disen 0.72651923-0.00000000 | temporal 0.67270046-0.00000000 | total 0.23825806\n",
      "Epoch 83, loss: 0.23825806-(best 0.23821108)\n",
      "\n",
      "Detailed Loss: recon 0.23909175-0.23909175 | disen 0.72511339-0.00000000 | temporal 0.67269182-0.00000000 | total 0.23909175\n",
      "Epoch 84, loss: 0.23909175-(best 0.23821108)\n",
      "\n",
      "Detailed Loss: recon 0.23834139-0.23834139 | disen 0.72655630-0.00000000 | temporal 0.67318863-0.00000000 | total 0.23834139\n",
      "Epoch 85, loss: 0.23834139-(best 0.23821108)\n",
      "\n",
      "Detailed Loss: recon 0.23842728-0.23842728 | disen 0.72604799-0.00000000 | temporal 0.67291552-0.00000000 | total 0.23842728\n",
      "Epoch 86, loss: 0.23842728-(best 0.23821108)\n",
      "\n",
      "Detailed Loss: recon 0.23607308-0.23607308 | disen 0.72619224-0.00000000 | temporal 0.67260206-0.00000000 | total 0.23607308\n",
      "Epoch 87, loss: 0.23607308-(best 0.23607308)\n",
      "\n",
      "Detailed Loss: recon 0.23523210-0.23523210 | disen 0.72541004-0.00000000 | temporal 0.67284048-0.00000000 | total 0.23523210\n",
      "Epoch 88, loss: 0.23523210-(best 0.23523210)\n",
      "\n",
      "Detailed Loss: recon 0.23594336-0.23594336 | disen 0.72619689-0.00000000 | temporal 0.67307717-0.00000000 | total 0.23594336\n",
      "Epoch 89, loss: 0.23594336-(best 0.23523210)\n",
      "\n",
      "Detailed Loss: recon 0.23440634-0.23440634 | disen 0.72493565-0.00000000 | temporal 0.67316794-0.00000000 | total 0.23440634\n",
      "Epoch 90, loss: 0.23440634-(best 0.23440634)\n",
      "\n",
      "Detailed Loss: recon 0.23594616-0.23594616 | disen 0.72510582-0.00000000 | temporal 0.67271471-0.00000000 | total 0.23594616\n",
      "Epoch 91, loss: 0.23594616-(best 0.23440634)\n",
      "\n",
      "Detailed Loss: recon 0.23592345-0.23592345 | disen 0.72383475-0.00000000 | temporal 0.67328268-0.00000000 | total 0.23592345\n",
      "Epoch 92, loss: 0.23592345-(best 0.23440634)\n",
      "\n",
      "Detailed Loss: recon 0.23509108-0.23509108 | disen 0.72602475-0.00000000 | temporal 0.67319781-0.00000000 | total 0.23509108\n",
      "Epoch 93, loss: 0.23509108-(best 0.23440634)\n",
      "\n",
      "Detailed Loss: recon 0.23378544-0.23378544 | disen 0.72779733-0.00000000 | temporal 0.67276198-0.00000000 | total 0.23378544\n",
      "Epoch 94, loss: 0.23378544-(best 0.23378544)\n",
      "\n",
      "Detailed Loss: recon 0.23372871-0.23372871 | disen 0.72553539-0.00000000 | temporal 0.67285228-0.00000000 | total 0.23372871\n",
      "Epoch 95, loss: 0.23372871-(best 0.23372871)\n",
      "\n",
      "Detailed Loss: recon 0.23457567-0.23457567 | disen 0.72371674-0.00000000 | temporal 0.67294163-0.00000000 | total 0.23457567\n",
      "Epoch 96, loss: 0.23457567-(best 0.23372871)\n",
      "\n",
      "Detailed Loss: recon 0.23404650-0.23404650 | disen 0.72552556-0.00000000 | temporal 0.67334276-0.00000000 | total 0.23404650\n",
      "Epoch 97, loss: 0.23404650-(best 0.23372871)\n",
      "\n",
      "Detailed Loss: recon 0.23490795-0.23490795 | disen 0.72474515-0.00000000 | temporal 0.67284566-0.00000000 | total 0.23490795\n",
      "Epoch 98, loss: 0.23490795-(best 0.23372871)\n",
      "\n",
      "Detailed Loss: recon 0.23111480-0.23111480 | disen 0.72523832-0.00000000 | temporal 0.67261493-0.00000000 | total 0.23111480\n",
      "Epoch 99, loss: 0.23111480-(best 0.23111480)\n",
      "\n",
      "Detailed Loss: recon 0.23364833-0.23364833 | disen 0.72411644-0.00000000 | temporal 0.67361277-0.00000000 | total 0.23364833\n",
      "Epoch 100, loss: 0.23364833-(best 0.23111480)\n",
      "\n",
      ">> Reached final epoch. Loading best model...\n",
      "TRAINING XGB PREDICTOR\n",
      ">> Ending training!\n",
      "\n",
      "-----------------------\n",
      "TIME COST: 6.611057281494141 s\n",
      "Best Val: REC 62.20 PRE 43.44 MF1 67.06 AUC 79.75 TP 311 FP 405 TN 1447 FN 189\n",
      "-----------------------\n",
      "\n",
      "ASSIGNED GRAPH FOR EVALUATION: 11758 {1: 2566, 0: 9192} Nodes, 1049758 Edges\n",
      "\n",
      "-------------------\n",
      "STARTING EVALUATION\n",
      "-------------------\n",
      "\n",
      "Dataset - Overall: REC 63.06 PRE 45.76 MF1 68.29 AUC 81.30 TP 1618 FP 1918 TN 7274 FN 948 | 11758 {1: 2566, 0: 9192}\n",
      "Dataset - Train: REC 69.03 PRE 50.24 MF1 72.03 AUC 85.38 TP 517 FP 512 TN 2266 FN 232 | 3527 {0: 2778, 1: 749}\n",
      "Dataset - Val: REC 62.00 PRE 42.76 MF1 66.61 AUC 80.04 TP 310 FP 415 TN 1437 FN 190 | 2352 {1: 500, 0: 1852}\n",
      "Dataset - Test: REC 60.06 PRE 44.39 MF1 66.76 AUC 79.39 TP 791 FP 991 TN 3571 FN 526 | 5879 {0: 4562, 1: 1317}\n",
      "PREDICTION STATUS - {'1-True': 2040, '0-True': 5621, '0-False': 3571, '1-False': 526}\n",
      "    >> 526 positive nodes left unpredicted...\n",
      "    >> 526 OG positive nodes left unpredicted...\n",
      "Selecting data as budgeted ground truth for next round...\n",
      "  > Starting round 1...\n",
      "\n",
      "+++++++++++++++++\n",
      "STARTING ROUND 1!\n",
      "+++++++++++++++++\n",
      "        \n",
      "ASSIGNED GRAPH FOR TRAINING: 11758 {1: 2566, 0: 9192} Nodes, 1049758 Edges\n",
      "\n",
      ">> Splitting to train-validation set...\n",
      "Alotting train-val-test split for round 1\n",
      "Initial pool: 5879, Prediction pool: 3536, Budget pool: 0, Full pool: 7661\n",
      "Full graph size: 11758, Training: 4596, Val: 3065, Test: 11758\n",
      "    >> INITIAL DATA SPLIT: 4596 train rows ({1: 1224, 0: 3372}) | 3065 val rows ({1: 816, 0: 2249}) | 4097 test rows ({0: 3571, 1: 526})\n",
      "    >> AUGMENTED DATA SPLIT: 4596 train rows ({1: 1224, 0: 3372}) | 3065 val rows ({1: 816, 0: 2249}) | 4097 test rows ({0: 3571, 1: 526})\n",
      "    >> Updated cross-entropy weight to 2.7549019607843137...\n",
      "\n",
      "-----------------------\n",
      "STARTING MODEL TRAININGps_l\n",
      "-----------------------\n",
      "\n",
      "TRAINING NODE EMBEDDER\n",
      "Detailed Loss: recon 0.20689899-0.20689899 | disen 0.58709478-0.00000000 | temporal 0.56798977-0.00000000 | total 0.20689899\n",
      "Epoch 1, loss: 0.20689899-(best 0.20689899)\n",
      "\n",
      "Detailed Loss: recon 0.40033329-0.40033329 | disen 0.58586091-0.00000000 | temporal 0.56603712-0.00000000 | total 0.40033329\n",
      "Epoch 2, loss: 0.40033329-(best 0.20689899)\n",
      "\n",
      "Detailed Loss: recon 0.24450007-0.24450007 | disen 0.58830810-0.00000000 | temporal 0.56792778-0.00000000 | total 0.24450007\n",
      "Epoch 3, loss: 0.24450007-(best 0.20689899)\n",
      "\n",
      "Detailed Loss: recon 0.23617992-0.23617992 | disen 0.58948183-0.00000000 | temporal 0.56912994-0.00000000 | total 0.23617992\n",
      "Epoch 4, loss: 0.23617992-(best 0.20689899)\n",
      "\n",
      "Detailed Loss: recon 0.28999293-0.28999293 | disen 0.58949983-0.00000000 | temporal 0.56958580-0.00000000 | total 0.28999293\n",
      "Epoch 5, loss: 0.28999293-(best 0.20689899)\n",
      "\n",
      "Detailed Loss: recon 0.26728863-0.26728863 | disen 0.59050095-0.00000000 | temporal 0.56952292-0.00000000 | total 0.26728863\n",
      "Epoch 6, loss: 0.26728863-(best 0.20689899)\n",
      "\n",
      "Detailed Loss: recon 0.24357358-0.24357358 | disen 0.59090674-0.00000000 | temporal 0.56925106-0.00000000 | total 0.24357358\n",
      "Epoch 7, loss: 0.24357358-(best 0.20689899)\n",
      "\n",
      "Detailed Loss: recon 0.22835085-0.22835085 | disen 0.59146976-0.00000000 | temporal 0.56894493-0.00000000 | total 0.22835085\n",
      "Epoch 8, loss: 0.22835085-(best 0.20689899)\n",
      "\n",
      "Detailed Loss: recon 0.22392569-0.22392569 | disen 0.59121680-0.00000000 | temporal 0.56858611-0.00000000 | total 0.22392569\n",
      "Epoch 9, loss: 0.22392569-(best 0.20689899)\n",
      "\n",
      "Detailed Loss: recon 0.23049057-0.23049057 | disen 0.59170753-0.00000000 | temporal 0.56828445-0.00000000 | total 0.23049057\n",
      "Epoch 10, loss: 0.23049057-(best 0.20689899)\n",
      "\n",
      "Detailed Loss: recon 0.23939055-0.23939055 | disen 0.59154117-0.00000000 | temporal 0.56810385-0.00000000 | total 0.23939055\n",
      "Epoch 11, loss: 0.23939055-(best 0.20689899)\n",
      "\n",
      "Detailed Loss: recon 0.24197122-0.24197122 | disen 0.59191442-0.00000000 | temporal 0.56806171-0.00000000 | total 0.24197122\n",
      "Epoch 12, loss: 0.24197122-(best 0.20689899)\n",
      "\n",
      "Detailed Loss: recon 0.24105142-0.24105142 | disen 0.59115207-0.00000000 | temporal 0.56809694-0.00000000 | total 0.24105142\n",
      "Epoch 13, loss: 0.24105142-(best 0.20689899)\n",
      "\n",
      "Detailed Loss: recon 0.23377860-0.23377860 | disen 0.59214795-0.00000000 | temporal 0.56828558-0.00000000 | total 0.23377860\n",
      "Epoch 14, loss: 0.23377860-(best 0.20689899)\n",
      "\n",
      "Detailed Loss: recon 0.22835568-0.22835568 | disen 0.59147930-0.00000000 | temporal 0.56848592-0.00000000 | total 0.22835568\n",
      "Epoch 15, loss: 0.22835568-(best 0.20689899)\n",
      "\n",
      "Detailed Loss: recon 0.22688846-0.22688846 | disen 0.59172446-0.00000000 | temporal 0.56867385-0.00000000 | total 0.22688846\n",
      "Epoch 16, loss: 0.22688846-(best 0.20689899)\n",
      "\n",
      "Detailed Loss: recon 0.22414272-0.22414272 | disen 0.59200788-0.00000000 | temporal 0.56881559-0.00000000 | total 0.22414272\n",
      "Epoch 17, loss: 0.22414272-(best 0.20689899)\n",
      "\n",
      "Detailed Loss: recon 0.22757146-0.22757146 | disen 0.59198296-0.00000000 | temporal 0.56892747-0.00000000 | total 0.22757146\n",
      "Epoch 18, loss: 0.22757146-(best 0.20689899)\n",
      "\n",
      "Detailed Loss: recon 0.23007411-0.23007411 | disen 0.59207964-0.00000000 | temporal 0.56893748-0.00000000 | total 0.23007411\n",
      "Epoch 19, loss: 0.23007411-(best 0.20689899)\n",
      "\n",
      "Detailed Loss: recon 0.22779565-0.22779565 | disen 0.59193599-0.00000000 | temporal 0.56889260-0.00000000 | total 0.22779565\n",
      "Epoch 20, loss: 0.22779565-(best 0.20689899)\n",
      "\n",
      "Detailed Loss: recon 0.22760503-0.22760503 | disen 0.59180903-0.00000000 | temporal 0.56881708-0.00000000 | total 0.22760503\n",
      "Epoch 21, loss: 0.22760503-(best 0.20689899)\n",
      "\n",
      "Detailed Loss: recon 0.22255556-0.22255556 | disen 0.59252763-0.00000000 | temporal 0.56873691-0.00000000 | total 0.22255556\n",
      "Epoch 22, loss: 0.22255556-(best 0.20689899)\n",
      "\n",
      "Detailed Loss: recon 0.21672842-0.21672842 | disen 0.59138399-0.00000000 | temporal 0.56861061-0.00000000 | total 0.21672842\n",
      "Epoch 23, loss: 0.21672842-(best 0.20689899)\n",
      "\n",
      "Detailed Loss: recon 0.21964946-0.21964946 | disen 0.59171563-0.00000000 | temporal 0.56852633-0.00000000 | total 0.21964946\n",
      "Epoch 24, loss: 0.21964946-(best 0.20689899)\n",
      "\n",
      "Detailed Loss: recon 0.21955107-0.21955107 | disen 0.59209400-0.00000000 | temporal 0.56846374-0.00000000 | total 0.21955107\n",
      "Epoch 25, loss: 0.21955107-(best 0.20689899)\n",
      "\n",
      "Detailed Loss: recon 0.21810822-0.21810822 | disen 0.59158421-0.00000000 | temporal 0.56843936-0.00000000 | total 0.21810822\n",
      "Epoch 26, loss: 0.21810822-(best 0.20689899)\n",
      "\n",
      "Detailed Loss: recon 0.21798310-0.21798310 | disen 0.59197992-0.00000000 | temporal 0.56848276-0.00000000 | total 0.21798310\n",
      "Epoch 27, loss: 0.21798310-(best 0.20689899)\n",
      "\n",
      "Detailed Loss: recon 0.21607989-0.21607989 | disen 0.59227556-0.00000000 | temporal 0.56852210-0.00000000 | total 0.21607989\n",
      "Epoch 28, loss: 0.21607989-(best 0.20689899)\n",
      "\n",
      "Detailed Loss: recon 0.21495315-0.21495315 | disen 0.59176564-0.00000000 | temporal 0.56861317-0.00000000 | total 0.21495315\n",
      "Epoch 29, loss: 0.21495315-(best 0.20689899)\n",
      "\n",
      "Detailed Loss: recon 0.21394061-0.21394061 | disen 0.59177589-0.00000000 | temporal 0.56868821-0.00000000 | total 0.21394061\n",
      "Epoch 30, loss: 0.21394061-(best 0.20689899)\n",
      "\n",
      "Detailed Loss: recon 0.21660964-0.21660964 | disen 0.59175318-0.00000000 | temporal 0.56873930-0.00000000 | total 0.21660964\n",
      "Epoch 31, loss: 0.21660964-(best 0.20689899)\n",
      "\n",
      "Detailed Loss: recon 0.21435058-0.21435058 | disen 0.59132385-0.00000000 | temporal 0.56875175-0.00000000 | total 0.21435058\n",
      "Epoch 32, loss: 0.21435058-(best 0.20689899)\n",
      "\n",
      "Detailed Loss: recon 0.21275640-0.21275640 | disen 0.59074175-0.00000000 | temporal 0.56872827-0.00000000 | total 0.21275640\n",
      "Epoch 33, loss: 0.21275640-(best 0.20689899)\n",
      "\n",
      "Detailed Loss: recon 0.21050909-0.21050909 | disen 0.59039891-0.00000000 | temporal 0.56867820-0.00000000 | total 0.21050909\n",
      "Epoch 34, loss: 0.21050909-(best 0.20689899)\n",
      "\n",
      "Detailed Loss: recon 0.21047261-0.21047261 | disen 0.59074241-0.00000000 | temporal 0.56864411-0.00000000 | total 0.21047261\n",
      "Epoch 35, loss: 0.21047261-(best 0.20689899)\n",
      "\n",
      "Detailed Loss: recon 0.21008401-0.21008401 | disen 0.59082425-0.00000000 | temporal 0.56858611-0.00000000 | total 0.21008401\n",
      "Epoch 36, loss: 0.21008401-(best 0.20689899)\n",
      "\n",
      "Detailed Loss: recon 0.20957303-0.20957303 | disen 0.59023511-0.00000000 | temporal 0.56857538-0.00000000 | total 0.20957303\n",
      "Epoch 37, loss: 0.20957303-(best 0.20689899)\n",
      "\n",
      "Detailed Loss: recon 0.20728889-0.20728889 | disen 0.59002888-0.00000000 | temporal 0.56863034-0.00000000 | total 0.20728889\n",
      "Epoch 38, loss: 0.20728889-(best 0.20689899)\n",
      "\n",
      "Detailed Loss: recon 0.20697647-0.20697647 | disen 0.58937585-0.00000000 | temporal 0.56869793-0.00000000 | total 0.20697647\n",
      "Epoch 39, loss: 0.20697647-(best 0.20689899)\n",
      "\n",
      "Detailed Loss: recon 0.20792848-0.20792848 | disen 0.58977950-0.00000000 | temporal 0.56871665-0.00000000 | total 0.20792848\n",
      "Epoch 40, loss: 0.20792848-(best 0.20689899)\n",
      "\n",
      "Detailed Loss: recon 0.20741421-0.20741421 | disen 0.59007859-0.00000000 | temporal 0.56875086-0.00000000 | total 0.20741421\n",
      "Epoch 41, loss: 0.20741421-(best 0.20689899)\n",
      "\n",
      "Detailed Loss: recon 0.20709144-0.20709144 | disen 0.58981168-0.00000000 | temporal 0.56876940-0.00000000 | total 0.20709144\n",
      "Epoch 42, loss: 0.20709144-(best 0.20689899)\n",
      "\n",
      "Detailed Loss: recon 0.20531341-0.20531341 | disen 0.58983326-0.00000000 | temporal 0.56872058-0.00000000 | total 0.20531341\n",
      "Epoch 43, loss: 0.20531341-(best 0.20531341)\n",
      "\n",
      "Detailed Loss: recon 0.20466053-0.20466053 | disen 0.58943361-0.00000000 | temporal 0.56866878-0.00000000 | total 0.20466053\n",
      "Epoch 44, loss: 0.20466053-(best 0.20466053)\n",
      "\n",
      "Detailed Loss: recon 0.20642172-0.20642172 | disen 0.58923531-0.00000000 | temporal 0.56865197-0.00000000 | total 0.20642172\n",
      "Epoch 45, loss: 0.20642172-(best 0.20466053)\n",
      "\n",
      "Detailed Loss: recon 0.20460686-0.20460686 | disen 0.58949322-0.00000000 | temporal 0.56858659-0.00000000 | total 0.20460686\n",
      "Epoch 46, loss: 0.20460686-(best 0.20460686)\n",
      "\n",
      "Detailed Loss: recon 0.20473950-0.20473950 | disen 0.58989799-0.00000000 | temporal 0.56855881-0.00000000 | total 0.20473950\n",
      "Epoch 47, loss: 0.20473950-(best 0.20460686)\n",
      "\n",
      "Detailed Loss: recon 0.20460062-0.20460062 | disen 0.58927333-0.00000000 | temporal 0.56852525-0.00000000 | total 0.20460062\n",
      "Epoch 48, loss: 0.20460062-(best 0.20460062)\n",
      "\n",
      "Detailed Loss: recon 0.20481366-0.20481366 | disen 0.58925867-0.00000000 | temporal 0.56850392-0.00000000 | total 0.20481366\n",
      "Epoch 49, loss: 0.20481366-(best 0.20460062)\n",
      "\n",
      "Detailed Loss: recon 0.20430624-0.20430624 | disen 0.58914781-0.00000000 | temporal 0.56842953-0.00000000 | total 0.20430624\n",
      "Epoch 50, loss: 0.20430624-(best 0.20430624)\n",
      "\n",
      ">> Reached final epoch. Loading best model...\n",
      "TRAINING XGB PREDICTOR\n",
      ">> Ending training!\n",
      "\n",
      "-----------------------\n",
      "TIME COST: 6.70015025138855 s\n",
      "Best Val: REC 52.57 PRE 52.96 MF1 67.86 AUC 77.90 TP 429 FP 381 TN 1868 FN 387\n",
      "-----------------------\n",
      "Generating additional positive data from adversary...\n",
      "Generating additional negative data by duplicating random nodes...\n",
      "\n",
      "ASSIGNED GRAPH FOR EVALUATION: 12345 {1: 2694, 0: 9651} Nodes, 1164318 Edges\n",
      "\n",
      "-------------------\n",
      "STARTING EVALUATION\n",
      "-------------------\n",
      "\n",
      "Dataset - Overall: REC 58.69 PRE 50.85 MF1 70.25 AUC 81.75 TP 1581 FP 1528 TN 8123 FN 1113 | 12345 {1: 2694, 0: 9651}\n",
      "Dataset - Train: REC 75.08 PRE 61.47 MF1 76.99 AUC 87.10 TP 919 FP 576 TN 2796 FN 305 | 4596 {1: 1224, 0: 3372}\n",
      "Dataset - Val: REC 55.02 PRE 47.77 MF1 65.76 AUC 76.57 TP 449 FP 491 TN 1758 FN 367 | 3065 {1: 816, 0: 2249}\n",
      "Dataset - Test: REC 32.57 PRE 31.60 MF1 60.43 AUC 76.22 TP 213 FP 461 TN 3569 FN 441 | 4684 {0: 4030, 1: 654}\n",
      "PREDICTION STATUS - {'1-True': 2253, '0-True': 6082, '0-False': 3569, '1-False': 441}\n",
      "    >> 441 positive nodes left unpredicted...\n",
      "    >> 355 OG positive nodes left unpredicted...\n",
      "-----------------------\n",
      "PREDICTION RESULT - ROUNDS\n",
      "Dataset - Round 0: REC 55.35 PRE 50.07 MF1 68.97 AUC 80.41 TP 729 FP 727 TN 3835 FN 588 | 5879 {0: 4562, 1: 1317}\n",
      "Dataset - Round 1: REC 32.81 PRE 33.87 MF1 57.56 AUC 67.81 TP 42 FP 82 TN 377 FN 86 | 587 {1: 128, 0: 459}\n",
      "-----------------------\n",
      "PREDICTION RESULT - SEEDS\n",
      "Seeds - Current: REC 37.50 PRE 36.64 MF1 59.63 AUC 70.45 TP 48 FP 83 TN 376 FN 80 | 587 {1: 128, 0: 459}\n",
      "Seeds - Prev: REC 0.00 PRE 0.00 MF1 37.55 AUC 70.45 TP 0 FP 106 TN 353 FN 128 | 587 {1: 128, 0: 459}\n",
      "Selecting data as budgeted ground truth for next round...\n",
      "  > Starting round 2...\n",
      "\n",
      "+++++++++++++++++\n",
      "STARTING ROUND 2!\n",
      "+++++++++++++++++\n",
      "        \n",
      "ASSIGNED GRAPH FOR TRAINING: 12345 {1: 2694, 0: 9651} Nodes, 1164318 Edges\n",
      "\n",
      ">> Splitting to train-validation set...\n",
      "Alotting train-val-test split for round 2\n",
      "Initial pool: 5879, Prediction pool: 6645, Budget pool: 0, Full pool: 8335\n",
      "Full graph size: 12345, Training: 5001, Val: 3334, Test: 12345\n",
      "    >> INITIAL DATA SPLIT: 5001 train rows ({1: 1352, 0: 3649}) | 3334 val rows ({0: 2433, 1: 901}) | 4010 test rows ({0: 3569, 1: 441})\n",
      "    >> AUGMENTED DATA SPLIT: 5001 train rows ({1: 1352, 0: 3649}) | 3334 val rows ({0: 2433, 1: 901}) | 4010 test rows ({0: 3569, 1: 441})\n",
      "    >> Updated cross-entropy weight to 2.69896449704142...\n",
      "\n",
      "-----------------------\n",
      "STARTING MODEL TRAININGps_l\n",
      "-----------------------\n",
      "\n",
      "TRAINING NODE EMBEDDER\n",
      "Detailed Loss: recon 0.20963240-0.20963240 | disen 0.58866900-0.00000000 | temporal 0.58893454-0.00000000 | total 0.20963240\n",
      "Epoch 1, loss: 0.20963240-(best 0.20963240)\n",
      "\n",
      "Detailed Loss: recon 0.22553955-0.22553955 | disen 0.58976084-0.00000000 | temporal 0.58935821-0.00000000 | total 0.22553955\n",
      "Epoch 2, loss: 0.22553955-(best 0.20963240)\n",
      "\n",
      "Detailed Loss: recon 0.24331668-0.24331668 | disen 0.58961558-0.00000000 | temporal 0.59012467-0.00000000 | total 0.24331668\n",
      "Epoch 3, loss: 0.24331668-(best 0.20963240)\n",
      "\n",
      "Detailed Loss: recon 0.21394560-0.21394560 | disen 0.59010410-0.00000000 | temporal 0.58960670-0.00000000 | total 0.21394560\n",
      "Epoch 4, loss: 0.21394560-(best 0.20963240)\n",
      "\n",
      "Detailed Loss: recon 0.21944588-0.21944588 | disen 0.58968985-0.00000000 | temporal 0.58901626-0.00000000 | total 0.21944588\n",
      "Epoch 5, loss: 0.21944588-(best 0.20963240)\n",
      "\n",
      "Detailed Loss: recon 0.22261243-0.22261243 | disen 0.58929574-0.00000000 | temporal 0.58876908-0.00000000 | total 0.22261243\n",
      "Epoch 6, loss: 0.22261243-(best 0.20963240)\n",
      "\n",
      "Detailed Loss: recon 0.21324727-0.21324727 | disen 0.58889550-0.00000000 | temporal 0.58887315-0.00000000 | total 0.21324727\n",
      "Epoch 7, loss: 0.21324727-(best 0.20963240)\n",
      "\n",
      "Detailed Loss: recon 0.21146914-0.21146914 | disen 0.58866751-0.00000000 | temporal 0.58906782-0.00000000 | total 0.21146914\n",
      "Epoch 8, loss: 0.21146914-(best 0.20963240)\n",
      "\n",
      "Detailed Loss: recon 0.21548501-0.21548501 | disen 0.58895642-0.00000000 | temporal 0.58919919-0.00000000 | total 0.21548501\n",
      "Epoch 9, loss: 0.21548501-(best 0.20963240)\n",
      "\n",
      "Detailed Loss: recon 0.21688281-0.21688281 | disen 0.58898938-0.00000000 | temporal 0.58917665-0.00000000 | total 0.21688281\n",
      "Epoch 10, loss: 0.21688281-(best 0.20963240)\n",
      "\n",
      "Detailed Loss: recon 0.21004960-0.21004960 | disen 0.58792764-0.00000000 | temporal 0.58904284-0.00000000 | total 0.21004960\n",
      "Epoch 11, loss: 0.21004960-(best 0.20963240)\n",
      "\n",
      "Detailed Loss: recon 0.21016815-0.21016815 | disen 0.58791316-0.00000000 | temporal 0.58888626-0.00000000 | total 0.21016815\n",
      "Epoch 12, loss: 0.21016815-(best 0.20963240)\n",
      "\n",
      "Detailed Loss: recon 0.21473894-0.21473894 | disen 0.58796889-0.00000000 | temporal 0.58887625-0.00000000 | total 0.21473894\n",
      "Epoch 13, loss: 0.21473894-(best 0.20963240)\n",
      "\n",
      "Detailed Loss: recon 0.21195349-0.21195349 | disen 0.58784938-0.00000000 | temporal 0.58894843-0.00000000 | total 0.21195349\n",
      "Epoch 14, loss: 0.21195349-(best 0.20963240)\n",
      "\n",
      "Detailed Loss: recon 0.20978759-0.20978759 | disen 0.58750749-0.00000000 | temporal 0.58914983-0.00000000 | total 0.20978759\n",
      "Epoch 15, loss: 0.20978759-(best 0.20963240)\n",
      "\n",
      "Detailed Loss: recon 0.20772660-0.20772660 | disen 0.58696598-0.00000000 | temporal 0.58931023-0.00000000 | total 0.20772660\n",
      "Epoch 16, loss: 0.20772660-(best 0.20772660)\n",
      "\n",
      "Detailed Loss: recon 0.21012971-0.21012971 | disen 0.58752900-0.00000000 | temporal 0.58936536-0.00000000 | total 0.21012971\n",
      "Epoch 17, loss: 0.21012971-(best 0.20772660)\n",
      "\n",
      "Detailed Loss: recon 0.20859098-0.20859098 | disen 0.58680522-0.00000000 | temporal 0.58938092-0.00000000 | total 0.20859098\n",
      "Epoch 18, loss: 0.20859098-(best 0.20772660)\n",
      "\n",
      "Detailed Loss: recon 0.20674434-0.20674434 | disen 0.58670932-0.00000000 | temporal 0.58929300-0.00000000 | total 0.20674434\n",
      "Epoch 19, loss: 0.20674434-(best 0.20674434)\n",
      "\n",
      "Detailed Loss: recon 0.20773391-0.20773391 | disen 0.58566713-0.00000000 | temporal 0.58920974-0.00000000 | total 0.20773391\n",
      "Epoch 20, loss: 0.20773391-(best 0.20674434)\n",
      "\n",
      "Detailed Loss: recon 0.20876944-0.20876944 | disen 0.58571804-0.00000000 | temporal 0.58917046-0.00000000 | total 0.20876944\n",
      "Epoch 21, loss: 0.20876944-(best 0.20674434)\n",
      "\n",
      "Detailed Loss: recon 0.20591782-0.20591782 | disen 0.58577412-0.00000000 | temporal 0.58920920-0.00000000 | total 0.20591782\n",
      "Epoch 22, loss: 0.20591782-(best 0.20591782)\n",
      "\n",
      "Detailed Loss: recon 0.20583266-0.20583266 | disen 0.58501416-0.00000000 | temporal 0.58934343-0.00000000 | total 0.20583266\n",
      "Epoch 23, loss: 0.20583266-(best 0.20583266)\n",
      "\n",
      "Detailed Loss: recon 0.20626783-0.20626783 | disen 0.58446002-0.00000000 | temporal 0.58944148-0.00000000 | total 0.20626783\n",
      "Epoch 24, loss: 0.20626783-(best 0.20583266)\n",
      "\n",
      "Detailed Loss: recon 0.20580862-0.20580862 | disen 0.58459318-0.00000000 | temporal 0.58946013-0.00000000 | total 0.20580862\n",
      "Epoch 25, loss: 0.20580862-(best 0.20580862)\n",
      "\n",
      "Detailed Loss: recon 0.20637083-0.20637083 | disen 0.58463085-0.00000000 | temporal 0.58938342-0.00000000 | total 0.20637083\n",
      "Epoch 26, loss: 0.20637083-(best 0.20580862)\n",
      "\n",
      "Detailed Loss: recon 0.20482951-0.20482951 | disen 0.58455110-0.00000000 | temporal 0.58930349-0.00000000 | total 0.20482951\n",
      "Epoch 27, loss: 0.20482951-(best 0.20482951)\n",
      "\n",
      "Detailed Loss: recon 0.20389596-0.20389596 | disen 0.58419001-0.00000000 | temporal 0.58918101-0.00000000 | total 0.20389596\n",
      "Epoch 28, loss: 0.20389596-(best 0.20389596)\n",
      "\n",
      "Detailed Loss: recon 0.20324586-0.20324586 | disen 0.58370543-0.00000000 | temporal 0.58928990-0.00000000 | total 0.20324586\n",
      "Epoch 29, loss: 0.20324586-(best 0.20324586)\n",
      "\n",
      "Detailed Loss: recon 0.20335065-0.20335065 | disen 0.58366710-0.00000000 | temporal 0.58933723-0.00000000 | total 0.20335065\n",
      "Epoch 30, loss: 0.20335065-(best 0.20324586)\n",
      "\n",
      "Detailed Loss: recon 0.20459703-0.20459703 | disen 0.58349502-0.00000000 | temporal 0.58943975-0.00000000 | total 0.20459703\n",
      "Epoch 31, loss: 0.20459703-(best 0.20324586)\n",
      "\n",
      "Detailed Loss: recon 0.20253408-0.20253408 | disen 0.58300471-0.00000000 | temporal 0.58945835-0.00000000 | total 0.20253408\n",
      "Epoch 32, loss: 0.20253408-(best 0.20253408)\n",
      "\n",
      "Detailed Loss: recon 0.20219007-0.20219007 | disen 0.58328122-0.00000000 | temporal 0.58940744-0.00000000 | total 0.20219007\n",
      "Epoch 33, loss: 0.20219007-(best 0.20219007)\n",
      "\n",
      "Detailed Loss: recon 0.20214498-0.20214498 | disen 0.58226055-0.00000000 | temporal 0.58939946-0.00000000 | total 0.20214498\n",
      "Epoch 34, loss: 0.20214498-(best 0.20214498)\n",
      "\n",
      "Detailed Loss: recon 0.20160130-0.20160130 | disen 0.58252585-0.00000000 | temporal 0.58943909-0.00000000 | total 0.20160130\n",
      "Epoch 35, loss: 0.20160130-(best 0.20160130)\n",
      "\n",
      "Detailed Loss: recon 0.20171420-0.20171420 | disen 0.58217239-0.00000000 | temporal 0.58949399-0.00000000 | total 0.20171420\n",
      "Epoch 36, loss: 0.20171420-(best 0.20160130)\n",
      "\n",
      "Detailed Loss: recon 0.20133457-0.20133457 | disen 0.58221054-0.00000000 | temporal 0.58951801-0.00000000 | total 0.20133457\n",
      "Epoch 37, loss: 0.20133457-(best 0.20133457)\n",
      "\n",
      "Detailed Loss: recon 0.20174295-0.20174295 | disen 0.58279848-0.00000000 | temporal 0.58955014-0.00000000 | total 0.20174295\n",
      "Epoch 38, loss: 0.20174295-(best 0.20133457)\n",
      "\n",
      "Detailed Loss: recon 0.20136862-0.20136862 | disen 0.58245409-0.00000000 | temporal 0.58954191-0.00000000 | total 0.20136862\n",
      "Epoch 39, loss: 0.20136862-(best 0.20133457)\n",
      "\n",
      "Detailed Loss: recon 0.20093334-0.20093334 | disen 0.58238596-0.00000000 | temporal 0.58955395-0.00000000 | total 0.20093334\n",
      "Epoch 40, loss: 0.20093334-(best 0.20093334)\n",
      "\n",
      "Detailed Loss: recon 0.20084791-0.20084791 | disen 0.58168113-0.00000000 | temporal 0.58959830-0.00000000 | total 0.20084791\n",
      "Epoch 41, loss: 0.20084791-(best 0.20084791)\n",
      "\n",
      "Detailed Loss: recon 0.19925499-0.19925499 | disen 0.58265096-0.00000000 | temporal 0.58968341-0.00000000 | total 0.19925499\n",
      "Epoch 42, loss: 0.19925499-(best 0.19925499)\n",
      "\n",
      "Detailed Loss: recon 0.20025736-0.20025736 | disen 0.58228666-0.00000000 | temporal 0.58966297-0.00000000 | total 0.20025736\n",
      "Epoch 43, loss: 0.20025736-(best 0.19925499)\n",
      "\n",
      "Detailed Loss: recon 0.19991523-0.19991523 | disen 0.58264351-0.00000000 | temporal 0.58970267-0.00000000 | total 0.19991523\n",
      "Epoch 44, loss: 0.19991523-(best 0.19925499)\n",
      "\n",
      "Detailed Loss: recon 0.19910857-0.19910857 | disen 0.58282626-0.00000000 | temporal 0.58969140-0.00000000 | total 0.19910857\n",
      "Epoch 45, loss: 0.19910857-(best 0.19910857)\n",
      "\n",
      "Detailed Loss: recon 0.19819218-0.19819218 | disen 0.58267927-0.00000000 | temporal 0.58976054-0.00000000 | total 0.19819218\n",
      "Epoch 46, loss: 0.19819218-(best 0.19819218)\n",
      "\n",
      "Detailed Loss: recon 0.19917963-0.19917963 | disen 0.58293802-0.00000000 | temporal 0.58979863-0.00000000 | total 0.19917963\n",
      "Epoch 47, loss: 0.19917963-(best 0.19819218)\n",
      "\n",
      "Detailed Loss: recon 0.19894123-0.19894123 | disen 0.58307040-0.00000000 | temporal 0.58979315-0.00000000 | total 0.19894123\n",
      "Epoch 48, loss: 0.19894123-(best 0.19819218)\n",
      "\n",
      "Detailed Loss: recon 0.19887935-0.19887935 | disen 0.58317983-0.00000000 | temporal 0.58978003-0.00000000 | total 0.19887935\n",
      "Epoch 49, loss: 0.19887935-(best 0.19819218)\n",
      "\n",
      "Detailed Loss: recon 0.19855528-0.19855528 | disen 0.58316070-0.00000000 | temporal 0.58976817-0.00000000 | total 0.19855528\n",
      "Epoch 50, loss: 0.19855528-(best 0.19819218)\n",
      "\n",
      ">> Reached final epoch. Loading best model...\n",
      "TRAINING XGB PREDICTOR\n",
      ">> Ending training!\n",
      "\n",
      "-----------------------\n",
      "TIME COST: 6.210601568222046 s\n",
      "Best Val: REC 53.72 PRE 49.29 MF1 66.14 AUC 75.60 TP 484 FP 498 TN 1935 FN 417\n",
      "-----------------------\n",
      "Generating additional positive data from adversary...\n",
      "Generating additional negative data by duplicating random nodes...\n",
      "\n",
      "ASSIGNED GRAPH FOR EVALUATION: 12932 {1: 2822, 0: 10110} Nodes, 1276070 Edges\n",
      "\n",
      "-------------------\n",
      "STARTING EVALUATION\n",
      "-------------------\n",
      "\n",
      "Dataset - Overall: REC 56.02 PRE 51.52 MF1 70.01 AUC 80.84 TP 1581 FP 1488 TN 8622 FN 1241 | 12932 {1: 2822, 0: 10110}\n",
      "Dataset - Train: REC 68.12 PRE 60.91 MF1 75.00 AUC 85.22 TP 921 FP 591 TN 3058 FN 431 | 5001 {1: 1352, 0: 3649}\n",
      "Dataset - Val: REC 56.38 PRE 48.71 MF1 66.31 AUC 75.27 TP 508 FP 535 TN 1898 FN 393 | 3334 {0: 2433, 1: 901}\n",
      "Dataset - Test: REC 26.71 PRE 29.57 MF1 59.23 AUC 74.68 TP 152 FP 362 TN 3666 FN 417 | 4597 {0: 4028, 1: 569}\n",
      "PREDICTION STATUS - {'1-True': 2405, '0-True': 6444, '0-False': 3666, '1-False': 417}\n",
      "    >> 417 positive nodes left unpredicted...\n",
      "    >> 255 OG positive nodes left unpredicted...\n",
      "-----------------------\n",
      "PREDICTION RESULT - ROUNDS\n",
      "Dataset - Round 0: REC 57.33 PRE 53.74 MF1 71.03 AUC 81.16 TP 755 FP 650 TN 3912 FN 562 | 5879 {0: 4562, 1: 1317}\n",
      "Dataset - Round 1: REC 35.16 PRE 45.00 MF1 62.44 AUC 72.14 TP 45 FP 55 TN 404 FN 83 | 587 {1: 128, 0: 459}\n",
      "Dataset - Round 2: REC 25.00 PRE 30.19 MF1 54.63 AUC 66.39 TP 32 FP 74 TN 385 FN 96 | 587 {1: 128, 0: 459}\n",
      "-----------------------\n",
      "PREDICTION RESULT - SEEDS\n",
      "Seeds - Current: REC 27.34 PRE 33.33 MF1 56.36 AUC 63.64 TP 35 FP 70 TN 389 FN 93 | 587 {1: 128, 0: 459}\n",
      "Seeds - Prev: REC 0.00 PRE 0.00 MF1 38.98 AUC 63.64 TP 0 FP 84 TN 375 FN 128 | 587 {1: 128, 0: 459}\n",
      "Selecting data as budgeted ground truth for next round...\n",
      "The file ../checkpoint/working_model_file_1730091655.7094383_epoch.py does not exist\n",
      "The file ../checkpoint/working_model_file_1730091655.7094383_round-M.json does not exist\n",
      "The file ../checkpoint/working_model_file_1730091655.7094383_round-T.json does not exist\n",
      "The file ../checkpoint/working_model_file_1730091655.7094383_round.pt does not exist\n",
      "Experiment ended, experienced 0 failures\n",
      "Elapsed experiment time 71.86771965s\n",
      "================\n",
      "++++++++++++++++\n",
      "TRIAL NUMBER 0\n",
      "++++++++++++++++\n",
      "================\n",
      "  > Rereading graph data...\n",
      "  > Initializing multiround object...\n",
      "11758 1049758\n",
      "\n",
      "=========================\n",
      "STARTING ADVER EXPERIMENT\n",
      "=========================\n",
      "MAIN CONFIG: [('device', 'cuda:0'), ('exp_type', 'ADVER'), ('full_oracle', False), ('node_expiration_time', 1), ('round_budget_neg', 0), ('round_budget_pos', 0), ('round_new_neg', 459), ('round_new_pos', 128), ('round_num', 3), ('save_df', True), ('save_embedding', False), ('save_temp_sne', False), ('setting_type', 'INDUCTIVE'), ('task_type', 'NODE'), ('verbose', 4)]\n",
      "MODEL CONFIG: [('act_name', 'LeakyReLU'), ('addon_internal_agg', 'OR'), ('addon_name', 'NONE'), ('addon_perc', 0.05), ('addon_round_window', 7), ('alpha', 0), ('att_heads', 2), ('beta', 1), ('boost_agg_backbone', <class 'models.benchmarks_supervised.booster.GIN_noparam'>), ('boost_metric', <function average_precision_score at 0x000001C8520A5760>), ('boost_predictor', <class 'xgboost.sklearn.XGBClassifier'>), ('device', 'cuda:0'), ('drop_rate', 0), ('dropout_rate', 0.1), ('early_stopping', 75), ('embed_type', 'temporal'), ('etypes', ['none']), ('gamma', 1), ('h_feats', 64), ('k', 1), ('lambda1', 1), ('lambda2', 1), ('lambda_', 1), ('loss_sample', True), ('loss_sample_ratio', 0.1), ('loss_type', 'ndist'), ('mlp_feats', 64), ('mlp_layers', 2), ('model_name', 'XGB-SP'), ('n_estimator', 500), ('norm_name', 'layer'), ('num_epoch', 100), ('num_layers', 2), ('num_round_epoch', 50), ('round_window', 7), ('temporal_agg', 'mean_final'), ('tloss_type', 'normal'), ('training_type', 'round'), ('verbose', 4)]\n",
      "STRAT CONFIG: [('augment_name', 'REAGE'), ('augment_neg_ratio', 0.09772070079945569), ('augment_pos_ratio', 0.02727929920054431), ('augment_prob', 0.5), ('augment_round_split', 8), ('device', 'cuda:0'), ('pseudo_name', 'NONE'), ('pseudo_new_pos_only', False), ('pseudo_skip_initial', False), ('verbose', 4)]\n",
      "TRAIN CONFIG: [('ce_weight', 2.69896449704142), ('device', 'cuda:0'), ('dset_name', 'tolokers_bid'), ('early_stopping', 75), ('learning_rate', 0.01), ('loss', <function cross_entropy at 0x000001C84AF6F880>), ('loss_gamma', 5), ('num_epoch', 100), ('num_round_epoch', 50), ('optimizer', <class 'torch.optim.adam.Adam'>), ('random_state', 7777777), ('ratio_initial', 0.5), ('ratio_test', 0.2), ('ratio_train', 0.6), ('ratio_val', 0.2), ('round_reset_model', False), ('round_train_list', []), ('stuck_stopping', 50), ('stuck_threshold', 0), ('train_max_retry', 10), ('verbose', 4)]\n",
      "ADVER CONFIG: [('adver_choose_name', 'GREEDY'), ('adver_conn_coef', 0.1), ('adver_feat_coef', 1), ('adver_mod_name', 'REPLAY'), ('device', 'cuda:0'), ('verbose', 4)]\n",
      "\n",
      "INITIAL GRAPH: 11758 {1: 2566, 0: 9192} Nodes, 1049758 Edges\n",
      "=========================\n",
      "        \n",
      ">> Initializing graph...\n",
      "MAIN GRAPH: 11758 {1: 2566, 0: 9192} Nodes, 1049758 Edges\n",
      "\n",
      ">> Initializing experiment type-specific modules:\n",
      "  >> Initializing adversarial modules...\n",
      ">> Initializing task-specific modules:\n",
      ">> Initializing classifier model...\n",
      ">> Initializing other strategies...\n",
      "\n",
      "FINISH INITIALIZATION\n",
      "=====================\n",
      "        \n",
      "  > Starting round 0...\n",
      "\n",
      "+++++++++++++++++\n",
      "STARTING ROUND 0!\n",
      "+++++++++++++++++\n",
      "        \n",
      "ASSIGNED GRAPH FOR TRAINING: 5879 {0: 4621, 1: 1258} Nodes, 257213 Edges\n",
      "\n",
      ">> Splitting to train-validation set...\n",
      "Alotting train-val-test split for round 0\n",
      "Initial pool: 0, Prediction pool: 0, Budget pool: 0, Full pool: 5879\n",
      "Full graph size: 5879, Training: 3527, Val: 2352, Test: 5879\n",
      "    >> INITIAL DATA SPLIT: 3527 train rows ({1: 755, 0: 2772}) | 2352 val rows ({0: 1849, 1: 503}) | 0 test rows ({})\n",
      "    >> AUGMENTED DATA SPLIT: 3527 train rows ({1: 755, 0: 2772}) | 2352 val rows ({0: 1849, 1: 503}) | 0 test rows ({})\n",
      "    >> Updated cross-entropy weight to 3.671523178807947...\n",
      "\n",
      "-----------------------\n",
      "STARTING MODEL TRAININGps_l\n",
      "-----------------------\n",
      "\n",
      "TRAINING NODE EMBEDDER\n",
      "Detailed Loss: recon 0.91031027-0.45515513 | disen 0.70382369-0.00000000 | temporal 0.64645481-0.32322741 | total 0.77838254\n",
      "Epoch 1, loss: 0.77838254-(best 0.77838254)\n",
      "\n",
      "Detailed Loss: recon 0.49990386-0.24995193 | disen 0.72873330-0.00000000 | temporal 0.66504240-0.33252120 | total 0.58247316\n",
      "Epoch 2, loss: 0.58247316-(best 0.58247316)\n",
      "\n",
      "Detailed Loss: recon 0.41442809-0.20721404 | disen 0.73744941-0.00000000 | temporal 0.67237341-0.33618671 | total 0.54340076\n",
      "Epoch 3, loss: 0.54340076-(best 0.54340076)\n",
      "\n",
      "Detailed Loss: recon 0.43035477-0.21517739 | disen 0.73740834-0.00000000 | temporal 0.67479002-0.33739501 | total 0.55257237\n",
      "Epoch 4, loss: 0.55257237-(best 0.54340076)\n",
      "\n",
      "Detailed Loss: recon 0.43826002-0.21913001 | disen 0.74163473-0.00000000 | temporal 0.67533791-0.33766896 | total 0.55679893\n",
      "Epoch 5, loss: 0.55679893-(best 0.54340076)\n",
      "\n",
      "Detailed Loss: recon 0.43386590-0.21693295 | disen 0.74471951-0.00000000 | temporal 0.67532706-0.33766353 | total 0.55459648\n",
      "Epoch 6, loss: 0.55459648-(best 0.54340076)\n",
      "\n",
      "Detailed Loss: recon 0.42227408-0.21113704 | disen 0.74013627-0.00000000 | temporal 0.67463034-0.33731517 | total 0.54845220\n",
      "Epoch 7, loss: 0.54845220-(best 0.54340076)\n",
      "\n",
      "Detailed Loss: recon 0.40189260-0.20094630 | disen 0.74413514-0.00000000 | temporal 0.67379534-0.33689767 | total 0.53784394\n",
      "Epoch 8, loss: 0.53784394-(best 0.53784394)\n",
      "\n",
      "Detailed Loss: recon 0.38374162-0.19187081 | disen 0.74141079-0.00000000 | temporal 0.67312968-0.33656484 | total 0.52843565\n",
      "Epoch 9, loss: 0.52843565-(best 0.52843565)\n",
      "\n",
      "Detailed Loss: recon 0.36723775-0.18361887 | disen 0.74369615-0.00000000 | temporal 0.67168385-0.33584192 | total 0.51946080\n",
      "Epoch 10, loss: 0.51946080-(best 0.51946080)\n",
      "\n",
      "Detailed Loss: recon 0.35515481-0.17757741 | disen 0.74706376-0.00000000 | temporal 0.67102706-0.33551353 | total 0.51309097\n",
      "Epoch 11, loss: 0.51309097-(best 0.51309097)\n",
      "\n",
      "Detailed Loss: recon 0.34855783-0.17427891 | disen 0.74462545-0.00000000 | temporal 0.67009902-0.33504951 | total 0.50932842\n",
      "Epoch 12, loss: 0.50932842-(best 0.50932842)\n",
      "\n",
      "Detailed Loss: recon 0.35175347-0.17587674 | disen 0.74501693-0.00000000 | temporal 0.66937429-0.33468714 | total 0.51056385\n",
      "Epoch 13, loss: 0.51056385-(best 0.50932842)\n",
      "\n",
      "Detailed Loss: recon 0.34675813-0.17337906 | disen 0.74611545-0.00000000 | temporal 0.66871697-0.33435848 | total 0.50773752\n",
      "Epoch 14, loss: 0.50773752-(best 0.50773752)\n",
      "\n",
      "Detailed Loss: recon 0.35013074-0.17506537 | disen 0.74465120-0.00000000 | temporal 0.66813552-0.33406776 | total 0.50913310\n",
      "Epoch 15, loss: 0.50913310-(best 0.50773752)\n",
      "\n",
      "Detailed Loss: recon 0.34534311-0.17267156 | disen 0.74230862-0.00000000 | temporal 0.66779888-0.33389944 | total 0.50657099\n",
      "Epoch 16, loss: 0.50657099-(best 0.50657099)\n",
      "\n",
      "Detailed Loss: recon 0.34451041-0.17225520 | disen 0.74264789-0.00000000 | temporal 0.66809052-0.33404526 | total 0.50630045\n",
      "Epoch 17, loss: 0.50630045-(best 0.50630045)\n",
      "\n",
      "Detailed Loss: recon 0.34325251-0.17162625 | disen 0.74348640-0.00000000 | temporal 0.66817999-0.33408999 | total 0.50571626\n",
      "Epoch 18, loss: 0.50571626-(best 0.50571626)\n",
      "\n",
      "Detailed Loss: recon 0.32351789-0.16175894 | disen 0.74220610-0.00000000 | temporal 0.66856080-0.33428040 | total 0.49603933\n",
      "Epoch 19, loss: 0.49603933-(best 0.49603933)\n",
      "\n",
      "Detailed Loss: recon 0.32597291-0.16298646 | disen 0.74369395-0.00000000 | temporal 0.66871667-0.33435833 | total 0.49734479\n",
      "Epoch 20, loss: 0.49734479-(best 0.49603933)\n",
      "\n",
      "Detailed Loss: recon 0.31744951-0.15872476 | disen 0.74604630-0.00000000 | temporal 0.66945058-0.33472529 | total 0.49345005\n",
      "Epoch 21, loss: 0.49345005-(best 0.49345005)\n",
      "\n",
      "Detailed Loss: recon 0.31088227-0.15544114 | disen 0.74304688-0.00000000 | temporal 0.67009038-0.33504519 | total 0.49048632\n",
      "Epoch 22, loss: 0.49048632-(best 0.49048632)\n",
      "\n",
      "Detailed Loss: recon 0.30998170-0.15499085 | disen 0.74154496-0.00000000 | temporal 0.67053515-0.33526757 | total 0.49025843\n",
      "Epoch 23, loss: 0.49025843-(best 0.49025843)\n",
      "\n",
      "Detailed Loss: recon 0.31019869-0.15509935 | disen 0.74205446-0.00000000 | temporal 0.67050970-0.33525485 | total 0.49035418\n",
      "Epoch 24, loss: 0.49035418-(best 0.49025843)\n",
      "\n",
      "Detailed Loss: recon 0.30893540-0.15446770 | disen 0.74285930-0.00000000 | temporal 0.67102063-0.33551031 | total 0.48997802\n",
      "Epoch 25, loss: 0.48997802-(best 0.48997802)\n",
      "\n",
      "Detailed Loss: recon 0.30254820-0.15127410 | disen 0.74059093-0.00000000 | temporal 0.67115265-0.33557633 | total 0.48685044\n",
      "Epoch 26, loss: 0.48685044-(best 0.48685044)\n",
      "\n",
      "Detailed Loss: recon 0.30237746-0.15118873 | disen 0.74162233-0.00000000 | temporal 0.67101336-0.33550668 | total 0.48669541\n",
      "Epoch 27, loss: 0.48669541-(best 0.48669541)\n",
      "\n",
      "Detailed Loss: recon 0.30009687-0.15004843 | disen 0.74252725-0.00000000 | temporal 0.67086840-0.33543420 | total 0.48548263\n",
      "Epoch 28, loss: 0.48548263-(best 0.48548263)\n",
      "\n",
      "Detailed Loss: recon 0.29905295-0.14952648 | disen 0.73886716-0.00000000 | temporal 0.67048681-0.33524340 | total 0.48476988\n",
      "Epoch 29, loss: 0.48476988-(best 0.48476988)\n",
      "\n",
      "Detailed Loss: recon 0.29395363-0.14697681 | disen 0.74150872-0.00000000 | temporal 0.67029965-0.33514982 | total 0.48212665\n",
      "Epoch 30, loss: 0.48212665-(best 0.48212665)\n",
      "\n",
      "Detailed Loss: recon 0.29598004-0.14799002 | disen 0.74264812-0.00000000 | temporal 0.67032647-0.33516324 | total 0.48315325\n",
      "Epoch 31, loss: 0.48315325-(best 0.48212665)\n",
      "\n",
      "Detailed Loss: recon 0.29638311-0.14819156 | disen 0.74006867-0.00000000 | temporal 0.67021644-0.33510822 | total 0.48329979\n",
      "Epoch 32, loss: 0.48329979-(best 0.48212665)\n",
      "\n",
      "Detailed Loss: recon 0.29470050-0.14735025 | disen 0.73741925-0.00000000 | temporal 0.67034787-0.33517393 | total 0.48252419\n",
      "Epoch 33, loss: 0.48252419-(best 0.48212665)\n",
      "\n",
      "Detailed Loss: recon 0.29173034-0.14586517 | disen 0.74104792-0.00000000 | temporal 0.67036557-0.33518279 | total 0.48104796\n",
      "Epoch 34, loss: 0.48104796-(best 0.48104796)\n",
      "\n",
      "Detailed Loss: recon 0.28784272-0.14392136 | disen 0.73606950-0.00000000 | temporal 0.67043626-0.33521813 | total 0.47913951\n",
      "Epoch 35, loss: 0.47913951-(best 0.47913951)\n",
      "\n",
      "Detailed Loss: recon 0.28768486-0.14384243 | disen 0.73994100-0.00000000 | temporal 0.67027253-0.33513626 | total 0.47897869\n",
      "Epoch 36, loss: 0.47897869-(best 0.47897869)\n",
      "\n",
      "Detailed Loss: recon 0.28103495-0.14051747 | disen 0.74114972-0.00000000 | temporal 0.67036986-0.33518493 | total 0.47570240\n",
      "Epoch 37, loss: 0.47570240-(best 0.47570240)\n",
      "\n",
      "Detailed Loss: recon 0.28393382-0.14196691 | disen 0.73768604-0.00000000 | temporal 0.67015415-0.33507708 | total 0.47704399\n",
      "Epoch 38, loss: 0.47704399-(best 0.47570240)\n",
      "\n",
      "Detailed Loss: recon 0.27512056-0.13756028 | disen 0.74104893-0.00000000 | temporal 0.67020750-0.33510375 | total 0.47266403\n",
      "Epoch 39, loss: 0.47266403-(best 0.47266403)\n",
      "\n",
      "Detailed Loss: recon 0.27908856-0.13954428 | disen 0.74079978-0.00000000 | temporal 0.67024064-0.33512032 | total 0.47466460\n",
      "Epoch 40, loss: 0.47466460-(best 0.47266403)\n",
      "\n",
      "Detailed Loss: recon 0.28035641-0.14017820 | disen 0.74171352-0.00000000 | temporal 0.67033154-0.33516577 | total 0.47534397\n",
      "Epoch 41, loss: 0.47534397-(best 0.47266403)\n",
      "\n",
      "Detailed Loss: recon 0.27963322-0.13981661 | disen 0.74139142-0.00000000 | temporal 0.67017293-0.33508646 | total 0.47490308\n",
      "Epoch 42, loss: 0.47490308-(best 0.47266403)\n",
      "\n",
      "Detailed Loss: recon 0.27826911-0.13913456 | disen 0.74045342-0.00000000 | temporal 0.67020845-0.33510423 | total 0.47423878\n",
      "Epoch 43, loss: 0.47423878-(best 0.47266403)\n",
      "\n",
      "Detailed Loss: recon 0.26834202-0.13417101 | disen 0.74202001-0.00000000 | temporal 0.67016852-0.33508426 | total 0.46925527\n",
      "Epoch 44, loss: 0.46925527-(best 0.46925527)\n",
      "\n",
      "Detailed Loss: recon 0.26978111-0.13489056 | disen 0.74130869-0.00000000 | temporal 0.67010576-0.33505288 | total 0.46994343\n",
      "Epoch 45, loss: 0.46994343-(best 0.46925527)\n",
      "\n",
      "Detailed Loss: recon 0.27284169-0.13642085 | disen 0.73952866-0.00000000 | temporal 0.67029381-0.33514690 | total 0.47156775\n",
      "Epoch 46, loss: 0.47156775-(best 0.46925527)\n",
      "\n",
      "Detailed Loss: recon 0.26716864-0.13358432 | disen 0.73822278-0.00000000 | temporal 0.67030466-0.33515233 | total 0.46873665\n",
      "Epoch 47, loss: 0.46873665-(best 0.46873665)\n",
      "\n",
      "Detailed Loss: recon 0.26739547-0.13369773 | disen 0.73693675-0.00000000 | temporal 0.67032939-0.33516470 | total 0.46886241\n",
      "Epoch 48, loss: 0.46886241-(best 0.46873665)\n",
      "\n",
      "Detailed Loss: recon 0.26516354-0.13258177 | disen 0.73818773-0.00000000 | temporal 0.67002457-0.33501229 | total 0.46759406\n",
      "Epoch 49, loss: 0.46759406-(best 0.46759406)\n",
      "\n",
      "Detailed Loss: recon 0.26965624-0.13482812 | disen 0.73648155-0.00000000 | temporal 0.66990077-0.33495039 | total 0.46977851\n",
      "Epoch 50, loss: 0.46977851-(best 0.46759406)\n",
      "\n",
      "Detailed Loss: recon 0.26514637-0.13257319 | disen 0.73858935-0.00000000 | temporal 0.66962296-0.33481148 | total 0.46738467\n",
      "Epoch 51, loss: 0.46738467-(best 0.46738467)\n",
      "\n",
      "Detailed Loss: recon 0.25882587-0.12941293 | disen 0.73989862-0.00000000 | temporal 0.66986716-0.33493358 | total 0.46434653\n",
      "Epoch 52, loss: 0.46434653-(best 0.46434653)\n",
      "\n",
      "Detailed Loss: recon 0.26301056-0.13150528 | disen 0.73817348-0.00000000 | temporal 0.66982704-0.33491352 | total 0.46641880\n",
      "Epoch 53, loss: 0.46641880-(best 0.46434653)\n",
      "\n",
      "Detailed Loss: recon 0.26086852-0.13043426 | disen 0.73891163-0.00000000 | temporal 0.66988695-0.33494347 | total 0.46537775\n",
      "Epoch 54, loss: 0.46537775-(best 0.46434653)\n",
      "\n",
      "Detailed Loss: recon 0.25630364-0.12815182 | disen 0.74313116-0.00000000 | temporal 0.66984987-0.33492494 | total 0.46307677\n",
      "Epoch 55, loss: 0.46307677-(best 0.46307677)\n",
      "\n",
      "Detailed Loss: recon 0.26116520-0.13058260 | disen 0.73853433-0.00000000 | temporal 0.67001301-0.33500651 | total 0.46558911\n",
      "Epoch 56, loss: 0.46558911-(best 0.46307677)\n",
      "\n",
      "Detailed Loss: recon 0.25827730-0.12913865 | disen 0.74144763-0.00000000 | temporal 0.67004532-0.33502266 | total 0.46416131\n",
      "Epoch 57, loss: 0.46416131-(best 0.46307677)\n",
      "\n",
      "Detailed Loss: recon 0.25653890-0.12826945 | disen 0.73835778-0.00000000 | temporal 0.66981274-0.33490637 | total 0.46317583\n",
      "Epoch 58, loss: 0.46317583-(best 0.46307677)\n",
      "\n",
      "Detailed Loss: recon 0.25750074-0.12875037 | disen 0.73831052-0.00000000 | temporal 0.67013544-0.33506772 | total 0.46381807\n",
      "Epoch 59, loss: 0.46381807-(best 0.46307677)\n",
      "\n",
      "Detailed Loss: recon 0.25585121-0.12792560 | disen 0.73912084-0.00000000 | temporal 0.66993004-0.33496502 | total 0.46289062\n",
      "Epoch 60, loss: 0.46289062-(best 0.46289062)\n",
      "\n",
      "Detailed Loss: recon 0.25516030-0.12758015 | disen 0.74052495-0.00000000 | temporal 0.66989887-0.33494943 | total 0.46252960\n",
      "Epoch 61, loss: 0.46252960-(best 0.46252960)\n",
      "\n",
      "Detailed Loss: recon 0.25593317-0.12796658 | disen 0.73916656-0.00000000 | temporal 0.66986293-0.33493146 | total 0.46289805\n",
      "Epoch 62, loss: 0.46289805-(best 0.46252960)\n",
      "\n",
      "Detailed Loss: recon 0.25113565-0.12556782 | disen 0.73937094-0.00000000 | temporal 0.67007214-0.33503607 | total 0.46060389\n",
      "Epoch 63, loss: 0.46060389-(best 0.46060389)\n",
      "\n",
      "Detailed Loss: recon 0.25037104-0.12518552 | disen 0.74323475-0.00000000 | temporal 0.67000800-0.33500400 | total 0.46018952\n",
      "Epoch 64, loss: 0.46018952-(best 0.46018952)\n",
      "\n",
      "Detailed Loss: recon 0.25086126-0.12543063 | disen 0.74002367-0.00000000 | temporal 0.66996622-0.33498311 | total 0.46041375\n",
      "Epoch 65, loss: 0.46041375-(best 0.46018952)\n",
      "\n",
      "Detailed Loss: recon 0.25101131-0.12550566 | disen 0.74378169-0.00000000 | temporal 0.66950446-0.33475223 | total 0.46025789\n",
      "Epoch 66, loss: 0.46025789-(best 0.46018952)\n",
      "\n",
      "Detailed Loss: recon 0.24917385-0.12458692 | disen 0.74135101-0.00000000 | temporal 0.66980445-0.33490223 | total 0.45948917\n",
      "Epoch 67, loss: 0.45948917-(best 0.45948917)\n",
      "\n",
      "Detailed Loss: recon 0.25103670-0.12551835 | disen 0.74644923-0.00000000 | temporal 0.66977358-0.33488679 | total 0.46040514\n",
      "Epoch 68, loss: 0.46040514-(best 0.45948917)\n",
      "\n",
      "Detailed Loss: recon 0.25043774-0.12521887 | disen 0.74079126-0.00000000 | temporal 0.66992205-0.33496103 | total 0.46017990\n",
      "Epoch 69, loss: 0.46017990-(best 0.45948917)\n",
      "\n",
      "Detailed Loss: recon 0.24966733-0.12483367 | disen 0.74388337-0.00000000 | temporal 0.67002666-0.33501333 | total 0.45984700\n",
      "Epoch 70, loss: 0.45984700-(best 0.45948917)\n",
      "\n",
      "Detailed Loss: recon 0.24932490-0.12466245 | disen 0.74465364-0.00000000 | temporal 0.66983950-0.33491975 | total 0.45958221\n",
      "Epoch 71, loss: 0.45958221-(best 0.45948917)\n",
      "\n",
      "Detailed Loss: recon 0.24826929-0.12413464 | disen 0.74223834-0.00000000 | temporal 0.67010701-0.33505350 | total 0.45918816\n",
      "Epoch 72, loss: 0.45918816-(best 0.45918816)\n",
      "\n",
      "Detailed Loss: recon 0.24685344-0.12342672 | disen 0.74473143-0.00000000 | temporal 0.67004842-0.33502421 | total 0.45845091\n",
      "Epoch 73, loss: 0.45845091-(best 0.45845091)\n",
      "\n",
      "Detailed Loss: recon 0.24606423-0.12303212 | disen 0.74167693-0.00000000 | temporal 0.67001307-0.33500654 | total 0.45803866\n",
      "Epoch 74, loss: 0.45803866-(best 0.45803866)\n",
      "\n",
      "Detailed Loss: recon 0.24633873-0.12316936 | disen 0.74255443-0.00000000 | temporal 0.66987735-0.33493868 | total 0.45810804\n",
      "Epoch 75, loss: 0.45810804-(best 0.45803866)\n",
      "\n",
      "Detailed Loss: recon 0.24661040-0.12330520 | disen 0.73873937-0.00000000 | temporal 0.66976672-0.33488336 | total 0.45818856\n",
      "Epoch 76, loss: 0.45818856-(best 0.45803866)\n",
      "\n",
      "Detailed Loss: recon 0.24458861-0.12229431 | disen 0.74138689-0.00000000 | temporal 0.66996759-0.33498380 | total 0.45727810\n",
      "Epoch 77, loss: 0.45727810-(best 0.45727810)\n",
      "\n",
      "Detailed Loss: recon 0.24613279-0.12306640 | disen 0.74162507-0.00000000 | temporal 0.66972286-0.33486143 | total 0.45792782\n",
      "Epoch 78, loss: 0.45792782-(best 0.45727810)\n",
      "\n",
      "Detailed Loss: recon 0.24540433-0.12270217 | disen 0.74610734-0.00000000 | temporal 0.66932166-0.33466083 | total 0.45736301\n",
      "Epoch 79, loss: 0.45736301-(best 0.45727810)\n",
      "\n",
      "Detailed Loss: recon 0.24103364-0.12051682 | disen 0.74694711-0.00000000 | temporal 0.66962564-0.33481282 | total 0.45532966\n",
      "Epoch 80, loss: 0.45532966-(best 0.45532966)\n",
      "\n",
      "Detailed Loss: recon 0.24252915-0.12126458 | disen 0.74173003-0.00000000 | temporal 0.66978008-0.33489004 | total 0.45615461\n",
      "Epoch 81, loss: 0.45615461-(best 0.45532966)\n",
      "\n",
      "Detailed Loss: recon 0.24265270-0.12132635 | disen 0.74388444-0.00000000 | temporal 0.66948432-0.33474216 | total 0.45606852\n",
      "Epoch 82, loss: 0.45606852-(best 0.45532966)\n",
      "\n",
      "Detailed Loss: recon 0.24428016-0.12214008 | disen 0.74648291-0.00000000 | temporal 0.66946572-0.33473286 | total 0.45687294\n",
      "Epoch 83, loss: 0.45687294-(best 0.45532966)\n",
      "\n",
      "Detailed Loss: recon 0.24386500-0.12193250 | disen 0.74031234-0.00000000 | temporal 0.66964650-0.33482325 | total 0.45675576\n",
      "Epoch 84, loss: 0.45675576-(best 0.45532966)\n",
      "\n",
      "Detailed Loss: recon 0.24338846-0.12169423 | disen 0.74429226-0.00000000 | temporal 0.66945976-0.33472988 | total 0.45642412\n",
      "Epoch 85, loss: 0.45642412-(best 0.45532966)\n",
      "\n",
      "Detailed Loss: recon 0.24378619-0.12189309 | disen 0.74339187-0.00000000 | temporal 0.66918522-0.33459261 | total 0.45648569\n",
      "Epoch 86, loss: 0.45648569-(best 0.45532966)\n",
      "\n",
      "Detailed Loss: recon 0.24471903-0.12235951 | disen 0.74314451-0.00000000 | temporal 0.66925246-0.33462623 | total 0.45698574\n",
      "Epoch 87, loss: 0.45698574-(best 0.45532966)\n",
      "\n",
      "Detailed Loss: recon 0.24095726-0.12047863 | disen 0.74355757-0.00000000 | temporal 0.66947502-0.33473751 | total 0.45521614\n",
      "Epoch 88, loss: 0.45521614-(best 0.45521614)\n",
      "\n",
      "Detailed Loss: recon 0.24340050-0.12170025 | disen 0.74468690-0.00000000 | temporal 0.66935784-0.33467892 | total 0.45637918\n",
      "Epoch 89, loss: 0.45637918-(best 0.45521614)\n",
      "\n",
      "Detailed Loss: recon 0.24125519-0.12062760 | disen 0.74644732-0.00000000 | temporal 0.66934055-0.33467028 | total 0.45529789\n",
      "Epoch 90, loss: 0.45529789-(best 0.45521614)\n",
      "\n",
      "Detailed Loss: recon 0.24345994-0.12172997 | disen 0.74563843-0.00000000 | temporal 0.66934890-0.33467445 | total 0.45640442\n",
      "Epoch 91, loss: 0.45640442-(best 0.45521614)\n",
      "\n",
      "Detailed Loss: recon 0.24146095-0.12073047 | disen 0.74594957-0.00000000 | temporal 0.66972393-0.33486196 | total 0.45559245\n",
      "Epoch 92, loss: 0.45559245-(best 0.45521614)\n",
      "\n",
      "Detailed Loss: recon 0.23952767-0.11976384 | disen 0.74339247-0.00000000 | temporal 0.66948324-0.33474162 | total 0.45450544\n",
      "Epoch 93, loss: 0.45450544-(best 0.45450544)\n",
      "\n",
      "Detailed Loss: recon 0.23920882-0.11960441 | disen 0.74721938-0.00000000 | temporal 0.66939926-0.33469963 | total 0.45430404\n",
      "Epoch 94, loss: 0.45430404-(best 0.45430404)\n",
      "\n",
      "Detailed Loss: recon 0.23900174-0.11950087 | disen 0.74601138-0.00000000 | temporal 0.66943014-0.33471507 | total 0.45421594\n",
      "Epoch 95, loss: 0.45421594-(best 0.45421594)\n",
      "\n",
      "Detailed Loss: recon 0.23892801-0.11946400 | disen 0.74651027-0.00000000 | temporal 0.66946882-0.33473441 | total 0.45419842\n",
      "Epoch 96, loss: 0.45419842-(best 0.45419842)\n",
      "\n",
      "Detailed Loss: recon 0.23790948-0.11895474 | disen 0.74667418-0.00000000 | temporal 0.66942710-0.33471355 | total 0.45366830\n",
      "Epoch 97, loss: 0.45366830-(best 0.45366830)\n",
      "\n",
      "Detailed Loss: recon 0.23773901-0.11886951 | disen 0.74577075-0.00000000 | temporal 0.66941059-0.33470529 | total 0.45357481\n",
      "Epoch 98, loss: 0.45357481-(best 0.45357481)\n",
      "\n",
      "Detailed Loss: recon 0.23637974-0.11818987 | disen 0.74435091-0.00000000 | temporal 0.66919762-0.33459881 | total 0.45278868\n",
      "Epoch 99, loss: 0.45278868-(best 0.45278868)\n",
      "\n",
      "Detailed Loss: recon 0.23736881-0.11868440 | disen 0.74520952-0.00000000 | temporal 0.66941601-0.33470801 | total 0.45339242\n",
      "Epoch 100, loss: 0.45339242-(best 0.45278868)\n",
      "\n",
      ">> Reached final epoch. Loading best model...\n",
      "TRAINING XGB PREDICTOR\n",
      ">> Ending training!\n",
      "\n",
      "-----------------------\n",
      "TIME COST: 6.283413410186768 s\n",
      "Best Val: REC 60.83 PRE 43.10 MF1 66.62 AUC 78.79 TP 306 FP 404 TN 1445 FN 197\n",
      "-----------------------\n",
      "\n",
      "ASSIGNED GRAPH FOR EVALUATION: 11758 {1: 2566, 0: 9192} Nodes, 1049758 Edges\n",
      "\n",
      "-------------------\n",
      "STARTING EVALUATION\n",
      "-------------------\n",
      "\n",
      "Dataset - Overall: REC 56.16 PRE 48.29 MF1 68.54 AUC 81.15 TP 1441 FP 1543 TN 7649 FN 1125 | 11758 {1: 2566, 0: 9192}\n",
      "Dataset - Train: REC 65.03 PRE 53.96 MF1 73.15 AUC 85.99 TP 491 FP 419 TN 2353 FN 264 | 3527 {1: 755, 0: 2772}\n",
      "Dataset - Val: REC 51.89 PRE 42.44 MF1 65.04 AUC 78.35 TP 261 FP 354 TN 1495 FN 242 | 2352 {0: 1849, 1: 503}\n",
      "Dataset - Test: REC 52.68 PRE 47.22 MF1 67.18 AUC 79.39 TP 689 FP 770 TN 3801 FN 619 | 5879 {1: 1308, 0: 4571}\n",
      "PREDICTION STATUS - {'1-True': 1947, '0-False': 3801, '1-False': 619, '0-True': 5391}\n",
      "    >> 619 positive nodes left unpredicted...\n",
      "    >> 619 OG positive nodes left unpredicted...\n",
      "Selecting data as budgeted ground truth for next round...\n",
      "  > Starting round 1...\n",
      "\n",
      "+++++++++++++++++\n",
      "STARTING ROUND 1!\n",
      "+++++++++++++++++\n",
      "        \n",
      "ASSIGNED GRAPH FOR TRAINING: 11758 {1: 2566, 0: 9192} Nodes, 1049758 Edges\n",
      "\n",
      ">> Splitting to train-validation set...\n",
      "Alotting train-val-test split for round 1\n",
      "Initial pool: 5879, Prediction pool: 2984, Budget pool: 0, Full pool: 7338\n",
      "Full graph size: 11758, Training: 4402, Val: 2936, Test: 11758\n",
      "    >> INITIAL DATA SPLIT: 4402 train rows ({1: 1168, 0: 3234}) | 2936 val rows ({0: 2157, 1: 779}) | 4420 test rows ({0: 3801, 1: 619})\n",
      "    >> AUGMENTED DATA SPLIT: 4402 train rows ({1: 1168, 0: 3234}) | 2936 val rows ({0: 2157, 1: 779}) | 4420 test rows ({0: 3801, 1: 619})\n",
      "    >> Updated cross-entropy weight to 2.768835616438356...\n",
      "\n",
      "-----------------------\n",
      "STARTING MODEL TRAININGps_l\n",
      "-----------------------\n",
      "\n",
      "TRAINING NODE EMBEDDER\n",
      "Detailed Loss: recon 0.21462548-0.10731274 | disen 0.60563511-0.00000000 | temporal 0.56142819-0.28071409 | total 0.38802683\n",
      "Epoch 1, loss: 0.38802683-(best 0.38802683)\n",
      "\n",
      "Detailed Loss: recon 0.29916927-0.14958464 | disen 0.60637653-0.00000000 | temporal 0.56034678-0.28017339 | total 0.42975801\n",
      "Epoch 2, loss: 0.42975801-(best 0.38802683)\n",
      "\n",
      "Detailed Loss: recon 0.23413956-0.11706978 | disen 0.60450500-0.00000000 | temporal 0.56166428-0.28083214 | total 0.39790192\n",
      "Epoch 3, loss: 0.39790192-(best 0.38802683)\n",
      "\n",
      "Detailed Loss: recon 0.25689286-0.12844643 | disen 0.60432547-0.00000000 | temporal 0.56201500-0.28100750 | total 0.40945393\n",
      "Epoch 4, loss: 0.40945393-(best 0.38802683)\n",
      "\n",
      "Detailed Loss: recon 0.21778196-0.10889098 | disen 0.60472262-0.00000000 | temporal 0.56159276-0.28079638 | total 0.38968736\n",
      "Epoch 5, loss: 0.38968736-(best 0.38802683)\n",
      "\n",
      "Detailed Loss: recon 0.23330794-0.11665397 | disen 0.60438001-0.00000000 | temporal 0.56123161-0.28061581 | total 0.39726979\n",
      "Epoch 6, loss: 0.39726979-(best 0.38802683)\n",
      "\n",
      "Detailed Loss: recon 0.24064992-0.12032496 | disen 0.60476673-0.00000000 | temporal 0.56112462-0.28056231 | total 0.40088728\n",
      "Epoch 7, loss: 0.40088728-(best 0.38802683)\n",
      "\n",
      "Detailed Loss: recon 0.22264192-0.11132096 | disen 0.60471362-0.00000000 | temporal 0.56127369-0.28063685 | total 0.39195782\n",
      "Epoch 8, loss: 0.39195782-(best 0.38802683)\n",
      "\n",
      "Detailed Loss: recon 0.21924767-0.10962383 | disen 0.60391569-0.00000000 | temporal 0.56150234-0.28075117 | total 0.39037502\n",
      "Epoch 9, loss: 0.39037502-(best 0.38802683)\n",
      "\n",
      "Detailed Loss: recon 0.22727188-0.11363594 | disen 0.60357600-0.00000000 | temporal 0.56163228-0.28081614 | total 0.39445210\n",
      "Epoch 10, loss: 0.39445210-(best 0.38802683)\n",
      "\n",
      "Detailed Loss: recon 0.22901806-0.11450903 | disen 0.60382378-0.00000000 | temporal 0.56160885-0.28080443 | total 0.39531344\n",
      "Epoch 11, loss: 0.39531344-(best 0.38802683)\n",
      "\n",
      "Detailed Loss: recon 0.22438088-0.11219044 | disen 0.60384929-0.00000000 | temporal 0.56152612-0.28076306 | total 0.39295352\n",
      "Epoch 12, loss: 0.39295352-(best 0.38802683)\n",
      "\n",
      "Detailed Loss: recon 0.22112899-0.11056449 | disen 0.60459208-0.00000000 | temporal 0.56132543-0.28066272 | total 0.39122722\n",
      "Epoch 13, loss: 0.39122722-(best 0.38802683)\n",
      "\n",
      "Detailed Loss: recon 0.22180751-0.11090375 | disen 0.60383624-0.00000000 | temporal 0.56115639-0.28057820 | total 0.39148194\n",
      "Epoch 14, loss: 0.39148194-(best 0.38802683)\n",
      "\n",
      "Detailed Loss: recon 0.22200391-0.11100195 | disen 0.60532153-0.00000000 | temporal 0.56095243-0.28047621 | total 0.39147818\n",
      "Epoch 15, loss: 0.39147818-(best 0.38802683)\n",
      "\n",
      "Detailed Loss: recon 0.22253023-0.11126512 | disen 0.60436165-0.00000000 | temporal 0.56094891-0.28047445 | total 0.39173958\n",
      "Epoch 16, loss: 0.39173958-(best 0.38802683)\n",
      "\n",
      "Detailed Loss: recon 0.22003399-0.11001699 | disen 0.60432971-0.00000000 | temporal 0.56098205-0.28049102 | total 0.39050803\n",
      "Epoch 17, loss: 0.39050803-(best 0.38802683)\n",
      "\n",
      "Detailed Loss: recon 0.21631919-0.10815959 | disen 0.60465741-0.00000000 | temporal 0.56108934-0.28054467 | total 0.38870427\n",
      "Epoch 18, loss: 0.38870427-(best 0.38802683)\n",
      "\n",
      "Detailed Loss: recon 0.21523039-0.10761520 | disen 0.60420942-0.00000000 | temporal 0.56117809-0.28058904 | total 0.38820425\n",
      "Epoch 19, loss: 0.38820425-(best 0.38802683)\n",
      "\n",
      "Detailed Loss: recon 0.21544640-0.10772320 | disen 0.60339016-0.00000000 | temporal 0.56121510-0.28060755 | total 0.38833076\n",
      "Epoch 20, loss: 0.38833076-(best 0.38802683)\n",
      "\n",
      "Detailed Loss: recon 0.21548454-0.10774227 | disen 0.60378987-0.00000000 | temporal 0.56117922-0.28058961 | total 0.38833189\n",
      "Epoch 21, loss: 0.38833189-(best 0.38802683)\n",
      "\n",
      "Detailed Loss: recon 0.21501389-0.10750695 | disen 0.60333741-0.00000000 | temporal 0.56104416-0.28052208 | total 0.38802904\n",
      "Epoch 22, loss: 0.38802904-(best 0.38802683)\n",
      "\n",
      "Detailed Loss: recon 0.21186835-0.10593417 | disen 0.60363388-0.00000000 | temporal 0.56091815-0.28045908 | total 0.38639325\n",
      "Epoch 23, loss: 0.38639325-(best 0.38639325)\n",
      "\n",
      "Detailed Loss: recon 0.21309023-0.10654511 | disen 0.60411096-0.00000000 | temporal 0.56076497-0.28038248 | total 0.38692760\n",
      "Epoch 24, loss: 0.38692760-(best 0.38639325)\n",
      "\n",
      "Detailed Loss: recon 0.21315473-0.10657737 | disen 0.60421586-0.00000000 | temporal 0.56070036-0.28035018 | total 0.38692755\n",
      "Epoch 25, loss: 0.38692755-(best 0.38639325)\n",
      "\n",
      "Detailed Loss: recon 0.21347421-0.10673711 | disen 0.60323393-0.00000000 | temporal 0.56067842-0.28033921 | total 0.38707632\n",
      "Epoch 26, loss: 0.38707632-(best 0.38639325)\n",
      "\n",
      "Detailed Loss: recon 0.21124649-0.10562325 | disen 0.60379577-0.00000000 | temporal 0.56073838-0.28036919 | total 0.38599244\n",
      "Epoch 27, loss: 0.38599244-(best 0.38599244)\n",
      "\n",
      "Detailed Loss: recon 0.21005826-0.10502913 | disen 0.60314655-0.00000000 | temporal 0.56071019-0.28035510 | total 0.38538423\n",
      "Epoch 28, loss: 0.38538423-(best 0.38538423)\n",
      "\n",
      "Detailed Loss: recon 0.20908701-0.10454351 | disen 0.60315174-0.00000000 | temporal 0.56072247-0.28036124 | total 0.38490474\n",
      "Epoch 29, loss: 0.38490474-(best 0.38490474)\n",
      "\n",
      "Detailed Loss: recon 0.20880739-0.10440370 | disen 0.60371816-0.00000000 | temporal 0.56071806-0.28035903 | total 0.38476273\n",
      "Epoch 30, loss: 0.38476273-(best 0.38476273)\n",
      "\n",
      "Detailed Loss: recon 0.20770660-0.10385330 | disen 0.60354286-0.00000000 | temporal 0.56070244-0.28035122 | total 0.38420451\n",
      "Epoch 31, loss: 0.38420451-(best 0.38420451)\n",
      "\n",
      "Detailed Loss: recon 0.20750973-0.10375486 | disen 0.60359144-0.00000000 | temporal 0.56065685-0.28032842 | total 0.38408327\n",
      "Epoch 32, loss: 0.38408327-(best 0.38408327)\n",
      "\n",
      "Detailed Loss: recon 0.20776656-0.10388328 | disen 0.60354936-0.00000000 | temporal 0.56060410-0.28030205 | total 0.38418531\n",
      "Epoch 33, loss: 0.38418531-(best 0.38408327)\n",
      "\n",
      "Detailed Loss: recon 0.20630190-0.10315095 | disen 0.60373175-0.00000000 | temporal 0.56056142-0.28028071 | total 0.38343167\n",
      "Epoch 34, loss: 0.38343167-(best 0.38343167)\n",
      "\n",
      "Detailed Loss: recon 0.20583044-0.10291522 | disen 0.60371625-0.00000000 | temporal 0.56053913-0.28026956 | total 0.38318479\n",
      "Epoch 35, loss: 0.38318479-(best 0.38318479)\n",
      "\n",
      "Detailed Loss: recon 0.20604035-0.10302018 | disen 0.60307437-0.00000000 | temporal 0.56048113-0.28024057 | total 0.38326073\n",
      "Epoch 36, loss: 0.38326073-(best 0.38318479)\n",
      "\n",
      "Detailed Loss: recon 0.20523517-0.10261758 | disen 0.60256851-0.00000000 | temporal 0.56053054-0.28026527 | total 0.38288286\n",
      "Epoch 37, loss: 0.38288286-(best 0.38288286)\n",
      "\n",
      "Detailed Loss: recon 0.20448636-0.10224318 | disen 0.60237169-0.00000000 | temporal 0.56050831-0.28025416 | total 0.38249734\n",
      "Epoch 38, loss: 0.38249734-(best 0.38249734)\n",
      "\n",
      "Detailed Loss: recon 0.20546779-0.10273390 | disen 0.60241157-0.00000000 | temporal 0.56044114-0.28022057 | total 0.38295448\n",
      "Epoch 39, loss: 0.38295448-(best 0.38249734)\n",
      "\n",
      "Detailed Loss: recon 0.20593058-0.10296529 | disen 0.60311306-0.00000000 | temporal 0.56038547-0.28019273 | total 0.38315803\n",
      "Epoch 40, loss: 0.38315803-(best 0.38249734)\n",
      "\n",
      "Detailed Loss: recon 0.20416668-0.10208334 | disen 0.60303640-0.00000000 | temporal 0.56032592-0.28016296 | total 0.38224632\n",
      "Epoch 41, loss: 0.38224632-(best 0.38224632)\n",
      "\n",
      "Detailed Loss: recon 0.20421956-0.10210978 | disen 0.60221070-0.00000000 | temporal 0.56030101-0.28015050 | total 0.38226029\n",
      "Epoch 42, loss: 0.38226029-(best 0.38224632)\n",
      "\n",
      "Detailed Loss: recon 0.20264253-0.10132127 | disen 0.60221004-0.00000000 | temporal 0.56023425-0.28011712 | total 0.38143837\n",
      "Epoch 43, loss: 0.38143837-(best 0.38143837)\n",
      "\n",
      "Detailed Loss: recon 0.20235889-0.10117944 | disen 0.60297942-0.00000000 | temporal 0.56019372-0.28009686 | total 0.38127631\n",
      "Epoch 44, loss: 0.38127631-(best 0.38127631)\n",
      "\n",
      "Detailed Loss: recon 0.20322952-0.10161476 | disen 0.60243523-0.00000000 | temporal 0.56022239-0.28011119 | total 0.38172597\n",
      "Epoch 45, loss: 0.38172597-(best 0.38127631)\n",
      "\n",
      "Detailed Loss: recon 0.20329392-0.10164696 | disen 0.60260177-0.00000000 | temporal 0.56021041-0.28010520 | total 0.38175216\n",
      "Epoch 46, loss: 0.38175216-(best 0.38127631)\n",
      "\n",
      "Detailed Loss: recon 0.20388556-0.10194278 | disen 0.60207522-0.00000000 | temporal 0.56022763-0.28011382 | total 0.38205659\n",
      "Epoch 47, loss: 0.38205659-(best 0.38127631)\n",
      "\n",
      "Detailed Loss: recon 0.20397952-0.10198976 | disen 0.60208881-0.00000000 | temporal 0.56011915-0.28005958 | total 0.38204932\n",
      "Epoch 48, loss: 0.38204932-(best 0.38127631)\n",
      "\n",
      "Detailed Loss: recon 0.20346513-0.10173257 | disen 0.60202563-0.00000000 | temporal 0.56013250-0.28006625 | total 0.38179880\n",
      "Epoch 49, loss: 0.38179880-(best 0.38127631)\n",
      "\n",
      "Detailed Loss: recon 0.20154357-0.10077178 | disen 0.60128832-0.00000000 | temporal 0.56006271-0.28003135 | total 0.38080314\n",
      "Epoch 50, loss: 0.38080314-(best 0.38080314)\n",
      "\n",
      ">> Reached final epoch. Loading best model...\n",
      "TRAINING XGB PREDICTOR\n",
      ">> Ending training!\n",
      "\n",
      "-----------------------\n",
      "TIME COST: 4.736652612686157 s\n",
      "Best Val: REC 61.62 PRE 51.50 MF1 69.03 AUC 79.43 TP 480 FP 452 TN 1705 FN 299\n",
      "-----------------------\n",
      "Generating additional positive data from adversary...\n",
      "Generating additional negative data by duplicating random nodes...\n",
      "\n",
      "ASSIGNED GRAPH FOR EVALUATION: 12345 {1: 2694, 0: 9651} Nodes, 1162830 Edges\n",
      "\n",
      "-------------------\n",
      "STARTING EVALUATION\n",
      "-------------------\n",
      "\n",
      "Dataset - Overall: REC 63.62 PRE 50.82 MF1 71.17 AUC 83.18 TP 1714 FP 1659 TN 7992 FN 980 | 12345 {1: 2694, 0: 9651}\n",
      "Dataset - Train: REC 79.37 PRE 61.43 MF1 77.91 AUC 88.89 TP 927 FP 582 TN 2652 FN 241 | 4402 {1: 1168, 0: 3234}\n",
      "Dataset - Val: REC 62.52 PRE 48.08 MF1 67.15 AUC 77.93 TP 487 FP 526 TN 1631 FN 292 | 2936 {0: 2157, 1: 779}\n",
      "Dataset - Test: REC 40.16 PRE 35.25 MF1 62.84 AUC 78.16 TP 300 FP 551 TN 3709 FN 447 | 5007 {0: 4260, 1: 747}\n",
      "PREDICTION STATUS - {'1-True': 2247, '0-False': 3709, '1-False': 447, '0-True': 5942}\n",
      "    >> 447 positive nodes left unpredicted...\n",
      "    >> 366 OG positive nodes left unpredicted...\n",
      "-----------------------\n",
      "PREDICTION RESULT - ROUNDS\n",
      "Dataset - Round 0: REC 64.07 PRE 48.95 MF1 70.06 AUC 81.97 TP 838 FP 874 TN 3697 FN 470 | 5879 {1: 1308, 0: 4571}\n",
      "Dataset - Round 1: REC 36.72 PRE 39.50 MF1 60.78 AUC 73.03 TP 47 FP 72 TN 387 FN 81 | 587 {1: 128, 0: 459}\n",
      "-----------------------\n",
      "PREDICTION RESULT - SEEDS\n",
      "Seeds - Current: REC 34.38 PRE 34.38 MF1 58.04 AUC 70.67 TP 44 FP 84 TN 375 FN 84 | 587 {1: 128, 0: 459}\n",
      "Seeds - Prev: REC 0.00 PRE 0.00 MF1 39.30 AUC 70.67 TP 0 FP 79 TN 380 FN 128 | 587 {1: 128, 0: 459}\n",
      "Selecting data as budgeted ground truth for next round...\n",
      "  > Starting round 2...\n",
      "\n",
      "+++++++++++++++++\n",
      "STARTING ROUND 2!\n",
      "+++++++++++++++++\n",
      "        \n",
      "ASSIGNED GRAPH FOR TRAINING: 12345 {1: 2694, 0: 9651} Nodes, 1162830 Edges\n",
      "\n",
      ">> Splitting to train-validation set...\n",
      "Alotting train-val-test split for round 2\n",
      "Initial pool: 5879, Prediction pool: 6357, Budget pool: 0, Full pool: 8189\n",
      "Full graph size: 12345, Training: 4913, Val: 3276, Test: 12345\n",
      "    >> INITIAL DATA SPLIT: 4913 train rows ({1: 1348, 0: 3565}) | 3276 val rows ({1: 899, 0: 2377}) | 4156 test rows ({0: 3709, 1: 447})\n",
      "    >> AUGMENTED DATA SPLIT: 4913 train rows ({1: 1348, 0: 3565}) | 3276 val rows ({1: 899, 0: 2377}) | 4156 test rows ({0: 3709, 1: 447})\n",
      "    >> Updated cross-entropy weight to 2.644658753709199...\n",
      "\n",
      "-----------------------\n",
      "STARTING MODEL TRAININGps_l\n",
      "-----------------------\n",
      "\n",
      "TRAINING NODE EMBEDDER\n",
      "Detailed Loss: recon 0.20597079-0.10298540 | disen 0.59859121-0.00000000 | temporal 0.58378631-0.29189315 | total 0.39487857\n",
      "Epoch 1, loss: 0.39487857-(best 0.39487857)\n",
      "\n",
      "Detailed Loss: recon 0.33683264-0.16841632 | disen 0.59672105-0.00000000 | temporal 0.58510786-0.29255393 | total 0.46097025\n",
      "Epoch 2, loss: 0.46097025-(best 0.39487857)\n",
      "\n",
      "Detailed Loss: recon 0.20972046-0.10486023 | disen 0.59652650-0.00000000 | temporal 0.58394784-0.29197392 | total 0.39683414\n",
      "Epoch 3, loss: 0.39683414-(best 0.39487857)\n",
      "\n",
      "Detailed Loss: recon 0.26268706-0.13134353 | disen 0.59737217-0.00000000 | temporal 0.58311641-0.29155821 | total 0.42290175\n",
      "Epoch 4, loss: 0.42290175-(best 0.39487857)\n",
      "\n",
      "Detailed Loss: recon 0.27515563-0.13757782 | disen 0.59566438-0.00000000 | temporal 0.58305138-0.29152569 | total 0.42910349\n",
      "Epoch 5, loss: 0.42910349-(best 0.39487857)\n",
      "\n",
      "Detailed Loss: recon 0.24145709-0.12072854 | disen 0.59595054-0.00000000 | temporal 0.58339059-0.29169530 | total 0.41242385\n",
      "Epoch 6, loss: 0.41242385-(best 0.39487857)\n",
      "\n",
      "Detailed Loss: recon 0.21992719-0.10996360 | disen 0.59470630-0.00000000 | temporal 0.58389628-0.29194814 | total 0.40191174\n",
      "Epoch 7, loss: 0.40191174-(best 0.39487857)\n",
      "\n",
      "Detailed Loss: recon 0.22051531-0.11025766 | disen 0.59473598-0.00000000 | temporal 0.58431441-0.29215720 | total 0.40241486\n",
      "Epoch 8, loss: 0.40241486-(best 0.39487857)\n",
      "\n",
      "Detailed Loss: recon 0.23305058-0.11652529 | disen 0.59479177-0.00000000 | temporal 0.58457029-0.29228514 | total 0.40881044\n",
      "Epoch 9, loss: 0.40881044-(best 0.39487857)\n",
      "\n",
      "Detailed Loss: recon 0.23289111-0.11644556 | disen 0.59515214-0.00000000 | temporal 0.58462101-0.29231051 | total 0.40875608\n",
      "Epoch 10, loss: 0.40875608-(best 0.39487857)\n",
      "\n",
      "Detailed Loss: recon 0.22540286-0.11270143 | disen 0.59543097-0.00000000 | temporal 0.58453232-0.29226616 | total 0.40496761\n",
      "Epoch 11, loss: 0.40496761-(best 0.39487857)\n",
      "\n",
      "Detailed Loss: recon 0.21882807-0.10941403 | disen 0.59547722-0.00000000 | temporal 0.58434474-0.29217237 | total 0.40158641\n",
      "Epoch 12, loss: 0.40158641-(best 0.39487857)\n",
      "\n",
      "Detailed Loss: recon 0.21879463-0.10939731 | disen 0.59563267-0.00000000 | temporal 0.58413613-0.29206806 | total 0.40146539\n",
      "Epoch 13, loss: 0.40146539-(best 0.39487857)\n",
      "\n",
      "Detailed Loss: recon 0.22150180-0.11075090 | disen 0.59636998-0.00000000 | temporal 0.58389449-0.29194725 | total 0.40269816\n",
      "Epoch 14, loss: 0.40269816-(best 0.39487857)\n",
      "\n",
      "Detailed Loss: recon 0.22406402-0.11203201 | disen 0.59580100-0.00000000 | temporal 0.58381104-0.29190552 | total 0.40393752\n",
      "Epoch 15, loss: 0.40393752-(best 0.39487857)\n",
      "\n",
      "Detailed Loss: recon 0.22582105-0.11291052 | disen 0.59651589-0.00000000 | temporal 0.58375454-0.29187727 | total 0.40478778\n",
      "Epoch 16, loss: 0.40478778-(best 0.39487857)\n",
      "\n",
      "Detailed Loss: recon 0.22195193-0.11097597 | disen 0.59593165-0.00000000 | temporal 0.58379519-0.29189759 | total 0.40287358\n",
      "Epoch 17, loss: 0.40287358-(best 0.39487857)\n",
      "\n",
      "Detailed Loss: recon 0.21486656-0.10743328 | disen 0.59514093-0.00000000 | temporal 0.58391464-0.29195732 | total 0.39939061\n",
      "Epoch 18, loss: 0.39939061-(best 0.39487857)\n",
      "\n",
      "Detailed Loss: recon 0.21701986-0.10850993 | disen 0.59505260-0.00000000 | temporal 0.58398902-0.29199451 | total 0.40050444\n",
      "Epoch 19, loss: 0.40050444-(best 0.39487857)\n",
      "\n",
      "Detailed Loss: recon 0.21631162-0.10815581 | disen 0.59450758-0.00000000 | temporal 0.58404481-0.29202241 | total 0.40017822\n",
      "Epoch 20, loss: 0.40017822-(best 0.39487857)\n",
      "\n",
      "Detailed Loss: recon 0.21661079-0.10830539 | disen 0.59417224-0.00000000 | temporal 0.58406991-0.29203495 | total 0.40034035\n",
      "Epoch 21, loss: 0.40034035-(best 0.39487857)\n",
      "\n",
      "Detailed Loss: recon 0.21757321-0.10878661 | disen 0.59415746-0.00000000 | temporal 0.58404756-0.29202378 | total 0.40081039\n",
      "Epoch 22, loss: 0.40081039-(best 0.39487857)\n",
      "\n",
      "Detailed Loss: recon 0.21297610-0.10648805 | disen 0.59509110-0.00000000 | temporal 0.58388454-0.29194227 | total 0.39843032\n",
      "Epoch 23, loss: 0.39843032-(best 0.39487857)\n",
      "\n",
      "Detailed Loss: recon 0.21354809-0.10677405 | disen 0.59519929-0.00000000 | temporal 0.58383143-0.29191571 | total 0.39868975\n",
      "Epoch 24, loss: 0.39868975-(best 0.39487857)\n",
      "\n",
      "Detailed Loss: recon 0.21101984-0.10550992 | disen 0.59522986-0.00000000 | temporal 0.58372355-0.29186177 | total 0.39737171\n",
      "Epoch 25, loss: 0.39737171-(best 0.39487857)\n",
      "\n",
      "Detailed Loss: recon 0.21221399-0.10610700 | disen 0.59541899-0.00000000 | temporal 0.58375394-0.29187697 | total 0.39798397\n",
      "Epoch 26, loss: 0.39798397-(best 0.39487857)\n",
      "\n",
      "Detailed Loss: recon 0.21061146-0.10530573 | disen 0.59455383-0.00000000 | temporal 0.58376926-0.29188463 | total 0.39719036\n",
      "Epoch 27, loss: 0.39719036-(best 0.39487857)\n",
      "\n",
      "Detailed Loss: recon 0.21078566-0.10539283 | disen 0.59564030-0.00000000 | temporal 0.58385307-0.29192653 | total 0.39731938\n",
      "Epoch 28, loss: 0.39731938-(best 0.39487857)\n",
      "\n",
      "Detailed Loss: recon 0.20890811-0.10445406 | disen 0.59441876-0.00000000 | temporal 0.58384174-0.29192087 | total 0.39637494\n",
      "Epoch 29, loss: 0.39637494-(best 0.39487857)\n",
      "\n",
      "Detailed Loss: recon 0.20934391-0.10467196 | disen 0.59526098-0.00000000 | temporal 0.58388621-0.29194310 | total 0.39661506\n",
      "Epoch 30, loss: 0.39661506-(best 0.39487857)\n",
      "\n",
      "Detailed Loss: recon 0.20877127-0.10438564 | disen 0.59567869-0.00000000 | temporal 0.58387089-0.29193544 | total 0.39632109\n",
      "Epoch 31, loss: 0.39632109-(best 0.39487857)\n",
      "\n",
      "Detailed Loss: recon 0.20747420-0.10373710 | disen 0.59644341-0.00000000 | temporal 0.58382541-0.29191270 | total 0.39564979\n",
      "Epoch 32, loss: 0.39564979-(best 0.39487857)\n",
      "\n",
      "Detailed Loss: recon 0.20690146-0.10345073 | disen 0.59521788-0.00000000 | temporal 0.58378869-0.29189435 | total 0.39534509\n",
      "Epoch 33, loss: 0.39534509-(best 0.39487857)\n",
      "\n",
      "Detailed Loss: recon 0.20835087-0.10417543 | disen 0.59536338-0.00000000 | temporal 0.58380944-0.29190472 | total 0.39608014\n",
      "Epoch 34, loss: 0.39608014-(best 0.39487857)\n",
      "\n",
      "Detailed Loss: recon 0.20540731-0.10270365 | disen 0.59537560-0.00000000 | temporal 0.58371896-0.29185948 | total 0.39456314\n",
      "Epoch 35, loss: 0.39456314-(best 0.39456314)\n",
      "\n",
      "Detailed Loss: recon 0.20741217-0.10370608 | disen 0.59589791-0.00000000 | temporal 0.58375007-0.29187503 | total 0.39558113\n",
      "Epoch 36, loss: 0.39558113-(best 0.39456314)\n",
      "\n",
      "Detailed Loss: recon 0.20744589-0.10372294 | disen 0.59512377-0.00000000 | temporal 0.58371192-0.29185596 | total 0.39557892\n",
      "Epoch 37, loss: 0.39557892-(best 0.39456314)\n",
      "\n",
      "Detailed Loss: recon 0.20763221-0.10381611 | disen 0.59502977-0.00000000 | temporal 0.58373946-0.29186973 | total 0.39568585\n",
      "Epoch 38, loss: 0.39568585-(best 0.39456314)\n",
      "\n",
      "Detailed Loss: recon 0.20674253-0.10337126 | disen 0.59559089-0.00000000 | temporal 0.58373201-0.29186600 | total 0.39523727\n",
      "Epoch 39, loss: 0.39523727-(best 0.39456314)\n",
      "\n",
      "Detailed Loss: recon 0.20402527-0.10201263 | disen 0.59499371-0.00000000 | temporal 0.58369261-0.29184631 | total 0.39385894\n",
      "Epoch 40, loss: 0.39385894-(best 0.39385894)\n",
      "\n",
      "Detailed Loss: recon 0.20537600-0.10268800 | disen 0.59574401-0.00000000 | temporal 0.58364886-0.29182443 | total 0.39451241\n",
      "Epoch 41, loss: 0.39451241-(best 0.39385894)\n",
      "\n",
      "Detailed Loss: recon 0.20426625-0.10213313 | disen 0.59602678-0.00000000 | temporal 0.58368301-0.29184151 | total 0.39397463\n",
      "Epoch 42, loss: 0.39397463-(best 0.39385894)\n",
      "\n",
      "Detailed Loss: recon 0.20456809-0.10228404 | disen 0.59619677-0.00000000 | temporal 0.58364785-0.29182392 | total 0.39410797\n",
      "Epoch 43, loss: 0.39410797-(best 0.39385894)\n",
      "\n",
      "Detailed Loss: recon 0.20480198-0.10240099 | disen 0.59592605-0.00000000 | temporal 0.58363706-0.29181853 | total 0.39421952\n",
      "Epoch 44, loss: 0.39421952-(best 0.39385894)\n",
      "\n",
      "Detailed Loss: recon 0.20374076-0.10187038 | disen 0.59609020-0.00000000 | temporal 0.58360106-0.29180053 | total 0.39367092\n",
      "Epoch 45, loss: 0.39367092-(best 0.39367092)\n",
      "\n",
      "Detailed Loss: recon 0.20328800-0.10164400 | disen 0.59613854-0.00000000 | temporal 0.58353823-0.29176912 | total 0.39341313\n",
      "Epoch 46, loss: 0.39341313-(best 0.39341313)\n",
      "\n",
      "Detailed Loss: recon 0.20446280-0.10223140 | disen 0.59681159-0.00000000 | temporal 0.58344042-0.29172021 | total 0.39395159\n",
      "Epoch 47, loss: 0.39395159-(best 0.39341313)\n",
      "\n",
      "Detailed Loss: recon 0.20413010-0.10206505 | disen 0.59635913-0.00000000 | temporal 0.58339441-0.29169720 | total 0.39376226\n",
      "Epoch 48, loss: 0.39376226-(best 0.39341313)\n",
      "\n",
      "Detailed Loss: recon 0.20313983-0.10156991 | disen 0.59552312-0.00000000 | temporal 0.58336252-0.29168126 | total 0.39325118\n",
      "Epoch 49, loss: 0.39325118-(best 0.39325118)\n",
      "\n",
      "Detailed Loss: recon 0.20163225-0.10081612 | disen 0.59598374-0.00000000 | temporal 0.58332950-0.29166475 | total 0.39248088\n",
      "Epoch 50, loss: 0.39248088-(best 0.39248088)\n",
      "\n",
      ">> Reached final epoch. Loading best model...\n",
      "TRAINING XGB PREDICTOR\n",
      ">> Ending training!\n",
      "\n",
      "-----------------------\n",
      "TIME COST: 5.023469924926758 s\n",
      "Best Val: REC 59.07 PRE 52.21 MF1 68.50 AUC 78.19 TP 531 FP 486 TN 1891 FN 368\n",
      "-----------------------\n",
      "Generating additional positive data from adversary...\n",
      "Generating additional negative data by duplicating random nodes...\n",
      "\n",
      "ASSIGNED GRAPH FOR EVALUATION: 12932 {1: 2822, 0: 10110} Nodes, 1276088 Edges\n",
      "\n",
      "-------------------\n",
      "STARTING EVALUATION\n",
      "-------------------\n",
      "\n",
      "Dataset - Overall: REC 62.58 PRE 50.30 MF1 70.72 AUC 82.07 TP 1766 FP 1745 TN 8365 FN 1056 | 12932 {1: 2822, 0: 10110}\n",
      "Dataset - Train: REC 73.52 PRE 57.38 MF1 74.13 AUC 85.16 TP 991 FP 736 TN 2829 FN 357 | 4913 {1: 1348, 0: 3565}\n",
      "Dataset - Val: REC 64.85 PRE 50.22 MF1 68.35 AUC 78.58 TP 583 FP 578 TN 1799 FN 316 | 3276 {1: 899, 0: 2377}\n",
      "Dataset - Test: REC 33.39 PRE 30.82 MF1 61.12 AUC 74.81 TP 192 FP 431 TN 3737 FN 383 | 4743 {0: 4168, 1: 575}\n",
      "PREDICTION STATUS - {'1-True': 2439, '0-False': 3737, '1-False': 383, '0-True': 6373}\n",
      "    >> 383 positive nodes left unpredicted...\n",
      "    >> 242 OG positive nodes left unpredicted...\n",
      "-----------------------\n",
      "PREDICTION RESULT - ROUNDS\n",
      "Dataset - Round 0: REC 67.66 PRE 49.52 MF1 70.95 AUC 82.33 TP 885 FP 902 TN 3669 FN 423 | 5879 {1: 1308, 0: 4571}\n",
      "Dataset - Round 1: REC 54.69 PRE 49.65 MF1 68.90 AUC 76.55 TP 70 FP 71 TN 388 FN 58 | 587 {1: 128, 0: 459}\n",
      "Dataset - Round 2: REC 28.91 PRE 30.08 MF1 55.15 AUC 65.65 TP 37 FP 86 TN 373 FN 91 | 587 {1: 128, 0: 459}\n",
      "-----------------------\n",
      "PREDICTION RESULT - SEEDS\n",
      "Seeds - Current: REC 32.03 PRE 34.75 MF1 57.83 AUC 65.19 TP 41 FP 77 TN 382 FN 87 | 587 {1: 128, 0: 459}\n",
      "Seeds - Prev: REC 0.00 PRE 0.00 MF1 39.36 AUC 65.19 TP 0 FP 78 TN 381 FN 128 | 587 {1: 128, 0: 459}\n",
      "Selecting data as budgeted ground truth for next round...\n",
      "The file ../checkpoint/working_model_file_1730091678.064546_epoch.py does not exist\n",
      "The file ../checkpoint/working_model_file_1730091678.064546_round-M.json does not exist\n",
      "The file ../checkpoint/working_model_file_1730091678.064546_round-T.json does not exist\n",
      "The file ../checkpoint/working_model_file_1730091678.064546_round.pt does not exist\n",
      "================\n",
      "++++++++++++++++\n",
      "TRIAL NUMBER 1\n",
      "++++++++++++++++\n",
      "================\n",
      "  > Rereading graph data...\n",
      "  > Initializing multiround object...\n",
      "11758 1049758\n",
      "\n",
      "=========================\n",
      "STARTING ADVER EXPERIMENT\n",
      "=========================\n",
      "MAIN CONFIG: [('device', 'cuda:0'), ('exp_type', 'ADVER'), ('full_oracle', False), ('node_expiration_time', 1), ('round_budget_neg', 0), ('round_budget_pos', 0), ('round_new_neg', 459), ('round_new_pos', 128), ('round_num', 3), ('save_df', True), ('save_embedding', False), ('save_temp_sne', False), ('setting_type', 'INDUCTIVE'), ('task_type', 'NODE'), ('verbose', 4)]\n",
      "MODEL CONFIG: [('act_name', 'LeakyReLU'), ('addon_internal_agg', 'OR'), ('addon_name', 'NONE'), ('addon_perc', 0.05), ('addon_round_window', 7), ('alpha', 0), ('att_heads', 2), ('beta', 1), ('boost_agg_backbone', <class 'models.benchmarks_supervised.booster.GIN_noparam'>), ('boost_metric', <function average_precision_score at 0x000001C8520A5760>), ('boost_predictor', <class 'xgboost.sklearn.XGBClassifier'>), ('device', 'cuda:0'), ('drop_rate', 0), ('dropout_rate', 0.1), ('early_stopping', 75), ('embed_type', 'temporal'), ('etypes', ['none']), ('gamma', 1), ('h_feats', 64), ('k', 1), ('lambda1', 1), ('lambda2', 1), ('lambda_', 1), ('loss_sample', True), ('loss_sample_ratio', 0.1), ('loss_type', 'ndist'), ('mlp_feats', 64), ('mlp_layers', 2), ('model_name', 'XGB-SP'), ('n_estimator', 500), ('norm_name', 'layer'), ('num_epoch', 100), ('num_layers', 2), ('num_round_epoch', 50), ('round_window', 7), ('temporal_agg', 'mean_final'), ('tloss_type', 'normal'), ('training_type', 'round'), ('verbose', 4)]\n",
      "STRAT CONFIG: [('augment_name', 'REAGE'), ('augment_neg_ratio', 0.09772070079945569), ('augment_pos_ratio', 0.02727929920054431), ('augment_prob', 0.5), ('augment_round_split', 8), ('device', 'cuda:0'), ('pseudo_name', 'NONE'), ('pseudo_new_pos_only', False), ('pseudo_skip_initial', False), ('verbose', 4)]\n",
      "TRAIN CONFIG: [('ce_weight', 2.644658753709199), ('device', 'cuda:0'), ('dset_name', 'tolokers_bid'), ('early_stopping', 75), ('learning_rate', 0.01), ('loss', <function cross_entropy at 0x000001C84AF6F880>), ('loss_gamma', 5), ('num_epoch', 100), ('num_round_epoch', 50), ('optimizer', <class 'torch.optim.adam.Adam'>), ('random_state', 7777777), ('ratio_initial', 0.5), ('ratio_test', 0.2), ('ratio_train', 0.6), ('ratio_val', 0.2), ('round_reset_model', False), ('round_train_list', []), ('stuck_stopping', 50), ('stuck_threshold', 0), ('train_max_retry', 10), ('verbose', 4)]\n",
      "ADVER CONFIG: [('adver_choose_name', 'GREEDY'), ('adver_conn_coef', 0.1), ('adver_feat_coef', 1), ('adver_mod_name', 'REPLAY'), ('device', 'cuda:0'), ('verbose', 4)]\n",
      "\n",
      "INITIAL GRAPH: 11758 {1: 2566, 0: 9192} Nodes, 1049758 Edges\n",
      "=========================\n",
      "        \n",
      ">> Initializing graph...\n",
      "MAIN GRAPH: 11758 {1: 2566, 0: 9192} Nodes, 1049758 Edges\n",
      "\n",
      ">> Initializing experiment type-specific modules:\n",
      "  >> Initializing adversarial modules...\n",
      ">> Initializing task-specific modules:\n",
      ">> Initializing classifier model...\n",
      ">> Initializing other strategies...\n",
      "\n",
      "FINISH INITIALIZATION\n",
      "=====================\n",
      "        \n",
      "  > Starting round 0...\n",
      "\n",
      "+++++++++++++++++\n",
      "STARTING ROUND 0!\n",
      "+++++++++++++++++\n",
      "        \n",
      "ASSIGNED GRAPH FOR TRAINING: 5879 {0: 4555, 1: 1324} Nodes, 262155 Edges\n",
      "\n",
      ">> Splitting to train-validation set...\n",
      "Alotting train-val-test split for round 0\n",
      "Initial pool: 0, Prediction pool: 0, Budget pool: 0, Full pool: 5879\n",
      "Full graph size: 5879, Training: 3527, Val: 2352, Test: 5879\n",
      "    >> INITIAL DATA SPLIT: 3527 train rows ({0: 2733, 1: 794}) | 2352 val rows ({0: 1822, 1: 530}) | 0 test rows ({})\n",
      "    >> AUGMENTED DATA SPLIT: 3527 train rows ({0: 2733, 1: 794}) | 2352 val rows ({0: 1822, 1: 530}) | 0 test rows ({})\n",
      "    >> Updated cross-entropy weight to 3.4420654911838793...\n",
      "\n",
      "-----------------------\n",
      "STARTING MODEL TRAININGps_l\n",
      "-----------------------\n",
      "\n",
      "TRAINING NODE EMBEDDER\n",
      "Detailed Loss: recon 0.74425828-0.37212914 | disen 0.69085145-0.00000000 | temporal 0.64392895-0.32196447 | total 0.69409359\n",
      "Epoch 1, loss: 0.69409359-(best 0.69409359)\n",
      "\n",
      "Detailed Loss: recon 0.41552186-0.20776093 | disen 0.71724170-0.00000000 | temporal 0.65862411-0.32931206 | total 0.53707302\n",
      "Epoch 2, loss: 0.53707302-(best 0.53707302)\n",
      "\n",
      "Detailed Loss: recon 0.37737751-0.18868876 | disen 0.71831763-0.00000000 | temporal 0.66237098-0.33118549 | total 0.51987422\n",
      "Epoch 3, loss: 0.51987422-(best 0.51987422)\n",
      "\n",
      "Detailed Loss: recon 0.37442908-0.18721454 | disen 0.72831047-0.00000000 | temporal 0.66293091-0.33146545 | total 0.51867998\n",
      "Epoch 4, loss: 0.51867998-(best 0.51867998)\n",
      "\n",
      "Detailed Loss: recon 0.36744279-0.18372139 | disen 0.72557539-0.00000000 | temporal 0.66359776-0.33179888 | total 0.51552027\n",
      "Epoch 5, loss: 0.51552027-(best 0.51552027)\n",
      "\n",
      "Detailed Loss: recon 0.35035366-0.17517683 | disen 0.73119164-0.00000000 | temporal 0.66423309-0.33211654 | total 0.50729334\n",
      "Epoch 6, loss: 0.50729334-(best 0.50729334)\n",
      "\n",
      "Detailed Loss: recon 0.34101433-0.17050716 | disen 0.73335147-0.00000000 | temporal 0.66427898-0.33213949 | total 0.50264668\n",
      "Epoch 7, loss: 0.50264668-(best 0.50264668)\n",
      "\n",
      "Detailed Loss: recon 0.32410765-0.16205382 | disen 0.73472691-0.00000000 | temporal 0.66367209-0.33183604 | total 0.49388987\n",
      "Epoch 8, loss: 0.49388987-(best 0.49388987)\n",
      "\n",
      "Detailed Loss: recon 0.30178982-0.15089491 | disen 0.73338532-0.00000000 | temporal 0.66313004-0.33156502 | total 0.48245993\n",
      "Epoch 9, loss: 0.48245993-(best 0.48245993)\n",
      "\n",
      "Detailed Loss: recon 0.29280329-0.14640164 | disen 0.73257881-0.00000000 | temporal 0.66234583-0.33117291 | total 0.47757456\n",
      "Epoch 10, loss: 0.47757456-(best 0.47757456)\n",
      "\n",
      "Detailed Loss: recon 0.30013067-0.15006533 | disen 0.73385465-0.00000000 | temporal 0.66146040-0.33073020 | total 0.48079553\n",
      "Epoch 11, loss: 0.48079553-(best 0.47757456)\n",
      "\n",
      "Detailed Loss: recon 0.29944313-0.14972156 | disen 0.73273742-0.00000000 | temporal 0.66081417-0.33040708 | total 0.48012865\n",
      "Epoch 12, loss: 0.48012865-(best 0.47757456)\n",
      "\n",
      "Detailed Loss: recon 0.30204892-0.15102446 | disen 0.73447132-0.00000000 | temporal 0.66067469-0.33033735 | total 0.48136181\n",
      "Epoch 13, loss: 0.48136181-(best 0.47757456)\n",
      "\n",
      "Detailed Loss: recon 0.29271662-0.14635831 | disen 0.73161256-0.00000000 | temporal 0.66086215-0.33043107 | total 0.47678939\n",
      "Epoch 14, loss: 0.47678939-(best 0.47678939)\n",
      "\n",
      "Detailed Loss: recon 0.28898615-0.14449307 | disen 0.73444456-0.00000000 | temporal 0.66131788-0.33065894 | total 0.47515202\n",
      "Epoch 15, loss: 0.47515202-(best 0.47515202)\n",
      "\n",
      "Detailed Loss: recon 0.28442103-0.14221051 | disen 0.73148519-0.00000000 | temporal 0.66133410-0.33066705 | total 0.47287756\n",
      "Epoch 16, loss: 0.47287756-(best 0.47287756)\n",
      "\n",
      "Detailed Loss: recon 0.27315611-0.13657805 | disen 0.73231971-0.00000000 | temporal 0.66245168-0.33122584 | total 0.46780390\n",
      "Epoch 17, loss: 0.46780390-(best 0.46780390)\n",
      "\n",
      "Detailed Loss: recon 0.27151281-0.13575640 | disen 0.73323131-0.00000000 | temporal 0.66279215-0.33139607 | total 0.46715248\n",
      "Epoch 18, loss: 0.46715248-(best 0.46715248)\n",
      "\n",
      "Detailed Loss: recon 0.26557124-0.13278562 | disen 0.73149902-0.00000000 | temporal 0.66324091-0.33162045 | total 0.46440607\n",
      "Epoch 19, loss: 0.46440607-(best 0.46440607)\n",
      "\n",
      "Detailed Loss: recon 0.26410449-0.13205224 | disen 0.73224622-0.00000000 | temporal 0.66342515-0.33171257 | total 0.46376482\n",
      "Epoch 20, loss: 0.46376482-(best 0.46376482)\n",
      "\n",
      "Detailed Loss: recon 0.26623726-0.13311863 | disen 0.73023021-0.00000000 | temporal 0.66383821-0.33191910 | total 0.46503773\n",
      "Epoch 21, loss: 0.46503773-(best 0.46376482)\n",
      "\n",
      "Detailed Loss: recon 0.26252347-0.13126174 | disen 0.73103189-0.00000000 | temporal 0.66356009-0.33178005 | total 0.46304178\n",
      "Epoch 22, loss: 0.46304178-(best 0.46304178)\n",
      "\n",
      "Detailed Loss: recon 0.25654840-0.12827420 | disen 0.73245740-0.00000000 | temporal 0.66364974-0.33182487 | total 0.46009907\n",
      "Epoch 23, loss: 0.46009907-(best 0.46009907)\n",
      "\n",
      "Detailed Loss: recon 0.25650221-0.12825111 | disen 0.72961324-0.00000000 | temporal 0.66351938-0.33175969 | total 0.46001080\n",
      "Epoch 24, loss: 0.46001080-(best 0.46001080)\n",
      "\n",
      "Detailed Loss: recon 0.25702581-0.12851290 | disen 0.73029709-0.00000000 | temporal 0.66348225-0.33174112 | total 0.46025401\n",
      "Epoch 25, loss: 0.46025401-(best 0.46001080)\n",
      "\n",
      "Detailed Loss: recon 0.25019443-0.12509722 | disen 0.73251975-0.00000000 | temporal 0.66302347-0.33151174 | total 0.45660895\n",
      "Epoch 26, loss: 0.45660895-(best 0.45660895)\n",
      "\n",
      "Detailed Loss: recon 0.25310796-0.12655398 | disen 0.73453677-0.00000000 | temporal 0.66301823-0.33150911 | total 0.45806310\n",
      "Epoch 27, loss: 0.45806310-(best 0.45660895)\n",
      "\n",
      "Detailed Loss: recon 0.24831221-0.12415610 | disen 0.72982872-0.00000000 | temporal 0.66310334-0.33155167 | total 0.45570779\n",
      "Epoch 28, loss: 0.45570779-(best 0.45570779)\n",
      "\n",
      "Detailed Loss: recon 0.24889587-0.12444793 | disen 0.73039556-0.00000000 | temporal 0.66319662-0.33159831 | total 0.45604625\n",
      "Epoch 29, loss: 0.45604625-(best 0.45570779)\n",
      "\n",
      "Detailed Loss: recon 0.24140272-0.12070136 | disen 0.73294806-0.00000000 | temporal 0.66348606-0.33174303 | total 0.45244437\n",
      "Epoch 30, loss: 0.45244437-(best 0.45244437)\n",
      "\n",
      "Detailed Loss: recon 0.23685315-0.11842658 | disen 0.73273832-0.00000000 | temporal 0.66372770-0.33186385 | total 0.45029044\n",
      "Epoch 31, loss: 0.45029044-(best 0.45029044)\n",
      "\n",
      "Detailed Loss: recon 0.24107735-0.12053867 | disen 0.73560405-0.00000000 | temporal 0.66380471-0.33190235 | total 0.45244104\n",
      "Epoch 32, loss: 0.45244104-(best 0.45029044)\n",
      "\n",
      "Detailed Loss: recon 0.23949218-0.11974609 | disen 0.73172224-0.00000000 | temporal 0.66369760-0.33184880 | total 0.45159489\n",
      "Epoch 33, loss: 0.45159489-(best 0.45029044)\n",
      "\n",
      "Detailed Loss: recon 0.23258522-0.11629261 | disen 0.73140299-0.00000000 | temporal 0.66365343-0.33182672 | total 0.44811934\n",
      "Epoch 34, loss: 0.44811934-(best 0.44811934)\n",
      "\n",
      "Detailed Loss: recon 0.23456943-0.11728472 | disen 0.73274076-0.00000000 | temporal 0.66372651-0.33186325 | total 0.44914797\n",
      "Epoch 35, loss: 0.44914797-(best 0.44811934)\n",
      "\n",
      "Detailed Loss: recon 0.23049319-0.11524659 | disen 0.73143762-0.00000000 | temporal 0.66360646-0.33180323 | total 0.44704983\n",
      "Epoch 36, loss: 0.44704983-(best 0.44704983)\n",
      "\n",
      "Detailed Loss: recon 0.23531204-0.11765602 | disen 0.73357153-0.00000000 | temporal 0.66388112-0.33194056 | total 0.44959658\n",
      "Epoch 37, loss: 0.44959658-(best 0.44704983)\n",
      "\n",
      "Detailed Loss: recon 0.23013461-0.11506730 | disen 0.73416018-0.00000000 | temporal 0.66394430-0.33197215 | total 0.44703946\n",
      "Epoch 38, loss: 0.44703946-(best 0.44703946)\n",
      "\n",
      "Detailed Loss: recon 0.22719234-0.11359617 | disen 0.73156315-0.00000000 | temporal 0.66409487-0.33204743 | total 0.44564360\n",
      "Epoch 39, loss: 0.44564360-(best 0.44564360)\n",
      "\n",
      "Detailed Loss: recon 0.22622794-0.11311397 | disen 0.73287714-0.00000000 | temporal 0.66409558-0.33204779 | total 0.44516176\n",
      "Epoch 40, loss: 0.44516176-(best 0.44516176)\n",
      "\n",
      "Detailed Loss: recon 0.22393158-0.11196579 | disen 0.72997916-0.00000000 | temporal 0.66433328-0.33216664 | total 0.44413245\n",
      "Epoch 41, loss: 0.44413245-(best 0.44413245)\n",
      "\n",
      "Detailed Loss: recon 0.22554533-0.11277267 | disen 0.73241878-0.00000000 | temporal 0.66405284-0.33202642 | total 0.44479910\n",
      "Epoch 42, loss: 0.44479910-(best 0.44413245)\n",
      "\n",
      "Detailed Loss: recon 0.22177266-0.11088633 | disen 0.73409891-0.00000000 | temporal 0.66394794-0.33197397 | total 0.44286031\n",
      "Epoch 43, loss: 0.44286031-(best 0.44286031)\n",
      "\n",
      "Detailed Loss: recon 0.21964943-0.10982472 | disen 0.73176885-0.00000000 | temporal 0.66389066-0.33194533 | total 0.44177005\n",
      "Epoch 44, loss: 0.44177005-(best 0.44177005)\n",
      "\n",
      "Detailed Loss: recon 0.21979722-0.10989861 | disen 0.73176271-0.00000000 | temporal 0.66389155-0.33194578 | total 0.44184440\n",
      "Epoch 45, loss: 0.44184440-(best 0.44177005)\n",
      "\n",
      "Detailed Loss: recon 0.22105154-0.11052577 | disen 0.73034048-0.00000000 | temporal 0.66377372-0.33188686 | total 0.44241261\n",
      "Epoch 46, loss: 0.44241261-(best 0.44177005)\n",
      "\n",
      "Detailed Loss: recon 0.21992883-0.10996442 | disen 0.72904146-0.00000000 | temporal 0.66373026-0.33186513 | total 0.44182956\n",
      "Epoch 47, loss: 0.44182956-(best 0.44177005)\n",
      "\n",
      "Detailed Loss: recon 0.21724206-0.10862103 | disen 0.73072231-0.00000000 | temporal 0.66390717-0.33195359 | total 0.44057462\n",
      "Epoch 48, loss: 0.44057462-(best 0.44057462)\n",
      "\n",
      "Detailed Loss: recon 0.21693824-0.10846912 | disen 0.73200381-0.00000000 | temporal 0.66406351-0.33203176 | total 0.44050089\n",
      "Epoch 49, loss: 0.44050089-(best 0.44050089)\n",
      "\n",
      "Detailed Loss: recon 0.21933626-0.10966813 | disen 0.73184431-0.00000000 | temporal 0.66400045-0.33200023 | total 0.44166836\n",
      "Epoch 50, loss: 0.44166836-(best 0.44050089)\n",
      "\n",
      "Detailed Loss: recon 0.21331042-0.10665521 | disen 0.73210245-0.00000000 | temporal 0.66431552-0.33215776 | total 0.43881297\n",
      "Epoch 51, loss: 0.43881297-(best 0.43881297)\n",
      "\n",
      "Detailed Loss: recon 0.21167687-0.10583843 | disen 0.73306811-0.00000000 | temporal 0.66441530-0.33220765 | total 0.43804610\n",
      "Epoch 52, loss: 0.43804610-(best 0.43804610)\n",
      "\n",
      "Detailed Loss: recon 0.21332315-0.10666157 | disen 0.73197675-0.00000000 | temporal 0.66420680-0.33210340 | total 0.43876499\n",
      "Epoch 53, loss: 0.43876499-(best 0.43804610)\n",
      "\n",
      "Detailed Loss: recon 0.21215725-0.10607862 | disen 0.73166722-0.00000000 | temporal 0.66383219-0.33191609 | total 0.43799472\n",
      "Epoch 54, loss: 0.43799472-(best 0.43799472)\n",
      "\n",
      "Detailed Loss: recon 0.21148479-0.10574239 | disen 0.72932035-0.00000000 | temporal 0.66403472-0.33201736 | total 0.43775976\n",
      "Epoch 55, loss: 0.43775976-(best 0.43775976)\n",
      "\n",
      "Detailed Loss: recon 0.20640212-0.10320106 | disen 0.73355472-0.00000000 | temporal 0.66400695-0.33200347 | total 0.43520454\n",
      "Epoch 56, loss: 0.43520454-(best 0.43520454)\n",
      "\n",
      "Detailed Loss: recon 0.21200234-0.10600117 | disen 0.73018837-0.00000000 | temporal 0.66400284-0.33200142 | total 0.43800259\n",
      "Epoch 57, loss: 0.43800259-(best 0.43520454)\n",
      "\n",
      "Detailed Loss: recon 0.20680366-0.10340183 | disen 0.73326802-0.00000000 | temporal 0.66387069-0.33193535 | total 0.43533719\n",
      "Epoch 58, loss: 0.43533719-(best 0.43520454)\n",
      "\n",
      "Detailed Loss: recon 0.20644830-0.10322415 | disen 0.73215485-0.00000000 | temporal 0.66378754-0.33189377 | total 0.43511793\n",
      "Epoch 59, loss: 0.43511793-(best 0.43511793)\n",
      "\n",
      "Detailed Loss: recon 0.20674913-0.10337456 | disen 0.73306870-0.00000000 | temporal 0.66401154-0.33200577 | total 0.43538034\n",
      "Epoch 60, loss: 0.43538034-(best 0.43511793)\n",
      "\n",
      "Detailed Loss: recon 0.20803587-0.10401794 | disen 0.73086774-0.00000000 | temporal 0.66394776-0.33197388 | total 0.43599182\n",
      "Epoch 61, loss: 0.43599182-(best 0.43511793)\n",
      "\n",
      "Detailed Loss: recon 0.20692602-0.10346301 | disen 0.73268628-0.00000000 | temporal 0.66401660-0.33200830 | total 0.43547130\n",
      "Epoch 62, loss: 0.43547130-(best 0.43511793)\n",
      "\n",
      "Detailed Loss: recon 0.20493183-0.10246591 | disen 0.73460704-0.00000000 | temporal 0.66379321-0.33189660 | total 0.43436253\n",
      "Epoch 63, loss: 0.43436253-(best 0.43436253)\n",
      "\n",
      "Detailed Loss: recon 0.20615348-0.10307674 | disen 0.73248708-0.00000000 | temporal 0.66376168-0.33188084 | total 0.43495756\n",
      "Epoch 64, loss: 0.43495756-(best 0.43436253)\n",
      "\n",
      "Detailed Loss: recon 0.20315914-0.10157957 | disen 0.73121285-0.00000000 | temporal 0.66397226-0.33198613 | total 0.43356571\n",
      "Epoch 65, loss: 0.43356571-(best 0.43356571)\n",
      "\n",
      "Detailed Loss: recon 0.20399708-0.10199854 | disen 0.73416722-0.00000000 | temporal 0.66374481-0.33187240 | total 0.43387094\n",
      "Epoch 66, loss: 0.43387094-(best 0.43356571)\n",
      "\n",
      "Detailed Loss: recon 0.20229028-0.10114514 | disen 0.73306048-0.00000000 | temporal 0.66377360-0.33188680 | total 0.43303195\n",
      "Epoch 67, loss: 0.43303195-(best 0.43303195)\n",
      "\n",
      "Detailed Loss: recon 0.20229380-0.10114690 | disen 0.73220134-0.00000000 | temporal 0.66364318-0.33182159 | total 0.43296850\n",
      "Epoch 68, loss: 0.43296850-(best 0.43296850)\n",
      "\n",
      "Detailed Loss: recon 0.20052037-0.10026018 | disen 0.73679644-0.00000000 | temporal 0.66384900-0.33192450 | total 0.43218470\n",
      "Epoch 69, loss: 0.43218470-(best 0.43218470)\n",
      "\n",
      "Detailed Loss: recon 0.20100477-0.10050239 | disen 0.73166752-0.00000000 | temporal 0.66395891-0.33197945 | total 0.43248183\n",
      "Epoch 70, loss: 0.43248183-(best 0.43218470)\n",
      "\n",
      "Detailed Loss: recon 0.20171694-0.10085847 | disen 0.73447716-0.00000000 | temporal 0.66383195-0.33191597 | total 0.43277445\n",
      "Epoch 71, loss: 0.43277445-(best 0.43218470)\n",
      "\n",
      "Detailed Loss: recon 0.20090157-0.10045078 | disen 0.73358834-0.00000000 | temporal 0.66370380-0.33185190 | total 0.43230268\n",
      "Epoch 72, loss: 0.43230268-(best 0.43218470)\n",
      "\n",
      "Detailed Loss: recon 0.19804822-0.09902411 | disen 0.73261178-0.00000000 | temporal 0.66378486-0.33189243 | total 0.43091655\n",
      "Epoch 73, loss: 0.43091655-(best 0.43091655)\n",
      "\n",
      "Detailed Loss: recon 0.20149074-0.10074537 | disen 0.73594737-0.00000000 | temporal 0.66371006-0.33185503 | total 0.43260041\n",
      "Epoch 74, loss: 0.43260041-(best 0.43091655)\n",
      "\n",
      "Detailed Loss: recon 0.19887447-0.09943724 | disen 0.73642892-0.00000000 | temporal 0.66366583-0.33183292 | total 0.43127015\n",
      "Epoch 75, loss: 0.43127015-(best 0.43091655)\n",
      "\n",
      "Detailed Loss: recon 0.19921838-0.09960919 | disen 0.73580664-0.00000000 | temporal 0.66385520-0.33192760 | total 0.43153679\n",
      "Epoch 76, loss: 0.43153679-(best 0.43091655)\n",
      "\n",
      "Detailed Loss: recon 0.19745088-0.09872544 | disen 0.73421395-0.00000000 | temporal 0.66384619-0.33192310 | total 0.43064854\n",
      "Epoch 77, loss: 0.43064854-(best 0.43064854)\n",
      "\n",
      "Detailed Loss: recon 0.19980904-0.09990452 | disen 0.73490143-0.00000000 | temporal 0.66386843-0.33193421 | total 0.43183875\n",
      "Epoch 78, loss: 0.43183875-(best 0.43064854)\n",
      "\n",
      "Detailed Loss: recon 0.20041808-0.10020904 | disen 0.73382926-0.00000000 | temporal 0.66350365-0.33175182 | total 0.43196088\n",
      "Epoch 79, loss: 0.43196088-(best 0.43064854)\n",
      "\n",
      "Detailed Loss: recon 0.19823079-0.09911539 | disen 0.73458111-0.00000000 | temporal 0.66371357-0.33185679 | total 0.43097219\n",
      "Epoch 80, loss: 0.43097219-(best 0.43064854)\n",
      "\n",
      "Detailed Loss: recon 0.19920902-0.09960451 | disen 0.73623157-0.00000000 | temporal 0.66385871-0.33192936 | total 0.43153387\n",
      "Epoch 81, loss: 0.43153387-(best 0.43064854)\n",
      "\n",
      "Detailed Loss: recon 0.19851610-0.09925805 | disen 0.73251814-0.00000000 | temporal 0.66328418-0.33164209 | total 0.43090016\n",
      "Epoch 82, loss: 0.43090016-(best 0.43064854)\n",
      "\n",
      "Detailed Loss: recon 0.19535235-0.09767617 | disen 0.73605466-0.00000000 | temporal 0.66339046-0.33169523 | total 0.42937142\n",
      "Epoch 83, loss: 0.42937142-(best 0.42937142)\n",
      "\n",
      "Detailed Loss: recon 0.19786821-0.09893411 | disen 0.73671359-0.00000000 | temporal 0.66362345-0.33181173 | total 0.43074584\n",
      "Epoch 84, loss: 0.43074584-(best 0.42937142)\n",
      "\n",
      "Detailed Loss: recon 0.19891278-0.09945639 | disen 0.73774886-0.00000000 | temporal 0.66324157-0.33162078 | total 0.43107718\n",
      "Epoch 85, loss: 0.43107718-(best 0.42937142)\n",
      "\n",
      "Detailed Loss: recon 0.19881639-0.09940819 | disen 0.73560905-0.00000000 | temporal 0.66336483-0.33168241 | total 0.43109059\n",
      "Epoch 86, loss: 0.43109059-(best 0.42937142)\n",
      "\n",
      "Detailed Loss: recon 0.19625458-0.09812729 | disen 0.73617649-0.00000000 | temporal 0.66327995-0.33163998 | total 0.42976725\n",
      "Epoch 87, loss: 0.42976725-(best 0.42937142)\n",
      "\n",
      "Detailed Loss: recon 0.19467269-0.09733634 | disen 0.73662412-0.00000000 | temporal 0.66287971-0.33143985 | total 0.42877620\n",
      "Epoch 88, loss: 0.42877620-(best 0.42877620)\n",
      "\n",
      "Detailed Loss: recon 0.19369814-0.09684907 | disen 0.73645592-0.00000000 | temporal 0.66327208-0.33163604 | total 0.42848510\n",
      "Epoch 89, loss: 0.42848510-(best 0.42848510)\n",
      "\n",
      "Detailed Loss: recon 0.19741134-0.09870567 | disen 0.73469830-0.00000000 | temporal 0.66339839-0.33169919 | total 0.43040487\n",
      "Epoch 90, loss: 0.43040487-(best 0.42848510)\n",
      "\n",
      "Detailed Loss: recon 0.19657525-0.09828763 | disen 0.73525208-0.00000000 | temporal 0.66298944-0.33149472 | total 0.42978233\n",
      "Epoch 91, loss: 0.42978233-(best 0.42848510)\n",
      "\n",
      "Detailed Loss: recon 0.19664088-0.09832044 | disen 0.73547828-0.00000000 | temporal 0.66281700-0.33140850 | total 0.42972893\n",
      "Epoch 92, loss: 0.42972893-(best 0.42848510)\n",
      "\n",
      "Detailed Loss: recon 0.19758150-0.09879075 | disen 0.73662984-0.00000000 | temporal 0.66349649-0.33174825 | total 0.43053901\n",
      "Epoch 93, loss: 0.43053901-(best 0.42848510)\n",
      "\n",
      "Detailed Loss: recon 0.19560558-0.09780279 | disen 0.73627239-0.00000000 | temporal 0.66308832-0.33154416 | total 0.42934695\n",
      "Epoch 94, loss: 0.42934695-(best 0.42848510)\n",
      "\n",
      "Detailed Loss: recon 0.19690715-0.09845357 | disen 0.73578930-0.00000000 | temporal 0.66289687-0.33144844 | total 0.42990202\n",
      "Epoch 95, loss: 0.42990202-(best 0.42848510)\n",
      "\n",
      "Detailed Loss: recon 0.19628769-0.09814385 | disen 0.73479521-0.00000000 | temporal 0.66326898-0.33163449 | total 0.42977834\n",
      "Epoch 96, loss: 0.42977834-(best 0.42848510)\n",
      "\n",
      "Detailed Loss: recon 0.19609484-0.09804742 | disen 0.73695612-0.00000000 | temporal 0.66342342-0.33171171 | total 0.42975914\n",
      "Epoch 97, loss: 0.42975914-(best 0.42848510)\n",
      "\n",
      "Detailed Loss: recon 0.19291642-0.09645821 | disen 0.73607206-0.00000000 | temporal 0.66352952-0.33176476 | total 0.42822295\n",
      "Epoch 98, loss: 0.42822295-(best 0.42822295)\n",
      "\n",
      "Detailed Loss: recon 0.19958296-0.09979148 | disen 0.73394871-0.00000000 | temporal 0.66283578-0.33141789 | total 0.43120939\n",
      "Epoch 99, loss: 0.43120939-(best 0.42822295)\n",
      "\n",
      "Detailed Loss: recon 0.19555435-0.09777717 | disen 0.73642665-0.00000000 | temporal 0.66321886-0.33160943 | total 0.42938662\n",
      "Epoch 100, loss: 0.42938662-(best 0.42822295)\n",
      "\n",
      ">> Reached final epoch. Loading best model...\n",
      "TRAINING XGB PREDICTOR\n",
      ">> Ending training!\n",
      "\n",
      "-----------------------\n",
      "TIME COST: 7.612291097640991 s\n",
      "Best Val: REC 65.47 PRE 48.67 MF1 69.99 AUC 81.35 TP 347 FP 366 TN 1456 FN 183\n",
      "-----------------------\n",
      "\n",
      "ASSIGNED GRAPH FOR EVALUATION: 11758 {1: 2566, 0: 9192} Nodes, 1049758 Edges\n",
      "\n",
      "-------------------\n",
      "STARTING EVALUATION\n",
      "-------------------\n",
      "\n",
      "Dataset - Overall: REC 62.86 PRE 46.51 MF1 68.70 AUC 81.05 TP 1613 FP 1855 TN 7337 FN 953 | 11758 {1: 2566, 0: 9192}\n",
      "Dataset - Train: REC 72.17 PRE 52.38 MF1 73.17 AUC 85.59 TP 573 FP 521 TN 2212 FN 221 | 3527 {0: 2733, 1: 794}\n",
      "Dataset - Val: REC 58.68 PRE 44.24 MF1 66.42 AUC 78.24 TP 311 FP 392 TN 1430 FN 219 | 2352 {0: 1822, 1: 530}\n",
      "Dataset - Test: REC 58.70 PRE 43.63 MF1 66.80 AUC 79.41 TP 729 FP 942 TN 3695 FN 513 | 5879 {1: 1242, 0: 4637}\n",
      "PREDICTION STATUS - {'1-True': 2053, '0-True': 5497, '0-False': 3695, '1-False': 513}\n",
      "    >> 513 positive nodes left unpredicted...\n",
      "    >> 513 OG positive nodes left unpredicted...\n",
      "Selecting data as budgeted ground truth for next round...\n",
      "  > Starting round 1...\n",
      "\n",
      "+++++++++++++++++\n",
      "STARTING ROUND 1!\n",
      "+++++++++++++++++\n",
      "        \n",
      "ASSIGNED GRAPH FOR TRAINING: 11758 {1: 2566, 0: 9192} Nodes, 1049758 Edges\n",
      "\n",
      ">> Splitting to train-validation set...\n",
      "Alotting train-val-test split for round 1\n",
      "Initial pool: 5879, Prediction pool: 3468, Budget pool: 0, Full pool: 7550\n",
      "Full graph size: 11758, Training: 4530, Val: 3020, Test: 11758\n",
      "    >> INITIAL DATA SPLIT: 4530 train rows ({1: 1232, 0: 3298}) | 3020 val rows ({1: 821, 0: 2199}) | 4208 test rows ({0: 3695, 1: 513})\n",
      "    >> AUGMENTED DATA SPLIT: 4530 train rows ({1: 1232, 0: 3298}) | 3020 val rows ({1: 821, 0: 2199}) | 4208 test rows ({0: 3695, 1: 513})\n",
      "    >> Updated cross-entropy weight to 2.676948051948052...\n",
      "\n",
      "-----------------------\n",
      "STARTING MODEL TRAININGps_l\n",
      "-----------------------\n",
      "\n",
      "TRAINING NODE EMBEDDER\n",
      "Detailed Loss: recon 0.18822424-0.09411212 | disen 0.59456760-0.00000000 | temporal 0.55969024-0.27984512 | total 0.37395725\n",
      "Epoch 1, loss: 0.37395725-(best 0.37395725)\n",
      "\n",
      "Detailed Loss: recon 0.45570534-0.22785267 | disen 0.60212517-0.00000000 | temporal 0.56105053-0.28052527 | total 0.50837791\n",
      "Epoch 2, loss: 0.50837791-(best 0.37395725)\n",
      "\n",
      "Detailed Loss: recon 0.20078771-0.10039385 | disen 0.59621179-0.00000000 | temporal 0.56009763-0.28004882 | total 0.38044268\n",
      "Epoch 3, loss: 0.38044268-(best 0.37395725)\n",
      "\n",
      "Detailed Loss: recon 0.21212403-0.10606202 | disen 0.59143817-0.00000000 | temporal 0.55919504-0.27959752 | total 0.38565955\n",
      "Epoch 4, loss: 0.38565955-(best 0.37395725)\n",
      "\n",
      "Detailed Loss: recon 0.25115204-0.12557602 | disen 0.58837432-0.00000000 | temporal 0.55886865-0.27943432 | total 0.40501034\n",
      "Epoch 5, loss: 0.40501034-(best 0.37395725)\n",
      "\n",
      "Detailed Loss: recon 0.24997109-0.12498555 | disen 0.58508754-0.00000000 | temporal 0.55907011-0.27953506 | total 0.40452060\n",
      "Epoch 6, loss: 0.40452060-(best 0.37395725)\n",
      "\n",
      "Detailed Loss: recon 0.21952073-0.10976037 | disen 0.58683968-0.00000000 | temporal 0.55959451-0.27979726 | total 0.38955763\n",
      "Epoch 7, loss: 0.38955763-(best 0.37395725)\n",
      "\n",
      "Detailed Loss: recon 0.20250271-0.10125136 | disen 0.58789265-0.00000000 | temporal 0.56010532-0.28005266 | total 0.38130403\n",
      "Epoch 8, loss: 0.38130403-(best 0.37395725)\n",
      "\n",
      "Detailed Loss: recon 0.20468710-0.10234355 | disen 0.58855480-0.00000000 | temporal 0.56048733-0.28024366 | total 0.38258722\n",
      "Epoch 9, loss: 0.38258722-(best 0.37395725)\n",
      "\n",
      "Detailed Loss: recon 0.21163201-0.10581601 | disen 0.58893454-0.00000000 | temporal 0.56075662-0.28037831 | total 0.38619432\n",
      "Epoch 10, loss: 0.38619432-(best 0.37395725)\n",
      "\n",
      "Detailed Loss: recon 0.22205655-0.11102828 | disen 0.58909804-0.00000000 | temporal 0.56091374-0.28045687 | total 0.39148515\n",
      "Epoch 11, loss: 0.39148515-(best 0.37395725)\n",
      "\n",
      "Detailed Loss: recon 0.22184592-0.11092296 | disen 0.58961451-0.00000000 | temporal 0.56091416-0.28045708 | total 0.39138004\n",
      "Epoch 12, loss: 0.39138004-(best 0.37395725)\n",
      "\n",
      "Detailed Loss: recon 0.21751228-0.10875614 | disen 0.58891058-0.00000000 | temporal 0.56083775-0.28041887 | total 0.38917500\n",
      "Epoch 13, loss: 0.38917500-(best 0.37395725)\n",
      "\n",
      "Detailed Loss: recon 0.21028537-0.10514268 | disen 0.58821738-0.00000000 | temporal 0.56062990-0.28031495 | total 0.38545763\n",
      "Epoch 14, loss: 0.38545763-(best 0.37395725)\n",
      "\n",
      "Detailed Loss: recon 0.20389573-0.10194787 | disen 0.58787787-0.00000000 | temporal 0.56038219-0.28019109 | total 0.38213897\n",
      "Epoch 15, loss: 0.38213897-(best 0.37395725)\n",
      "\n",
      "Detailed Loss: recon 0.20180720-0.10090360 | disen 0.58673131-0.00000000 | temporal 0.56014907-0.28007454 | total 0.38097814\n",
      "Epoch 16, loss: 0.38097814-(best 0.37395725)\n",
      "\n",
      "Detailed Loss: recon 0.20259909-0.10129955 | disen 0.58548212-0.00000000 | temporal 0.55996048-0.27998024 | total 0.38127980\n",
      "Epoch 17, loss: 0.38127980-(best 0.37395725)\n",
      "\n",
      "Detailed Loss: recon 0.20492515-0.10246257 | disen 0.58593273-0.00000000 | temporal 0.55969584-0.27984792 | total 0.38231051\n",
      "Epoch 18, loss: 0.38231051-(best 0.37395725)\n",
      "\n",
      "Detailed Loss: recon 0.20515470-0.10257735 | disen 0.58325231-0.00000000 | temporal 0.55959535-0.27979767 | total 0.38237503\n",
      "Epoch 19, loss: 0.38237503-(best 0.37395725)\n",
      "\n",
      "Detailed Loss: recon 0.20836352-0.10418176 | disen 0.58503866-0.00000000 | temporal 0.55957955-0.27978978 | total 0.38397154\n",
      "Epoch 20, loss: 0.38397154-(best 0.37395725)\n",
      "\n",
      "Detailed Loss: recon 0.20213008-0.10106504 | disen 0.58651489-0.00000000 | temporal 0.55959404-0.27979702 | total 0.38086206\n",
      "Epoch 21, loss: 0.38086206-(best 0.37395725)\n",
      "\n",
      "Detailed Loss: recon 0.19610874-0.09805437 | disen 0.58597255-0.00000000 | temporal 0.55958521-0.27979261 | total 0.37784699\n",
      "Epoch 22, loss: 0.37784699-(best 0.37395725)\n",
      "\n",
      "Detailed Loss: recon 0.19275337-0.09637669 | disen 0.58570731-0.00000000 | temporal 0.55976164-0.27988082 | total 0.37625751\n",
      "Epoch 23, loss: 0.37625751-(best 0.37395725)\n",
      "\n",
      "Detailed Loss: recon 0.19372773-0.09686387 | disen 0.58708286-0.00000000 | temporal 0.55982262-0.27991131 | total 0.37677518\n",
      "Epoch 24, loss: 0.37677518-(best 0.37395725)\n",
      "\n",
      "Detailed Loss: recon 0.19493531-0.09746765 | disen 0.58744341-0.00000000 | temporal 0.55993044-0.27996522 | total 0.37743288\n",
      "Epoch 25, loss: 0.37743288-(best 0.37395725)\n",
      "\n",
      "Detailed Loss: recon 0.19584316-0.09792158 | disen 0.58768427-0.00000000 | temporal 0.55995697-0.27997848 | total 0.37790006\n",
      "Epoch 26, loss: 0.37790006-(best 0.37395725)\n",
      "\n",
      "Detailed Loss: recon 0.19423166-0.09711583 | disen 0.58834743-0.00000000 | temporal 0.55991447-0.27995723 | total 0.37707305\n",
      "Epoch 27, loss: 0.37707305-(best 0.37395725)\n",
      "\n",
      "Detailed Loss: recon 0.19275251-0.09637626 | disen 0.58784533-0.00000000 | temporal 0.55988050-0.27994025 | total 0.37631649\n",
      "Epoch 28, loss: 0.37631649-(best 0.37395725)\n",
      "\n",
      "Detailed Loss: recon 0.18907106-0.09453553 | disen 0.58817351-0.00000000 | temporal 0.55978167-0.27989084 | total 0.37442636\n",
      "Epoch 29, loss: 0.37442636-(best 0.37395725)\n",
      "\n",
      "Detailed Loss: recon 0.19141898-0.09570949 | disen 0.58813906-0.00000000 | temporal 0.55961382-0.27980691 | total 0.37551641\n",
      "Epoch 30, loss: 0.37551641-(best 0.37395725)\n",
      "\n",
      "Detailed Loss: recon 0.18922743-0.09461372 | disen 0.58920622-0.00000000 | temporal 0.55955821-0.27977911 | total 0.37439281\n",
      "Epoch 31, loss: 0.37439281-(best 0.37395725)\n",
      "\n",
      "Detailed Loss: recon 0.18920265-0.09460133 | disen 0.58883154-0.00000000 | temporal 0.55946672-0.27973336 | total 0.37433469\n",
      "Epoch 32, loss: 0.37433469-(best 0.37395725)\n",
      "\n",
      "Detailed Loss: recon 0.18881834-0.09440917 | disen 0.58886409-0.00000000 | temporal 0.55947495-0.27973747 | total 0.37414664\n",
      "Epoch 33, loss: 0.37414664-(best 0.37395725)\n",
      "\n",
      "Detailed Loss: recon 0.18748567-0.09374283 | disen 0.58927232-0.00000000 | temporal 0.55953306-0.27976653 | total 0.37350935\n",
      "Epoch 34, loss: 0.37350935-(best 0.37350935)\n",
      "\n",
      "Detailed Loss: recon 0.18755099-0.09377550 | disen 0.58938134-0.00000000 | temporal 0.55956846-0.27978423 | total 0.37355971\n",
      "Epoch 35, loss: 0.37355971-(best 0.37350935)\n",
      "\n",
      "Detailed Loss: recon 0.18613410-0.09306705 | disen 0.59020311-0.00000000 | temporal 0.55958533-0.27979267 | total 0.37285972\n",
      "Epoch 36, loss: 0.37285972-(best 0.37285972)\n",
      "\n",
      "Detailed Loss: recon 0.18639150-0.09319575 | disen 0.58934242-0.00000000 | temporal 0.55960661-0.27980331 | total 0.37299907\n",
      "Epoch 37, loss: 0.37299907-(best 0.37285972)\n",
      "\n",
      "Detailed Loss: recon 0.18434906-0.09217453 | disen 0.59035695-0.00000000 | temporal 0.55960643-0.27980322 | total 0.37197775\n",
      "Epoch 38, loss: 0.37197775-(best 0.37197775)\n",
      "\n",
      "Detailed Loss: recon 0.18566200-0.09283100 | disen 0.58971375-0.00000000 | temporal 0.55951661-0.27975830 | total 0.37258929\n",
      "Epoch 39, loss: 0.37258929-(best 0.37197775)\n",
      "\n",
      "Detailed Loss: recon 0.18475482-0.09237741 | disen 0.59046578-0.00000000 | temporal 0.55955184-0.27977592 | total 0.37215334\n",
      "Epoch 40, loss: 0.37215334-(best 0.37197775)\n",
      "\n",
      "Detailed Loss: recon 0.18222958-0.09111479 | disen 0.59041953-0.00000000 | temporal 0.55942452-0.27971226 | total 0.37082705\n",
      "Epoch 41, loss: 0.37082705-(best 0.37082705)\n",
      "\n",
      "Detailed Loss: recon 0.18286160-0.09143080 | disen 0.59064943-0.00000000 | temporal 0.55941850-0.27970925 | total 0.37114006\n",
      "Epoch 42, loss: 0.37114006-(best 0.37082705)\n",
      "\n",
      "Detailed Loss: recon 0.18132173-0.09066086 | disen 0.59157300-0.00000000 | temporal 0.55939329-0.27969664 | total 0.37035751\n",
      "Epoch 43, loss: 0.37035751-(best 0.37035751)\n",
      "\n",
      "Detailed Loss: recon 0.18274036-0.09137018 | disen 0.59095567-0.00000000 | temporal 0.55937755-0.27968878 | total 0.37105894\n",
      "Epoch 44, loss: 0.37105894-(best 0.37035751)\n",
      "\n",
      "Detailed Loss: recon 0.18222535-0.09111267 | disen 0.59161264-0.00000000 | temporal 0.55946541-0.27973270 | total 0.37084538\n",
      "Epoch 45, loss: 0.37084538-(best 0.37035751)\n",
      "\n",
      "Detailed Loss: recon 0.18125784-0.09062892 | disen 0.59226531-0.00000000 | temporal 0.55941141-0.27970570 | total 0.37033463\n",
      "Epoch 46, loss: 0.37033463-(best 0.37033463)\n",
      "\n",
      "Detailed Loss: recon 0.17946076-0.08973038 | disen 0.59196746-0.00000000 | temporal 0.55943578-0.27971789 | total 0.36944827\n",
      "Epoch 47, loss: 0.36944827-(best 0.36944827)\n",
      "\n",
      "Detailed Loss: recon 0.17956367-0.08978184 | disen 0.59181952-0.00000000 | temporal 0.55935752-0.27967876 | total 0.36946058\n",
      "Epoch 48, loss: 0.36946058-(best 0.36944827)\n",
      "\n",
      "Detailed Loss: recon 0.17969166-0.08984583 | disen 0.59146142-0.00000000 | temporal 0.55935067-0.27967533 | total 0.36952117\n",
      "Epoch 49, loss: 0.36952117-(best 0.36944827)\n",
      "\n",
      "Detailed Loss: recon 0.17868482-0.08934241 | disen 0.59265113-0.00000000 | temporal 0.55920178-0.27960089 | total 0.36894330\n",
      "Epoch 50, loss: 0.36894330-(best 0.36894330)\n",
      "\n",
      ">> Reached final epoch. Loading best model...\n",
      "TRAINING XGB PREDICTOR\n",
      ">> Ending training!\n",
      "\n",
      "-----------------------\n",
      "TIME COST: 6.6616387367248535 s\n",
      "Best Val: REC 72.11 PRE 48.60 MF1 68.35 AUC 78.67 TP 592 FP 626 TN 1573 FN 229\n",
      "-----------------------\n",
      "Generating additional positive data from adversary...\n",
      "Generating additional negative data by duplicating random nodes...\n",
      "\n",
      "ASSIGNED GRAPH FOR EVALUATION: 12345 {1: 2694, 0: 9651} Nodes, 1148028 Edges\n",
      "\n",
      "-------------------\n",
      "STARTING EVALUATION\n",
      "-------------------\n",
      "\n",
      "Dataset - Overall: REC 59.43 PRE 54.00 MF1 71.84 AUC 83.64 TP 1601 FP 1364 TN 8287 FN 1093 | 12345 {1: 2694, 0: 9651}\n",
      "Dataset - Train: REC 77.44 PRE 65.57 MF1 79.39 AUC 90.12 TP 954 FP 501 TN 2797 FN 278 | 4530 {1: 1232, 0: 3298}\n",
      "Dataset - Val: REC 56.39 PRE 48.74 MF1 66.25 AUC 77.12 TP 463 FP 487 TN 1712 FN 358 | 3020 {1: 821, 0: 2199}\n",
      "Dataset - Test: REC 28.71 PRE 32.86 MF1 60.36 AUC 77.17 TP 184 FP 376 TN 3778 FN 457 | 4795 {0: 4154, 1: 641}\n",
      "PREDICTION STATUS - {'1-True': 2237, '0-True': 5873, '0-False': 3778, '1-False': 457}\n",
      "    >> 457 positive nodes left unpredicted...\n",
      "    >> 361 OG positive nodes left unpredicted...\n",
      "-----------------------\n",
      "PREDICTION RESULT - ROUNDS\n",
      "Dataset - Round 0: REC 56.44 PRE 50.32 MF1 69.85 AUC 81.82 TP 701 FP 692 TN 3945 FN 541 | 5879 {1: 1242, 0: 4637}\n",
      "Dataset - Round 1: REC 25.00 PRE 39.51 MF1 57.80 AUC 70.81 TP 32 FP 49 TN 410 FN 96 | 587 {1: 128, 0: 459}\n",
      "-----------------------\n",
      "PREDICTION RESULT - SEEDS\n",
      "Seeds - Current: REC 23.44 PRE 27.52 MF1 53.21 AUC 66.13 TP 30 FP 79 TN 380 FN 98 | 587 {1: 128, 0: 459}\n",
      "Seeds - Prev: REC 0.00 PRE 0.00 MF1 38.60 AUC 66.13 TP 0 FP 90 TN 369 FN 128 | 587 {1: 128, 0: 459}\n",
      "Selecting data as budgeted ground truth for next round...\n",
      "  > Starting round 2...\n",
      "\n",
      "+++++++++++++++++\n",
      "STARTING ROUND 2!\n",
      "+++++++++++++++++\n",
      "        \n",
      "ASSIGNED GRAPH FOR TRAINING: 12345 {1: 2694, 0: 9651} Nodes, 1148028 Edges\n",
      "\n",
      ">> Splitting to train-validation set...\n",
      "Alotting train-val-test split for round 2\n",
      "Initial pool: 5879, Prediction pool: 6433, Budget pool: 0, Full pool: 8110\n",
      "Full graph size: 12345, Training: 4866, Val: 3244, Test: 12345\n",
      "    >> INITIAL DATA SPLIT: 4866 train rows ({1: 1342, 0: 3524}) | 3244 val rows ({0: 2349, 1: 895}) | 4235 test rows ({0: 3778, 1: 457})\n",
      "    >> AUGMENTED DATA SPLIT: 4866 train rows ({1: 1342, 0: 3524}) | 3244 val rows ({0: 2349, 1: 895}) | 4235 test rows ({0: 3778, 1: 457})\n",
      "    >> Updated cross-entropy weight to 2.6259314456035767...\n",
      "\n",
      "-----------------------\n",
      "STARTING MODEL TRAININGps_l\n",
      "-----------------------\n",
      "\n",
      "TRAINING NODE EMBEDDER\n",
      "Detailed Loss: recon 0.17983165-0.08991583 | disen 0.58945441-0.00000000 | temporal 0.58012182-0.29006091 | total 0.37997675\n",
      "Epoch 1, loss: 0.37997675-(best 0.37997675)\n",
      "\n",
      "Detailed Loss: recon 0.19976747-0.09988374 | disen 0.58984762-0.00000000 | temporal 0.57934964-0.28967482 | total 0.38955855\n",
      "Epoch 2, loss: 0.38955855-(best 0.37997675)\n",
      "\n",
      "Detailed Loss: recon 0.20291620-0.10145810 | disen 0.59302276-0.00000000 | temporal 0.57975215-0.28987607 | total 0.39133418\n",
      "Epoch 3, loss: 0.39133418-(best 0.37997675)\n",
      "\n",
      "Detailed Loss: recon 0.19007662-0.09503831 | disen 0.59084493-0.00000000 | temporal 0.57920170-0.28960085 | total 0.38463914\n",
      "Epoch 4, loss: 0.38463914-(best 0.37997675)\n",
      "\n",
      "Detailed Loss: recon 0.19516240-0.09758120 | disen 0.59045959-0.00000000 | temporal 0.57901323-0.28950661 | total 0.38708782\n",
      "Epoch 5, loss: 0.38708782-(best 0.37997675)\n",
      "\n",
      "Detailed Loss: recon 0.18632013-0.09316006 | disen 0.59014547-0.00000000 | temporal 0.57932097-0.28966048 | total 0.38282055\n",
      "Epoch 6, loss: 0.38282055-(best 0.37997675)\n",
      "\n",
      "Detailed Loss: recon 0.18505584-0.09252792 | disen 0.59095842-0.00000000 | temporal 0.57982254-0.28991127 | total 0.38243920\n",
      "Epoch 7, loss: 0.38243920-(best 0.37997675)\n",
      "\n",
      "Detailed Loss: recon 0.18695006-0.09347503 | disen 0.59086823-0.00000000 | temporal 0.57998478-0.28999239 | total 0.38346744\n",
      "Epoch 8, loss: 0.38346744-(best 0.37997675)\n",
      "\n",
      "Detailed Loss: recon 0.18567982-0.09283991 | disen 0.59119993-0.00000000 | temporal 0.57990879-0.28995439 | total 0.38279432\n",
      "Epoch 9, loss: 0.38279432-(best 0.37997675)\n",
      "\n",
      "Detailed Loss: recon 0.18738990-0.09369495 | disen 0.59110117-0.00000000 | temporal 0.57982516-0.28991258 | total 0.38360754\n",
      "Epoch 10, loss: 0.38360754-(best 0.37997675)\n",
      "\n",
      "Detailed Loss: recon 0.18678015-0.09339008 | disen 0.59047413-0.00000000 | temporal 0.57991230-0.28995615 | total 0.38334623\n",
      "Epoch 11, loss: 0.38334623-(best 0.37997675)\n",
      "\n",
      "Detailed Loss: recon 0.18552671-0.09276336 | disen 0.59034860-0.00000000 | temporal 0.57999367-0.28999683 | total 0.38276020\n",
      "Epoch 12, loss: 0.38276020-(best 0.37997675)\n",
      "\n",
      "Detailed Loss: recon 0.18307999-0.09153999 | disen 0.59009987-0.00000000 | temporal 0.58005947-0.29002973 | total 0.38156974\n",
      "Epoch 13, loss: 0.38156974-(best 0.37997675)\n",
      "\n",
      "Detailed Loss: recon 0.18276131-0.09138066 | disen 0.59144467-0.00000000 | temporal 0.58004785-0.29002392 | total 0.38140458\n",
      "Epoch 14, loss: 0.38140458-(best 0.37997675)\n",
      "\n",
      "Detailed Loss: recon 0.18105648-0.09052824 | disen 0.59043586-0.00000000 | temporal 0.57986200-0.28993100 | total 0.38045925\n",
      "Epoch 15, loss: 0.38045925-(best 0.37997675)\n",
      "\n",
      "Detailed Loss: recon 0.17942226-0.08971113 | disen 0.58987570-0.00000000 | temporal 0.57973081-0.28986540 | total 0.37957653\n",
      "Epoch 16, loss: 0.37957653-(best 0.37957653)\n",
      "\n",
      "Detailed Loss: recon 0.17936939-0.08968470 | disen 0.59033310-0.00000000 | temporal 0.57963151-0.28981575 | total 0.37950045\n",
      "Epoch 17, loss: 0.37950045-(best 0.37950045)\n",
      "\n",
      "Detailed Loss: recon 0.17847824-0.08923912 | disen 0.59075093-0.00000000 | temporal 0.57959127-0.28979564 | total 0.37903476\n",
      "Epoch 18, loss: 0.37903476-(best 0.37903476)\n",
      "\n",
      "Detailed Loss: recon 0.17909548-0.08954774 | disen 0.59166831-0.00000000 | temporal 0.57963133-0.28981566 | total 0.37936342\n",
      "Epoch 19, loss: 0.37936342-(best 0.37903476)\n",
      "\n",
      "Detailed Loss: recon 0.17818046-0.08909023 | disen 0.59287763-0.00000000 | temporal 0.57960486-0.28980243 | total 0.37889266\n",
      "Epoch 20, loss: 0.37889266-(best 0.37889266)\n",
      "\n",
      "Detailed Loss: recon 0.17722750-0.08861375 | disen 0.59239644-0.00000000 | temporal 0.57963169-0.28981584 | total 0.37842959\n",
      "Epoch 21, loss: 0.37842959-(best 0.37842959)\n",
      "\n",
      "Detailed Loss: recon 0.17540222-0.08770111 | disen 0.59291667-0.00000000 | temporal 0.57961643-0.28980821 | total 0.37750933\n",
      "Epoch 22, loss: 0.37750933-(best 0.37750933)\n",
      "\n",
      "Detailed Loss: recon 0.17572580-0.08786290 | disen 0.59226304-0.00000000 | temporal 0.57952178-0.28976089 | total 0.37762380\n",
      "Epoch 23, loss: 0.37762380-(best 0.37750933)\n",
      "\n",
      "Detailed Loss: recon 0.17655671-0.08827835 | disen 0.59275907-0.00000000 | temporal 0.57951880-0.28975940 | total 0.37803775\n",
      "Epoch 24, loss: 0.37803775-(best 0.37750933)\n",
      "\n",
      "Detailed Loss: recon 0.17460810-0.08730405 | disen 0.59274507-0.00000000 | temporal 0.57946920-0.28973460 | total 0.37703866\n",
      "Epoch 25, loss: 0.37703866-(best 0.37703866)\n",
      "\n",
      "Detailed Loss: recon 0.17390768-0.08695384 | disen 0.59286201-0.00000000 | temporal 0.57948172-0.28974086 | total 0.37669471\n",
      "Epoch 26, loss: 0.37669471-(best 0.37669471)\n",
      "\n",
      "Detailed Loss: recon 0.17442171-0.08721086 | disen 0.59280545-0.00000000 | temporal 0.57943451-0.28971726 | total 0.37692812\n",
      "Epoch 27, loss: 0.37692812-(best 0.37669471)\n",
      "\n",
      "Detailed Loss: recon 0.17578542-0.08789271 | disen 0.59321702-0.00000000 | temporal 0.57943618-0.28971809 | total 0.37761080\n",
      "Epoch 28, loss: 0.37761080-(best 0.37669471)\n",
      "\n",
      "Detailed Loss: recon 0.17369115-0.08684558 | disen 0.59318572-0.00000000 | temporal 0.57934350-0.28967175 | total 0.37651733\n",
      "Epoch 29, loss: 0.37651733-(best 0.37651733)\n",
      "\n",
      "Detailed Loss: recon 0.17421564-0.08710782 | disen 0.59224617-0.00000000 | temporal 0.57926744-0.28963372 | total 0.37674153\n",
      "Epoch 30, loss: 0.37674153-(best 0.37651733)\n",
      "\n",
      "Detailed Loss: recon 0.17405902-0.08702951 | disen 0.59298575-0.00000000 | temporal 0.57926929-0.28963464 | total 0.37666416\n",
      "Epoch 31, loss: 0.37666416-(best 0.37651733)\n",
      "\n",
      "Detailed Loss: recon 0.17244175-0.08622088 | disen 0.59287977-0.00000000 | temporal 0.57924360-0.28962180 | total 0.37584269\n",
      "Epoch 32, loss: 0.37584269-(best 0.37584269)\n",
      "\n",
      "Detailed Loss: recon 0.17264758-0.08632379 | disen 0.59320259-0.00000000 | temporal 0.57930773-0.28965387 | total 0.37597767\n",
      "Epoch 33, loss: 0.37597767-(best 0.37584269)\n",
      "\n",
      "Detailed Loss: recon 0.17191723-0.08595861 | disen 0.59318531-0.00000000 | temporal 0.57938963-0.28969482 | total 0.37565345\n",
      "Epoch 34, loss: 0.37565345-(best 0.37565345)\n",
      "\n",
      "Detailed Loss: recon 0.17187975-0.08593988 | disen 0.59276843-0.00000000 | temporal 0.57928646-0.28964323 | total 0.37558311\n",
      "Epoch 35, loss: 0.37558311-(best 0.37558311)\n",
      "\n",
      "Detailed Loss: recon 0.17189857-0.08594929 | disen 0.59248769-0.00000000 | temporal 0.57917351-0.28958675 | total 0.37553602\n",
      "Epoch 36, loss: 0.37553602-(best 0.37553602)\n",
      "\n",
      "Detailed Loss: recon 0.17152904-0.08576452 | disen 0.59139109-0.00000000 | temporal 0.57919973-0.28959987 | total 0.37536439\n",
      "Epoch 37, loss: 0.37536439-(best 0.37536439)\n",
      "\n",
      "Detailed Loss: recon 0.17096905-0.08548453 | disen 0.59234130-0.00000000 | temporal 0.57938939-0.28969470 | total 0.37517923\n",
      "Epoch 38, loss: 0.37517923-(best 0.37517923)\n",
      "\n",
      "Detailed Loss: recon 0.17121689-0.08560845 | disen 0.59221303-0.00000000 | temporal 0.57950264-0.28975132 | total 0.37535977\n",
      "Epoch 39, loss: 0.37535977-(best 0.37517923)\n",
      "\n",
      "Detailed Loss: recon 0.17019665-0.08509833 | disen 0.59274155-0.00000000 | temporal 0.57939649-0.28969824 | total 0.37479657\n",
      "Epoch 40, loss: 0.37479657-(best 0.37479657)\n",
      "\n",
      "Detailed Loss: recon 0.17026584-0.08513292 | disen 0.59209633-0.00000000 | temporal 0.57924098-0.28962049 | total 0.37475342\n",
      "Epoch 41, loss: 0.37475342-(best 0.37475342)\n",
      "\n",
      "Detailed Loss: recon 0.16984808-0.08492404 | disen 0.59281433-0.00000000 | temporal 0.57913208-0.28956604 | total 0.37449008\n",
      "Epoch 42, loss: 0.37449008-(best 0.37449008)\n",
      "\n",
      "Detailed Loss: recon 0.17087220-0.08543610 | disen 0.59195042-0.00000000 | temporal 0.57910180-0.28955090 | total 0.37498701\n",
      "Epoch 43, loss: 0.37498701-(best 0.37449008)\n",
      "\n",
      "Detailed Loss: recon 0.17130005-0.08565003 | disen 0.59333491-0.00000000 | temporal 0.57922637-0.28961319 | total 0.37526321\n",
      "Epoch 44, loss: 0.37526321-(best 0.37449008)\n",
      "\n",
      "Detailed Loss: recon 0.16824827-0.08412413 | disen 0.59499717-0.00000000 | temporal 0.57919002-0.28959501 | total 0.37371916\n",
      "Epoch 45, loss: 0.37371916-(best 0.37371916)\n",
      "\n",
      "Detailed Loss: recon 0.17102472-0.08551236 | disen 0.59385419-0.00000000 | temporal 0.57905006-0.28952503 | total 0.37503740\n",
      "Epoch 46, loss: 0.37503740-(best 0.37371916)\n",
      "\n",
      "Detailed Loss: recon 0.17216641-0.08608320 | disen 0.59376758-0.00000000 | temporal 0.57905865-0.28952932 | total 0.37561253\n",
      "Epoch 47, loss: 0.37561253-(best 0.37371916)\n",
      "\n",
      "Detailed Loss: recon 0.17340687-0.08670343 | disen 0.59376490-0.00000000 | temporal 0.57899308-0.28949654 | total 0.37619996\n",
      "Epoch 48, loss: 0.37619996-(best 0.37371916)\n",
      "\n",
      "Detailed Loss: recon 0.17214702-0.08607351 | disen 0.59379601-0.00000000 | temporal 0.57896304-0.28948152 | total 0.37555504\n",
      "Epoch 49, loss: 0.37555504-(best 0.37371916)\n",
      "\n",
      "Detailed Loss: recon 0.16977435-0.08488718 | disen 0.59411192-0.00000000 | temporal 0.57900107-0.28950053 | total 0.37438771\n",
      "Epoch 50, loss: 0.37438771-(best 0.37371916)\n",
      "\n",
      ">> Reached final epoch. Loading best model...\n",
      "TRAINING XGB PREDICTOR\n",
      ">> Ending training!\n",
      "\n",
      "-----------------------\n",
      "TIME COST: 6.563235282897949 s\n",
      "Best Val: REC 61.56 PRE 50.32 MF1 67.82 AUC 78.15 TP 551 FP 544 TN 1805 FN 344\n",
      "-----------------------\n",
      "Generating additional positive data from adversary...\n",
      "Generating additional negative data by duplicating random nodes...\n",
      "\n",
      "ASSIGNED GRAPH FOR EVALUATION: 12932 {1: 2822, 0: 10110} Nodes, 1254198 Edges\n",
      "\n",
      "-------------------\n",
      "STARTING EVALUATION\n",
      "-------------------\n",
      "\n",
      "Dataset - Overall: REC 52.52 PRE 55.40 MF1 70.74 AUC 82.53 TP 1482 FP 1193 TN 8917 FN 1340 | 12932 {1: 2822, 0: 10110}\n",
      "Dataset - Train: REC 64.68 PRE 64.78 MF1 75.65 AUC 86.40 TP 868 FP 472 TN 3052 FN 474 | 4866 {1: 1342, 0: 3524}\n",
      "Dataset - Val: REC 52.85 PRE 50.86 MF1 66.49 AUC 77.00 TP 473 FP 457 TN 1892 FN 422 | 3244 {0: 2349, 1: 895}\n",
      "Dataset - Test: REC 24.10 PRE 34.81 MF1 60.15 AUC 77.09 TP 141 FP 264 TN 3973 FN 444 | 4822 {0: 4237, 1: 585}\n",
      "PREDICTION STATUS - {'1-True': 2378, '0-True': 6137, '0-False': 3973, '1-False': 444}\n",
      "    >> 444 positive nodes left unpredicted...\n",
      "    >> 275 OG positive nodes left unpredicted...\n",
      "-----------------------\n",
      "PREDICTION RESULT - ROUNDS\n",
      "Dataset - Round 0: REC 52.17 PRE 54.78 MF1 70.67 AUC 82.37 TP 648 FP 535 TN 4102 FN 594 | 5879 {1: 1242, 0: 4637}\n",
      "Dataset - Round 1: REC 36.72 PRE 47.47 MF1 63.68 AUC 73.16 TP 47 FP 52 TN 407 FN 81 | 587 {1: 128, 0: 459}\n",
      "Dataset - Round 2: REC 23.44 PRE 40.54 MF1 57.55 AUC 70.89 TP 30 FP 44 TN 415 FN 98 | 587 {1: 128, 0: 459}\n",
      "-----------------------\n",
      "PREDICTION RESULT - SEEDS\n",
      "Seeds - Current: REC 31.25 PRE 41.67 MF1 60.28 AUC 70.18 TP 40 FP 56 TN 403 FN 88 | 587 {1: 128, 0: 459}\n",
      "Seeds - Prev: REC 0.00 PRE 0.00 MF1 39.79 AUC 70.18 TP 0 FP 71 TN 388 FN 128 | 587 {1: 128, 0: 459}\n",
      "Selecting data as budgeted ground truth for next round...\n",
      "The file ../checkpoint/working_model_file_1730091696.8500998_epoch.py does not exist\n",
      "The file ../checkpoint/working_model_file_1730091696.8500998_round-M.json does not exist\n",
      "The file ../checkpoint/working_model_file_1730091696.8500998_round-T.json does not exist\n",
      "The file ../checkpoint/working_model_file_1730091696.8500998_round.pt does not exist\n",
      "================\n",
      "++++++++++++++++\n",
      "TRIAL NUMBER 2\n",
      "++++++++++++++++\n",
      "================\n",
      "  > Rereading graph data...\n",
      "  > Initializing multiround object...\n",
      "11758 1049758\n",
      "\n",
      "=========================\n",
      "STARTING ADVER EXPERIMENT\n",
      "=========================\n",
      "MAIN CONFIG: [('device', 'cuda:0'), ('exp_type', 'ADVER'), ('full_oracle', False), ('node_expiration_time', 1), ('round_budget_neg', 0), ('round_budget_pos', 0), ('round_new_neg', 459), ('round_new_pos', 128), ('round_num', 3), ('save_df', True), ('save_embedding', False), ('save_temp_sne', False), ('setting_type', 'INDUCTIVE'), ('task_type', 'NODE'), ('verbose', 4)]\n",
      "MODEL CONFIG: [('act_name', 'LeakyReLU'), ('addon_internal_agg', 'OR'), ('addon_name', 'NONE'), ('addon_perc', 0.05), ('addon_round_window', 7), ('alpha', 0), ('att_heads', 2), ('beta', 1), ('boost_agg_backbone', <class 'models.benchmarks_supervised.booster.GIN_noparam'>), ('boost_metric', <function average_precision_score at 0x000001C8520A5760>), ('boost_predictor', <class 'xgboost.sklearn.XGBClassifier'>), ('device', 'cuda:0'), ('drop_rate', 0), ('dropout_rate', 0.1), ('early_stopping', 75), ('embed_type', 'temporal'), ('etypes', ['none']), ('gamma', 1), ('h_feats', 64), ('k', 1), ('lambda1', 1), ('lambda2', 1), ('lambda_', 1), ('loss_sample', True), ('loss_sample_ratio', 0.1), ('loss_type', 'ndist'), ('mlp_feats', 64), ('mlp_layers', 2), ('model_name', 'XGB-SP'), ('n_estimator', 500), ('norm_name', 'layer'), ('num_epoch', 100), ('num_layers', 2), ('num_round_epoch', 50), ('round_window', 7), ('temporal_agg', 'mean_final'), ('tloss_type', 'normal'), ('training_type', 'round'), ('verbose', 4)]\n",
      "STRAT CONFIG: [('augment_name', 'REAGE'), ('augment_neg_ratio', 0.09772070079945569), ('augment_pos_ratio', 0.02727929920054431), ('augment_prob', 0.5), ('augment_round_split', 8), ('device', 'cuda:0'), ('pseudo_name', 'NONE'), ('pseudo_new_pos_only', False), ('pseudo_skip_initial', False), ('verbose', 4)]\n",
      "TRAIN CONFIG: [('ce_weight', 2.6259314456035767), ('device', 'cuda:0'), ('dset_name', 'tolokers_bid'), ('early_stopping', 75), ('learning_rate', 0.01), ('loss', <function cross_entropy at 0x000001C84AF6F880>), ('loss_gamma', 5), ('num_epoch', 100), ('num_round_epoch', 50), ('optimizer', <class 'torch.optim.adam.Adam'>), ('random_state', 7777777), ('ratio_initial', 0.5), ('ratio_test', 0.2), ('ratio_train', 0.6), ('ratio_val', 0.2), ('round_reset_model', False), ('round_train_list', []), ('stuck_stopping', 50), ('stuck_threshold', 0), ('train_max_retry', 10), ('verbose', 4)]\n",
      "ADVER CONFIG: [('adver_choose_name', 'GREEDY'), ('adver_conn_coef', 0.1), ('adver_feat_coef', 1), ('adver_mod_name', 'REPLAY'), ('device', 'cuda:0'), ('verbose', 4)]\n",
      "\n",
      "INITIAL GRAPH: 11758 {1: 2566, 0: 9192} Nodes, 1049758 Edges\n",
      "=========================\n",
      "        \n",
      ">> Initializing graph...\n",
      "MAIN GRAPH: 11758 {1: 2566, 0: 9192} Nodes, 1049758 Edges\n",
      "\n",
      ">> Initializing experiment type-specific modules:\n",
      "  >> Initializing adversarial modules...\n",
      ">> Initializing task-specific modules:\n",
      ">> Initializing classifier model...\n",
      ">> Initializing other strategies...\n",
      "\n",
      "FINISH INITIALIZATION\n",
      "=====================\n",
      "        \n",
      "  > Starting round 0...\n",
      "\n",
      "+++++++++++++++++\n",
      "STARTING ROUND 0!\n",
      "+++++++++++++++++\n",
      "        \n",
      "ASSIGNED GRAPH FOR TRAINING: 5879 {0: 4554, 1: 1325} Nodes, 276143 Edges\n",
      "\n",
      ">> Splitting to train-validation set...\n",
      "Alotting train-val-test split for round 0\n",
      "Initial pool: 0, Prediction pool: 0, Budget pool: 0, Full pool: 5879\n",
      "Full graph size: 5879, Training: 3527, Val: 2352, Test: 5879\n",
      "    >> INITIAL DATA SPLIT: 3527 train rows ({0: 2732, 1: 795}) | 2352 val rows ({0: 1822, 1: 530}) | 0 test rows ({})\n",
      "    >> AUGMENTED DATA SPLIT: 3527 train rows ({0: 2732, 1: 795}) | 2352 val rows ({0: 1822, 1: 530}) | 0 test rows ({})\n",
      "    >> Updated cross-entropy weight to 3.4364779874213838...\n",
      "\n",
      "-----------------------\n",
      "STARTING MODEL TRAININGps_l\n",
      "-----------------------\n",
      "\n",
      "TRAINING NODE EMBEDDER\n",
      "Detailed Loss: recon 0.79775709-0.39887854 | disen 0.69689029-0.00000000 | temporal 0.64042926-0.32021463 | total 0.71909320\n",
      "Epoch 1, loss: 0.71909320-(best 0.71909320)\n",
      "\n",
      "Detailed Loss: recon 0.44035697-0.22017848 | disen 0.71832633-0.00000000 | temporal 0.65514690-0.32757345 | total 0.54775190\n",
      "Epoch 2, loss: 0.54775190-(best 0.54775190)\n",
      "\n",
      "Detailed Loss: recon 0.39461851-0.19730926 | disen 0.72774971-0.00000000 | temporal 0.65856558-0.32928279 | total 0.52659202\n",
      "Epoch 3, loss: 0.52659202-(best 0.52659202)\n",
      "\n",
      "Detailed Loss: recon 0.38783979-0.19391990 | disen 0.72825384-0.00000000 | temporal 0.65973240-0.32986620 | total 0.52378607\n",
      "Epoch 4, loss: 0.52378607-(best 0.52378607)\n",
      "\n",
      "Detailed Loss: recon 0.38756913-0.19378456 | disen 0.73438179-0.00000000 | temporal 0.66091681-0.33045840 | total 0.52424300\n",
      "Epoch 5, loss: 0.52424300-(best 0.52378607)\n",
      "\n",
      "Detailed Loss: recon 0.36135659-0.18067829 | disen 0.72878528-0.00000000 | temporal 0.66035599-0.33017799 | total 0.51085627\n",
      "Epoch 6, loss: 0.51085627-(best 0.51085627)\n",
      "\n",
      "Detailed Loss: recon 0.34684277-0.17342138 | disen 0.73254037-0.00000000 | temporal 0.65983307-0.32991654 | total 0.50333792\n",
      "Epoch 7, loss: 0.50333792-(best 0.50333792)\n",
      "\n",
      "Detailed Loss: recon 0.32615384-0.16307692 | disen 0.73247385-0.00000000 | temporal 0.65878254-0.32939127 | total 0.49246818\n",
      "Epoch 8, loss: 0.49246818-(best 0.49246818)\n",
      "\n",
      "Detailed Loss: recon 0.32104021-0.16052011 | disen 0.73290241-0.00000000 | temporal 0.65789884-0.32894942 | total 0.48946953\n",
      "Epoch 9, loss: 0.48946953-(best 0.48946953)\n",
      "\n",
      "Detailed Loss: recon 0.31980056-0.15990028 | disen 0.73162329-0.00000000 | temporal 0.65697658-0.32848829 | total 0.48838857\n",
      "Epoch 10, loss: 0.48838857-(best 0.48838857)\n",
      "\n",
      "Detailed Loss: recon 0.31491742-0.15745871 | disen 0.72995329-0.00000000 | temporal 0.65609282-0.32804641 | total 0.48550510\n",
      "Epoch 11, loss: 0.48550510-(best 0.48550510)\n",
      "\n",
      "Detailed Loss: recon 0.32197183-0.16098592 | disen 0.73269641-0.00000000 | temporal 0.65573853-0.32786927 | total 0.48885518\n",
      "Epoch 12, loss: 0.48885518-(best 0.48550510)\n",
      "\n",
      "Detailed Loss: recon 0.31610772-0.15805386 | disen 0.73146951-0.00000000 | temporal 0.65558046-0.32779023 | total 0.48584408\n",
      "Epoch 13, loss: 0.48584408-(best 0.48550510)\n",
      "\n",
      "Detailed Loss: recon 0.30957341-0.15478671 | disen 0.73237145-0.00000000 | temporal 0.65581673-0.32790837 | total 0.48269507\n",
      "Epoch 14, loss: 0.48269507-(best 0.48269507)\n",
      "\n",
      "Detailed Loss: recon 0.29786146-0.14893073 | disen 0.72994250-0.00000000 | temporal 0.65655434-0.32827717 | total 0.47720790\n",
      "Epoch 15, loss: 0.47720790-(best 0.47720790)\n",
      "\n",
      "Detailed Loss: recon 0.29053643-0.14526822 | disen 0.73262286-0.00000000 | temporal 0.65723097-0.32861549 | total 0.47388369\n",
      "Epoch 16, loss: 0.47388369-(best 0.47388369)\n",
      "\n",
      "Detailed Loss: recon 0.28576085-0.14288042 | disen 0.73138517-0.00000000 | temporal 0.65783316-0.32891658 | total 0.47179699\n",
      "Epoch 17, loss: 0.47179699-(best 0.47179699)\n",
      "\n",
      "Detailed Loss: recon 0.28492489-0.14246245 | disen 0.73121512-0.00000000 | temporal 0.65814245-0.32907122 | total 0.47153366\n",
      "Epoch 18, loss: 0.47153366-(best 0.47153366)\n",
      "\n",
      "Detailed Loss: recon 0.28385127-0.14192563 | disen 0.73318708-0.00000000 | temporal 0.65836203-0.32918102 | total 0.47110665\n",
      "Epoch 19, loss: 0.47110665-(best 0.47110665)\n",
      "\n",
      "Detailed Loss: recon 0.27515805-0.13757902 | disen 0.73344314-0.00000000 | temporal 0.65864569-0.32932284 | total 0.46690187\n",
      "Epoch 20, loss: 0.46690187-(best 0.46690187)\n",
      "\n",
      "Detailed Loss: recon 0.26642984-0.13321492 | disen 0.73112565-0.00000000 | temporal 0.65843296-0.32921648 | total 0.46243140\n",
      "Epoch 21, loss: 0.46243140-(best 0.46243140)\n",
      "\n",
      "Detailed Loss: recon 0.26647866-0.13323933 | disen 0.73094690-0.00000000 | temporal 0.65838319-0.32919160 | total 0.46243092\n",
      "Epoch 22, loss: 0.46243092-(best 0.46243092)\n",
      "\n",
      "Detailed Loss: recon 0.26135993-0.13067997 | disen 0.73316932-0.00000000 | temporal 0.65830028-0.32915014 | total 0.45983011\n",
      "Epoch 23, loss: 0.45983011-(best 0.45983011)\n",
      "\n",
      "Detailed Loss: recon 0.26348004-0.13174002 | disen 0.73327649-0.00000000 | temporal 0.65812218-0.32906109 | total 0.46080112\n",
      "Epoch 24, loss: 0.46080112-(best 0.45983011)\n",
      "\n",
      "Detailed Loss: recon 0.25259992-0.12629996 | disen 0.73000729-0.00000000 | temporal 0.65818453-0.32909226 | total 0.45539224\n",
      "Epoch 25, loss: 0.45539224-(best 0.45539224)\n",
      "\n",
      "Detailed Loss: recon 0.25447428-0.12723714 | disen 0.73146415-0.00000000 | temporal 0.65810418-0.32905209 | total 0.45628923\n",
      "Epoch 26, loss: 0.45628923-(best 0.45539224)\n",
      "\n",
      "Detailed Loss: recon 0.25268400-0.12634200 | disen 0.73185706-0.00000000 | temporal 0.65834588-0.32917294 | total 0.45551494\n",
      "Epoch 27, loss: 0.45551494-(best 0.45539224)\n",
      "\n",
      "Detailed Loss: recon 0.25119030-0.12559515 | disen 0.73209298-0.00000000 | temporal 0.65849555-0.32924777 | total 0.45484293\n",
      "Epoch 28, loss: 0.45484293-(best 0.45484293)\n",
      "\n",
      "Detailed Loss: recon 0.24579856-0.12289928 | disen 0.73220938-0.00000000 | temporal 0.65849620-0.32924810 | total 0.45214736\n",
      "Epoch 29, loss: 0.45214736-(best 0.45214736)\n",
      "\n",
      "Detailed Loss: recon 0.24494575-0.12247287 | disen 0.72891915-0.00000000 | temporal 0.65846664-0.32923332 | total 0.45170620\n",
      "Epoch 30, loss: 0.45170620-(best 0.45170620)\n",
      "\n",
      "Detailed Loss: recon 0.24255300-0.12127650 | disen 0.73180634-0.00000000 | temporal 0.65836483-0.32918242 | total 0.45045891\n",
      "Epoch 31, loss: 0.45045891-(best 0.45045891)\n",
      "\n",
      "Detailed Loss: recon 0.24311313-0.12155657 | disen 0.72920811-0.00000000 | temporal 0.65851992-0.32925996 | total 0.45081651\n",
      "Epoch 32, loss: 0.45081651-(best 0.45045891)\n",
      "\n",
      "Detailed Loss: recon 0.24074775-0.12037387 | disen 0.73193854-0.00000000 | temporal 0.65864432-0.32932216 | total 0.44969603\n",
      "Epoch 33, loss: 0.44969603-(best 0.44969603)\n",
      "\n",
      "Detailed Loss: recon 0.23582453-0.11791226 | disen 0.72844350-0.00000000 | temporal 0.65865815-0.32932907 | total 0.44724134\n",
      "Epoch 34, loss: 0.44724134-(best 0.44724134)\n",
      "\n",
      "Detailed Loss: recon 0.23598087-0.11799043 | disen 0.72987545-0.00000000 | temporal 0.65854853-0.32927427 | total 0.44726470\n",
      "Epoch 35, loss: 0.44726470-(best 0.44724134)\n",
      "\n",
      "Detailed Loss: recon 0.23673335-0.11836667 | disen 0.73187840-0.00000000 | temporal 0.65841526-0.32920763 | total 0.44757432\n",
      "Epoch 36, loss: 0.44757432-(best 0.44724134)\n",
      "\n",
      "Detailed Loss: recon 0.23388565-0.11694282 | disen 0.73100710-0.00000000 | temporal 0.65823704-0.32911852 | total 0.44606134\n",
      "Epoch 37, loss: 0.44606134-(best 0.44606134)\n",
      "\n",
      "Detailed Loss: recon 0.23391777-0.11695889 | disen 0.73182368-0.00000000 | temporal 0.65820903-0.32910451 | total 0.44606340\n",
      "Epoch 38, loss: 0.44606340-(best 0.44606134)\n",
      "\n",
      "Detailed Loss: recon 0.23375890-0.11687945 | disen 0.73295701-0.00000000 | temporal 0.65825951-0.32912976 | total 0.44600922\n",
      "Epoch 39, loss: 0.44600922-(best 0.44600922)\n",
      "\n",
      "Detailed Loss: recon 0.23073667-0.11536834 | disen 0.73139226-0.00000000 | temporal 0.65808427-0.32904214 | total 0.44441047\n",
      "Epoch 40, loss: 0.44441047-(best 0.44441047)\n",
      "\n",
      "Detailed Loss: recon 0.23301981-0.11650991 | disen 0.73038495-0.00000000 | temporal 0.65824515-0.32912257 | total 0.44563249\n",
      "Epoch 41, loss: 0.44563249-(best 0.44441047)\n",
      "\n",
      "Detailed Loss: recon 0.22942530-0.11471265 | disen 0.73337674-0.00000000 | temporal 0.65838856-0.32919428 | total 0.44390693\n",
      "Epoch 42, loss: 0.44390693-(best 0.44390693)\n",
      "\n",
      "Detailed Loss: recon 0.23109937-0.11554968 | disen 0.73168254-0.00000000 | temporal 0.65834707-0.32917354 | total 0.44472322\n",
      "Epoch 43, loss: 0.44472322-(best 0.44390693)\n",
      "\n",
      "Detailed Loss: recon 0.23059407-0.11529703 | disen 0.73353839-0.00000000 | temporal 0.65843523-0.32921761 | total 0.44451463\n",
      "Epoch 44, loss: 0.44451463-(best 0.44390693)\n",
      "\n",
      "Detailed Loss: recon 0.22941193-0.11470596 | disen 0.73142922-0.00000000 | temporal 0.65853858-0.32926929 | total 0.44397527\n",
      "Epoch 45, loss: 0.44397527-(best 0.44390693)\n",
      "\n",
      "Detailed Loss: recon 0.22660294-0.11330147 | disen 0.73469192-0.00000000 | temporal 0.65848184-0.32924092 | total 0.44254237\n",
      "Epoch 46, loss: 0.44254237-(best 0.44254237)\n",
      "\n",
      "Detailed Loss: recon 0.22721854-0.11360927 | disen 0.73444545-0.00000000 | temporal 0.65823275-0.32911637 | total 0.44272566\n",
      "Epoch 47, loss: 0.44272566-(best 0.44254237)\n",
      "\n",
      "Detailed Loss: recon 0.22632138-0.11316069 | disen 0.73288745-0.00000000 | temporal 0.65847784-0.32923892 | total 0.44239962\n",
      "Epoch 48, loss: 0.44239962-(best 0.44239962)\n",
      "\n",
      "Detailed Loss: recon 0.22450703-0.11225352 | disen 0.72855240-0.00000000 | temporal 0.65855551-0.32927775 | total 0.44153127\n",
      "Epoch 49, loss: 0.44153127-(best 0.44153127)\n",
      "\n",
      "Detailed Loss: recon 0.22602293-0.11301146 | disen 0.73210555-0.00000000 | temporal 0.65851867-0.32925934 | total 0.44227082\n",
      "Epoch 50, loss: 0.44227082-(best 0.44153127)\n",
      "\n",
      "Detailed Loss: recon 0.22633688-0.11316844 | disen 0.73113835-0.00000000 | temporal 0.65825135-0.32912567 | total 0.44229412\n",
      "Epoch 51, loss: 0.44229412-(best 0.44153127)\n",
      "\n",
      "Detailed Loss: recon 0.22548276-0.11274138 | disen 0.73459172-0.00000000 | temporal 0.65792197-0.32896098 | total 0.44170237\n",
      "Epoch 52, loss: 0.44170237-(best 0.44153127)\n",
      "\n",
      "Detailed Loss: recon 0.22340111-0.11170056 | disen 0.73151004-0.00000000 | temporal 0.65812171-0.32906085 | total 0.44076142\n",
      "Epoch 53, loss: 0.44076142-(best 0.44076142)\n",
      "\n",
      "Detailed Loss: recon 0.22119586-0.11059793 | disen 0.73318613-0.00000000 | temporal 0.65824109-0.32912055 | total 0.43971848\n",
      "Epoch 54, loss: 0.43971848-(best 0.43971848)\n",
      "\n",
      "Detailed Loss: recon 0.22116324-0.11058162 | disen 0.73281014-0.00000000 | temporal 0.65809071-0.32904536 | total 0.43962699\n",
      "Epoch 55, loss: 0.43962699-(best 0.43962699)\n",
      "\n",
      "Detailed Loss: recon 0.22019306-0.11009653 | disen 0.73235691-0.00000000 | temporal 0.65817165-0.32908583 | total 0.43918234\n",
      "Epoch 56, loss: 0.43918234-(best 0.43918234)\n",
      "\n",
      "Detailed Loss: recon 0.22160362-0.11080181 | disen 0.73470747-0.00000000 | temporal 0.65803248-0.32901624 | total 0.43981805\n",
      "Epoch 57, loss: 0.43981805-(best 0.43918234)\n",
      "\n",
      "Detailed Loss: recon 0.21957883-0.10978942 | disen 0.73305392-0.00000000 | temporal 0.65795177-0.32897589 | total 0.43876529\n",
      "Epoch 58, loss: 0.43876529-(best 0.43876529)\n",
      "\n",
      "Detailed Loss: recon 0.22244386-0.11122193 | disen 0.73103929-0.00000000 | temporal 0.65798461-0.32899231 | total 0.44021425\n",
      "Epoch 59, loss: 0.44021425-(best 0.43876529)\n",
      "\n",
      "Detailed Loss: recon 0.22201769-0.11100885 | disen 0.73429495-0.00000000 | temporal 0.65801883-0.32900941 | total 0.44001827\n",
      "Epoch 60, loss: 0.44001827-(best 0.43876529)\n",
      "\n",
      "Detailed Loss: recon 0.22060615-0.11030307 | disen 0.73385507-0.00000000 | temporal 0.65799463-0.32899731 | total 0.43930039\n",
      "Epoch 61, loss: 0.43930039-(best 0.43876529)\n",
      "\n",
      "Detailed Loss: recon 0.21813272-0.10906636 | disen 0.73431718-0.00000000 | temporal 0.65781057-0.32890528 | total 0.43797165\n",
      "Epoch 62, loss: 0.43797165-(best 0.43797165)\n",
      "\n",
      "Detailed Loss: recon 0.22299778-0.11149889 | disen 0.73631525-0.00000000 | temporal 0.65809947-0.32904974 | total 0.44054863\n",
      "Epoch 63, loss: 0.44054863-(best 0.43797165)\n",
      "\n",
      "Detailed Loss: recon 0.22050904-0.11025452 | disen 0.73443305-0.00000000 | temporal 0.65815026-0.32907513 | total 0.43932965\n",
      "Epoch 64, loss: 0.43932965-(best 0.43797165)\n",
      "\n",
      "Detailed Loss: recon 0.21695016-0.10847508 | disen 0.73481846-0.00000000 | temporal 0.65804386-0.32902193 | total 0.43749702\n",
      "Epoch 65, loss: 0.43749702-(best 0.43749702)\n",
      "\n",
      "Detailed Loss: recon 0.21804762-0.10902381 | disen 0.73639798-0.00000000 | temporal 0.65767342-0.32883671 | total 0.43786052\n",
      "Epoch 66, loss: 0.43786052-(best 0.43749702)\n",
      "\n",
      "Detailed Loss: recon 0.21736196-0.10868098 | disen 0.73625326-0.00000000 | temporal 0.65789777-0.32894889 | total 0.43762988\n",
      "Epoch 67, loss: 0.43762988-(best 0.43749702)\n",
      "\n",
      "Detailed Loss: recon 0.21874270-0.10937135 | disen 0.73492694-0.00000000 | temporal 0.65788782-0.32894391 | total 0.43831527\n",
      "Epoch 68, loss: 0.43831527-(best 0.43749702)\n",
      "\n",
      "Detailed Loss: recon 0.21928239-0.10964119 | disen 0.73455751-0.00000000 | temporal 0.65781820-0.32890910 | total 0.43855029\n",
      "Epoch 69, loss: 0.43855029-(best 0.43749702)\n",
      "\n",
      "Detailed Loss: recon 0.21914832-0.10957416 | disen 0.73829257-0.00000000 | temporal 0.65741879-0.32870939 | total 0.43828356\n",
      "Epoch 70, loss: 0.43828356-(best 0.43749702)\n",
      "\n",
      "Detailed Loss: recon 0.21767728-0.10883864 | disen 0.73585653-0.00000000 | temporal 0.65740907-0.32870454 | total 0.43754318\n",
      "Epoch 71, loss: 0.43754318-(best 0.43749702)\n",
      "\n",
      "Detailed Loss: recon 0.21775454-0.10887727 | disen 0.73574972-0.00000000 | temporal 0.65757179-0.32878590 | total 0.43766317\n",
      "Epoch 72, loss: 0.43766317-(best 0.43749702)\n",
      "\n",
      "Detailed Loss: recon 0.22078012-0.11039006 | disen 0.73612094-0.00000000 | temporal 0.65745658-0.32872829 | total 0.43911836\n",
      "Epoch 73, loss: 0.43911836-(best 0.43749702)\n",
      "\n",
      "Detailed Loss: recon 0.21843961-0.10921980 | disen 0.73474967-0.00000000 | temporal 0.65732753-0.32866377 | total 0.43788356\n",
      "Epoch 74, loss: 0.43788356-(best 0.43749702)\n",
      "\n",
      "Detailed Loss: recon 0.21829024-0.10914512 | disen 0.73749042-0.00000000 | temporal 0.65752161-0.32876080 | total 0.43790591\n",
      "Epoch 75, loss: 0.43790591-(best 0.43749702)\n",
      "\n",
      "Detailed Loss: recon 0.21879625-0.10939813 | disen 0.73618799-0.00000000 | temporal 0.65760875-0.32880437 | total 0.43820250\n",
      "Epoch 76, loss: 0.43820250-(best 0.43749702)\n",
      "\n",
      "Detailed Loss: recon 0.21731316-0.10865658 | disen 0.73781776-0.00000000 | temporal 0.65715730-0.32857865 | total 0.43723524\n",
      "Epoch 77, loss: 0.43723524-(best 0.43723524)\n",
      "\n",
      "Detailed Loss: recon 0.21798234-0.10899117 | disen 0.73595047-0.00000000 | temporal 0.65698797-0.32849398 | total 0.43748516\n",
      "Epoch 78, loss: 0.43748516-(best 0.43723524)\n",
      "\n",
      "Detailed Loss: recon 0.21806300-0.10903150 | disen 0.73657763-0.00000000 | temporal 0.65746087-0.32873043 | total 0.43776193\n",
      "Epoch 79, loss: 0.43776193-(best 0.43723524)\n",
      "\n",
      "Detailed Loss: recon 0.21711336-0.10855668 | disen 0.73843753-0.00000000 | temporal 0.65708691-0.32854345 | total 0.43710014\n",
      "Epoch 80, loss: 0.43710014-(best 0.43710014)\n",
      "\n",
      "Detailed Loss: recon 0.22120589-0.11060295 | disen 0.73784375-0.00000000 | temporal 0.65666240-0.32833120 | total 0.43893415\n",
      "Epoch 81, loss: 0.43893415-(best 0.43710014)\n",
      "\n",
      "Detailed Loss: recon 0.21814728-0.10907364 | disen 0.73777801-0.00000000 | temporal 0.65735269-0.32867634 | total 0.43774998\n",
      "Epoch 82, loss: 0.43774998-(best 0.43710014)\n",
      "\n",
      "Detailed Loss: recon 0.21935773-0.10967886 | disen 0.73782003-0.00000000 | temporal 0.65725964-0.32862982 | total 0.43830869\n",
      "Epoch 83, loss: 0.43830869-(best 0.43710014)\n",
      "\n",
      "Detailed Loss: recon 0.21704987-0.10852493 | disen 0.73796552-0.00000000 | temporal 0.65683758-0.32841879 | total 0.43694371\n",
      "Epoch 84, loss: 0.43694371-(best 0.43694371)\n",
      "\n",
      "Detailed Loss: recon 0.21850370-0.10925185 | disen 0.73784375-0.00000000 | temporal 0.65676844-0.32838422 | total 0.43763608\n",
      "Epoch 85, loss: 0.43763608-(best 0.43694371)\n",
      "\n",
      "Detailed Loss: recon 0.21567044-0.10783522 | disen 0.73847556-0.00000000 | temporal 0.65754908-0.32877454 | total 0.43660975\n",
      "Epoch 86, loss: 0.43660975-(best 0.43660975)\n",
      "\n",
      "Detailed Loss: recon 0.21663395-0.10831697 | disen 0.73801684-0.00000000 | temporal 0.65744007-0.32872003 | total 0.43703699\n",
      "Epoch 87, loss: 0.43703699-(best 0.43660975)\n",
      "\n",
      "Detailed Loss: recon 0.21717732-0.10858866 | disen 0.73899019-0.00000000 | temporal 0.65661651-0.32830825 | total 0.43689692\n",
      "Epoch 88, loss: 0.43689692-(best 0.43660975)\n",
      "\n",
      "Detailed Loss: recon 0.21692120-0.10846060 | disen 0.73727423-0.00000000 | temporal 0.65666348-0.32833174 | total 0.43679234\n",
      "Epoch 89, loss: 0.43679234-(best 0.43660975)\n",
      "\n",
      "Detailed Loss: recon 0.21830413-0.10915206 | disen 0.73941100-0.00000000 | temporal 0.65717793-0.32858896 | total 0.43774104\n",
      "Epoch 90, loss: 0.43774104-(best 0.43660975)\n",
      "\n",
      "Detailed Loss: recon 0.21497694-0.10748847 | disen 0.73705196-0.00000000 | temporal 0.65733576-0.32866788 | total 0.43615633\n",
      "Epoch 91, loss: 0.43615633-(best 0.43615633)\n",
      "\n",
      "Detailed Loss: recon 0.21694478-0.10847239 | disen 0.73908865-0.00000000 | temporal 0.65667546-0.32833773 | total 0.43681014\n",
      "Epoch 92, loss: 0.43681014-(best 0.43615633)\n",
      "\n",
      "Detailed Loss: recon 0.21596597-0.10798299 | disen 0.73758924-0.00000000 | temporal 0.65644187-0.32822093 | total 0.43620393\n",
      "Epoch 93, loss: 0.43620393-(best 0.43615633)\n",
      "\n",
      "Detailed Loss: recon 0.21489380-0.10744690 | disen 0.74017560-0.00000000 | temporal 0.65670598-0.32835299 | total 0.43579990\n",
      "Epoch 94, loss: 0.43579990-(best 0.43579990)\n",
      "\n",
      "Detailed Loss: recon 0.21643358-0.10821679 | disen 0.73753446-0.00000000 | temporal 0.65708542-0.32854271 | total 0.43675950\n",
      "Epoch 95, loss: 0.43675950-(best 0.43579990)\n",
      "\n",
      "Detailed Loss: recon 0.21452549-0.10726275 | disen 0.73520100-0.00000000 | temporal 0.65677965-0.32838982 | total 0.43565255\n",
      "Epoch 96, loss: 0.43565255-(best 0.43565255)\n",
      "\n",
      "Detailed Loss: recon 0.21475556-0.10737778 | disen 0.73643601-0.00000000 | temporal 0.65660143-0.32830071 | total 0.43567848\n",
      "Epoch 97, loss: 0.43567848-(best 0.43565255)\n",
      "\n",
      "Detailed Loss: recon 0.21384315-0.10692158 | disen 0.73791516-0.00000000 | temporal 0.65666515-0.32833257 | total 0.43525416\n",
      "Epoch 98, loss: 0.43525416-(best 0.43525416)\n",
      "\n",
      "Detailed Loss: recon 0.21270320-0.10635160 | disen 0.73951024-0.00000000 | temporal 0.65691787-0.32845894 | total 0.43481052\n",
      "Epoch 99, loss: 0.43481052-(best 0.43481052)\n",
      "\n",
      "Detailed Loss: recon 0.21414420-0.10707210 | disen 0.73721462-0.00000000 | temporal 0.65690249-0.32845125 | total 0.43552333\n",
      "Epoch 100, loss: 0.43552333-(best 0.43481052)\n",
      "\n",
      ">> Reached final epoch. Loading best model...\n",
      "TRAINING XGB PREDICTOR\n",
      ">> Ending training!\n",
      "\n",
      "-----------------------\n",
      "TIME COST: 6.331077337265015 s\n",
      "Best Val: REC 52.26 PRE 48.77 MF1 67.68 AUC 79.02 TP 277 FP 291 TN 1531 FN 253\n",
      "-----------------------\n",
      "\n",
      "ASSIGNED GRAPH FOR EVALUATION: 11758 {1: 2566, 0: 9192} Nodes, 1049758 Edges\n",
      "\n",
      "-------------------\n",
      "STARTING EVALUATION\n",
      "-------------------\n",
      "\n",
      "Dataset - Overall: REC 63.21 PRE 45.69 MF1 68.27 AUC 81.10 TP 1622 FP 1928 TN 7264 FN 944 | 11758 {1: 2566, 0: 9192}\n",
      "Dataset - Train: REC 71.82 PRE 52.19 MF1 73.00 AUC 85.35 TP 571 FP 523 TN 2209 FN 224 | 3527 {0: 2732, 1: 795}\n",
      "Dataset - Val: REC 61.89 PRE 44.32 MF1 66.89 AUC 78.73 TP 328 FP 412 TN 1410 FN 202 | 2352 {0: 1822, 1: 530}\n",
      "Dataset - Test: REC 58.26 PRE 42.13 MF1 65.87 AUC 79.36 TP 723 FP 993 TN 3645 FN 518 | 5879 {1: 1241, 0: 4638}\n",
      "PREDICTION STATUS - {'1-True': 2048, '0-True': 5547, '1-False': 518, '0-False': 3645}\n",
      "    >> 518 positive nodes left unpredicted...\n",
      "    >> 518 OG positive nodes left unpredicted...\n",
      "Selecting data as budgeted ground truth for next round...\n",
      "  > Starting round 1...\n",
      "\n",
      "+++++++++++++++++\n",
      "STARTING ROUND 1!\n",
      "+++++++++++++++++\n",
      "        \n",
      "ASSIGNED GRAPH FOR TRAINING: 11758 {1: 2566, 0: 9192} Nodes, 1049758 Edges\n",
      "\n",
      ">> Splitting to train-validation set...\n",
      "Alotting train-val-test split for round 1\n",
      "Initial pool: 5879, Prediction pool: 3550, Budget pool: 0, Full pool: 7595\n",
      "Full graph size: 11758, Training: 4557, Val: 3038, Test: 11758\n",
      "    >> INITIAL DATA SPLIT: 4557 train rows ({1: 1229, 0: 3328}) | 3038 val rows ({1: 819, 0: 2219}) | 4163 test rows ({1: 518, 0: 3645})\n",
      "    >> AUGMENTED DATA SPLIT: 4557 train rows ({1: 1229, 0: 3328}) | 3038 val rows ({1: 819, 0: 2219}) | 4163 test rows ({1: 518, 0: 3645})\n",
      "    >> Updated cross-entropy weight to 2.7078925956061837...\n",
      "\n",
      "-----------------------\n",
      "STARTING MODEL TRAININGps_l\n",
      "-----------------------\n",
      "\n",
      "TRAINING NODE EMBEDDER\n",
      "Detailed Loss: recon 0.21140993-0.10570496 | disen 0.60556674-0.00000000 | temporal 0.55985928-0.27992964 | total 0.38563460\n",
      "Epoch 1, loss: 0.38563460-(best 0.38563460)\n",
      "\n",
      "Detailed Loss: recon 0.24584588-0.12292294 | disen 0.60339499-0.00000000 | temporal 0.55919874-0.27959937 | total 0.40252233\n",
      "Epoch 2, loss: 0.40252233-(best 0.38563460)\n",
      "\n",
      "Detailed Loss: recon 0.26234013-0.13117006 | disen 0.60643733-0.00000000 | temporal 0.56078547-0.28039274 | total 0.41156280\n",
      "Epoch 3, loss: 0.41156280-(best 0.38563460)\n",
      "\n",
      "Detailed Loss: recon 0.23688982-0.11844491 | disen 0.60562485-0.00000000 | temporal 0.56040388-0.28020194 | total 0.39864686\n",
      "Epoch 4, loss: 0.39864686-(best 0.38563460)\n",
      "\n",
      "Detailed Loss: recon 0.23139255-0.11569627 | disen 0.60429347-0.00000000 | temporal 0.55988121-0.27994061 | total 0.39563689\n",
      "Epoch 5, loss: 0.39563689-(best 0.38563460)\n",
      "\n",
      "Detailed Loss: recon 0.23509926-0.11754963 | disen 0.60256118-0.00000000 | temporal 0.55948019-0.27974010 | total 0.39728972\n",
      "Epoch 6, loss: 0.39728972-(best 0.38563460)\n",
      "\n",
      "Detailed Loss: recon 0.22956377-0.11478189 | disen 0.60173899-0.00000000 | temporal 0.55941975-0.27970988 | total 0.39449176\n",
      "Epoch 7, loss: 0.39449176-(best 0.38563460)\n",
      "\n",
      "Detailed Loss: recon 0.22359787-0.11179893 | disen 0.60146701-0.00000000 | temporal 0.55961537-0.27980769 | total 0.39160663\n",
      "Epoch 8, loss: 0.39160663-(best 0.38563460)\n",
      "\n",
      "Detailed Loss: recon 0.21879111-0.10939556 | disen 0.60233557-0.00000000 | temporal 0.55995417-0.27997708 | total 0.38937265\n",
      "Epoch 9, loss: 0.38937265-(best 0.38563460)\n",
      "\n",
      "Detailed Loss: recon 0.21547644-0.10773822 | disen 0.60183781-0.00000000 | temporal 0.56015825-0.28007913 | total 0.38781735\n",
      "Epoch 10, loss: 0.38781735-(best 0.38563460)\n",
      "\n",
      "Detailed Loss: recon 0.22023974-0.11011987 | disen 0.60068434-0.00000000 | temporal 0.56017470-0.28008735 | total 0.39020723\n",
      "Epoch 11, loss: 0.39020723-(best 0.38563460)\n",
      "\n",
      "Detailed Loss: recon 0.21806726-0.10903363 | disen 0.60064203-0.00000000 | temporal 0.56008035-0.28004017 | total 0.38907379\n",
      "Epoch 12, loss: 0.38907379-(best 0.38563460)\n",
      "\n",
      "Detailed Loss: recon 0.21678349-0.10839175 | disen 0.60048318-0.00000000 | temporal 0.55990696-0.27995348 | total 0.38834524\n",
      "Epoch 13, loss: 0.38834524-(best 0.38563460)\n",
      "\n",
      "Detailed Loss: recon 0.21374588-0.10687294 | disen 0.59930730-0.00000000 | temporal 0.55982149-0.27991074 | total 0.38678369\n",
      "Epoch 14, loss: 0.38678369-(best 0.38563460)\n",
      "\n",
      "Detailed Loss: recon 0.21289763-0.10644881 | disen 0.60029101-0.00000000 | temporal 0.55985665-0.27992833 | total 0.38637716\n",
      "Epoch 15, loss: 0.38637716-(best 0.38563460)\n",
      "\n",
      "Detailed Loss: recon 0.21377042-0.10688521 | disen 0.59978843-0.00000000 | temporal 0.55987555-0.27993777 | total 0.38682300\n",
      "Epoch 16, loss: 0.38682300-(best 0.38563460)\n",
      "\n",
      "Detailed Loss: recon 0.21117817-0.10558908 | disen 0.59933454-0.00000000 | temporal 0.55972272-0.27986136 | total 0.38545045\n",
      "Epoch 17, loss: 0.38545045-(best 0.38545045)\n",
      "\n",
      "Detailed Loss: recon 0.21036696-0.10518348 | disen 0.59934956-0.00000000 | temporal 0.55960381-0.27980191 | total 0.38498539\n",
      "Epoch 18, loss: 0.38498539-(best 0.38498539)\n",
      "\n",
      "Detailed Loss: recon 0.20722511-0.10361256 | disen 0.59882420-0.00000000 | temporal 0.55951476-0.27975738 | total 0.38336992\n",
      "Epoch 19, loss: 0.38336992-(best 0.38336992)\n",
      "\n",
      "Detailed Loss: recon 0.20729592-0.10364796 | disen 0.59919071-0.00000000 | temporal 0.55948752-0.27974376 | total 0.38339174\n",
      "Epoch 20, loss: 0.38339174-(best 0.38336992)\n",
      "\n",
      "Detailed Loss: recon 0.20982151-0.10491075 | disen 0.59922963-0.00000000 | temporal 0.55945796-0.27972898 | total 0.38463974\n",
      "Epoch 21, loss: 0.38463974-(best 0.38336992)\n",
      "\n",
      "Detailed Loss: recon 0.20407769-0.10203885 | disen 0.60087001-0.00000000 | temporal 0.55956620-0.27978310 | total 0.38182193\n",
      "Epoch 22, loss: 0.38182193-(best 0.38182193)\n",
      "\n",
      "Detailed Loss: recon 0.20537780-0.10268890 | disen 0.60018188-0.00000000 | temporal 0.55949938-0.27974969 | total 0.38243860\n",
      "Epoch 23, loss: 0.38243860-(best 0.38182193)\n",
      "\n",
      "Detailed Loss: recon 0.20518115-0.10259058 | disen 0.59930503-0.00000000 | temporal 0.55942833-0.27971417 | total 0.38230473\n",
      "Epoch 24, loss: 0.38230473-(best 0.38182193)\n",
      "\n",
      "Detailed Loss: recon 0.20326422-0.10163211 | disen 0.59933925-0.00000000 | temporal 0.55933714-0.27966857 | total 0.38130069\n",
      "Epoch 25, loss: 0.38130069-(best 0.38130069)\n",
      "\n",
      "Detailed Loss: recon 0.20097855-0.10048927 | disen 0.59891355-0.00000000 | temporal 0.55933017-0.27966508 | total 0.38015437\n",
      "Epoch 26, loss: 0.38015437-(best 0.38015437)\n",
      "\n",
      "Detailed Loss: recon 0.20143567-0.10071784 | disen 0.59921610-0.00000000 | temporal 0.55927467-0.27963734 | total 0.38035518\n",
      "Epoch 27, loss: 0.38035518-(best 0.38015437)\n",
      "\n",
      "Detailed Loss: recon 0.20654276-0.10327138 | disen 0.59944475-0.00000000 | temporal 0.55918902-0.27959451 | total 0.38286591\n",
      "Epoch 28, loss: 0.38286591-(best 0.38015437)\n",
      "\n",
      "Detailed Loss: recon 0.20220971-0.10110486 | disen 0.60033464-0.00000000 | temporal 0.55925035-0.27962518 | total 0.38073003\n",
      "Epoch 29, loss: 0.38073003-(best 0.38015437)\n",
      "\n",
      "Detailed Loss: recon 0.20562628-0.10281314 | disen 0.59966892-0.00000000 | temporal 0.55914694-0.27957347 | total 0.38238662\n",
      "Epoch 30, loss: 0.38238662-(best 0.38015437)\n",
      "\n",
      "Detailed Loss: recon 0.20390348-0.10195174 | disen 0.60028100-0.00000000 | temporal 0.55910808-0.27955404 | total 0.38150579\n",
      "Epoch 31, loss: 0.38150579-(best 0.38015437)\n",
      "\n",
      "Detailed Loss: recon 0.20389375-0.10194688 | disen 0.59947228-0.00000000 | temporal 0.55897766-0.27948883 | total 0.38143569\n",
      "Epoch 32, loss: 0.38143569-(best 0.38015437)\n",
      "\n",
      "Detailed Loss: recon 0.20295046-0.10147523 | disen 0.59917426-0.00000000 | temporal 0.55894548-0.27947274 | total 0.38094798\n",
      "Epoch 33, loss: 0.38094798-(best 0.38015437)\n",
      "\n",
      "Detailed Loss: recon 0.20342116-0.10171058 | disen 0.59993070-0.00000000 | temporal 0.55900186-0.27950093 | total 0.38121152\n",
      "Epoch 34, loss: 0.38121152-(best 0.38015437)\n",
      "\n",
      "Detailed Loss: recon 0.20455068-0.10227534 | disen 0.60057527-0.00000000 | temporal 0.55906868-0.27953434 | total 0.38180968\n",
      "Epoch 35, loss: 0.38180968-(best 0.38015437)\n",
      "\n",
      "Detailed Loss: recon 0.20092392-0.10046196 | disen 0.60082197-0.00000000 | temporal 0.55905247-0.27952623 | total 0.37998819\n",
      "Epoch 36, loss: 0.37998819-(best 0.37998819)\n",
      "\n",
      "Detailed Loss: recon 0.19931087-0.09965543 | disen 0.60079181-0.00000000 | temporal 0.55898368-0.27949184 | total 0.37914729\n",
      "Epoch 37, loss: 0.37914729-(best 0.37914729)\n",
      "\n",
      "Detailed Loss: recon 0.19968197-0.09984098 | disen 0.60078967-0.00000000 | temporal 0.55898386-0.27949193 | total 0.37933290\n",
      "Epoch 38, loss: 0.37933290-(best 0.37914729)\n",
      "\n",
      "Detailed Loss: recon 0.19746743-0.09873372 | disen 0.60054439-0.00000000 | temporal 0.55906266-0.27953133 | total 0.37826505\n",
      "Epoch 39, loss: 0.37826505-(best 0.37826505)\n",
      "\n",
      "Detailed Loss: recon 0.20126557-0.10063279 | disen 0.60113370-0.00000000 | temporal 0.55902725-0.27951363 | total 0.38014641\n",
      "Epoch 40, loss: 0.38014641-(best 0.37826505)\n",
      "\n",
      "Detailed Loss: recon 0.19872609-0.09936304 | disen 0.60016036-0.00000000 | temporal 0.55881113-0.27940556 | total 0.37876862\n",
      "Epoch 41, loss: 0.37876862-(best 0.37826505)\n",
      "\n",
      "Detailed Loss: recon 0.20236710-0.10118355 | disen 0.60032380-0.00000000 | temporal 0.55860418-0.27930209 | total 0.38048565\n",
      "Epoch 42, loss: 0.38048565-(best 0.37826505)\n",
      "\n",
      "Detailed Loss: recon 0.20192719-0.10096359 | disen 0.60072488-0.00000000 | temporal 0.55860204-0.27930102 | total 0.38026461\n",
      "Epoch 43, loss: 0.38026461-(best 0.37826505)\n",
      "\n",
      "Detailed Loss: recon 0.20096681-0.10048340 | disen 0.60116243-0.00000000 | temporal 0.55861068-0.27930534 | total 0.37978876\n",
      "Epoch 44, loss: 0.37978876-(best 0.37826505)\n",
      "\n",
      "Detailed Loss: recon 0.20209657-0.10104828 | disen 0.60022444-0.00000000 | temporal 0.55860651-0.27930325 | total 0.38035154\n",
      "Epoch 45, loss: 0.38035154-(best 0.37826505)\n",
      "\n",
      "Detailed Loss: recon 0.20105711-0.10052855 | disen 0.60160005-0.00000000 | temporal 0.55859584-0.27929792 | total 0.37982649\n",
      "Epoch 46, loss: 0.37982649-(best 0.37826505)\n",
      "\n",
      "Detailed Loss: recon 0.20194492-0.10097246 | disen 0.59943044-0.00000000 | temporal 0.55858302-0.27929151 | total 0.38026398\n",
      "Epoch 47, loss: 0.38026398-(best 0.37826505)\n",
      "\n",
      "Detailed Loss: recon 0.19786963-0.09893481 | disen 0.59974599-0.00000000 | temporal 0.55866635-0.27933317 | total 0.37826800\n",
      "Epoch 48, loss: 0.37826800-(best 0.37826505)\n",
      "\n",
      "Detailed Loss: recon 0.19993818-0.09996909 | disen 0.59844583-0.00000000 | temporal 0.55867922-0.27933961 | total 0.37930870\n",
      "Epoch 49, loss: 0.37930870-(best 0.37826505)\n",
      "\n",
      "Detailed Loss: recon 0.19882357-0.09941179 | disen 0.59985250-0.00000000 | temporal 0.55879349-0.27939674 | total 0.37880853\n",
      "Epoch 50, loss: 0.37880853-(best 0.37826505)\n",
      "\n",
      ">> Reached final epoch. Loading best model...\n",
      "TRAINING XGB PREDICTOR\n",
      ">> Ending training!\n",
      "\n",
      "-----------------------\n",
      "TIME COST: 7.434353351593018 s\n",
      "Best Val: REC 54.95 PRE 47.47 MF1 65.41 AUC 73.82 TP 450 FP 498 TN 1721 FN 369\n",
      "-----------------------\n",
      "Generating additional positive data from adversary...\n",
      "Generating additional negative data by duplicating random nodes...\n",
      "\n",
      "ASSIGNED GRAPH FOR EVALUATION: 12345 {1: 2694, 0: 9651} Nodes, 1143392 Edges\n",
      "\n",
      "-------------------\n",
      "STARTING EVALUATION\n",
      "-------------------\n",
      "\n",
      "Dataset - Overall: REC 53.45 PRE 48.05 MF1 67.91 AUC 78.33 TP 1440 FP 1557 TN 8094 FN 1254 | 12345 {1: 2694, 0: 9651}\n",
      "Dataset - Train: REC 64.85 PRE 57.46 MF1 72.60 AUC 81.81 TP 797 FP 590 TN 2738 FN 432 | 4557 {1: 1229, 0: 3328}\n",
      "Dataset - Val: REC 52.38 PRE 46.18 MF1 64.26 AUC 73.51 TP 429 FP 500 TN 1719 FN 390 | 3038 {1: 819, 0: 2219}\n",
      "Dataset - Test: REC 33.13 PRE 31.42 MF1 60.63 AUC 73.66 TP 214 FP 467 TN 3637 FN 432 | 4750 {1: 646, 0: 4104}\n",
      "PREDICTION STATUS - {'1-True': 2262, '0-True': 6014, '0-False': 3637, '1-False': 432}\n",
      "    >> 432 positive nodes left unpredicted...\n",
      "    >> 350 OG positive nodes left unpredicted...\n",
      "-----------------------\n",
      "PREDICTION RESULT - ROUNDS\n",
      "Dataset - Round 0: REC 50.36 PRE 46.02 MF1 66.68 AUC 77.04 TP 625 FP 733 TN 3905 FN 616 | 5879 {1: 1241, 0: 4638}\n",
      "Dataset - Round 1: REC 35.94 PRE 46.00 MF1 62.99 AUC 72.96 TP 46 FP 54 TN 405 FN 82 | 587 {1: 128, 0: 459}\n",
      "-----------------------\n",
      "PREDICTION RESULT - SEEDS\n",
      "Seeds - Current: REC 40.62 PRE 41.27 MF1 62.32 AUC 70.32 TP 52 FP 74 TN 385 FN 76 | 587 {1: 128, 0: 459}\n",
      "Seeds - Prev: REC 0.00 PRE 0.00 MF1 38.15 AUC 70.32 TP 0 FP 97 TN 362 FN 128 | 587 {1: 128, 0: 459}\n",
      "Selecting data as budgeted ground truth for next round...\n",
      "  > Starting round 2...\n",
      "\n",
      "+++++++++++++++++\n",
      "STARTING ROUND 2!\n",
      "+++++++++++++++++\n",
      "        \n",
      "ASSIGNED GRAPH FOR TRAINING: 12345 {1: 2694, 0: 9651} Nodes, 1143392 Edges\n",
      "\n",
      ">> Splitting to train-validation set...\n",
      "Alotting train-val-test split for round 2\n",
      "Initial pool: 5879, Prediction pool: 6547, Budget pool: 0, Full pool: 8276\n",
      "Full graph size: 12345, Training: 4965, Val: 3311, Test: 12345\n",
      "    >> INITIAL DATA SPLIT: 4965 train rows ({1: 1357, 0: 3608}) | 3311 val rows ({0: 2406, 1: 905}) | 4069 test rows ({0: 3637, 1: 432})\n",
      "    >> AUGMENTED DATA SPLIT: 4965 train rows ({1: 1357, 0: 3608}) | 3311 val rows ({0: 2406, 1: 905}) | 4069 test rows ({0: 3637, 1: 432})\n",
      "    >> Updated cross-entropy weight to 2.658806190125276...\n",
      "\n",
      "-----------------------\n",
      "STARTING MODEL TRAININGps_l\n",
      "-----------------------\n",
      "\n",
      "TRAINING NODE EMBEDDER\n",
      "Detailed Loss: recon 0.20168369-0.10084184 | disen 0.59996331-0.00000000 | temporal 0.58189529-0.29094765 | total 0.39178950\n",
      "Epoch 1, loss: 0.39178950-(best 0.39178950)\n",
      "\n",
      "Detailed Loss: recon 0.26198989-0.13099495 | disen 0.59690195-0.00000000 | temporal 0.58073819-0.29036909 | total 0.42136404\n",
      "Epoch 2, loss: 0.42136404-(best 0.39178950)\n",
      "\n",
      "Detailed Loss: recon 0.22283964-0.11141982 | disen 0.59991068-0.00000000 | temporal 0.58206695-0.29103348 | total 0.40245330\n",
      "Epoch 3, loss: 0.40245330-(best 0.39178950)\n",
      "\n",
      "Detailed Loss: recon 0.22442274-0.11221137 | disen 0.60044074-0.00000000 | temporal 0.58246231-0.29123116 | total 0.40344253\n",
      "Epoch 4, loss: 0.40344253-(best 0.39178950)\n",
      "\n",
      "Detailed Loss: recon 0.21557014-0.10778507 | disen 0.60063136-0.00000000 | temporal 0.58233774-0.29116887 | total 0.39895394\n",
      "Epoch 5, loss: 0.39895394-(best 0.39178950)\n",
      "\n",
      "Detailed Loss: recon 0.21988557-0.10994279 | disen 0.59935981-0.00000000 | temporal 0.58212882-0.29106441 | total 0.40100721\n",
      "Epoch 6, loss: 0.40100721-(best 0.39178950)\n",
      "\n",
      "Detailed Loss: recon 0.22802910-0.11401455 | disen 0.60104597-0.00000000 | temporal 0.58209395-0.29104698 | total 0.40506154\n",
      "Epoch 7, loss: 0.40506154-(best 0.39178950)\n",
      "\n",
      "Detailed Loss: recon 0.22432083-0.11216041 | disen 0.60007876-0.00000000 | temporal 0.58219135-0.29109567 | total 0.40325609\n",
      "Epoch 8, loss: 0.40325609-(best 0.39178950)\n",
      "\n",
      "Detailed Loss: recon 0.21233425-0.10616712 | disen 0.59896916-0.00000000 | temporal 0.58240801-0.29120401 | total 0.39737111\n",
      "Epoch 9, loss: 0.39737111-(best 0.39178950)\n",
      "\n",
      "Detailed Loss: recon 0.21267162-0.10633581 | disen 0.60073125-0.00000000 | temporal 0.58257037-0.29128519 | total 0.39762101\n",
      "Epoch 10, loss: 0.39762101-(best 0.39178950)\n",
      "\n",
      "Detailed Loss: recon 0.21361536-0.10680768 | disen 0.60014862-0.00000000 | temporal 0.58270466-0.29135233 | total 0.39816001\n",
      "Epoch 11, loss: 0.39816001-(best 0.39178950)\n",
      "\n",
      "Detailed Loss: recon 0.21445136-0.10722568 | disen 0.59953159-0.00000000 | temporal 0.58259988-0.29129994 | total 0.39852563\n",
      "Epoch 12, loss: 0.39852563-(best 0.39178950)\n",
      "\n",
      "Detailed Loss: recon 0.20743565-0.10371783 | disen 0.59936440-0.00000000 | temporal 0.58235717-0.29117858 | total 0.39489642\n",
      "Epoch 13, loss: 0.39489642-(best 0.39178950)\n",
      "\n",
      "Detailed Loss: recon 0.21028730-0.10514365 | disen 0.59797537-0.00000000 | temporal 0.58202404-0.29101202 | total 0.39615566\n",
      "Epoch 14, loss: 0.39615566-(best 0.39178950)\n",
      "\n",
      "Detailed Loss: recon 0.21111381-0.10555691 | disen 0.59799635-0.00000000 | temporal 0.58185506-0.29092753 | total 0.39648443\n",
      "Epoch 15, loss: 0.39648443-(best 0.39178950)\n",
      "\n",
      "Detailed Loss: recon 0.21088776-0.10544388 | disen 0.59778237-0.00000000 | temporal 0.58188999-0.29094499 | total 0.39638889\n",
      "Epoch 16, loss: 0.39638889-(best 0.39178950)\n",
      "\n",
      "Detailed Loss: recon 0.20540038-0.10270019 | disen 0.59825909-0.00000000 | temporal 0.58200026-0.29100013 | total 0.39370030\n",
      "Epoch 17, loss: 0.39370030-(best 0.39178950)\n",
      "\n",
      "Detailed Loss: recon 0.20700926-0.10350463 | disen 0.59859955-0.00000000 | temporal 0.58208728-0.29104364 | total 0.39454827\n",
      "Epoch 18, loss: 0.39454827-(best 0.39178950)\n",
      "\n",
      "Detailed Loss: recon 0.20857243-0.10428622 | disen 0.59888214-0.00000000 | temporal 0.58216912-0.29108456 | total 0.39537078\n",
      "Epoch 19, loss: 0.39537078-(best 0.39178950)\n",
      "\n",
      "Detailed Loss: recon 0.20567912-0.10283956 | disen 0.60041773-0.00000000 | temporal 0.58199710-0.29099855 | total 0.39383811\n",
      "Epoch 20, loss: 0.39383811-(best 0.39178950)\n",
      "\n",
      "Detailed Loss: recon 0.20522326-0.10261163 | disen 0.59811085-0.00000000 | temporal 0.58177805-0.29088902 | total 0.39350066\n",
      "Epoch 21, loss: 0.39350066-(best 0.39178950)\n",
      "\n",
      "Detailed Loss: recon 0.20711559-0.10355780 | disen 0.59827733-0.00000000 | temporal 0.58175033-0.29087517 | total 0.39443296\n",
      "Epoch 22, loss: 0.39443296-(best 0.39178950)\n",
      "\n",
      "Detailed Loss: recon 0.20394288-0.10197144 | disen 0.59887487-0.00000000 | temporal 0.58184630-0.29092315 | total 0.39289460\n",
      "Epoch 23, loss: 0.39289460-(best 0.39178950)\n",
      "\n",
      "Detailed Loss: recon 0.20449863-0.10224932 | disen 0.59922707-0.00000000 | temporal 0.58194935-0.29097468 | total 0.39322400\n",
      "Epoch 24, loss: 0.39322400-(best 0.39178950)\n",
      "\n",
      "Detailed Loss: recon 0.20482898-0.10241449 | disen 0.59970582-0.00000000 | temporal 0.58204985-0.29102492 | total 0.39343941\n",
      "Epoch 25, loss: 0.39343941-(best 0.39178950)\n",
      "\n",
      "Detailed Loss: recon 0.20267701-0.10133851 | disen 0.59887719-0.00000000 | temporal 0.58205748-0.29102874 | total 0.39236724\n",
      "Epoch 26, loss: 0.39236724-(best 0.39178950)\n",
      "\n",
      "Detailed Loss: recon 0.20282066-0.10141033 | disen 0.59943426-0.00000000 | temporal 0.58195698-0.29097849 | total 0.39238882\n",
      "Epoch 27, loss: 0.39238882-(best 0.39178950)\n",
      "\n",
      "Detailed Loss: recon 0.20300035-0.10150018 | disen 0.59875542-0.00000000 | temporal 0.58191890-0.29095945 | total 0.39245963\n",
      "Epoch 28, loss: 0.39245963-(best 0.39178950)\n",
      "\n",
      "Detailed Loss: recon 0.20213950-0.10106975 | disen 0.59861463-0.00000000 | temporal 0.58194280-0.29097140 | total 0.39204115\n",
      "Epoch 29, loss: 0.39204115-(best 0.39178950)\n",
      "\n",
      "Detailed Loss: recon 0.20097350-0.10048675 | disen 0.59915364-0.00000000 | temporal 0.58197796-0.29098898 | total 0.39147574\n",
      "Epoch 30, loss: 0.39147574-(best 0.39147574)\n",
      "\n",
      "Detailed Loss: recon 0.19925271-0.09962635 | disen 0.59911424-0.00000000 | temporal 0.58205312-0.29102656 | total 0.39065292\n",
      "Epoch 31, loss: 0.39065292-(best 0.39065292)\n",
      "\n",
      "Detailed Loss: recon 0.20111357-0.10055678 | disen 0.59933889-0.00000000 | temporal 0.58206397-0.29103199 | total 0.39158878\n",
      "Epoch 32, loss: 0.39158878-(best 0.39065292)\n",
      "\n",
      "Detailed Loss: recon 0.19958261-0.09979130 | disen 0.59895062-0.00000000 | temporal 0.58199710-0.29099855 | total 0.39078987\n",
      "Epoch 33, loss: 0.39078987-(best 0.39065292)\n",
      "\n",
      "Detailed Loss: recon 0.19968592-0.09984296 | disen 0.59865355-0.00000000 | temporal 0.58186179-0.29093090 | total 0.39077386\n",
      "Epoch 34, loss: 0.39077386-(best 0.39065292)\n",
      "\n",
      "Detailed Loss: recon 0.20057353-0.10028677 | disen 0.59944040-0.00000000 | temporal 0.58177376-0.29088688 | total 0.39117366\n",
      "Epoch 35, loss: 0.39117366-(best 0.39065292)\n",
      "\n",
      "Detailed Loss: recon 0.20031337-0.10015669 | disen 0.59983140-0.00000000 | temporal 0.58178431-0.29089215 | total 0.39104885\n",
      "Epoch 36, loss: 0.39104885-(best 0.39065292)\n",
      "\n",
      "Detailed Loss: recon 0.19886753-0.09943376 | disen 0.59867513-0.00000000 | temporal 0.58178508-0.29089254 | total 0.39032632\n",
      "Epoch 37, loss: 0.39032632-(best 0.39032632)\n",
      "\n",
      "Detailed Loss: recon 0.19917750-0.09958875 | disen 0.59895074-0.00000000 | temporal 0.58174121-0.29087061 | total 0.39045936\n",
      "Epoch 38, loss: 0.39045936-(best 0.39032632)\n",
      "\n",
      "Detailed Loss: recon 0.19994268-0.09997134 | disen 0.59924626-0.00000000 | temporal 0.58167750-0.29083875 | total 0.39081007\n",
      "Epoch 39, loss: 0.39081007-(best 0.39032632)\n",
      "\n",
      "Detailed Loss: recon 0.19985907-0.09992953 | disen 0.59901363-0.00000000 | temporal 0.58154494-0.29077247 | total 0.39070201\n",
      "Epoch 40, loss: 0.39070201-(best 0.39032632)\n",
      "\n",
      "Detailed Loss: recon 0.19838718-0.09919359 | disen 0.59824717-0.00000000 | temporal 0.58148664-0.29074332 | total 0.38993692\n",
      "Epoch 41, loss: 0.38993692-(best 0.38993692)\n",
      "\n",
      "Detailed Loss: recon 0.19805793-0.09902897 | disen 0.59832168-0.00000000 | temporal 0.58148170-0.29074085 | total 0.38976982\n",
      "Epoch 42, loss: 0.38976982-(best 0.38976982)\n",
      "\n",
      "Detailed Loss: recon 0.19796839-0.09898420 | disen 0.59894097-0.00000000 | temporal 0.58157253-0.29078627 | total 0.38977045\n",
      "Epoch 43, loss: 0.38977045-(best 0.38976982)\n",
      "\n",
      "Detailed Loss: recon 0.19897442-0.09948721 | disen 0.59866929-0.00000000 | temporal 0.58155054-0.29077527 | total 0.39026248\n",
      "Epoch 44, loss: 0.39026248-(best 0.38976982)\n",
      "\n",
      "Detailed Loss: recon 0.19751757-0.09875879 | disen 0.59830576-0.00000000 | temporal 0.58151120-0.29075560 | total 0.38951439\n",
      "Epoch 45, loss: 0.38951439-(best 0.38951439)\n",
      "\n",
      "Detailed Loss: recon 0.19838379-0.09919190 | disen 0.59837258-0.00000000 | temporal 0.58145249-0.29072624 | total 0.38991815\n",
      "Epoch 46, loss: 0.38991815-(best 0.38951439)\n",
      "\n",
      "Detailed Loss: recon 0.19802630-0.09901315 | disen 0.59846556-0.00000000 | temporal 0.58140808-0.29070404 | total 0.38971719\n",
      "Epoch 47, loss: 0.38971719-(best 0.38951439)\n",
      "\n",
      "Detailed Loss: recon 0.19633129-0.09816565 | disen 0.59831250-0.00000000 | temporal 0.58148694-0.29074347 | total 0.38890910\n",
      "Epoch 48, loss: 0.38890910-(best 0.38890910)\n",
      "\n",
      "Detailed Loss: recon 0.19675849-0.09837925 | disen 0.59795517-0.00000000 | temporal 0.58150834-0.29075417 | total 0.38913342\n",
      "Epoch 49, loss: 0.38913342-(best 0.38890910)\n",
      "\n",
      "Detailed Loss: recon 0.19649354-0.09824677 | disen 0.59808135-0.00000000 | temporal 0.58143264-0.29071632 | total 0.38896310\n",
      "Epoch 50, loss: 0.38896310-(best 0.38890910)\n",
      "\n",
      ">> Reached final epoch. Loading best model...\n",
      "TRAINING XGB PREDICTOR\n",
      ">> Ending training!\n",
      "\n",
      "-----------------------\n",
      "TIME COST: 10.129687070846558 s\n",
      "Best Val: REC 65.64 PRE 46.15 MF1 65.77 AUC 73.78 TP 594 FP 693 TN 1713 FN 311\n",
      "-----------------------\n",
      "Generating additional positive data from adversary...\n",
      "Generating additional negative data by duplicating random nodes...\n",
      "\n",
      "ASSIGNED GRAPH FOR EVALUATION: 12932 {1: 2822, 0: 10110} Nodes, 1242132 Edges\n",
      "\n",
      "-------------------\n",
      "STARTING EVALUATION\n",
      "-------------------\n",
      "\n",
      "Dataset - Overall: REC 51.17 PRE 49.35 MF1 68.01 AUC 78.26 TP 1444 FP 1482 TN 8628 FN 1378 | 12932 {1: 2822, 0: 10110}\n",
      "Dataset - Train: REC 61.16 PRE 58.70 MF1 72.19 AUC 81.24 TP 830 FP 584 TN 3024 FN 527 | 4965 {1: 1357, 0: 3608}\n",
      "Dataset - Val: REC 50.17 PRE 46.80 MF1 64.03 AUC 72.56 TP 454 FP 516 TN 1890 FN 451 | 3311 {0: 2406, 1: 905}\n",
      "Dataset - Test: REC 28.57 PRE 29.52 MF1 59.76 AUC 73.49 TP 160 FP 382 TN 3714 FN 400 | 4656 {0: 4096, 1: 560}\n",
      "PREDICTION STATUS - {'1-True': 2422, '0-True': 6396, '0-False': 3714, '1-False': 400}\n",
      "    >> 400 positive nodes left unpredicted...\n",
      "    >> 249 OG positive nodes left unpredicted...\n",
      "-----------------------\n",
      "PREDICTION RESULT - ROUNDS\n",
      "Dataset - Round 0: REC 52.70 PRE 48.95 MF1 68.47 AUC 78.75 TP 654 FP 682 TN 3956 FN 587 | 5879 {1: 1241, 0: 4638}\n",
      "Dataset - Round 1: REC 42.97 PRE 45.08 MF1 64.42 AUC 75.68 TP 55 FP 67 TN 392 FN 73 | 587 {1: 128, 0: 459}\n",
      "Dataset - Round 2: REC 25.78 PRE 34.38 MF1 56.42 AUC 66.75 TP 33 FP 63 TN 396 FN 95 | 587 {1: 128, 0: 459}\n",
      "-----------------------\n",
      "PREDICTION RESULT - SEEDS\n",
      "Seeds - Current: REC 30.47 PRE 33.91 MF1 57.19 AUC 65.23 TP 39 FP 76 TN 383 FN 89 | 587 {1: 128, 0: 459}\n",
      "Seeds - Prev: REC 0.00 PRE 0.00 MF1 39.11 AUC 65.23 TP 0 FP 82 TN 377 FN 128 | 587 {1: 128, 0: 459}\n",
      "Selecting data as budgeted ground truth for next round...\n",
      "The file ../checkpoint/working_model_file_1730091720.938262_epoch.py does not exist\n",
      "The file ../checkpoint/working_model_file_1730091720.938262_round-M.json does not exist\n",
      "The file ../checkpoint/working_model_file_1730091720.938262_round-T.json does not exist\n",
      "The file ../checkpoint/working_model_file_1730091720.938262_round.pt does not exist\n",
      "Experiment ended, experienced 0 failures\n",
      "Elapsed experiment time 69.96220469s\n",
      "================\n",
      "++++++++++++++++\n",
      "TRIAL NUMBER 0\n",
      "++++++++++++++++\n",
      "================\n",
      "  > Rereading graph data...\n",
      "  > Initializing multiround object...\n",
      "11758 1049758\n",
      "\n",
      "=========================\n",
      "STARTING ADVER EXPERIMENT\n",
      "=========================\n",
      "MAIN CONFIG: [('device', 'cuda:0'), ('exp_type', 'ADVER'), ('full_oracle', False), ('node_expiration_time', 1), ('round_budget_neg', 0), ('round_budget_pos', 0), ('round_new_neg', 459), ('round_new_pos', 128), ('round_num', 3), ('save_df', True), ('save_embedding', False), ('save_temp_sne', False), ('setting_type', 'INDUCTIVE'), ('task_type', 'NODE'), ('verbose', 4)]\n",
      "MODEL CONFIG: [('act_name', 'LeakyReLU'), ('addon_internal_agg', 'OR'), ('addon_name', 'NONE'), ('addon_perc', 0.05), ('addon_round_window', 7), ('alpha', 1), ('att_heads', 2), ('beta', 0), ('boost_agg_backbone', <class 'models.benchmarks_supervised.booster.GIN_noparam'>), ('boost_metric', <function average_precision_score at 0x000001C8520A5760>), ('boost_predictor', <class 'xgboost.sklearn.XGBClassifier'>), ('device', 'cuda:0'), ('drop_rate', 0), ('dropout_rate', 0.1), ('early_stopping', 75), ('embed_type', 'temporal'), ('etypes', ['none']), ('gamma', 1), ('h_feats', 64), ('k', 1), ('lambda1', 1), ('lambda2', 1), ('lambda_', 1), ('loss_sample', True), ('loss_sample_ratio', 0.1), ('loss_type', 'ndist'), ('mlp_feats', 64), ('mlp_layers', 2), ('model_name', 'XGB-SP'), ('n_estimator', 500), ('norm_name', 'layer'), ('num_epoch', 100), ('num_layers', 2), ('num_round_epoch', 50), ('round_window', 7), ('temporal_agg', 'mean_final'), ('tloss_type', 'normal'), ('training_type', 'round'), ('verbose', 4)]\n",
      "STRAT CONFIG: [('augment_name', 'REAGE'), ('augment_neg_ratio', 0.09772070079945569), ('augment_pos_ratio', 0.02727929920054431), ('augment_prob', 0.5), ('augment_round_split', 8), ('device', 'cuda:0'), ('pseudo_name', 'NONE'), ('pseudo_new_pos_only', False), ('pseudo_skip_initial', False), ('verbose', 4)]\n",
      "TRAIN CONFIG: [('ce_weight', 2.658806190125276), ('device', 'cuda:0'), ('dset_name', 'tolokers_bid'), ('early_stopping', 75), ('learning_rate', 0.01), ('loss', <function cross_entropy at 0x000001C84AF6F880>), ('loss_gamma', 5), ('num_epoch', 100), ('num_round_epoch', 50), ('optimizer', <class 'torch.optim.adam.Adam'>), ('random_state', 7777777), ('ratio_initial', 0.5), ('ratio_test', 0.2), ('ratio_train', 0.6), ('ratio_val', 0.2), ('round_reset_model', False), ('round_train_list', []), ('stuck_stopping', 50), ('stuck_threshold', 0), ('train_max_retry', 10), ('verbose', 4)]\n",
      "ADVER CONFIG: [('adver_choose_name', 'GREEDY'), ('adver_conn_coef', 0.1), ('adver_feat_coef', 1), ('adver_mod_name', 'REPLAY'), ('device', 'cuda:0'), ('verbose', 4)]\n",
      "\n",
      "INITIAL GRAPH: 11758 {1: 2566, 0: 9192} Nodes, 1049758 Edges\n",
      "=========================\n",
      "        \n",
      ">> Initializing graph...\n",
      "MAIN GRAPH: 11758 {1: 2566, 0: 9192} Nodes, 1049758 Edges\n",
      "\n",
      ">> Initializing experiment type-specific modules:\n",
      "  >> Initializing adversarial modules...\n",
      ">> Initializing task-specific modules:\n",
      ">> Initializing classifier model...\n",
      ">> Initializing other strategies...\n",
      "\n",
      "FINISH INITIALIZATION\n",
      "=====================\n",
      "        \n",
      "  > Starting round 0...\n",
      "\n",
      "+++++++++++++++++\n",
      "STARTING ROUND 0!\n",
      "+++++++++++++++++\n",
      "        \n",
      "ASSIGNED GRAPH FOR TRAINING: 5879 {0: 4600, 1: 1279} Nodes, 262259 Edges\n",
      "\n",
      ">> Splitting to train-validation set...\n",
      "Alotting train-val-test split for round 0\n",
      "Initial pool: 0, Prediction pool: 0, Budget pool: 0, Full pool: 5879\n",
      "Full graph size: 5879, Training: 3527, Val: 2352, Test: 5879\n",
      "    >> INITIAL DATA SPLIT: 3527 train rows ({0: 2760, 1: 767}) | 2352 val rows ({0: 1840, 1: 512}) | 0 test rows ({})\n",
      "    >> AUGMENTED DATA SPLIT: 3527 train rows ({0: 2760, 1: 767}) | 2352 val rows ({0: 1840, 1: 512}) | 0 test rows ({})\n",
      "    >> Updated cross-entropy weight to 3.5984354628422426...\n",
      "\n",
      "-----------------------\n",
      "STARTING MODEL TRAININGps_l\n",
      "-----------------------\n",
      "\n",
      "TRAINING NODE EMBEDDER\n",
      "Detailed Loss: recon 0.68801796-0.34400898 | disen 0.70241648-0.35120824 | temporal 0.64106089-0.00000000 | total 0.69521725\n",
      "Epoch 1, loss: 0.69521725-(best 0.69521725)\n",
      "\n",
      "Detailed Loss: recon 0.43095192-0.21547596 | disen 0.71230125-0.35615063 | temporal 0.65412420-0.00000000 | total 0.57162660\n",
      "Epoch 2, loss: 0.57162660-(best 0.57162660)\n",
      "\n",
      "Detailed Loss: recon 0.37235722-0.18617861 | disen 0.71997559-0.35998780 | temporal 0.65848064-0.00000000 | total 0.54616642\n",
      "Epoch 3, loss: 0.54616642-(best 0.54616642)\n",
      "\n",
      "Detailed Loss: recon 0.34723932-0.17361966 | disen 0.71249211-0.35624605 | temporal 0.66059679-0.00000000 | total 0.52986574\n",
      "Epoch 4, loss: 0.52986574-(best 0.52986574)\n",
      "\n",
      "Detailed Loss: recon 0.33793724-0.16896862 | disen 0.70996404-0.35498202 | temporal 0.66205156-0.00000000 | total 0.52395064\n",
      "Epoch 5, loss: 0.52395064-(best 0.52395064)\n",
      "\n",
      "Detailed Loss: recon 0.31334090-0.15667045 | disen 0.70989370-0.35494685 | temporal 0.66252953-0.00000000 | total 0.51161730\n",
      "Epoch 6, loss: 0.51161730-(best 0.51161730)\n",
      "\n",
      "Detailed Loss: recon 0.31360787-0.15680394 | disen 0.70391178-0.35195589 | temporal 0.66272634-0.00000000 | total 0.50875986\n",
      "Epoch 7, loss: 0.50875986-(best 0.50875986)\n",
      "\n",
      "Detailed Loss: recon 0.31333584-0.15666792 | disen 0.70385218-0.35192609 | temporal 0.66298002-0.00000000 | total 0.50859404\n",
      "Epoch 8, loss: 0.50859404-(best 0.50859404)\n",
      "\n",
      "Detailed Loss: recon 0.29509386-0.14754693 | disen 0.69721317-0.34860659 | temporal 0.66318315-0.00000000 | total 0.49615353\n",
      "Epoch 9, loss: 0.49615353-(best 0.49615353)\n",
      "\n",
      "Detailed Loss: recon 0.29760623-0.14880311 | disen 0.69364184-0.34682092 | temporal 0.66373187-0.00000000 | total 0.49562404\n",
      "Epoch 10, loss: 0.49562404-(best 0.49562404)\n",
      "\n",
      "Detailed Loss: recon 0.29058856-0.14529428 | disen 0.69237459-0.34618729 | temporal 0.66424382-0.00000000 | total 0.49148157\n",
      "Epoch 11, loss: 0.49148157-(best 0.49148157)\n",
      "\n",
      "Detailed Loss: recon 0.28332683-0.14166342 | disen 0.69210625-0.34605312 | temporal 0.66504341-0.00000000 | total 0.48771656\n",
      "Epoch 12, loss: 0.48771656-(best 0.48771656)\n",
      "\n",
      "Detailed Loss: recon 0.27647844-0.13823922 | disen 0.68719602-0.34359801 | temporal 0.66587335-0.00000000 | total 0.48183721\n",
      "Epoch 13, loss: 0.48183721-(best 0.48183721)\n",
      "\n",
      "Detailed Loss: recon 0.27089396-0.13544698 | disen 0.68533552-0.34266776 | temporal 0.66649139-0.00000000 | total 0.47811472\n",
      "Epoch 14, loss: 0.47811472-(best 0.47811472)\n",
      "\n",
      "Detailed Loss: recon 0.27038407-0.13519204 | disen 0.68138492-0.34069246 | temporal 0.66686237-0.00000000 | total 0.47588450\n",
      "Epoch 15, loss: 0.47588450-(best 0.47588450)\n",
      "\n",
      "Detailed Loss: recon 0.26397127-0.13198563 | disen 0.67518902-0.33759451 | temporal 0.66682482-0.00000000 | total 0.46958014\n",
      "Epoch 16, loss: 0.46958014-(best 0.46958014)\n",
      "\n",
      "Detailed Loss: recon 0.26459005-0.13229503 | disen 0.68133610-0.34066805 | temporal 0.66672951-0.00000000 | total 0.47296309\n",
      "Epoch 17, loss: 0.47296309-(best 0.46958014)\n",
      "\n",
      "Detailed Loss: recon 0.26220137-0.13110068 | disen 0.66989851-0.33494925 | temporal 0.66663176-0.00000000 | total 0.46604994\n",
      "Epoch 18, loss: 0.46604994-(best 0.46604994)\n",
      "\n",
      "Detailed Loss: recon 0.26273015-0.13136508 | disen 0.67468935-0.33734468 | temporal 0.66640162-0.00000000 | total 0.46870977\n",
      "Epoch 19, loss: 0.46870977-(best 0.46604994)\n",
      "\n",
      "Detailed Loss: recon 0.25081319-0.12540659 | disen 0.66206497-0.33103248 | temporal 0.66667366-0.00000000 | total 0.45643908\n",
      "Epoch 20, loss: 0.45643908-(best 0.45643908)\n",
      "\n",
      "Detailed Loss: recon 0.25521725-0.12760863 | disen 0.66488409-0.33244205 | temporal 0.66706055-0.00000000 | total 0.46005067\n",
      "Epoch 21, loss: 0.46005067-(best 0.45643908)\n",
      "\n",
      "Detailed Loss: recon 0.24914929-0.12457465 | disen 0.65446794-0.32723397 | temporal 0.66730142-0.00000000 | total 0.45180863\n",
      "Epoch 22, loss: 0.45180863-(best 0.45180863)\n",
      "\n",
      "Detailed Loss: recon 0.24568312-0.12284156 | disen 0.65614128-0.32807064 | temporal 0.66737050-0.00000000 | total 0.45091221\n",
      "Epoch 23, loss: 0.45091221-(best 0.45091221)\n",
      "\n",
      "Detailed Loss: recon 0.24366236-0.12183118 | disen 0.64247954-0.32123977 | temporal 0.66731179-0.00000000 | total 0.44307095\n",
      "Epoch 24, loss: 0.44307095-(best 0.44307095)\n",
      "\n",
      "Detailed Loss: recon 0.24208552-0.12104276 | disen 0.64170152-0.32085076 | temporal 0.66716272-0.00000000 | total 0.44189352\n",
      "Epoch 25, loss: 0.44189352-(best 0.44189352)\n",
      "\n",
      "Detailed Loss: recon 0.24017772-0.12008886 | disen 0.64016163-0.32008082 | temporal 0.66753000-0.00000000 | total 0.44016969\n",
      "Epoch 26, loss: 0.44016969-(best 0.44016969)\n",
      "\n",
      "Detailed Loss: recon 0.24167511-0.12083755 | disen 0.64817101-0.32408550 | temporal 0.66739392-0.00000000 | total 0.44492304\n",
      "Epoch 27, loss: 0.44492304-(best 0.44016969)\n",
      "\n",
      "Detailed Loss: recon 0.24080282-0.12040141 | disen 0.64461082-0.32230541 | temporal 0.66735935-0.00000000 | total 0.44270682\n",
      "Epoch 28, loss: 0.44270682-(best 0.44016969)\n",
      "\n",
      "Detailed Loss: recon 0.24184626-0.12092313 | disen 0.63185191-0.31592596 | temporal 0.66740376-0.00000000 | total 0.43684909\n",
      "Epoch 29, loss: 0.43684909-(best 0.43684909)\n",
      "\n",
      "Detailed Loss: recon 0.24303250-0.12151625 | disen 0.63937962-0.31968981 | temporal 0.66771108-0.00000000 | total 0.44120607\n",
      "Epoch 30, loss: 0.44120607-(best 0.43684909)\n",
      "\n",
      "Detailed Loss: recon 0.24124722-0.12062361 | disen 0.63182318-0.31591159 | temporal 0.66781294-0.00000000 | total 0.43653521\n",
      "Epoch 31, loss: 0.43653521-(best 0.43653521)\n",
      "\n",
      "Detailed Loss: recon 0.23414761-0.11707380 | disen 0.62127995-0.31063998 | temporal 0.66771030-0.00000000 | total 0.42771378\n",
      "Epoch 32, loss: 0.42771378-(best 0.42771378)\n",
      "\n",
      "Detailed Loss: recon 0.24052474-0.12026237 | disen 0.61793303-0.30896652 | temporal 0.66780037-0.00000000 | total 0.42922890\n",
      "Epoch 33, loss: 0.42922890-(best 0.42771378)\n",
      "\n",
      "Detailed Loss: recon 0.23571479-0.11785740 | disen 0.61751378-0.30875689 | temporal 0.66762263-0.00000000 | total 0.42661428\n",
      "Epoch 34, loss: 0.42661428-(best 0.42661428)\n",
      "\n",
      "Detailed Loss: recon 0.23620152-0.11810076 | disen 0.60757279-0.30378640 | temporal 0.66751707-0.00000000 | total 0.42188716\n",
      "Epoch 35, loss: 0.42188716-(best 0.42188716)\n",
      "\n",
      "Detailed Loss: recon 0.23908781-0.11954390 | disen 0.60764384-0.30382192 | temporal 0.66746300-0.00000000 | total 0.42336583\n",
      "Epoch 36, loss: 0.42336583-(best 0.42188716)\n",
      "\n",
      "Detailed Loss: recon 0.23810963-0.11905482 | disen 0.60604084-0.30302042 | temporal 0.66766709-0.00000000 | total 0.42207524\n",
      "Epoch 37, loss: 0.42207524-(best 0.42188716)\n",
      "\n",
      "Detailed Loss: recon 0.23730622-0.11865311 | disen 0.59938914-0.29969457 | temporal 0.66760957-0.00000000 | total 0.41834769\n",
      "Epoch 38, loss: 0.41834769-(best 0.41834769)\n",
      "\n",
      "Detailed Loss: recon 0.23185608-0.11592804 | disen 0.59817207-0.29908603 | temporal 0.66785890-0.00000000 | total 0.41501409\n",
      "Epoch 39, loss: 0.41501409-(best 0.41501409)\n",
      "\n",
      "Detailed Loss: recon 0.23637675-0.11818837 | disen 0.58911180-0.29455590 | temporal 0.66778135-0.00000000 | total 0.41274428\n",
      "Epoch 40, loss: 0.41274428-(best 0.41274428)\n",
      "\n",
      "Detailed Loss: recon 0.23561469-0.11780734 | disen 0.58316767-0.29158384 | temporal 0.66796672-0.00000000 | total 0.40939116\n",
      "Epoch 41, loss: 0.40939116-(best 0.40939116)\n",
      "\n",
      "Detailed Loss: recon 0.23175041-0.11587521 | disen 0.57955146-0.28977573 | temporal 0.66782546-0.00000000 | total 0.40565094\n",
      "Epoch 42, loss: 0.40565094-(best 0.40565094)\n",
      "\n",
      "Detailed Loss: recon 0.23319623-0.11659811 | disen 0.57468951-0.28734475 | temporal 0.66767329-0.00000000 | total 0.40394288\n",
      "Epoch 43, loss: 0.40394288-(best 0.40394288)\n",
      "\n",
      "Detailed Loss: recon 0.23403141-0.11701570 | disen 0.57226187-0.28613093 | temporal 0.66792613-0.00000000 | total 0.40314662\n",
      "Epoch 44, loss: 0.40314662-(best 0.40314662)\n",
      "\n",
      "Detailed Loss: recon 0.23257327-0.11628664 | disen 0.56404364-0.28202182 | temporal 0.66774219-0.00000000 | total 0.39830846\n",
      "Epoch 45, loss: 0.39830846-(best 0.39830846)\n",
      "\n",
      "Detailed Loss: recon 0.23033428-0.11516714 | disen 0.56670797-0.28335398 | temporal 0.66791707-0.00000000 | total 0.39852113\n",
      "Epoch 46, loss: 0.39852113-(best 0.39830846)\n",
      "\n",
      "Detailed Loss: recon 0.23210150-0.11605075 | disen 0.55829704-0.27914852 | temporal 0.66773003-0.00000000 | total 0.39519927\n",
      "Epoch 47, loss: 0.39519927-(best 0.39519927)\n",
      "\n",
      "Detailed Loss: recon 0.23268723-0.11634362 | disen 0.56337965-0.28168982 | temporal 0.66779733-0.00000000 | total 0.39803344\n",
      "Epoch 48, loss: 0.39803344-(best 0.39519927)\n",
      "\n",
      "Detailed Loss: recon 0.23265031-0.11632515 | disen 0.55092049-0.27546024 | temporal 0.66783041-0.00000000 | total 0.39178538\n",
      "Epoch 49, loss: 0.39178538-(best 0.39178538)\n",
      "\n",
      "Detailed Loss: recon 0.22879609-0.11439805 | disen 0.55630392-0.27815196 | temporal 0.66761816-0.00000000 | total 0.39254999\n",
      "Epoch 50, loss: 0.39254999-(best 0.39178538)\n",
      "\n",
      "Detailed Loss: recon 0.23289424-0.11644712 | disen 0.54964542-0.27482271 | temporal 0.66716361-0.00000000 | total 0.39126983\n",
      "Epoch 51, loss: 0.39126983-(best 0.39126983)\n",
      "\n",
      "Detailed Loss: recon 0.23232734-0.11616367 | disen 0.54701519-0.27350760 | temporal 0.66769320-0.00000000 | total 0.38967127\n",
      "Epoch 52, loss: 0.38967127-(best 0.38967127)\n",
      "\n",
      "Detailed Loss: recon 0.22940896-0.11470448 | disen 0.54267311-0.27133656 | temporal 0.66744345-0.00000000 | total 0.38604105\n",
      "Epoch 53, loss: 0.38604105-(best 0.38604105)\n",
      "\n",
      "Detailed Loss: recon 0.23147628-0.11573814 | disen 0.53338671-0.26669335 | temporal 0.66731858-0.00000000 | total 0.38243151\n",
      "Epoch 54, loss: 0.38243151-(best 0.38243151)\n",
      "\n",
      "Detailed Loss: recon 0.23186362-0.11593181 | disen 0.54224443-0.27112222 | temporal 0.66762477-0.00000000 | total 0.38705403\n",
      "Epoch 55, loss: 0.38705403-(best 0.38243151)\n",
      "\n",
      "Detailed Loss: recon 0.22922990-0.11461495 | disen 0.53430378-0.26715189 | temporal 0.66782433-0.00000000 | total 0.38176686\n",
      "Epoch 56, loss: 0.38176686-(best 0.38176686)\n",
      "\n",
      "Detailed Loss: recon 0.23011725-0.11505862 | disen 0.52051544-0.26025772 | temporal 0.66724503-0.00000000 | total 0.37531635\n",
      "Epoch 57, loss: 0.37531635-(best 0.37531635)\n",
      "\n",
      "Detailed Loss: recon 0.23002890-0.11501445 | disen 0.51962018-0.25981009 | temporal 0.66694885-0.00000000 | total 0.37482452\n",
      "Epoch 58, loss: 0.37482452-(best 0.37482452)\n",
      "\n",
      "Detailed Loss: recon 0.23280859-0.11640429 | disen 0.51643759-0.25821880 | temporal 0.66717523-0.00000000 | total 0.37462309\n",
      "Epoch 59, loss: 0.37462309-(best 0.37462309)\n",
      "\n",
      "Detailed Loss: recon 0.23334262-0.11667131 | disen 0.50940424-0.25470212 | temporal 0.66683710-0.00000000 | total 0.37137341\n",
      "Epoch 60, loss: 0.37137341-(best 0.37137341)\n",
      "\n",
      "Detailed Loss: recon 0.23692551-0.11846276 | disen 0.51798600-0.25899300 | temporal 0.66676301-0.00000000 | total 0.37745577\n",
      "Epoch 61, loss: 0.37745577-(best 0.37137341)\n",
      "\n",
      "Detailed Loss: recon 0.23272185-0.11636093 | disen 0.50916278-0.25458139 | temporal 0.66704386-0.00000000 | total 0.37094232\n",
      "Epoch 62, loss: 0.37094232-(best 0.37094232)\n",
      "\n",
      "Detailed Loss: recon 0.22958672-0.11479336 | disen 0.50476277-0.25238138 | temporal 0.66677958-0.00000000 | total 0.36717474\n",
      "Epoch 63, loss: 0.36717474-(best 0.36717474)\n",
      "\n",
      "Detailed Loss: recon 0.23528138-0.11764069 | disen 0.49846905-0.24923453 | temporal 0.66619110-0.00000000 | total 0.36687523\n",
      "Epoch 64, loss: 0.36687523-(best 0.36687523)\n",
      "\n",
      "Detailed Loss: recon 0.23270707-0.11635353 | disen 0.49687445-0.24843723 | temporal 0.66710949-0.00000000 | total 0.36479077\n",
      "Epoch 65, loss: 0.36479077-(best 0.36479077)\n",
      "\n",
      "Detailed Loss: recon 0.23285007-0.11642504 | disen 0.49791288-0.24895644 | temporal 0.66670310-0.00000000 | total 0.36538148\n",
      "Epoch 66, loss: 0.36538148-(best 0.36479077)\n",
      "\n",
      "Detailed Loss: recon 0.23974793-0.11987396 | disen 0.48501825-0.24250913 | temporal 0.66553849-0.00000000 | total 0.36238310\n",
      "Epoch 67, loss: 0.36238310-(best 0.36238310)\n",
      "\n",
      "Detailed Loss: recon 0.23016229-0.11508115 | disen 0.48408830-0.24204415 | temporal 0.66665387-0.00000000 | total 0.35712528\n",
      "Epoch 68, loss: 0.35712528-(best 0.35712528)\n",
      "\n",
      "Detailed Loss: recon 0.23556346-0.11778173 | disen 0.48182505-0.24091253 | temporal 0.66684407-0.00000000 | total 0.35869426\n",
      "Epoch 69, loss: 0.35869426-(best 0.35712528)\n",
      "\n",
      "Detailed Loss: recon 0.24314079-0.12157039 | disen 0.48193240-0.24096620 | temporal 0.66540235-0.00000000 | total 0.36253661\n",
      "Epoch 70, loss: 0.36253661-(best 0.35712528)\n",
      "\n",
      "Detailed Loss: recon 0.23310018-0.11655009 | disen 0.48020440-0.24010220 | temporal 0.66640639-0.00000000 | total 0.35665229\n",
      "Epoch 71, loss: 0.35665229-(best 0.35665229)\n",
      "\n",
      "Detailed Loss: recon 0.23409717-0.11704858 | disen 0.47309756-0.23654878 | temporal 0.66614765-0.00000000 | total 0.35359737\n",
      "Epoch 72, loss: 0.35359737-(best 0.35359737)\n",
      "\n",
      "Detailed Loss: recon 0.23736775-0.11868387 | disen 0.47188526-0.23594263 | temporal 0.66584730-0.00000000 | total 0.35462651\n",
      "Epoch 73, loss: 0.35462651-(best 0.35359737)\n",
      "\n",
      "Detailed Loss: recon 0.23508069-0.11754034 | disen 0.47617155-0.23808578 | temporal 0.66586584-0.00000000 | total 0.35562611\n",
      "Epoch 74, loss: 0.35562611-(best 0.35359737)\n",
      "\n",
      "Detailed Loss: recon 0.23320810-0.11660405 | disen 0.47388476-0.23694238 | temporal 0.66592562-0.00000000 | total 0.35354644\n",
      "Epoch 75, loss: 0.35354644-(best 0.35354644)\n",
      "\n",
      "Detailed Loss: recon 0.23244318-0.11622159 | disen 0.46614003-0.23307002 | temporal 0.66591215-0.00000000 | total 0.34929162\n",
      "Epoch 76, loss: 0.34929162-(best 0.34929162)\n",
      "\n",
      "Detailed Loss: recon 0.23560433-0.11780217 | disen 0.45800346-0.22900173 | temporal 0.66509587-0.00000000 | total 0.34680390\n",
      "Epoch 77, loss: 0.34680390-(best 0.34680390)\n",
      "\n",
      "Detailed Loss: recon 0.23693638-0.11846819 | disen 0.45976847-0.22988424 | temporal 0.66454971-0.00000000 | total 0.34835243\n",
      "Epoch 78, loss: 0.34835243-(best 0.34680390)\n",
      "\n",
      "Detailed Loss: recon 0.23265646-0.11632823 | disen 0.45545721-0.22772861 | temporal 0.66552275-0.00000000 | total 0.34405684\n",
      "Epoch 79, loss: 0.34405684-(best 0.34405684)\n",
      "\n",
      "Detailed Loss: recon 0.23225972-0.11612986 | disen 0.45123869-0.22561935 | temporal 0.66512638-0.00000000 | total 0.34174919\n",
      "Epoch 80, loss: 0.34174919-(best 0.34174919)\n",
      "\n",
      "Detailed Loss: recon 0.24001086-0.12000543 | disen 0.44419181-0.22209591 | temporal 0.66451448-0.00000000 | total 0.34210134\n",
      "Epoch 81, loss: 0.34210134-(best 0.34174919)\n",
      "\n",
      "Detailed Loss: recon 0.23291081-0.11645541 | disen 0.46315408-0.23157704 | temporal 0.66541433-0.00000000 | total 0.34803244\n",
      "Epoch 82, loss: 0.34803244-(best 0.34174919)\n",
      "\n",
      "Detailed Loss: recon 0.23503846-0.11751923 | disen 0.45605260-0.22802630 | temporal 0.66460299-0.00000000 | total 0.34554553\n",
      "Epoch 83, loss: 0.34554553-(best 0.34174919)\n",
      "\n",
      "Detailed Loss: recon 0.23515856-0.11757928 | disen 0.44488913-0.22244456 | temporal 0.66481256-0.00000000 | total 0.34002385\n",
      "Epoch 84, loss: 0.34002385-(best 0.34002385)\n",
      "\n",
      "Detailed Loss: recon 0.23523629-0.11761814 | disen 0.44290346-0.22145173 | temporal 0.66505802-0.00000000 | total 0.33906987\n",
      "Epoch 85, loss: 0.33906987-(best 0.33906987)\n",
      "\n",
      "Detailed Loss: recon 0.23527496-0.11763748 | disen 0.43775582-0.21887791 | temporal 0.66458386-0.00000000 | total 0.33651540\n",
      "Epoch 86, loss: 0.33651540-(best 0.33651540)\n",
      "\n",
      "Detailed Loss: recon 0.23650986-0.11825493 | disen 0.44118476-0.22059238 | temporal 0.66428477-0.00000000 | total 0.33884731\n",
      "Epoch 87, loss: 0.33884731-(best 0.33651540)\n",
      "\n",
      "Detailed Loss: recon 0.23841682-0.11920841 | disen 0.44051379-0.22025689 | temporal 0.66491854-0.00000000 | total 0.33946532\n",
      "Epoch 88, loss: 0.33946532-(best 0.33651540)\n",
      "\n",
      "Detailed Loss: recon 0.23627150-0.11813575 | disen 0.44183147-0.22091573 | temporal 0.66516095-0.00000000 | total 0.33905149\n",
      "Epoch 89, loss: 0.33905149-(best 0.33651540)\n",
      "\n",
      "Detailed Loss: recon 0.24144685-0.12072343 | disen 0.43280512-0.21640256 | temporal 0.66384566-0.00000000 | total 0.33712599\n",
      "Epoch 90, loss: 0.33712599-(best 0.33651540)\n",
      "\n",
      "Detailed Loss: recon 0.23983519-0.11991759 | disen 0.42958492-0.21479246 | temporal 0.66403836-0.00000000 | total 0.33471006\n",
      "Epoch 91, loss: 0.33471006-(best 0.33471006)\n",
      "\n",
      "Detailed Loss: recon 0.23863363-0.11931682 | disen 0.42062747-0.21031374 | temporal 0.66401827-0.00000000 | total 0.32963055\n",
      "Epoch 92, loss: 0.32963055-(best 0.32963055)\n",
      "\n",
      "Detailed Loss: recon 0.23785201-0.11892600 | disen 0.42490739-0.21245369 | temporal 0.66381353-0.00000000 | total 0.33137971\n",
      "Epoch 93, loss: 0.33137971-(best 0.32963055)\n",
      "\n",
      "Detailed Loss: recon 0.23755610-0.11877805 | disen 0.41745913-0.20872957 | temporal 0.66285425-0.00000000 | total 0.32750762\n",
      "Epoch 94, loss: 0.32750762-(best 0.32750762)\n",
      "\n",
      "Detailed Loss: recon 0.23841694-0.11920847 | disen 0.41970855-0.20985427 | temporal 0.66358572-0.00000000 | total 0.32906276\n",
      "Epoch 95, loss: 0.32906276-(best 0.32750762)\n",
      "\n",
      "Detailed Loss: recon 0.23410019-0.11705010 | disen 0.41871405-0.20935702 | temporal 0.66359198-0.00000000 | total 0.32640713\n",
      "Epoch 96, loss: 0.32640713-(best 0.32640713)\n",
      "\n",
      "Detailed Loss: recon 0.23728266-0.11864133 | disen 0.41220087-0.20610043 | temporal 0.66333884-0.00000000 | total 0.32474178\n",
      "Epoch 97, loss: 0.32474178-(best 0.32474178)\n",
      "\n",
      "Detailed Loss: recon 0.24186596-0.12093298 | disen 0.41441071-0.20720536 | temporal 0.66321903-0.00000000 | total 0.32813835\n",
      "Epoch 98, loss: 0.32813835-(best 0.32474178)\n",
      "\n",
      "Detailed Loss: recon 0.23508243-0.11754122 | disen 0.41369712-0.20684856 | temporal 0.66369826-0.00000000 | total 0.32438979\n",
      "Epoch 99, loss: 0.32438979-(best 0.32438979)\n",
      "\n",
      "Detailed Loss: recon 0.24255201-0.12127601 | disen 0.41220129-0.20610064 | temporal 0.66270000-0.00000000 | total 0.32737666\n",
      "Epoch 100, loss: 0.32737666-(best 0.32438979)\n",
      "\n",
      ">> Reached final epoch. Loading best model...\n",
      "TRAINING XGB PREDICTOR\n",
      ">> Ending training!\n",
      "\n",
      "-----------------------\n",
      "TIME COST: 6.224073648452759 s\n",
      "Best Val: REC 54.88 PRE 45.99 MF1 67.19 AUC 79.14 TP 281 FP 330 TN 1510 FN 231\n",
      "-----------------------\n",
      "\n",
      "ASSIGNED GRAPH FOR EVALUATION: 11758 {1: 2566, 0: 9192} Nodes, 1049758 Edges\n",
      "\n",
      "-------------------\n",
      "STARTING EVALUATION\n",
      "-------------------\n",
      "\n",
      "Dataset - Overall: REC 58.61 PRE 43.56 MF1 66.38 AUC 79.01 TP 1504 FP 1949 TN 7243 FN 1062 | 11758 {1: 2566, 0: 9192}\n",
      "Dataset - Train: REC 59.58 PRE 49.25 MF1 69.67 AUC 82.67 TP 457 FP 471 TN 2289 FN 310 | 3527 {0: 2760, 1: 767}\n",
      "Dataset - Val: REC 49.22 PRE 41.58 MF1 63.98 AUC 76.17 TP 252 FP 354 TN 1486 FN 260 | 2352 {0: 1840, 1: 512}\n",
      "Dataset - Test: REC 61.77 PRE 41.43 MF1 65.35 AUC 78.04 TP 795 FP 1124 TN 3468 FN 492 | 5879 {1: 1287, 0: 4592}\n",
      "PREDICTION STATUS - {'1-True': 2074, '0-False': 3468, '0-True': 5724, '1-False': 492}\n",
      "    >> 492 positive nodes left unpredicted...\n",
      "    >> 492 OG positive nodes left unpredicted...\n",
      "Selecting data as budgeted ground truth for next round...\n",
      "  > Starting round 1...\n",
      "\n",
      "+++++++++++++++++\n",
      "STARTING ROUND 1!\n",
      "+++++++++++++++++\n",
      "        \n",
      "ASSIGNED GRAPH FOR TRAINING: 11758 {1: 2566, 0: 9192} Nodes, 1049758 Edges\n",
      "\n",
      ">> Splitting to train-validation set...\n",
      "Alotting train-val-test split for round 1\n",
      "Initial pool: 5879, Prediction pool: 3453, Budget pool: 0, Full pool: 7798\n",
      "Full graph size: 11758, Training: 4678, Val: 3120, Test: 11758\n",
      "    >> INITIAL DATA SPLIT: 4678 train rows ({1: 1244, 0: 3434}) | 3120 val rows ({1: 830, 0: 2290}) | 3960 test rows ({0: 3468, 1: 492})\n",
      "    >> AUGMENTED DATA SPLIT: 4678 train rows ({1: 1244, 0: 3434}) | 3120 val rows ({1: 830, 0: 2290}) | 3960 test rows ({0: 3468, 1: 492})\n",
      "    >> Updated cross-entropy weight to 2.760450160771704...\n",
      "\n",
      "-----------------------\n",
      "STARTING MODEL TRAININGps_l\n",
      "-----------------------\n",
      "\n",
      "TRAINING NODE EMBEDDER\n",
      "Detailed Loss: recon 0.21995960-0.10997980 | disen 0.37147129-0.18573564 | temporal 0.56276435-0.00000000 | total 0.29571545\n",
      "Epoch 1, loss: 0.29571545-(best 0.29571545)\n",
      "\n",
      "Detailed Loss: recon 0.33412966-0.16706483 | disen 0.42057967-0.21028984 | temporal 0.56512409-0.00000000 | total 0.37735468\n",
      "Epoch 2, loss: 0.37735468-(best 0.29571545)\n",
      "\n",
      "Detailed Loss: recon 0.29018253-0.14509127 | disen 0.43152624-0.21576312 | temporal 0.56805706-0.00000000 | total 0.36085439\n",
      "Epoch 3, loss: 0.36085439-(best 0.29571545)\n",
      "\n",
      "Detailed Loss: recon 0.28835821-0.14417911 | disen 0.41803700-0.20901850 | temporal 0.56821221-0.00000000 | total 0.35319760\n",
      "Epoch 4, loss: 0.35319760-(best 0.29571545)\n",
      "\n",
      "Detailed Loss: recon 0.26797700-0.13398850 | disen 0.41013348-0.20506674 | temporal 0.56798887-0.00000000 | total 0.33905524\n",
      "Epoch 5, loss: 0.33905524-(best 0.29571545)\n",
      "\n",
      "Detailed Loss: recon 0.24570233-0.12285116 | disen 0.40520799-0.20260400 | temporal 0.56765908-0.00000000 | total 0.32545516\n",
      "Epoch 6, loss: 0.32545516-(best 0.29571545)\n",
      "\n",
      "Detailed Loss: recon 0.24109571-0.12054785 | disen 0.40763181-0.20381591 | temporal 0.56738061-0.00000000 | total 0.32436377\n",
      "Epoch 7, loss: 0.32436377-(best 0.29571545)\n",
      "\n",
      "Detailed Loss: recon 0.22622626-0.11311313 | disen 0.40883672-0.20441836 | temporal 0.56699818-0.00000000 | total 0.31753150\n",
      "Epoch 8, loss: 0.31753150-(best 0.29571545)\n",
      "\n",
      "Detailed Loss: recon 0.21597674-0.10798837 | disen 0.41004121-0.20502061 | temporal 0.56679702-0.00000000 | total 0.31300896\n",
      "Epoch 9, loss: 0.31300896-(best 0.29571545)\n",
      "\n",
      "Detailed Loss: recon 0.21954879-0.10977440 | disen 0.41135067-0.20567533 | temporal 0.56671882-0.00000000 | total 0.31544971\n",
      "Epoch 10, loss: 0.31544971-(best 0.29571545)\n",
      "\n",
      "Detailed Loss: recon 0.21774393-0.10887197 | disen 0.40788710-0.20394355 | temporal 0.56667030-0.00000000 | total 0.31281552\n",
      "Epoch 11, loss: 0.31281552-(best 0.29571545)\n",
      "\n",
      "Detailed Loss: recon 0.21736805-0.10868403 | disen 0.40405613-0.20202807 | temporal 0.56653315-0.00000000 | total 0.31071210\n",
      "Epoch 12, loss: 0.31071210-(best 0.29571545)\n",
      "\n",
      "Detailed Loss: recon 0.21246648-0.10623324 | disen 0.40347230-0.20173615 | temporal 0.56653678-0.00000000 | total 0.30796939\n",
      "Epoch 13, loss: 0.30796939-(best 0.29571545)\n",
      "\n",
      "Detailed Loss: recon 0.21187156-0.10593578 | disen 0.39907628-0.19953814 | temporal 0.56656575-0.00000000 | total 0.30547392\n",
      "Epoch 14, loss: 0.30547392-(best 0.29571545)\n",
      "\n",
      "Detailed Loss: recon 0.20858729-0.10429364 | disen 0.39651263-0.19825631 | temporal 0.56649625-0.00000000 | total 0.30254996\n",
      "Epoch 15, loss: 0.30254996-(best 0.29571545)\n",
      "\n",
      "Detailed Loss: recon 0.21218801-0.10609400 | disen 0.39447081-0.19723541 | temporal 0.56639171-0.00000000 | total 0.30332941\n",
      "Epoch 16, loss: 0.30332941-(best 0.29571545)\n",
      "\n",
      "Detailed Loss: recon 0.20819160-0.10409580 | disen 0.38446105-0.19223052 | temporal 0.56626356-0.00000000 | total 0.29632634\n",
      "Epoch 17, loss: 0.29632634-(best 0.29571545)\n",
      "\n",
      "Detailed Loss: recon 0.21011478-0.10505739 | disen 0.38597643-0.19298822 | temporal 0.56629872-0.00000000 | total 0.29804561\n",
      "Epoch 18, loss: 0.29804561-(best 0.29571545)\n",
      "\n",
      "Detailed Loss: recon 0.21210396-0.10605198 | disen 0.37773871-0.18886936 | temporal 0.56603211-0.00000000 | total 0.29492134\n",
      "Epoch 19, loss: 0.29492134-(best 0.29492134)\n",
      "\n",
      "Detailed Loss: recon 0.21192114-0.10596057 | disen 0.37743366-0.18871683 | temporal 0.56599838-0.00000000 | total 0.29467741\n",
      "Epoch 20, loss: 0.29467741-(best 0.29467741)\n",
      "\n",
      "Detailed Loss: recon 0.20817243-0.10408621 | disen 0.37339348-0.18669674 | temporal 0.56571817-0.00000000 | total 0.29078296\n",
      "Epoch 21, loss: 0.29078296-(best 0.29078296)\n",
      "\n",
      "Detailed Loss: recon 0.20812428-0.10406214 | disen 0.36890614-0.18445307 | temporal 0.56553495-0.00000000 | total 0.28851521\n",
      "Epoch 22, loss: 0.28851521-(best 0.28851521)\n",
      "\n",
      "Detailed Loss: recon 0.20763920-0.10381960 | disen 0.36606318-0.18303159 | temporal 0.56544256-0.00000000 | total 0.28685120\n",
      "Epoch 23, loss: 0.28685120-(best 0.28685120)\n",
      "\n",
      "Detailed Loss: recon 0.20615834-0.10307917 | disen 0.36152202-0.18076101 | temporal 0.56525576-0.00000000 | total 0.28384018\n",
      "Epoch 24, loss: 0.28384018-(best 0.28384018)\n",
      "\n",
      "Detailed Loss: recon 0.20844643-0.10422321 | disen 0.35859793-0.17929897 | temporal 0.56526595-0.00000000 | total 0.28352219\n",
      "Epoch 25, loss: 0.28352219-(best 0.28352219)\n",
      "\n",
      "Detailed Loss: recon 0.20846209-0.10423104 | disen 0.35732585-0.17866293 | temporal 0.56525666-0.00000000 | total 0.28289396\n",
      "Epoch 26, loss: 0.28289396-(best 0.28289396)\n",
      "\n",
      "Detailed Loss: recon 0.20675588-0.10337794 | disen 0.35439724-0.17719862 | temporal 0.56526679-0.00000000 | total 0.28057656\n",
      "Epoch 27, loss: 0.28057656-(best 0.28057656)\n",
      "\n",
      "Detailed Loss: recon 0.20746705-0.10373352 | disen 0.35180378-0.17590189 | temporal 0.56522346-0.00000000 | total 0.27963543\n",
      "Epoch 28, loss: 0.27963543-(best 0.27963543)\n",
      "\n",
      "Detailed Loss: recon 0.20651257-0.10325629 | disen 0.34888834-0.17444417 | temporal 0.56511313-0.00000000 | total 0.27770045\n",
      "Epoch 29, loss: 0.27770045-(best 0.27770045)\n",
      "\n",
      "Detailed Loss: recon 0.20227501-0.10113750 | disen 0.35009372-0.17504686 | temporal 0.56512475-0.00000000 | total 0.27618438\n",
      "Epoch 30, loss: 0.27618438-(best 0.27618438)\n",
      "\n",
      "Detailed Loss: recon 0.20263174-0.10131587 | disen 0.34598398-0.17299199 | temporal 0.56511092-0.00000000 | total 0.27430785\n",
      "Epoch 31, loss: 0.27430785-(best 0.27430785)\n",
      "\n",
      "Detailed Loss: recon 0.20203845-0.10101923 | disen 0.34613699-0.17306849 | temporal 0.56521696-0.00000000 | total 0.27408773\n",
      "Epoch 32, loss: 0.27408773-(best 0.27408773)\n",
      "\n",
      "Detailed Loss: recon 0.20149486-0.10074743 | disen 0.34406096-0.17203048 | temporal 0.56521666-0.00000000 | total 0.27277792\n",
      "Epoch 33, loss: 0.27277792-(best 0.27277792)\n",
      "\n",
      "Detailed Loss: recon 0.20312393-0.10156196 | disen 0.34138572-0.17069286 | temporal 0.56522059-0.00000000 | total 0.27225482\n",
      "Epoch 34, loss: 0.27225482-(best 0.27225482)\n",
      "\n",
      "Detailed Loss: recon 0.20295951-0.10147975 | disen 0.34163290-0.17081645 | temporal 0.56528312-0.00000000 | total 0.27229619\n",
      "Epoch 35, loss: 0.27229619-(best 0.27225482)\n",
      "\n",
      "Detailed Loss: recon 0.20259027-0.10129514 | disen 0.33625215-0.16812608 | temporal 0.56513977-0.00000000 | total 0.26942122\n",
      "Epoch 36, loss: 0.26942122-(best 0.26942122)\n",
      "\n",
      "Detailed Loss: recon 0.20375919-0.10187960 | disen 0.33735925-0.16867962 | temporal 0.56527847-0.00000000 | total 0.27055922\n",
      "Epoch 37, loss: 0.27055922-(best 0.26942122)\n",
      "\n",
      "Detailed Loss: recon 0.19818883-0.09909441 | disen 0.33472025-0.16736013 | temporal 0.56517738-0.00000000 | total 0.26645455\n",
      "Epoch 38, loss: 0.26645455-(best 0.26645455)\n",
      "\n",
      "Detailed Loss: recon 0.20205036-0.10102518 | disen 0.33526599-0.16763300 | temporal 0.56517220-0.00000000 | total 0.26865816\n",
      "Epoch 39, loss: 0.26865816-(best 0.26645455)\n",
      "\n",
      "Detailed Loss: recon 0.20143287-0.10071643 | disen 0.33339477-0.16669738 | temporal 0.56521940-0.00000000 | total 0.26741382\n",
      "Epoch 40, loss: 0.26741382-(best 0.26645455)\n",
      "\n",
      "Detailed Loss: recon 0.19859421-0.09929711 | disen 0.33472437-0.16736218 | temporal 0.56528968-0.00000000 | total 0.26665929\n",
      "Epoch 41, loss: 0.26665929-(best 0.26645455)\n",
      "\n",
      "Detailed Loss: recon 0.19654965-0.09827483 | disen 0.33272219-0.16636109 | temporal 0.56517631-0.00000000 | total 0.26463592\n",
      "Epoch 42, loss: 0.26463592-(best 0.26463592)\n",
      "\n",
      "Detailed Loss: recon 0.19664386-0.09832193 | disen 0.33257264-0.16628632 | temporal 0.56523627-0.00000000 | total 0.26460826\n",
      "Epoch 43, loss: 0.26460826-(best 0.26460826)\n",
      "\n",
      "Detailed Loss: recon 0.19559877-0.09779938 | disen 0.33035243-0.16517621 | temporal 0.56512713-0.00000000 | total 0.26297560\n",
      "Epoch 44, loss: 0.26297560-(best 0.26297560)\n",
      "\n",
      "Detailed Loss: recon 0.19647871-0.09823935 | disen 0.33033353-0.16516677 | temporal 0.56502730-0.00000000 | total 0.26340613\n",
      "Epoch 45, loss: 0.26340613-(best 0.26297560)\n",
      "\n",
      "Detailed Loss: recon 0.19619057-0.09809528 | disen 0.33018625-0.16509312 | temporal 0.56504381-0.00000000 | total 0.26318842\n",
      "Epoch 46, loss: 0.26318842-(best 0.26297560)\n",
      "\n",
      "Detailed Loss: recon 0.19674733-0.09837367 | disen 0.32789683-0.16394842 | temporal 0.56495059-0.00000000 | total 0.26232207\n",
      "Epoch 47, loss: 0.26232207-(best 0.26232207)\n",
      "\n",
      "Detailed Loss: recon 0.19546807-0.09773403 | disen 0.32659245-0.16329622 | temporal 0.56499672-0.00000000 | total 0.26103026\n",
      "Epoch 48, loss: 0.26103026-(best 0.26103026)\n",
      "\n",
      "Detailed Loss: recon 0.19795305-0.09897652 | disen 0.32480693-0.16240346 | temporal 0.56506294-0.00000000 | total 0.26137999\n",
      "Epoch 49, loss: 0.26137999-(best 0.26103026)\n",
      "\n",
      "Detailed Loss: recon 0.19607982-0.09803991 | disen 0.32371086-0.16185543 | temporal 0.56516743-0.00000000 | total 0.25989532\n",
      "Epoch 50, loss: 0.25989532-(best 0.25989532)\n",
      "\n",
      ">> Reached final epoch. Loading best model...\n",
      "TRAINING XGB PREDICTOR\n",
      ">> Ending training!\n",
      "\n",
      "-----------------------\n",
      "TIME COST: 6.583891153335571 s\n",
      "Best Val: REC 69.28 PRE 47.48 MF1 67.56 AUC 77.31 TP 575 FP 636 TN 1654 FN 255\n",
      "-----------------------\n",
      "Generating additional positive data from adversary...\n",
      "Generating additional negative data by duplicating random nodes...\n",
      "\n",
      "ASSIGNED GRAPH FOR EVALUATION: 12345 {1: 2694, 0: 9651} Nodes, 1166076 Edges\n",
      "\n",
      "-------------------\n",
      "STARTING EVALUATION\n",
      "-------------------\n",
      "\n",
      "Dataset - Overall: REC 53.56 PRE 49.81 MF1 68.73 AUC 79.76 TP 1443 FP 1454 TN 8197 FN 1251 | 12345 {1: 2694, 0: 9651}\n",
      "Dataset - Train: REC 64.47 PRE 58.50 MF1 73.17 AUC 83.43 TP 802 FP 569 TN 2865 FN 442 | 4678 {1: 1244, 0: 3434}\n",
      "Dataset - Val: REC 52.77 PRE 47.20 MF1 65.08 AUC 74.66 TP 438 FP 490 TN 1800 FN 392 | 3120 {1: 830, 0: 2290}\n",
      "Dataset - Test: REC 32.74 PRE 33.95 MF1 61.51 AUC 75.58 TP 203 FP 395 TN 3532 FN 417 | 4547 {0: 3927, 1: 620}\n",
      "PREDICTION STATUS - {'1-True': 2277, '0-False': 3532, '0-True': 6119, '1-False': 417}\n",
      "    >> 417 positive nodes left unpredicted...\n",
      "    >> 330 OG positive nodes left unpredicted...\n",
      "-----------------------\n",
      "PREDICTION RESULT - ROUNDS\n",
      "Dataset - Round 0: REC 54.00 PRE 48.98 MF1 68.41 AUC 79.10 TP 695 FP 724 TN 3868 FN 592 | 5879 {1: 1287, 0: 4592}\n",
      "Dataset - Round 1: REC 32.03 PRE 40.20 MF1 59.99 AUC 69.89 TP 41 FP 61 TN 398 FN 87 | 587 {1: 128, 0: 459}\n",
      "-----------------------\n",
      "PREDICTION RESULT - SEEDS\n",
      "Seeds - Current: REC 35.16 PRE 41.28 MF1 61.14 AUC 71.50 TP 45 FP 64 TN 395 FN 83 | 587 {1: 128, 0: 459}\n",
      "Seeds - Prev: REC 0.00 PRE 0.00 MF1 37.88 AUC 71.50 TP 0 FP 101 TN 358 FN 128 | 587 {1: 128, 0: 459}\n",
      "Selecting data as budgeted ground truth for next round...\n",
      "  > Starting round 2...\n",
      "\n",
      "+++++++++++++++++\n",
      "STARTING ROUND 2!\n",
      "+++++++++++++++++\n",
      "        \n",
      "ASSIGNED GRAPH FOR TRAINING: 12345 {1: 2694, 0: 9651} Nodes, 1166076 Edges\n",
      "\n",
      ">> Splitting to train-validation set...\n",
      "Alotting train-val-test split for round 2\n",
      "Initial pool: 5879, Prediction pool: 6350, Budget pool: 0, Full pool: 8396\n",
      "Full graph size: 12345, Training: 5037, Val: 3359, Test: 12345\n",
      "    >> INITIAL DATA SPLIT: 5037 train rows ({1: 1366, 0: 3671}) | 3359 val rows ({1: 911, 0: 2448}) | 3949 test rows ({0: 3532, 1: 417})\n",
      "    >> AUGMENTED DATA SPLIT: 5037 train rows ({1: 1366, 0: 3671}) | 3359 val rows ({1: 911, 0: 2448}) | 3949 test rows ({0: 3532, 1: 417})\n",
      "    >> Updated cross-entropy weight to 2.6874084919472914...\n",
      "\n",
      "-----------------------\n",
      "STARTING MODEL TRAININGps_l\n",
      "-----------------------\n",
      "\n",
      "TRAINING NODE EMBEDDER\n",
      "Detailed Loss: recon 0.19609970-0.09804985 | disen 0.34448248-0.17224124 | temporal 0.58538944-0.00000000 | total 0.27029109\n",
      "Epoch 1, loss: 0.27029109-(best 0.27029109)\n",
      "\n",
      "Detailed Loss: recon 0.22436188-0.11218094 | disen 0.36163485-0.18081743 | temporal 0.58677864-0.00000000 | total 0.29299837\n",
      "Epoch 2, loss: 0.29299837-(best 0.27029109)\n",
      "\n",
      "Detailed Loss: recon 0.22506234-0.11253117 | disen 0.36205816-0.18102908 | temporal 0.58673120-0.00000000 | total 0.29356027\n",
      "Epoch 3, loss: 0.29356027-(best 0.27029109)\n",
      "\n",
      "Detailed Loss: recon 0.20693170-0.10346585 | disen 0.35632759-0.17816380 | temporal 0.58642435-0.00000000 | total 0.28162965\n",
      "Epoch 4, loss: 0.28162965-(best 0.27029109)\n",
      "\n",
      "Detailed Loss: recon 0.20648853-0.10324427 | disen 0.35900974-0.17950487 | temporal 0.58665502-0.00000000 | total 0.28274915\n",
      "Epoch 5, loss: 0.28274915-(best 0.27029109)\n",
      "\n",
      "Detailed Loss: recon 0.20008013-0.10004006 | disen 0.35599238-0.17799619 | temporal 0.58646929-0.00000000 | total 0.27803624\n",
      "Epoch 6, loss: 0.27803624-(best 0.27029109)\n",
      "\n",
      "Detailed Loss: recon 0.20565711-0.10282855 | disen 0.35551435-0.17775717 | temporal 0.58640289-0.00000000 | total 0.28058574\n",
      "Epoch 7, loss: 0.28058574-(best 0.27029109)\n",
      "\n",
      "Detailed Loss: recon 0.19855468-0.09927734 | disen 0.35559499-0.17779750 | temporal 0.58656979-0.00000000 | total 0.27707484\n",
      "Epoch 8, loss: 0.27707484-(best 0.27029109)\n",
      "\n",
      "Detailed Loss: recon 0.19866601-0.09933300 | disen 0.35673314-0.17836657 | temporal 0.58682430-0.00000000 | total 0.27769959\n",
      "Epoch 9, loss: 0.27769959-(best 0.27029109)\n",
      "\n",
      "Detailed Loss: recon 0.19788055-0.09894028 | disen 0.35473037-0.17736518 | temporal 0.58688593-0.00000000 | total 0.27630547\n",
      "Epoch 10, loss: 0.27630547-(best 0.27029109)\n",
      "\n",
      "Detailed Loss: recon 0.19520214-0.09760107 | disen 0.35262454-0.17631227 | temporal 0.58687067-0.00000000 | total 0.27391332\n",
      "Epoch 11, loss: 0.27391332-(best 0.27029109)\n",
      "\n",
      "Detailed Loss: recon 0.19439256-0.09719628 | disen 0.34828925-0.17414463 | temporal 0.58674675-0.00000000 | total 0.27134091\n",
      "Epoch 12, loss: 0.27134091-(best 0.27029109)\n",
      "\n",
      "Detailed Loss: recon 0.19271293-0.09635647 | disen 0.34570569-0.17285284 | temporal 0.58666909-0.00000000 | total 0.26920933\n",
      "Epoch 13, loss: 0.26920933-(best 0.26920933)\n",
      "\n",
      "Detailed Loss: recon 0.19289981-0.09644990 | disen 0.34342325-0.17171162 | temporal 0.58661652-0.00000000 | total 0.26816154\n",
      "Epoch 14, loss: 0.26816154-(best 0.26816154)\n",
      "\n",
      "Detailed Loss: recon 0.19265869-0.09632935 | disen 0.34282255-0.17141128 | temporal 0.58662367-0.00000000 | total 0.26774061\n",
      "Epoch 15, loss: 0.26774061-(best 0.26774061)\n",
      "\n",
      "Detailed Loss: recon 0.19186257-0.09593128 | disen 0.34064651-0.17032325 | temporal 0.58656567-0.00000000 | total 0.26625454\n",
      "Epoch 16, loss: 0.26625454-(best 0.26625454)\n",
      "\n",
      "Detailed Loss: recon 0.19092713-0.09546357 | disen 0.34076607-0.17038304 | temporal 0.58652788-0.00000000 | total 0.26584661\n",
      "Epoch 17, loss: 0.26584661-(best 0.26584661)\n",
      "\n",
      "Detailed Loss: recon 0.18976222-0.09488111 | disen 0.33675224-0.16837612 | temporal 0.58635330-0.00000000 | total 0.26325724\n",
      "Epoch 18, loss: 0.26325724-(best 0.26325724)\n",
      "\n",
      "Detailed Loss: recon 0.18979983-0.09489992 | disen 0.33686334-0.16843167 | temporal 0.58638191-0.00000000 | total 0.26333159\n",
      "Epoch 19, loss: 0.26333159-(best 0.26325724)\n",
      "\n",
      "Detailed Loss: recon 0.18886313-0.09443156 | disen 0.33206636-0.16603318 | temporal 0.58616227-0.00000000 | total 0.26046473\n",
      "Epoch 20, loss: 0.26046473-(best 0.26046473)\n",
      "\n",
      "Detailed Loss: recon 0.19005659-0.09502830 | disen 0.32866573-0.16433287 | temporal 0.58599240-0.00000000 | total 0.25936115\n",
      "Epoch 21, loss: 0.25936115-(best 0.25936115)\n",
      "\n",
      "Detailed Loss: recon 0.19000696-0.09500348 | disen 0.33002698-0.16501349 | temporal 0.58602846-0.00000000 | total 0.26001698\n",
      "Epoch 22, loss: 0.26001698-(best 0.25936115)\n",
      "\n",
      "Detailed Loss: recon 0.18847995-0.09423997 | disen 0.32708395-0.16354197 | temporal 0.58594787-0.00000000 | total 0.25778195\n",
      "Epoch 23, loss: 0.25778195-(best 0.25778195)\n",
      "\n",
      "Detailed Loss: recon 0.18887827-0.09443913 | disen 0.32564181-0.16282091 | temporal 0.58595866-0.00000000 | total 0.25726002\n",
      "Epoch 24, loss: 0.25726002-(best 0.25726002)\n",
      "\n",
      "Detailed Loss: recon 0.18767266-0.09383633 | disen 0.32299370-0.16149685 | temporal 0.58596808-0.00000000 | total 0.25533319\n",
      "Epoch 25, loss: 0.25533319-(best 0.25533319)\n",
      "\n",
      "Detailed Loss: recon 0.18970351-0.09485175 | disen 0.32329082-0.16164541 | temporal 0.58600944-0.00000000 | total 0.25649717\n",
      "Epoch 26, loss: 0.25649717-(best 0.25533319)\n",
      "\n",
      "Detailed Loss: recon 0.18809567-0.09404784 | disen 0.32242918-0.16121459 | temporal 0.58597362-0.00000000 | total 0.25526243\n",
      "Epoch 27, loss: 0.25526243-(best 0.25526243)\n",
      "\n",
      "Detailed Loss: recon 0.18864435-0.09432217 | disen 0.32143074-0.16071537 | temporal 0.58596003-0.00000000 | total 0.25503755\n",
      "Epoch 28, loss: 0.25503755-(best 0.25503755)\n",
      "\n",
      "Detailed Loss: recon 0.19440036-0.09720018 | disen 0.31858498-0.15929249 | temporal 0.58588094-0.00000000 | total 0.25649267\n",
      "Epoch 29, loss: 0.25649267-(best 0.25503755)\n",
      "\n",
      "Detailed Loss: recon 0.19056611-0.09528305 | disen 0.31788564-0.15894282 | temporal 0.58590877-0.00000000 | total 0.25422588\n",
      "Epoch 30, loss: 0.25422588-(best 0.25422588)\n",
      "\n",
      "Detailed Loss: recon 0.18728253-0.09364127 | disen 0.31922859-0.15961429 | temporal 0.58583379-0.00000000 | total 0.25325555\n",
      "Epoch 31, loss: 0.25325555-(best 0.25325555)\n",
      "\n",
      "Detailed Loss: recon 0.19019559-0.09509780 | disen 0.31602526-0.15801263 | temporal 0.58572328-0.00000000 | total 0.25311041\n",
      "Epoch 32, loss: 0.25311041-(best 0.25311041)\n",
      "\n",
      "Detailed Loss: recon 0.18472204-0.09236102 | disen 0.31753916-0.15876958 | temporal 0.58597487-0.00000000 | total 0.25113058\n",
      "Epoch 33, loss: 0.25113058-(best 0.25113058)\n",
      "\n",
      "Detailed Loss: recon 0.18694055-0.09347028 | disen 0.31963092-0.15981546 | temporal 0.58610010-0.00000000 | total 0.25328574\n",
      "Epoch 34, loss: 0.25328574-(best 0.25113058)\n",
      "\n",
      "Detailed Loss: recon 0.18644685-0.09322342 | disen 0.31471634-0.15735817 | temporal 0.58603621-0.00000000 | total 0.25058159\n",
      "Epoch 35, loss: 0.25058159-(best 0.25058159)\n",
      "\n",
      "Detailed Loss: recon 0.18607210-0.09303605 | disen 0.31669146-0.15834573 | temporal 0.58601451-0.00000000 | total 0.25138178\n",
      "Epoch 36, loss: 0.25138178-(best 0.25058159)\n",
      "\n",
      "Detailed Loss: recon 0.18626140-0.09313070 | disen 0.31387830-0.15693915 | temporal 0.58596760-0.00000000 | total 0.25006986\n",
      "Epoch 37, loss: 0.25006986-(best 0.25006986)\n",
      "\n",
      "Detailed Loss: recon 0.18785946-0.09392973 | disen 0.31306666-0.15653333 | temporal 0.58596593-0.00000000 | total 0.25046307\n",
      "Epoch 38, loss: 0.25046307-(best 0.25006986)\n",
      "\n",
      "Detailed Loss: recon 0.18456946-0.09228473 | disen 0.31198817-0.15599409 | temporal 0.58591491-0.00000000 | total 0.24827883\n",
      "Epoch 39, loss: 0.24827883-(best 0.24827883)\n",
      "\n",
      "Detailed Loss: recon 0.18619131-0.09309565 | disen 0.31268585-0.15634292 | temporal 0.58594233-0.00000000 | total 0.24943858\n",
      "Epoch 40, loss: 0.24943858-(best 0.24827883)\n",
      "\n",
      "Detailed Loss: recon 0.18454063-0.09227031 | disen 0.31185836-0.15592918 | temporal 0.58592194-0.00000000 | total 0.24819949\n",
      "Epoch 41, loss: 0.24819949-(best 0.24819949)\n",
      "\n",
      "Detailed Loss: recon 0.18356369-0.09178185 | disen 0.31198961-0.15599480 | temporal 0.58597738-0.00000000 | total 0.24777666\n",
      "Epoch 42, loss: 0.24777666-(best 0.24777666)\n",
      "\n",
      "Detailed Loss: recon 0.18492943-0.09246472 | disen 0.31001592-0.15500796 | temporal 0.58596712-0.00000000 | total 0.24747267\n",
      "Epoch 43, loss: 0.24747267-(best 0.24747267)\n",
      "\n",
      "Detailed Loss: recon 0.18713653-0.09356827 | disen 0.30884385-0.15442193 | temporal 0.58587348-0.00000000 | total 0.24799019\n",
      "Epoch 44, loss: 0.24799019-(best 0.24747267)\n",
      "\n",
      "Detailed Loss: recon 0.18687379-0.09343690 | disen 0.30921739-0.15460870 | temporal 0.58591974-0.00000000 | total 0.24804559\n",
      "Epoch 45, loss: 0.24804559-(best 0.24747267)\n",
      "\n",
      "Detailed Loss: recon 0.18883161-0.09441581 | disen 0.30794311-0.15397155 | temporal 0.58601195-0.00000000 | total 0.24838737\n",
      "Epoch 46, loss: 0.24838737-(best 0.24747267)\n",
      "\n",
      "Detailed Loss: recon 0.18723935-0.09361967 | disen 0.30895030-0.15447515 | temporal 0.58603168-0.00000000 | total 0.24809483\n",
      "Epoch 47, loss: 0.24809483-(best 0.24747267)\n",
      "\n",
      "Detailed Loss: recon 0.18458913-0.09229457 | disen 0.30824476-0.15412238 | temporal 0.58591622-0.00000000 | total 0.24641696\n",
      "Epoch 48, loss: 0.24641696-(best 0.24641696)\n",
      "\n",
      "Detailed Loss: recon 0.18545870-0.09272935 | disen 0.30706620-0.15353310 | temporal 0.58597612-0.00000000 | total 0.24626246\n",
      "Epoch 49, loss: 0.24626246-(best 0.24626246)\n",
      "\n",
      "Detailed Loss: recon 0.18759368-0.09379684 | disen 0.30560023-0.15280011 | temporal 0.58607036-0.00000000 | total 0.24659696\n",
      "Epoch 50, loss: 0.24659696-(best 0.24626246)\n",
      "\n",
      ">> Reached final epoch. Loading best model...\n",
      "TRAINING XGB PREDICTOR\n",
      ">> Ending training!\n",
      "\n",
      "-----------------------\n",
      "TIME COST: 7.393831491470337 s\n",
      "Best Val: REC 54.99 PRE 45.96 MF1 64.45 AUC 73.35 TP 501 FP 589 TN 1859 FN 410\n",
      "-----------------------\n",
      "Generating additional positive data from adversary...\n",
      "Generating additional negative data by duplicating random nodes...\n",
      "\n",
      "ASSIGNED GRAPH FOR EVALUATION: 12932 {1: 2822, 0: 10110} Nodes, 1281476 Edges\n",
      "\n",
      "-------------------\n",
      "STARTING EVALUATION\n",
      "-------------------\n",
      "\n",
      "Dataset - Overall: REC 59.39 PRE 48.26 MF1 69.10 AUC 79.33 TP 1676 FP 1797 TN 8313 FN 1146 | 12932 {1: 2822, 0: 10110}\n",
      "Dataset - Train: REC 71.52 PRE 58.75 MF1 74.63 AUC 84.39 TP 977 FP 686 TN 2985 FN 389 | 5037 {1: 1366, 0: 3671}\n",
      "Dataset - Val: REC 55.65 PRE 44.40 MF1 63.56 AUC 72.12 TP 507 FP 635 TN 1813 FN 404 | 3359 {1: 911, 0: 2448}\n",
      "Dataset - Test: REC 35.23 PRE 28.74 MF1 60.55 AUC 72.80 TP 192 FP 476 TN 3515 FN 353 | 4536 {0: 3991, 1: 545}\n",
      "PREDICTION STATUS - {'1-True': 2469, '0-False': 3515, '0-True': 6595, '1-False': 353}\n",
      "    >> 353 positive nodes left unpredicted...\n",
      "    >> 220 OG positive nodes left unpredicted...\n",
      "-----------------------\n",
      "PREDICTION RESULT - ROUNDS\n",
      "Dataset - Round 0: REC 59.52 PRE 48.27 MF1 69.10 AUC 79.22 TP 766 FP 821 TN 3771 FN 521 | 5879 {1: 1287, 0: 4592}\n",
      "Dataset - Round 1: REC 44.53 PRE 44.53 MF1 64.53 AUC 72.03 TP 57 FP 71 TN 388 FN 71 | 587 {1: 128, 0: 459}\n",
      "Dataset - Round 2: REC 39.84 PRE 37.23 MF1 60.28 AUC 68.67 TP 51 FP 86 TN 373 FN 77 | 587 {1: 128, 0: 459}\n",
      "-----------------------\n",
      "PREDICTION RESULT - SEEDS\n",
      "Seeds - Current: REC 39.84 PRE 40.48 MF1 61.82 AUC 69.42 TP 51 FP 75 TN 384 FN 77 | 587 {1: 128, 0: 459}\n",
      "Seeds - Prev: REC 0.00 PRE 0.00 MF1 39.86 AUC 69.42 TP 0 FP 70 TN 389 FN 128 | 587 {1: 128, 0: 459}\n",
      "Selecting data as budgeted ground truth for next round...\n",
      "The file ../checkpoint/working_model_file_1730091748.0390356_epoch.py does not exist\n",
      "The file ../checkpoint/working_model_file_1730091748.0390356_round-M.json does not exist\n",
      "The file ../checkpoint/working_model_file_1730091748.0390356_round-T.json does not exist\n",
      "The file ../checkpoint/working_model_file_1730091748.0390356_round.pt does not exist\n",
      "================\n",
      "++++++++++++++++\n",
      "TRIAL NUMBER 1\n",
      "++++++++++++++++\n",
      "================\n",
      "  > Rereading graph data...\n",
      "  > Initializing multiround object...\n",
      "11758 1049758\n",
      "\n",
      "=========================\n",
      "STARTING ADVER EXPERIMENT\n",
      "=========================\n",
      "MAIN CONFIG: [('device', 'cuda:0'), ('exp_type', 'ADVER'), ('full_oracle', False), ('node_expiration_time', 1), ('round_budget_neg', 0), ('round_budget_pos', 0), ('round_new_neg', 459), ('round_new_pos', 128), ('round_num', 3), ('save_df', True), ('save_embedding', False), ('save_temp_sne', False), ('setting_type', 'INDUCTIVE'), ('task_type', 'NODE'), ('verbose', 4)]\n",
      "MODEL CONFIG: [('act_name', 'LeakyReLU'), ('addon_internal_agg', 'OR'), ('addon_name', 'NONE'), ('addon_perc', 0.05), ('addon_round_window', 7), ('alpha', 1), ('att_heads', 2), ('beta', 0), ('boost_agg_backbone', <class 'models.benchmarks_supervised.booster.GIN_noparam'>), ('boost_metric', <function average_precision_score at 0x000001C8520A5760>), ('boost_predictor', <class 'xgboost.sklearn.XGBClassifier'>), ('device', 'cuda:0'), ('drop_rate', 0), ('dropout_rate', 0.1), ('early_stopping', 75), ('embed_type', 'temporal'), ('etypes', ['none']), ('gamma', 1), ('h_feats', 64), ('k', 1), ('lambda1', 1), ('lambda2', 1), ('lambda_', 1), ('loss_sample', True), ('loss_sample_ratio', 0.1), ('loss_type', 'ndist'), ('mlp_feats', 64), ('mlp_layers', 2), ('model_name', 'XGB-SP'), ('n_estimator', 500), ('norm_name', 'layer'), ('num_epoch', 100), ('num_layers', 2), ('num_round_epoch', 50), ('round_window', 7), ('temporal_agg', 'mean_final'), ('tloss_type', 'normal'), ('training_type', 'round'), ('verbose', 4)]\n",
      "STRAT CONFIG: [('augment_name', 'REAGE'), ('augment_neg_ratio', 0.09772070079945569), ('augment_pos_ratio', 0.02727929920054431), ('augment_prob', 0.5), ('augment_round_split', 8), ('device', 'cuda:0'), ('pseudo_name', 'NONE'), ('pseudo_new_pos_only', False), ('pseudo_skip_initial', False), ('verbose', 4)]\n",
      "TRAIN CONFIG: [('ce_weight', 2.6874084919472914), ('device', 'cuda:0'), ('dset_name', 'tolokers_bid'), ('early_stopping', 75), ('learning_rate', 0.01), ('loss', <function cross_entropy at 0x000001C84AF6F880>), ('loss_gamma', 5), ('num_epoch', 100), ('num_round_epoch', 50), ('optimizer', <class 'torch.optim.adam.Adam'>), ('random_state', 7777777), ('ratio_initial', 0.5), ('ratio_test', 0.2), ('ratio_train', 0.6), ('ratio_val', 0.2), ('round_reset_model', False), ('round_train_list', []), ('stuck_stopping', 50), ('stuck_threshold', 0), ('train_max_retry', 10), ('verbose', 4)]\n",
      "ADVER CONFIG: [('adver_choose_name', 'GREEDY'), ('adver_conn_coef', 0.1), ('adver_feat_coef', 1), ('adver_mod_name', 'REPLAY'), ('device', 'cuda:0'), ('verbose', 4)]\n",
      "\n",
      "INITIAL GRAPH: 11758 {1: 2566, 0: 9192} Nodes, 1049758 Edges\n",
      "=========================\n",
      "        \n",
      ">> Initializing graph...\n",
      "MAIN GRAPH: 11758 {1: 2566, 0: 9192} Nodes, 1049758 Edges\n",
      "\n",
      ">> Initializing experiment type-specific modules:\n",
      "  >> Initializing adversarial modules...\n",
      ">> Initializing task-specific modules:\n",
      ">> Initializing classifier model...\n",
      ">> Initializing other strategies...\n",
      "\n",
      "FINISH INITIALIZATION\n",
      "=====================\n",
      "        \n",
      "  > Starting round 0...\n",
      "\n",
      "+++++++++++++++++\n",
      "STARTING ROUND 0!\n",
      "+++++++++++++++++\n",
      "        \n",
      "ASSIGNED GRAPH FOR TRAINING: 5879 {0: 4650, 1: 1229} Nodes, 264191 Edges\n",
      "\n",
      ">> Splitting to train-validation set...\n",
      "Alotting train-val-test split for round 0\n",
      "Initial pool: 0, Prediction pool: 0, Budget pool: 0, Full pool: 5879\n",
      "Full graph size: 5879, Training: 3527, Val: 2352, Test: 5879\n",
      "    >> INITIAL DATA SPLIT: 3527 train rows ({0: 2790, 1: 737}) | 2352 val rows ({0: 1860, 1: 492}) | 0 test rows ({})\n",
      "    >> AUGMENTED DATA SPLIT: 3527 train rows ({0: 2790, 1: 737}) | 2352 val rows ({0: 1860, 1: 492}) | 0 test rows ({})\n",
      "    >> Updated cross-entropy weight to 3.78561736770692...\n",
      "\n",
      "-----------------------\n",
      "STARTING MODEL TRAININGps_l\n",
      "-----------------------\n",
      "\n",
      "TRAINING NODE EMBEDDER\n",
      "Detailed Loss: recon 0.75330943-0.37665471 | disen 0.69444025-0.34722012 | temporal 0.64131206-0.00000000 | total 0.72387481\n",
      "Epoch 1, loss: 0.72387481-(best 0.72387481)\n",
      "\n",
      "Detailed Loss: recon 0.48274380-0.24137190 | disen 0.71369237-0.35684618 | temporal 0.65576071-0.00000000 | total 0.59821808\n",
      "Epoch 2, loss: 0.59821808-(best 0.59821808)\n",
      "\n",
      "Detailed Loss: recon 0.42860016-0.21430008 | disen 0.71506310-0.35753155 | temporal 0.66066366-0.00000000 | total 0.57183164\n",
      "Epoch 3, loss: 0.57183164-(best 0.57183164)\n",
      "\n",
      "Detailed Loss: recon 0.42974600-0.21487300 | disen 0.72013783-0.36006892 | temporal 0.66267973-0.00000000 | total 0.57494193\n",
      "Epoch 4, loss: 0.57494193-(best 0.57183164)\n",
      "\n",
      "Detailed Loss: recon 0.41201711-0.20600855 | disen 0.71594107-0.35797054 | temporal 0.66328651-0.00000000 | total 0.56397909\n",
      "Epoch 5, loss: 0.56397909-(best 0.56397909)\n",
      "\n",
      "Detailed Loss: recon 0.40175304-0.20087652 | disen 0.71518457-0.35759228 | temporal 0.66372865-0.00000000 | total 0.55846882\n",
      "Epoch 6, loss: 0.55846882-(best 0.55846882)\n",
      "\n",
      "Detailed Loss: recon 0.38184986-0.19092493 | disen 0.71206963-0.35603482 | temporal 0.66360098-0.00000000 | total 0.54695976\n",
      "Epoch 7, loss: 0.54695976-(best 0.54695976)\n",
      "\n",
      "Detailed Loss: recon 0.36591840-0.18295920 | disen 0.71424770-0.35712385 | temporal 0.66305780-0.00000000 | total 0.54008305\n",
      "Epoch 8, loss: 0.54008305-(best 0.54008305)\n",
      "\n",
      "Detailed Loss: recon 0.35577250-0.17788625 | disen 0.71800911-0.35900456 | temporal 0.66197580-0.00000000 | total 0.53689080\n",
      "Epoch 9, loss: 0.53689080-(best 0.53689080)\n",
      "\n",
      "Detailed Loss: recon 0.35011607-0.17505804 | disen 0.70866752-0.35433376 | temporal 0.66174477-0.00000000 | total 0.52939177\n",
      "Epoch 10, loss: 0.52939177-(best 0.52939177)\n",
      "\n",
      "Detailed Loss: recon 0.34404379-0.17202190 | disen 0.70453596-0.35226798 | temporal 0.66181082-0.00000000 | total 0.52428985\n",
      "Epoch 11, loss: 0.52428985-(best 0.52428985)\n",
      "\n",
      "Detailed Loss: recon 0.34703887-0.17351943 | disen 0.70176065-0.35088032 | temporal 0.66197407-0.00000000 | total 0.52439976\n",
      "Epoch 12, loss: 0.52439976-(best 0.52428985)\n",
      "\n",
      "Detailed Loss: recon 0.33242664-0.16621332 | disen 0.70433956-0.35216978 | temporal 0.66232324-0.00000000 | total 0.51838309\n",
      "Epoch 13, loss: 0.51838309-(best 0.51838309)\n",
      "\n",
      "Detailed Loss: recon 0.32162708-0.16081354 | disen 0.69560820-0.34780410 | temporal 0.66340661-0.00000000 | total 0.50861764\n",
      "Epoch 14, loss: 0.50861764-(best 0.50861764)\n",
      "\n",
      "Detailed Loss: recon 0.31114417-0.15557209 | disen 0.69681913-0.34840956 | temporal 0.66430593-0.00000000 | total 0.50398165\n",
      "Epoch 15, loss: 0.50398165-(best 0.50398165)\n",
      "\n",
      "Detailed Loss: recon 0.31178933-0.15589467 | disen 0.69022930-0.34511465 | temporal 0.66520000-0.00000000 | total 0.50100935\n",
      "Epoch 16, loss: 0.50100935-(best 0.50100935)\n",
      "\n",
      "Detailed Loss: recon 0.30230558-0.15115279 | disen 0.69302988-0.34651494 | temporal 0.66571629-0.00000000 | total 0.49766773\n",
      "Epoch 17, loss: 0.49766773-(best 0.49766773)\n",
      "\n",
      "Detailed Loss: recon 0.30428180-0.15214090 | disen 0.68220961-0.34110481 | temporal 0.66625452-0.00000000 | total 0.49324572\n",
      "Epoch 18, loss: 0.49324572-(best 0.49324572)\n",
      "\n",
      "Detailed Loss: recon 0.29975921-0.14987960 | disen 0.68379402-0.34189701 | temporal 0.66651303-0.00000000 | total 0.49177662\n",
      "Epoch 19, loss: 0.49177662-(best 0.49177662)\n",
      "\n",
      "Detailed Loss: recon 0.29186884-0.14593442 | disen 0.68402672-0.34201336 | temporal 0.66660082-0.00000000 | total 0.48794776\n",
      "Epoch 20, loss: 0.48794776-(best 0.48794776)\n",
      "\n",
      "Detailed Loss: recon 0.29353851-0.14676926 | disen 0.66914916-0.33457458 | temporal 0.66655582-0.00000000 | total 0.48134384\n",
      "Epoch 21, loss: 0.48134384-(best 0.48134384)\n",
      "\n",
      "Detailed Loss: recon 0.28829810-0.14414905 | disen 0.67563820-0.33781910 | temporal 0.66666436-0.00000000 | total 0.48196816\n",
      "Epoch 22, loss: 0.48196816-(best 0.48134384)\n",
      "\n",
      "Detailed Loss: recon 0.28551298-0.14275649 | disen 0.66399050-0.33199525 | temporal 0.66648126-0.00000000 | total 0.47475174\n",
      "Epoch 23, loss: 0.47475174-(best 0.47475174)\n",
      "\n",
      "Detailed Loss: recon 0.28050917-0.14025459 | disen 0.66131997-0.33065999 | temporal 0.66675478-0.00000000 | total 0.47091457\n",
      "Epoch 24, loss: 0.47091457-(best 0.47091457)\n",
      "\n",
      "Detailed Loss: recon 0.27759954-0.13879977 | disen 0.65368205-0.32684103 | temporal 0.66698360-0.00000000 | total 0.46564078\n",
      "Epoch 25, loss: 0.46564078-(best 0.46564078)\n",
      "\n",
      "Detailed Loss: recon 0.27648792-0.13824396 | disen 0.65161431-0.32580715 | temporal 0.66739732-0.00000000 | total 0.46405113\n",
      "Epoch 26, loss: 0.46405113-(best 0.46405113)\n",
      "\n",
      "Detailed Loss: recon 0.27425444-0.13712722 | disen 0.64914429-0.32457215 | temporal 0.66785794-0.00000000 | total 0.46169937\n",
      "Epoch 27, loss: 0.46169937-(best 0.46169937)\n",
      "\n",
      "Detailed Loss: recon 0.27026126-0.13513063 | disen 0.64565516-0.32282758 | temporal 0.66780114-0.00000000 | total 0.45795822\n",
      "Epoch 28, loss: 0.45795822-(best 0.45795822)\n",
      "\n",
      "Detailed Loss: recon 0.27401409-0.13700704 | disen 0.62898731-0.31449366 | temporal 0.66797704-0.00000000 | total 0.45150071\n",
      "Epoch 29, loss: 0.45150071-(best 0.45150071)\n",
      "\n",
      "Detailed Loss: recon 0.27380031-0.13690016 | disen 0.64170849-0.32085425 | temporal 0.66817820-0.00000000 | total 0.45775440\n",
      "Epoch 30, loss: 0.45775440-(best 0.45150071)\n",
      "\n",
      "Detailed Loss: recon 0.26783705-0.13391852 | disen 0.62007678-0.31003839 | temporal 0.66836643-0.00000000 | total 0.44395691\n",
      "Epoch 31, loss: 0.44395691-(best 0.44395691)\n",
      "\n",
      "Detailed Loss: recon 0.26826364-0.13413182 | disen 0.61086965-0.30543482 | temporal 0.66813463-0.00000000 | total 0.43956664\n",
      "Epoch 32, loss: 0.43956664-(best 0.43956664)\n",
      "\n",
      "Detailed Loss: recon 0.26822373-0.13411187 | disen 0.61919850-0.30959925 | temporal 0.66785663-0.00000000 | total 0.44371110\n",
      "Epoch 33, loss: 0.44371110-(best 0.43956664)\n",
      "\n",
      "Detailed Loss: recon 0.26343691-0.13171846 | disen 0.61207336-0.30603668 | temporal 0.66803569-0.00000000 | total 0.43775514\n",
      "Epoch 34, loss: 0.43775514-(best 0.43775514)\n",
      "\n",
      "Detailed Loss: recon 0.26424384-0.13212192 | disen 0.60899138-0.30449569 | temporal 0.66817272-0.00000000 | total 0.43661761\n",
      "Epoch 35, loss: 0.43661761-(best 0.43661761)\n",
      "\n",
      "Detailed Loss: recon 0.26195872-0.13097936 | disen 0.60512954-0.30256477 | temporal 0.66861469-0.00000000 | total 0.43354413\n",
      "Epoch 36, loss: 0.43354413-(best 0.43354413)\n",
      "\n",
      "Detailed Loss: recon 0.26565441-0.13282721 | disen 0.61116779-0.30558389 | temporal 0.66861117-0.00000000 | total 0.43841112\n",
      "Epoch 37, loss: 0.43841112-(best 0.43354413)\n",
      "\n",
      "Detailed Loss: recon 0.26487333-0.13243666 | disen 0.59221590-0.29610795 | temporal 0.66857368-0.00000000 | total 0.42854461\n",
      "Epoch 38, loss: 0.42854461-(best 0.42854461)\n",
      "\n",
      "Detailed Loss: recon 0.25837553-0.12918776 | disen 0.58802396-0.29401198 | temporal 0.66841018-0.00000000 | total 0.42319974\n",
      "Epoch 39, loss: 0.42319974-(best 0.42319974)\n",
      "\n",
      "Detailed Loss: recon 0.26314175-0.13157088 | disen 0.59563971-0.29781985 | temporal 0.66846102-0.00000000 | total 0.42939073\n",
      "Epoch 40, loss: 0.42939073-(best 0.42319974)\n",
      "\n",
      "Detailed Loss: recon 0.25794914-0.12897457 | disen 0.59586608-0.29793304 | temporal 0.66862828-0.00000000 | total 0.42690760\n",
      "Epoch 41, loss: 0.42690760-(best 0.42319974)\n",
      "\n",
      "Detailed Loss: recon 0.25580922-0.12790461 | disen 0.58731574-0.29365787 | temporal 0.66867030-0.00000000 | total 0.42156249\n",
      "Epoch 42, loss: 0.42156249-(best 0.42156249)\n",
      "\n",
      "Detailed Loss: recon 0.25765747-0.12882873 | disen 0.58418012-0.29209006 | temporal 0.66860199-0.00000000 | total 0.42091879\n",
      "Epoch 43, loss: 0.42091879-(best 0.42091879)\n",
      "\n",
      "Detailed Loss: recon 0.25641328-0.12820664 | disen 0.58298361-0.29149181 | temporal 0.66853875-0.00000000 | total 0.41969845\n",
      "Epoch 44, loss: 0.41969845-(best 0.41969845)\n",
      "\n",
      "Detailed Loss: recon 0.25235298-0.12617649 | disen 0.56692743-0.28346372 | temporal 0.66851968-0.00000000 | total 0.40964019\n",
      "Epoch 45, loss: 0.40964019-(best 0.40964019)\n",
      "\n",
      "Detailed Loss: recon 0.25318137-0.12659068 | disen 0.57395172-0.28697586 | temporal 0.66853774-0.00000000 | total 0.41356653\n",
      "Epoch 46, loss: 0.41356653-(best 0.40964019)\n",
      "\n",
      "Detailed Loss: recon 0.25158688-0.12579344 | disen 0.56942105-0.28471053 | temporal 0.66872460-0.00000000 | total 0.41050398\n",
      "Epoch 47, loss: 0.41050398-(best 0.40964019)\n",
      "\n",
      "Detailed Loss: recon 0.25509402-0.12754701 | disen 0.55984640-0.27992320 | temporal 0.66819811-0.00000000 | total 0.40747023\n",
      "Epoch 48, loss: 0.40747023-(best 0.40747023)\n",
      "\n",
      "Detailed Loss: recon 0.25578886-0.12789443 | disen 0.56561100-0.28280550 | temporal 0.66891193-0.00000000 | total 0.41069993\n",
      "Epoch 49, loss: 0.41069993-(best 0.40747023)\n",
      "\n",
      "Detailed Loss: recon 0.25324938-0.12662469 | disen 0.55736804-0.27868402 | temporal 0.66880065-0.00000000 | total 0.40530872\n",
      "Epoch 50, loss: 0.40530872-(best 0.40530872)\n",
      "\n",
      "Detailed Loss: recon 0.25814876-0.12907438 | disen 0.55521226-0.27760613 | temporal 0.66842598-0.00000000 | total 0.40668052\n",
      "Epoch 51, loss: 0.40668052-(best 0.40530872)\n",
      "\n",
      "Detailed Loss: recon 0.25325176-0.12662588 | disen 0.55743104-0.27871552 | temporal 0.66825122-0.00000000 | total 0.40534139\n",
      "Epoch 52, loss: 0.40534139-(best 0.40530872)\n",
      "\n",
      "Detailed Loss: recon 0.25541043-0.12770522 | disen 0.55129969-0.27564985 | temporal 0.66806209-0.00000000 | total 0.40335506\n",
      "Epoch 53, loss: 0.40335506-(best 0.40335506)\n",
      "\n",
      "Detailed Loss: recon 0.25335962-0.12667981 | disen 0.54917395-0.27458698 | temporal 0.66820085-0.00000000 | total 0.40126678\n",
      "Epoch 54, loss: 0.40126678-(best 0.40126678)\n",
      "\n",
      "Detailed Loss: recon 0.25147414-0.12573707 | disen 0.54793781-0.27396891 | temporal 0.66834098-0.00000000 | total 0.39970598\n",
      "Epoch 55, loss: 0.39970598-(best 0.39970598)\n",
      "\n",
      "Detailed Loss: recon 0.25148451-0.12574226 | disen 0.53536606-0.26768303 | temporal 0.66819966-0.00000000 | total 0.39342529\n",
      "Epoch 56, loss: 0.39342529-(best 0.39342529)\n",
      "\n",
      "Detailed Loss: recon 0.25748655-0.12874328 | disen 0.54883265-0.27441633 | temporal 0.66796678-0.00000000 | total 0.40315962\n",
      "Epoch 57, loss: 0.40315962-(best 0.39342529)\n",
      "\n",
      "Detailed Loss: recon 0.25406623-0.12703311 | disen 0.53933591-0.26966795 | temporal 0.66813004-0.00000000 | total 0.39670107\n",
      "Epoch 58, loss: 0.39670107-(best 0.39342529)\n",
      "\n",
      "Detailed Loss: recon 0.25104919-0.12552460 | disen 0.54320014-0.27160007 | temporal 0.66827190-0.00000000 | total 0.39712465\n",
      "Epoch 59, loss: 0.39712465-(best 0.39342529)\n",
      "\n",
      "Detailed Loss: recon 0.25549898-0.12774949 | disen 0.52539432-0.26269716 | temporal 0.66791320-0.00000000 | total 0.39044666\n",
      "Epoch 60, loss: 0.39044666-(best 0.39044666)\n",
      "\n",
      "Detailed Loss: recon 0.25619927-0.12809964 | disen 0.53236890-0.26618445 | temporal 0.66825467-0.00000000 | total 0.39428407\n",
      "Epoch 61, loss: 0.39428407-(best 0.39044666)\n",
      "\n",
      "Detailed Loss: recon 0.25250357-0.12625179 | disen 0.53203475-0.26601738 | temporal 0.66816181-0.00000000 | total 0.39226916\n",
      "Epoch 62, loss: 0.39226916-(best 0.39044666)\n",
      "\n",
      "Detailed Loss: recon 0.25405231-0.12702616 | disen 0.53105044-0.26552522 | temporal 0.66782922-0.00000000 | total 0.39255136\n",
      "Epoch 63, loss: 0.39255136-(best 0.39044666)\n",
      "\n",
      "Detailed Loss: recon 0.25221992-0.12610996 | disen 0.52209640-0.26104820 | temporal 0.66780806-0.00000000 | total 0.38715816\n",
      "Epoch 64, loss: 0.38715816-(best 0.38715816)\n",
      "\n",
      "Detailed Loss: recon 0.25025913-0.12512957 | disen 0.52397728-0.26198864 | temporal 0.66744256-0.00000000 | total 0.38711822\n",
      "Epoch 65, loss: 0.38711822-(best 0.38711822)\n",
      "\n",
      "Detailed Loss: recon 0.25075704-0.12537852 | disen 0.52278787-0.26139393 | temporal 0.66761661-0.00000000 | total 0.38677245\n",
      "Epoch 66, loss: 0.38677245-(best 0.38677245)\n",
      "\n",
      "Detailed Loss: recon 0.25157917-0.12578958 | disen 0.51619613-0.25809807 | temporal 0.66752827-0.00000000 | total 0.38388765\n",
      "Epoch 67, loss: 0.38388765-(best 0.38388765)\n",
      "\n",
      "Detailed Loss: recon 0.25259590-0.12629795 | disen 0.52619839-0.26309919 | temporal 0.66798371-0.00000000 | total 0.38939714\n",
      "Epoch 68, loss: 0.38939714-(best 0.38388765)\n",
      "\n",
      "Detailed Loss: recon 0.25233686-0.12616843 | disen 0.51717389-0.25858694 | temporal 0.66785634-0.00000000 | total 0.38475537\n",
      "Epoch 69, loss: 0.38475537-(best 0.38388765)\n",
      "\n",
      "Detailed Loss: recon 0.25568220-0.12784110 | disen 0.51804268-0.25902134 | temporal 0.66794956-0.00000000 | total 0.38686246\n",
      "Epoch 70, loss: 0.38686246-(best 0.38388765)\n",
      "\n",
      "Detailed Loss: recon 0.25266135-0.12633067 | disen 0.51479888-0.25739944 | temporal 0.66736418-0.00000000 | total 0.38373011\n",
      "Epoch 71, loss: 0.38373011-(best 0.38373011)\n",
      "\n",
      "Detailed Loss: recon 0.25310725-0.12655362 | disen 0.51570094-0.25785047 | temporal 0.66746914-0.00000000 | total 0.38440409\n",
      "Epoch 72, loss: 0.38440409-(best 0.38373011)\n",
      "\n",
      "Detailed Loss: recon 0.24978636-0.12489318 | disen 0.51368207-0.25684103 | temporal 0.66765648-0.00000000 | total 0.38173422\n",
      "Epoch 73, loss: 0.38173422-(best 0.38173422)\n",
      "\n",
      "Detailed Loss: recon 0.25755650-0.12877825 | disen 0.50602734-0.25301367 | temporal 0.66729677-0.00000000 | total 0.38179192\n",
      "Epoch 74, loss: 0.38179192-(best 0.38173422)\n",
      "\n",
      "Detailed Loss: recon 0.25574362-0.12787181 | disen 0.51115084-0.25557542 | temporal 0.66732913-0.00000000 | total 0.38344723\n",
      "Epoch 75, loss: 0.38344723-(best 0.38173422)\n",
      "\n",
      "Detailed Loss: recon 0.25623614-0.12811807 | disen 0.50263739-0.25131869 | temporal 0.66736764-0.00000000 | total 0.37943676\n",
      "Epoch 76, loss: 0.37943676-(best 0.37943676)\n",
      "\n",
      "Detailed Loss: recon 0.25668234-0.12834117 | disen 0.49817002-0.24908501 | temporal 0.66757971-0.00000000 | total 0.37742618\n",
      "Epoch 77, loss: 0.37742618-(best 0.37742618)\n",
      "\n",
      "Detailed Loss: recon 0.25650686-0.12825343 | disen 0.49478978-0.24739489 | temporal 0.66734958-0.00000000 | total 0.37564832\n",
      "Epoch 78, loss: 0.37564832-(best 0.37564832)\n",
      "\n",
      "Detailed Loss: recon 0.25538480-0.12769240 | disen 0.49507260-0.24753630 | temporal 0.66714525-0.00000000 | total 0.37522870\n",
      "Epoch 79, loss: 0.37522870-(best 0.37522870)\n",
      "\n",
      "Detailed Loss: recon 0.25648689-0.12824345 | disen 0.49566734-0.24783367 | temporal 0.66697270-0.00000000 | total 0.37607712\n",
      "Epoch 80, loss: 0.37607712-(best 0.37522870)\n",
      "\n",
      "Detailed Loss: recon 0.25950533-0.12975267 | disen 0.49460471-0.24730235 | temporal 0.66707689-0.00000000 | total 0.37705502\n",
      "Epoch 81, loss: 0.37705502-(best 0.37522870)\n",
      "\n",
      "Detailed Loss: recon 0.25626221-0.12813111 | disen 0.49124116-0.24562058 | temporal 0.66710842-0.00000000 | total 0.37375170\n",
      "Epoch 82, loss: 0.37375170-(best 0.37375170)\n",
      "\n",
      "Detailed Loss: recon 0.25416514-0.12708257 | disen 0.48787183-0.24393591 | temporal 0.66671360-0.00000000 | total 0.37101847\n",
      "Epoch 83, loss: 0.37101847-(best 0.37101847)\n",
      "\n",
      "Detailed Loss: recon 0.26042804-0.13021402 | disen 0.48817450-0.24408725 | temporal 0.66693336-0.00000000 | total 0.37430125\n",
      "Epoch 84, loss: 0.37430125-(best 0.37101847)\n",
      "\n",
      "Detailed Loss: recon 0.25630593-0.12815297 | disen 0.49404395-0.24702197 | temporal 0.66709048-0.00000000 | total 0.37517494\n",
      "Epoch 85, loss: 0.37517494-(best 0.37101847)\n",
      "\n",
      "Detailed Loss: recon 0.25882995-0.12941498 | disen 0.49499607-0.24749804 | temporal 0.66717112-0.00000000 | total 0.37691301\n",
      "Epoch 86, loss: 0.37691301-(best 0.37101847)\n",
      "\n",
      "Detailed Loss: recon 0.25807950-0.12903975 | disen 0.48710650-0.24355325 | temporal 0.66667837-0.00000000 | total 0.37259299\n",
      "Epoch 87, loss: 0.37259299-(best 0.37101847)\n",
      "\n",
      "Detailed Loss: recon 0.25716680-0.12858340 | disen 0.47979265-0.23989633 | temporal 0.66689271-0.00000000 | total 0.36847973\n",
      "Epoch 88, loss: 0.36847973-(best 0.36847973)\n",
      "\n",
      "Detailed Loss: recon 0.25524306-0.12762153 | disen 0.49323380-0.24661690 | temporal 0.66707194-0.00000000 | total 0.37423843\n",
      "Epoch 89, loss: 0.37423843-(best 0.36847973)\n",
      "\n",
      "Detailed Loss: recon 0.25954270-0.12977135 | disen 0.48018003-0.24009001 | temporal 0.66682166-0.00000000 | total 0.36986136\n",
      "Epoch 90, loss: 0.36986136-(best 0.36847973)\n",
      "\n",
      "Detailed Loss: recon 0.26073590-0.13036795 | disen 0.47204196-0.23602098 | temporal 0.66642278-0.00000000 | total 0.36638892\n",
      "Epoch 91, loss: 0.36638892-(best 0.36638892)\n",
      "\n",
      "Detailed Loss: recon 0.26063788-0.13031894 | disen 0.47806871-0.23903435 | temporal 0.66704923-0.00000000 | total 0.36935329\n",
      "Epoch 92, loss: 0.36935329-(best 0.36638892)\n",
      "\n",
      "Detailed Loss: recon 0.25414270-0.12707135 | disen 0.48047906-0.24023953 | temporal 0.66737646-0.00000000 | total 0.36731088\n",
      "Epoch 93, loss: 0.36731088-(best 0.36638892)\n",
      "\n",
      "Detailed Loss: recon 0.26098365-0.13049182 | disen 0.47708136-0.23854068 | temporal 0.66698521-0.00000000 | total 0.36903250\n",
      "Epoch 94, loss: 0.36903250-(best 0.36638892)\n",
      "\n",
      "Detailed Loss: recon 0.25679219-0.12839609 | disen 0.48247439-0.24123719 | temporal 0.66698235-0.00000000 | total 0.36963329\n",
      "Epoch 95, loss: 0.36963329-(best 0.36638892)\n",
      "\n",
      "Detailed Loss: recon 0.25972223-0.12986112 | disen 0.47426569-0.23713285 | temporal 0.66618830-0.00000000 | total 0.36699396\n",
      "Epoch 96, loss: 0.36699396-(best 0.36638892)\n",
      "\n",
      "Detailed Loss: recon 0.25644389-0.12822194 | disen 0.47270590-0.23635295 | temporal 0.66668975-0.00000000 | total 0.36457491\n",
      "Epoch 97, loss: 0.36457491-(best 0.36457491)\n",
      "\n",
      "Detailed Loss: recon 0.25661457-0.12830728 | disen 0.46949524-0.23474762 | temporal 0.66640007-0.00000000 | total 0.36305490\n",
      "Epoch 98, loss: 0.36305490-(best 0.36305490)\n",
      "\n",
      "Detailed Loss: recon 0.25611219-0.12805609 | disen 0.47355002-0.23677501 | temporal 0.66631049-0.00000000 | total 0.36483109\n",
      "Epoch 99, loss: 0.36483109-(best 0.36305490)\n",
      "\n",
      "Detailed Loss: recon 0.25736165-0.12868083 | disen 0.46757203-0.23378602 | temporal 0.66652757-0.00000000 | total 0.36246684\n",
      "Epoch 100, loss: 0.36246684-(best 0.36246684)\n",
      "\n",
      ">> Reached final epoch. Loading best model...\n",
      "TRAINING XGB PREDICTOR\n",
      ">> Ending training!\n",
      "\n",
      "-----------------------\n",
      "TIME COST: 6.1748740673065186 s\n",
      "Best Val: REC 49.19 PRE 46.72 MF1 66.84 AUC 79.14 TP 242 FP 276 TN 1584 FN 250\n",
      "-----------------------\n",
      "\n",
      "ASSIGNED GRAPH FOR EVALUATION: 11758 {1: 2566, 0: 9192} Nodes, 1049758 Edges\n",
      "\n",
      "-------------------\n",
      "STARTING EVALUATION\n",
      "-------------------\n",
      "\n",
      "Dataset - Overall: REC 51.87 PRE 49.13 MF1 68.07 AUC 80.21 TP 1331 FP 1378 TN 7814 FN 1235 | 11758 {1: 2566, 0: 9192}\n",
      "Dataset - Train: REC 61.19 PRE 53.56 MF1 72.38 AUC 84.66 TP 451 FP 391 TN 2399 FN 286 | 3527 {0: 2790, 1: 737}\n",
      "Dataset - Val: REC 46.34 PRE 42.14 MF1 64.21 AUC 77.13 TP 228 FP 313 TN 1547 FN 264 | 2352 {0: 1860, 1: 492}\n",
      "Dataset - Test: REC 48.77 PRE 49.17 MF1 67.01 AUC 78.92 TP 652 FP 674 TN 3868 FN 685 | 5879 {1: 1337, 0: 4542}\n",
      "PREDICTION STATUS - {'1-True': 1881, '0-True': 5324, '0-False': 3868, '1-False': 685}\n",
      "    >> 685 positive nodes left unpredicted...\n",
      "    >> 685 OG positive nodes left unpredicted...\n",
      "Selecting data as budgeted ground truth for next round...\n",
      "  > Starting round 1...\n",
      "\n",
      "+++++++++++++++++\n",
      "STARTING ROUND 1!\n",
      "+++++++++++++++++\n",
      "        \n",
      "ASSIGNED GRAPH FOR TRAINING: 11758 {1: 2566, 0: 9192} Nodes, 1049758 Edges\n",
      "\n",
      ">> Splitting to train-validation set...\n",
      "Alotting train-val-test split for round 1\n",
      "Initial pool: 5879, Prediction pool: 2709, Budget pool: 0, Full pool: 7205\n",
      "Full graph size: 11758, Training: 4323, Val: 2882, Test: 11758\n",
      "    >> INITIAL DATA SPLIT: 4323 train rows ({1: 1129, 0: 3194}) | 2882 val rows ({0: 2130, 1: 752}) | 4553 test rows ({0: 3868, 1: 685})\n",
      "    >> AUGMENTED DATA SPLIT: 4323 train rows ({1: 1129, 0: 3194}) | 2882 val rows ({0: 2130, 1: 752}) | 4553 test rows ({0: 3868, 1: 685})\n",
      "    >> Updated cross-entropy weight to 2.829052258635961...\n",
      "\n",
      "-----------------------\n",
      "STARTING MODEL TRAININGps_l\n",
      "-----------------------\n",
      "\n",
      "TRAINING NODE EMBEDDER\n",
      "Detailed Loss: recon 0.23662834-0.11831417 | disen 0.43954605-0.21977302 | temporal 0.56020761-0.00000000 | total 0.33808720\n",
      "Epoch 1, loss: 0.33808720-(best 0.33808720)\n",
      "\n",
      "Detailed Loss: recon 0.42033666-0.21016833 | disen 0.39847147-0.19923574 | temporal 0.56018597-0.00000000 | total 0.40940407\n",
      "Epoch 2, loss: 0.40940407-(best 0.33808720)\n",
      "\n",
      "Detailed Loss: recon 0.26526242-0.13263121 | disen 0.39325655-0.19662827 | temporal 0.56050497-0.00000000 | total 0.32925949\n",
      "Epoch 3, loss: 0.32925949-(best 0.32925949)\n",
      "\n",
      "Detailed Loss: recon 0.30773878-0.15386939 | disen 0.41512358-0.20756179 | temporal 0.56121892-0.00000000 | total 0.36143118\n",
      "Epoch 4, loss: 0.36143118-(best 0.32925949)\n",
      "\n",
      "Detailed Loss: recon 0.25394931-0.12697466 | disen 0.40463048-0.20231524 | temporal 0.56121784-0.00000000 | total 0.32928991\n",
      "Epoch 5, loss: 0.32928991-(best 0.32925949)\n",
      "\n",
      "Detailed Loss: recon 0.25185925-0.12592962 | disen 0.39824700-0.19912350 | temporal 0.56111991-0.00000000 | total 0.32505313\n",
      "Epoch 6, loss: 0.32505313-(best 0.32505313)\n",
      "\n",
      "Detailed Loss: recon 0.26596680-0.13298340 | disen 0.39757651-0.19878826 | temporal 0.56112659-0.00000000 | total 0.33177167\n",
      "Epoch 7, loss: 0.33177167-(best 0.32505313)\n",
      "\n",
      "Detailed Loss: recon 0.27191815-0.13595907 | disen 0.39313972-0.19656986 | temporal 0.56104320-0.00000000 | total 0.33252895\n",
      "Epoch 8, loss: 0.33252895-(best 0.32505313)\n",
      "\n",
      "Detailed Loss: recon 0.25849634-0.12924817 | disen 0.39522606-0.19761303 | temporal 0.56111747-0.00000000 | total 0.32686120\n",
      "Epoch 9, loss: 0.32686120-(best 0.32505313)\n",
      "\n",
      "Detailed Loss: recon 0.25045002-0.12522501 | disen 0.39510113-0.19755057 | temporal 0.56129187-0.00000000 | total 0.32277557\n",
      "Epoch 10, loss: 0.32277557-(best 0.32277557)\n",
      "\n",
      "Detailed Loss: recon 0.24606660-0.12303330 | disen 0.39346623-0.19673312 | temporal 0.56144297-0.00000000 | total 0.31976640\n",
      "Epoch 11, loss: 0.31976640-(best 0.31976640)\n",
      "\n",
      "Detailed Loss: recon 0.24651051-0.12325525 | disen 0.38906199-0.19453099 | temporal 0.56151825-0.00000000 | total 0.31778625\n",
      "Epoch 12, loss: 0.31778625-(best 0.31778625)\n",
      "\n",
      "Detailed Loss: recon 0.24683699-0.12341850 | disen 0.38809854-0.19404927 | temporal 0.56160021-0.00000000 | total 0.31746775\n",
      "Epoch 13, loss: 0.31746775-(best 0.31746775)\n",
      "\n",
      "Detailed Loss: recon 0.24750946-0.12375473 | disen 0.38493502-0.19246751 | temporal 0.56162512-0.00000000 | total 0.31622225\n",
      "Epoch 14, loss: 0.31622225-(best 0.31622225)\n",
      "\n",
      "Detailed Loss: recon 0.24509856-0.12254928 | disen 0.38318932-0.19159466 | temporal 0.56150103-0.00000000 | total 0.31414396\n",
      "Epoch 15, loss: 0.31414396-(best 0.31414396)\n",
      "\n",
      "Detailed Loss: recon 0.24382739-0.12191369 | disen 0.37802160-0.18901080 | temporal 0.56134683-0.00000000 | total 0.31092450\n",
      "Epoch 16, loss: 0.31092450-(best 0.31092450)\n",
      "\n",
      "Detailed Loss: recon 0.24275099-0.12137549 | disen 0.37471277-0.18735638 | temporal 0.56113195-0.00000000 | total 0.30873188\n",
      "Epoch 17, loss: 0.30873188-(best 0.30873188)\n",
      "\n",
      "Detailed Loss: recon 0.24216175-0.12108088 | disen 0.37065005-0.18532503 | temporal 0.56106991-0.00000000 | total 0.30640590\n",
      "Epoch 18, loss: 0.30640590-(best 0.30640590)\n",
      "\n",
      "Detailed Loss: recon 0.23854916-0.11927458 | disen 0.37064815-0.18532407 | temporal 0.56110537-0.00000000 | total 0.30459866\n",
      "Epoch 19, loss: 0.30459866-(best 0.30459866)\n",
      "\n",
      "Detailed Loss: recon 0.23555714-0.11777857 | disen 0.37018037-0.18509018 | temporal 0.56121659-0.00000000 | total 0.30286875\n",
      "Epoch 20, loss: 0.30286875-(best 0.30286875)\n",
      "\n",
      "Detailed Loss: recon 0.24017853-0.12008926 | disen 0.36791891-0.18395945 | temporal 0.56126630-0.00000000 | total 0.30404872\n",
      "Epoch 21, loss: 0.30404872-(best 0.30286875)\n",
      "\n",
      "Detailed Loss: recon 0.23881833-0.11940917 | disen 0.36550868-0.18275434 | temporal 0.56127161-0.00000000 | total 0.30216351\n",
      "Epoch 22, loss: 0.30216351-(best 0.30216351)\n",
      "\n",
      "Detailed Loss: recon 0.23637879-0.11818939 | disen 0.36327285-0.18163642 | temporal 0.56128687-0.00000000 | total 0.29982582\n",
      "Epoch 23, loss: 0.29982582-(best 0.29982582)\n",
      "\n",
      "Detailed Loss: recon 0.23540609-0.11770304 | disen 0.36125106-0.18062553 | temporal 0.56120706-0.00000000 | total 0.29832858\n",
      "Epoch 24, loss: 0.29832858-(best 0.29832858)\n",
      "\n",
      "Detailed Loss: recon 0.23424886-0.11712443 | disen 0.35764909-0.17882454 | temporal 0.56109416-0.00000000 | total 0.29594898\n",
      "Epoch 25, loss: 0.29594898-(best 0.29594898)\n",
      "\n",
      "Detailed Loss: recon 0.23611283-0.11805642 | disen 0.35827053-0.17913526 | temporal 0.56110263-0.00000000 | total 0.29719168\n",
      "Epoch 26, loss: 0.29719168-(best 0.29594898)\n",
      "\n",
      "Detailed Loss: recon 0.23392797-0.11696398 | disen 0.35505915-0.17752957 | temporal 0.56103832-0.00000000 | total 0.29449356\n",
      "Epoch 27, loss: 0.29449356-(best 0.29449356)\n",
      "\n",
      "Detailed Loss: recon 0.23405781-0.11702891 | disen 0.35387474-0.17693737 | temporal 0.56097215-0.00000000 | total 0.29396629\n",
      "Epoch 28, loss: 0.29396629-(best 0.29396629)\n",
      "\n",
      "Detailed Loss: recon 0.23354748-0.11677374 | disen 0.35150862-0.17575431 | temporal 0.56093478-0.00000000 | total 0.29252803\n",
      "Epoch 29, loss: 0.29252803-(best 0.29252803)\n",
      "\n",
      "Detailed Loss: recon 0.22978231-0.11489116 | disen 0.35052228-0.17526114 | temporal 0.56085581-0.00000000 | total 0.29015231\n",
      "Epoch 30, loss: 0.29015231-(best 0.29015231)\n",
      "\n",
      "Detailed Loss: recon 0.23190594-0.11595297 | disen 0.34773844-0.17386922 | temporal 0.56071895-0.00000000 | total 0.28982219\n",
      "Epoch 31, loss: 0.28982219-(best 0.28982219)\n",
      "\n",
      "Detailed Loss: recon 0.23111713-0.11555856 | disen 0.34417713-0.17208856 | temporal 0.56055123-0.00000000 | total 0.28764713\n",
      "Epoch 32, loss: 0.28764713-(best 0.28764713)\n",
      "\n",
      "Detailed Loss: recon 0.23222886-0.11611443 | disen 0.34395456-0.17197728 | temporal 0.56051207-0.00000000 | total 0.28809172\n",
      "Epoch 33, loss: 0.28809172-(best 0.28764713)\n",
      "\n",
      "Detailed Loss: recon 0.23104833-0.11552417 | disen 0.34185106-0.17092553 | temporal 0.56047362-0.00000000 | total 0.28644970\n",
      "Epoch 34, loss: 0.28644970-(best 0.28644970)\n",
      "\n",
      "Detailed Loss: recon 0.22952621-0.11476310 | disen 0.33957529-0.16978765 | temporal 0.56035334-0.00000000 | total 0.28455076\n",
      "Epoch 35, loss: 0.28455076-(best 0.28455076)\n",
      "\n",
      "Detailed Loss: recon 0.22777003-0.11388502 | disen 0.34088117-0.17044058 | temporal 0.56032228-0.00000000 | total 0.28432560\n",
      "Epoch 36, loss: 0.28432560-(best 0.28432560)\n",
      "\n",
      "Detailed Loss: recon 0.22877532-0.11438766 | disen 0.33905250-0.16952625 | temporal 0.56019336-0.00000000 | total 0.28391391\n",
      "Epoch 37, loss: 0.28391391-(best 0.28391391)\n",
      "\n",
      "Detailed Loss: recon 0.23113486-0.11556743 | disen 0.33753115-0.16876557 | temporal 0.56017637-0.00000000 | total 0.28433299\n",
      "Epoch 38, loss: 0.28433299-(best 0.28391391)\n",
      "\n",
      "Detailed Loss: recon 0.22971402-0.11485701 | disen 0.33593106-0.16796553 | temporal 0.56011963-0.00000000 | total 0.28282255\n",
      "Epoch 39, loss: 0.28282255-(best 0.28282255)\n",
      "\n",
      "Detailed Loss: recon 0.22699845-0.11349922 | disen 0.33391047-0.16695523 | temporal 0.55999160-0.00000000 | total 0.28045446\n",
      "Epoch 40, loss: 0.28045446-(best 0.28045446)\n",
      "\n",
      "Detailed Loss: recon 0.22880787-0.11440393 | disen 0.33407861-0.16703930 | temporal 0.55997014-0.00000000 | total 0.28144324\n",
      "Epoch 41, loss: 0.28144324-(best 0.28045446)\n",
      "\n",
      "Detailed Loss: recon 0.22591673-0.11295836 | disen 0.33220124-0.16610062 | temporal 0.55991828-0.00000000 | total 0.27905899\n",
      "Epoch 42, loss: 0.27905899-(best 0.27905899)\n",
      "\n",
      "Detailed Loss: recon 0.22706696-0.11353348 | disen 0.33308661-0.16654330 | temporal 0.55998307-0.00000000 | total 0.28007680\n",
      "Epoch 43, loss: 0.28007680-(best 0.27905899)\n",
      "\n",
      "Detailed Loss: recon 0.22537088-0.11268544 | disen 0.33157742-0.16578871 | temporal 0.55990523-0.00000000 | total 0.27847415\n",
      "Epoch 44, loss: 0.27847415-(best 0.27847415)\n",
      "\n",
      "Detailed Loss: recon 0.22521363-0.11260682 | disen 0.32971185-0.16485593 | temporal 0.55981082-0.00000000 | total 0.27746275\n",
      "Epoch 45, loss: 0.27746275-(best 0.27746275)\n",
      "\n",
      "Detailed Loss: recon 0.22497080-0.11248540 | disen 0.33054698-0.16527349 | temporal 0.55977505-0.00000000 | total 0.27775890\n",
      "Epoch 46, loss: 0.27775890-(best 0.27746275)\n",
      "\n",
      "Detailed Loss: recon 0.22344965-0.11172482 | disen 0.32861918-0.16430959 | temporal 0.55974197-0.00000000 | total 0.27603441\n",
      "Epoch 47, loss: 0.27603441-(best 0.27603441)\n",
      "\n",
      "Detailed Loss: recon 0.22471905-0.11235952 | disen 0.32851619-0.16425809 | temporal 0.55965060-0.00000000 | total 0.27661762\n",
      "Epoch 48, loss: 0.27661762-(best 0.27603441)\n",
      "\n",
      "Detailed Loss: recon 0.22276470-0.11138235 | disen 0.32632858-0.16316429 | temporal 0.55959851-0.00000000 | total 0.27454662\n",
      "Epoch 49, loss: 0.27454662-(best 0.27454662)\n",
      "\n",
      "Detailed Loss: recon 0.22358088-0.11179044 | disen 0.32618904-0.16309452 | temporal 0.55962789-0.00000000 | total 0.27488497\n",
      "Epoch 50, loss: 0.27488497-(best 0.27454662)\n",
      "\n",
      ">> Reached final epoch. Loading best model...\n",
      "TRAINING XGB PREDICTOR\n",
      ">> Ending training!\n",
      "\n",
      "-----------------------\n",
      "TIME COST: 5.6952760219573975 s\n",
      "Best Val: REC 69.15 PRE 49.06 MF1 68.93 AUC 79.98 TP 520 FP 540 TN 1590 FN 232\n",
      "-----------------------\n",
      "Generating additional positive data from adversary...\n",
      "Generating additional negative data by duplicating random nodes...\n",
      "\n",
      "ASSIGNED GRAPH FOR EVALUATION: 12345 {1: 2694, 0: 9651} Nodes, 1149774 Edges\n",
      "\n",
      "-------------------\n",
      "STARTING EVALUATION\n",
      "-------------------\n",
      "\n",
      "Dataset - Overall: REC 61.06 PRE 51.75 MF1 71.15 AUC 83.22 TP 1645 FP 1534 TN 8117 FN 1049 | 12345 {1: 2694, 0: 9651}\n",
      "Dataset - Train: REC 80.07 PRE 62.73 MF1 78.90 AUC 90.14 TP 904 FP 537 TN 2657 FN 225 | 4323 {1: 1129, 0: 3194}\n",
      "Dataset - Val: REC 63.70 PRE 48.24 MF1 67.66 AUC 78.94 TP 479 FP 514 TN 1616 FN 273 | 2882 {0: 2130, 1: 752}\n",
      "Dataset - Test: REC 32.23 PRE 35.17 MF1 60.89 AUC 76.76 TP 262 FP 483 TN 3844 FN 551 | 5140 {0: 4327, 1: 813}\n",
      "PREDICTION STATUS - {'1-True': 2143, '0-True': 5807, '0-False': 3844, '1-False': 551}\n",
      "    >> 551 positive nodes left unpredicted...\n",
      "    >> 459 OG positive nodes left unpredicted...\n",
      "-----------------------\n",
      "PREDICTION RESULT - ROUNDS\n",
      "Dataset - Round 0: REC 56.77 PRE 51.01 MF1 69.55 AUC 81.15 TP 759 FP 729 TN 3813 FN 578 | 5879 {1: 1337, 0: 4542}\n",
      "Dataset - Round 1: REC 28.12 PRE 34.62 MF1 57.02 AUC 72.05 TP 36 FP 68 TN 391 FN 92 | 587 {1: 128, 0: 459}\n",
      "-----------------------\n",
      "PREDICTION RESULT - SEEDS\n",
      "Seeds - Current: REC 32.81 PRE 33.60 MF1 57.43 AUC 69.36 TP 42 FP 83 TN 376 FN 86 | 587 {1: 128, 0: 459}\n",
      "Seeds - Prev: REC 0.00 PRE 0.00 MF1 39.92 AUC 69.36 TP 0 FP 69 TN 390 FN 128 | 587 {1: 128, 0: 459}\n",
      "Selecting data as budgeted ground truth for next round...\n",
      "  > Starting round 2...\n",
      "\n",
      "+++++++++++++++++\n",
      "STARTING ROUND 2!\n",
      "+++++++++++++++++\n",
      "        \n",
      "ASSIGNED GRAPH FOR TRAINING: 12345 {1: 2694, 0: 9651} Nodes, 1149774 Edges\n",
      "\n",
      ">> Splitting to train-validation set...\n",
      "Alotting train-val-test split for round 2\n",
      "Initial pool: 5879, Prediction pool: 5888, Budget pool: 0, Full pool: 7950\n",
      "Full graph size: 12345, Training: 4770, Val: 3180, Test: 12345\n",
      "    >> INITIAL DATA SPLIT: 4770 train rows ({1: 1286, 0: 3484}) | 3180 val rows ({0: 2323, 1: 857}) | 4395 test rows ({0: 3844, 1: 551})\n",
      "    >> AUGMENTED DATA SPLIT: 4770 train rows ({1: 1286, 0: 3484}) | 3180 val rows ({0: 2323, 1: 857}) | 4395 test rows ({0: 3844, 1: 551})\n",
      "    >> Updated cross-entropy weight to 2.7091757387247277...\n",
      "\n",
      "-----------------------\n",
      "STARTING MODEL TRAININGps_l\n",
      "-----------------------\n",
      "\n",
      "TRAINING NODE EMBEDDER\n",
      "Detailed Loss: recon 0.23463669-0.11731835 | disen 0.33834434-0.16917217 | temporal 0.58076829-0.00000000 | total 0.28649050\n",
      "Epoch 1, loss: 0.28649050-(best 0.28649050)\n",
      "\n",
      "Detailed Loss: recon 0.29238838-0.14619419 | disen 0.35728735-0.17864367 | temporal 0.58111000-0.00000000 | total 0.32483786\n",
      "Epoch 2, loss: 0.32483786-(best 0.28649050)\n",
      "\n",
      "Detailed Loss: recon 0.26793194-0.13396597 | disen 0.34882033-0.17441016 | temporal 0.58211017-0.00000000 | total 0.30837613\n",
      "Epoch 3, loss: 0.30837613-(best 0.28649050)\n",
      "\n",
      "Detailed Loss: recon 0.25351685-0.12675843 | disen 0.35519391-0.17759696 | temporal 0.58210504-0.00000000 | total 0.30435538\n",
      "Epoch 4, loss: 0.30435538-(best 0.28649050)\n",
      "\n",
      "Detailed Loss: recon 0.23208588-0.11604294 | disen 0.35664940-0.17832470 | temporal 0.58167857-0.00000000 | total 0.29436764\n",
      "Epoch 5, loss: 0.29436764-(best 0.28649050)\n",
      "\n",
      "Detailed Loss: recon 0.24069297-0.12034649 | disen 0.34909260-0.17454630 | temporal 0.58132428-0.00000000 | total 0.29489279\n",
      "Epoch 6, loss: 0.29489279-(best 0.28649050)\n",
      "\n",
      "Detailed Loss: recon 0.23539726-0.11769863 | disen 0.34822577-0.17411289 | temporal 0.58141273-0.00000000 | total 0.29181153\n",
      "Epoch 7, loss: 0.29181153-(best 0.28649050)\n",
      "\n",
      "Detailed Loss: recon 0.23694925-0.11847463 | disen 0.34777826-0.17388913 | temporal 0.58165061-0.00000000 | total 0.29236376\n",
      "Epoch 8, loss: 0.29236376-(best 0.28649050)\n",
      "\n",
      "Detailed Loss: recon 0.23589414-0.11794707 | disen 0.34714067-0.17357033 | temporal 0.58172578-0.00000000 | total 0.29151741\n",
      "Epoch 9, loss: 0.29151741-(best 0.28649050)\n",
      "\n",
      "Detailed Loss: recon 0.23438215-0.11719108 | disen 0.34296787-0.17148393 | temporal 0.58163190-0.00000000 | total 0.28867501\n",
      "Epoch 10, loss: 0.28867501-(best 0.28649050)\n",
      "\n",
      "Detailed Loss: recon 0.23256366-0.11628183 | disen 0.34033942-0.17016971 | temporal 0.58162749-0.00000000 | total 0.28645155\n",
      "Epoch 11, loss: 0.28645155-(best 0.28645155)\n",
      "\n",
      "Detailed Loss: recon 0.23341101-0.11670551 | disen 0.33917826-0.16958913 | temporal 0.58165514-0.00000000 | total 0.28629464\n",
      "Epoch 12, loss: 0.28629464-(best 0.28629464)\n",
      "\n",
      "Detailed Loss: recon 0.23131716-0.11565858 | disen 0.33543056-0.16771528 | temporal 0.58163208-0.00000000 | total 0.28337386\n",
      "Epoch 13, loss: 0.28337386-(best 0.28337386)\n",
      "\n",
      "Detailed Loss: recon 0.23183051-0.11591525 | disen 0.33350742-0.16675371 | temporal 0.58159769-0.00000000 | total 0.28266895\n",
      "Epoch 14, loss: 0.28266895-(best 0.28266895)\n",
      "\n",
      "Detailed Loss: recon 0.22992685-0.11496343 | disen 0.32978427-0.16489214 | temporal 0.58162314-0.00000000 | total 0.27985555\n",
      "Epoch 15, loss: 0.27985555-(best 0.27985555)\n",
      "\n",
      "Detailed Loss: recon 0.23245895-0.11622947 | disen 0.33086866-0.16543433 | temporal 0.58177257-0.00000000 | total 0.28166381\n",
      "Epoch 16, loss: 0.28166381-(best 0.27985555)\n",
      "\n",
      "Detailed Loss: recon 0.23236659-0.11618330 | disen 0.32800663-0.16400331 | temporal 0.58164537-0.00000000 | total 0.28018659\n",
      "Epoch 17, loss: 0.28018659-(best 0.27985555)\n",
      "\n",
      "Detailed Loss: recon 0.23222446-0.11611223 | disen 0.32735878-0.16367939 | temporal 0.58171135-0.00000000 | total 0.27979162\n",
      "Epoch 18, loss: 0.27979162-(best 0.27979162)\n",
      "\n",
      "Detailed Loss: recon 0.23265138-0.11632569 | disen 0.32475919-0.16237959 | temporal 0.58163708-0.00000000 | total 0.27870530\n",
      "Epoch 19, loss: 0.27870530-(best 0.27870530)\n",
      "\n",
      "Detailed Loss: recon 0.23077574-0.11538787 | disen 0.32155412-0.16077706 | temporal 0.58171529-0.00000000 | total 0.27616495\n",
      "Epoch 20, loss: 0.27616495-(best 0.27616495)\n",
      "\n",
      "Detailed Loss: recon 0.23045939-0.11522970 | disen 0.32159132-0.16079566 | temporal 0.58184695-0.00000000 | total 0.27602535\n",
      "Epoch 21, loss: 0.27602535-(best 0.27602535)\n",
      "\n",
      "Detailed Loss: recon 0.22936234-0.11468117 | disen 0.32097125-0.16048563 | temporal 0.58179510-0.00000000 | total 0.27516681\n",
      "Epoch 22, loss: 0.27516681-(best 0.27516681)\n",
      "\n",
      "Detailed Loss: recon 0.22841021-0.11420511 | disen 0.31889671-0.15944836 | temporal 0.58172375-0.00000000 | total 0.27365345\n",
      "Epoch 23, loss: 0.27365345-(best 0.27365345)\n",
      "\n",
      "Detailed Loss: recon 0.22812155-0.11406077 | disen 0.31915039-0.15957519 | temporal 0.58173609-0.00000000 | total 0.27363598\n",
      "Epoch 24, loss: 0.27363598-(best 0.27363598)\n",
      "\n",
      "Detailed Loss: recon 0.22819179-0.11409590 | disen 0.31865877-0.15932938 | temporal 0.58174795-0.00000000 | total 0.27342528\n",
      "Epoch 25, loss: 0.27342528-(best 0.27342528)\n",
      "\n",
      "Detailed Loss: recon 0.22569416-0.11284708 | disen 0.31705064-0.15852532 | temporal 0.58173484-0.00000000 | total 0.27137241\n",
      "Epoch 26, loss: 0.27137241-(best 0.27137241)\n",
      "\n",
      "Detailed Loss: recon 0.22612149-0.11306074 | disen 0.31609374-0.15804687 | temporal 0.58158499-0.00000000 | total 0.27110761\n",
      "Epoch 27, loss: 0.27110761-(best 0.27110761)\n",
      "\n",
      "Detailed Loss: recon 0.22703680-0.11351840 | disen 0.31582826-0.15791413 | temporal 0.58163255-0.00000000 | total 0.27143252\n",
      "Epoch 28, loss: 0.27143252-(best 0.27110761)\n",
      "\n",
      "Detailed Loss: recon 0.22710268-0.11355134 | disen 0.31230867-0.15615433 | temporal 0.58147126-0.00000000 | total 0.26970568\n",
      "Epoch 29, loss: 0.26970568-(best 0.26970568)\n",
      "\n",
      "Detailed Loss: recon 0.22683933-0.11341967 | disen 0.31170857-0.15585428 | temporal 0.58142620-0.00000000 | total 0.26927394\n",
      "Epoch 30, loss: 0.26927394-(best 0.26927394)\n",
      "\n",
      "Detailed Loss: recon 0.22627346-0.11313673 | disen 0.31023091-0.15511546 | temporal 0.58132160-0.00000000 | total 0.26825219\n",
      "Epoch 31, loss: 0.26825219-(best 0.26825219)\n",
      "\n",
      "Detailed Loss: recon 0.22583181-0.11291590 | disen 0.30946982-0.15473491 | temporal 0.58147746-0.00000000 | total 0.26765081\n",
      "Epoch 32, loss: 0.26765081-(best 0.26765081)\n",
      "\n",
      "Detailed Loss: recon 0.22725986-0.11362993 | disen 0.30775332-0.15387666 | temporal 0.58147544-0.00000000 | total 0.26750660\n",
      "Epoch 33, loss: 0.26750660-(best 0.26750660)\n",
      "\n",
      "Detailed Loss: recon 0.22648382-0.11324191 | disen 0.30694991-0.15347496 | temporal 0.58133090-0.00000000 | total 0.26671687\n",
      "Epoch 34, loss: 0.26671687-(best 0.26671687)\n",
      "\n",
      "Detailed Loss: recon 0.22637102-0.11318551 | disen 0.30528486-0.15264243 | temporal 0.58129114-0.00000000 | total 0.26582795\n",
      "Epoch 35, loss: 0.26582795-(best 0.26582795)\n",
      "\n",
      "Detailed Loss: recon 0.22788504-0.11394252 | disen 0.30536717-0.15268359 | temporal 0.58131707-0.00000000 | total 0.26662612\n",
      "Epoch 36, loss: 0.26662612-(best 0.26582795)\n",
      "\n",
      "Detailed Loss: recon 0.22721057-0.11360528 | disen 0.30455917-0.15227959 | temporal 0.58133048-0.00000000 | total 0.26588488\n",
      "Epoch 37, loss: 0.26588488-(best 0.26582795)\n",
      "\n",
      "Detailed Loss: recon 0.22529608-0.11264804 | disen 0.30673665-0.15336832 | temporal 0.58139306-0.00000000 | total 0.26601636\n",
      "Epoch 38, loss: 0.26601636-(best 0.26582795)\n",
      "\n",
      "Detailed Loss: recon 0.22653443-0.11326721 | disen 0.30268812-0.15134406 | temporal 0.58138484-0.00000000 | total 0.26461127\n",
      "Epoch 39, loss: 0.26461127-(best 0.26461127)\n",
      "\n",
      "Detailed Loss: recon 0.22740088-0.11370044 | disen 0.30329156-0.15164578 | temporal 0.58139491-0.00000000 | total 0.26534623\n",
      "Epoch 40, loss: 0.26534623-(best 0.26461127)\n",
      "\n",
      "Detailed Loss: recon 0.22746283-0.11373141 | disen 0.30042958-0.15021479 | temporal 0.58128899-0.00000000 | total 0.26394621\n",
      "Epoch 41, loss: 0.26394621-(best 0.26394621)\n",
      "\n",
      "Detailed Loss: recon 0.22624999-0.11312500 | disen 0.29858768-0.14929384 | temporal 0.58139092-0.00000000 | total 0.26241884\n",
      "Epoch 42, loss: 0.26241884-(best 0.26241884)\n",
      "\n",
      "Detailed Loss: recon 0.22514138-0.11257069 | disen 0.30024201-0.15012100 | temporal 0.58145887-0.00000000 | total 0.26269168\n",
      "Epoch 43, loss: 0.26269168-(best 0.26241884)\n",
      "\n",
      "Detailed Loss: recon 0.22427559-0.11213779 | disen 0.29878449-0.14939225 | temporal 0.58143699-0.00000000 | total 0.26153004\n",
      "Epoch 44, loss: 0.26153004-(best 0.26153004)\n",
      "\n",
      "Detailed Loss: recon 0.22579341-0.11289670 | disen 0.29806978-0.14903489 | temporal 0.58147168-0.00000000 | total 0.26193160\n",
      "Epoch 45, loss: 0.26193160-(best 0.26153004)\n",
      "\n",
      "Detailed Loss: recon 0.22274539-0.11137269 | disen 0.29849350-0.14924675 | temporal 0.58149534-0.00000000 | total 0.26061946\n",
      "Epoch 46, loss: 0.26061946-(best 0.26061946)\n",
      "\n",
      "Detailed Loss: recon 0.22231898-0.11115949 | disen 0.29727072-0.14863536 | temporal 0.58139616-0.00000000 | total 0.25979483\n",
      "Epoch 47, loss: 0.25979483-(best 0.25979483)\n",
      "\n",
      "Detailed Loss: recon 0.22165975-0.11082987 | disen 0.29664993-0.14832497 | temporal 0.58139914-0.00000000 | total 0.25915486\n",
      "Epoch 48, loss: 0.25915486-(best 0.25915486)\n",
      "\n",
      "Detailed Loss: recon 0.22191450-0.11095725 | disen 0.29545844-0.14772922 | temporal 0.58139354-0.00000000 | total 0.25868648\n",
      "Epoch 49, loss: 0.25868648-(best 0.25868648)\n",
      "\n",
      "Detailed Loss: recon 0.22513562-0.11256781 | disen 0.29476434-0.14738217 | temporal 0.58125931-0.00000000 | total 0.25994998\n",
      "Epoch 50, loss: 0.25994998-(best 0.25868648)\n",
      "\n",
      ">> Reached final epoch. Loading best model...\n",
      "TRAINING XGB PREDICTOR\n",
      ">> Ending training!\n",
      "\n",
      "-----------------------\n",
      "TIME COST: 6.8932318687438965 s\n",
      "Best Val: REC 59.98 PRE 46.64 MF1 65.66 AUC 75.37 TP 514 FP 588 TN 1735 FN 343\n",
      "-----------------------\n",
      "Generating additional positive data from adversary...\n",
      "Generating additional negative data by duplicating random nodes...\n",
      "\n",
      "ASSIGNED GRAPH FOR EVALUATION: 12932 {1: 2822, 0: 10110} Nodes, 1271118 Edges\n",
      "\n",
      "-------------------\n",
      "STARTING EVALUATION\n",
      "-------------------\n",
      "\n",
      "Dataset - Overall: REC 53.26 PRE 52.79 MF1 69.92 AUC 80.95 TP 1503 FP 1344 TN 8766 FN 1319 | 12932 {1: 2822, 0: 10110}\n",
      "Dataset - Train: REC 69.83 PRE 65.12 MF1 77.38 AUC 87.36 TP 898 FP 481 TN 3003 FN 388 | 4770 {1: 1286, 0: 3484}\n",
      "Dataset - Val: REC 52.16 PRE 46.56 MF1 64.44 AUC 74.43 TP 447 FP 513 TN 1810 FN 410 | 3180 {0: 2323, 1: 857}\n",
      "Dataset - Test: REC 23.27 PRE 31.10 MF1 58.35 AUC 74.00 TP 158 FP 350 TN 3953 FN 521 | 4982 {0: 4303, 1: 679}\n",
      "PREDICTION STATUS - {'1-True': 2301, '0-True': 6157, '0-False': 3953, '1-False': 521}\n",
      "    >> 521 positive nodes left unpredicted...\n",
      "    >> 354 OG positive nodes left unpredicted...\n",
      "-----------------------\n",
      "PREDICTION RESULT - ROUNDS\n",
      "Dataset - Round 0: REC 54.00 PRE 53.28 MF1 69.94 AUC 80.98 TP 722 FP 633 TN 3909 FN 615 | 5879 {1: 1337, 0: 4542}\n",
      "Dataset - Round 1: REC 39.84 PRE 48.11 MF1 64.77 AUC 72.61 TP 51 FP 55 TN 404 FN 77 | 587 {1: 128, 0: 459}\n",
      "Dataset - Round 2: REC 22.66 PRE 31.87 MF1 54.81 AUC 68.19 TP 29 FP 62 TN 397 FN 99 | 587 {1: 128, 0: 459}\n",
      "-----------------------\n",
      "PREDICTION RESULT - SEEDS\n",
      "Seeds - Current: REC 25.78 PRE 29.46 MF1 54.44 AUC 65.33 TP 33 FP 79 TN 380 FN 95 | 587 {1: 128, 0: 459}\n",
      "Seeds - Prev: REC 0.00 PRE 0.00 MF1 39.73 AUC 65.33 TP 0 FP 72 TN 387 FN 128 | 587 {1: 128, 0: 459}\n",
      "Selecting data as budgeted ground truth for next round...\n",
      "The file ../checkpoint/working_model_file_1730091771.3557496_epoch.py does not exist\n",
      "The file ../checkpoint/working_model_file_1730091771.3557496_round-M.json does not exist\n",
      "The file ../checkpoint/working_model_file_1730091771.3557496_round-T.json does not exist\n",
      "The file ../checkpoint/working_model_file_1730091771.3557496_round.pt does not exist\n",
      "================\n",
      "++++++++++++++++\n",
      "TRIAL NUMBER 2\n",
      "++++++++++++++++\n",
      "================\n",
      "  > Rereading graph data...\n",
      "  > Initializing multiround object...\n",
      "11758 1049758\n",
      "\n",
      "=========================\n",
      "STARTING ADVER EXPERIMENT\n",
      "=========================\n",
      "MAIN CONFIG: [('device', 'cuda:0'), ('exp_type', 'ADVER'), ('full_oracle', False), ('node_expiration_time', 1), ('round_budget_neg', 0), ('round_budget_pos', 0), ('round_new_neg', 459), ('round_new_pos', 128), ('round_num', 3), ('save_df', True), ('save_embedding', False), ('save_temp_sne', False), ('setting_type', 'INDUCTIVE'), ('task_type', 'NODE'), ('verbose', 4)]\n",
      "MODEL CONFIG: [('act_name', 'LeakyReLU'), ('addon_internal_agg', 'OR'), ('addon_name', 'NONE'), ('addon_perc', 0.05), ('addon_round_window', 7), ('alpha', 1), ('att_heads', 2), ('beta', 0), ('boost_agg_backbone', <class 'models.benchmarks_supervised.booster.GIN_noparam'>), ('boost_metric', <function average_precision_score at 0x000001C8520A5760>), ('boost_predictor', <class 'xgboost.sklearn.XGBClassifier'>), ('device', 'cuda:0'), ('drop_rate', 0), ('dropout_rate', 0.1), ('early_stopping', 75), ('embed_type', 'temporal'), ('etypes', ['none']), ('gamma', 1), ('h_feats', 64), ('k', 1), ('lambda1', 1), ('lambda2', 1), ('lambda_', 1), ('loss_sample', True), ('loss_sample_ratio', 0.1), ('loss_type', 'ndist'), ('mlp_feats', 64), ('mlp_layers', 2), ('model_name', 'XGB-SP'), ('n_estimator', 500), ('norm_name', 'layer'), ('num_epoch', 100), ('num_layers', 2), ('num_round_epoch', 50), ('round_window', 7), ('temporal_agg', 'mean_final'), ('tloss_type', 'normal'), ('training_type', 'round'), ('verbose', 4)]\n",
      "STRAT CONFIG: [('augment_name', 'REAGE'), ('augment_neg_ratio', 0.09772070079945569), ('augment_pos_ratio', 0.02727929920054431), ('augment_prob', 0.5), ('augment_round_split', 8), ('device', 'cuda:0'), ('pseudo_name', 'NONE'), ('pseudo_new_pos_only', False), ('pseudo_skip_initial', False), ('verbose', 4)]\n",
      "TRAIN CONFIG: [('ce_weight', 2.7091757387247277), ('device', 'cuda:0'), ('dset_name', 'tolokers_bid'), ('early_stopping', 75), ('learning_rate', 0.01), ('loss', <function cross_entropy at 0x000001C84AF6F880>), ('loss_gamma', 5), ('num_epoch', 100), ('num_round_epoch', 50), ('optimizer', <class 'torch.optim.adam.Adam'>), ('random_state', 7777777), ('ratio_initial', 0.5), ('ratio_test', 0.2), ('ratio_train', 0.6), ('ratio_val', 0.2), ('round_reset_model', False), ('round_train_list', []), ('stuck_stopping', 50), ('stuck_threshold', 0), ('train_max_retry', 10), ('verbose', 4)]\n",
      "ADVER CONFIG: [('adver_choose_name', 'GREEDY'), ('adver_conn_coef', 0.1), ('adver_feat_coef', 1), ('adver_mod_name', 'REPLAY'), ('device', 'cuda:0'), ('verbose', 4)]\n",
      "\n",
      "INITIAL GRAPH: 11758 {1: 2566, 0: 9192} Nodes, 1049758 Edges\n",
      "=========================\n",
      "        \n",
      ">> Initializing graph...\n",
      "MAIN GRAPH: 11758 {1: 2566, 0: 9192} Nodes, 1049758 Edges\n",
      "\n",
      ">> Initializing experiment type-specific modules:\n",
      "  >> Initializing adversarial modules...\n",
      ">> Initializing task-specific modules:\n",
      ">> Initializing classifier model...\n",
      ">> Initializing other strategies...\n",
      "\n",
      "FINISH INITIALIZATION\n",
      "=====================\n",
      "        \n",
      "  > Starting round 0...\n",
      "\n",
      "+++++++++++++++++\n",
      "STARTING ROUND 0!\n",
      "+++++++++++++++++\n",
      "        \n",
      "ASSIGNED GRAPH FOR TRAINING: 5879 {1: 1289, 0: 4590} Nodes, 271551 Edges\n",
      "\n",
      ">> Splitting to train-validation set...\n",
      "Alotting train-val-test split for round 0\n",
      "Initial pool: 0, Prediction pool: 0, Budget pool: 0, Full pool: 5879\n",
      "Full graph size: 5879, Training: 3527, Val: 2352, Test: 5879\n",
      "    >> INITIAL DATA SPLIT: 3527 train rows ({0: 2754, 1: 773}) | 2352 val rows ({1: 516, 0: 1836}) | 0 test rows ({})\n",
      "    >> AUGMENTED DATA SPLIT: 3527 train rows ({0: 2754, 1: 773}) | 2352 val rows ({1: 516, 0: 1836}) | 0 test rows ({})\n",
      "    >> Updated cross-entropy weight to 3.5627425614489003...\n",
      "\n",
      "-----------------------\n",
      "STARTING MODEL TRAININGps_l\n",
      "-----------------------\n",
      "\n",
      "TRAINING NODE EMBEDDER\n",
      "Detailed Loss: recon 0.92561024-0.46280512 | disen 0.69348723-0.34674361 | temporal 0.64482009-0.00000000 | total 0.80954874\n",
      "Epoch 1, loss: 0.80954874-(best 0.80954874)\n",
      "\n",
      "Detailed Loss: recon 0.55281442-0.27640721 | disen 0.71433556-0.35716778 | temporal 0.66147989-0.00000000 | total 0.63357496\n",
      "Epoch 2, loss: 0.63357496-(best 0.63357496)\n",
      "\n",
      "Detailed Loss: recon 0.48755288-0.24377644 | disen 0.72137511-0.36068755 | temporal 0.66673309-0.00000000 | total 0.60446399\n",
      "Epoch 3, loss: 0.60446399-(best 0.60446399)\n",
      "\n",
      "Detailed Loss: recon 0.48323798-0.24161899 | disen 0.72037423-0.36018711 | temporal 0.66891396-0.00000000 | total 0.60180610\n",
      "Epoch 4, loss: 0.60180610-(best 0.60180610)\n",
      "\n",
      "Detailed Loss: recon 0.47446737-0.23723368 | disen 0.72303975-0.36151987 | temporal 0.67026657-0.00000000 | total 0.59875357\n",
      "Epoch 5, loss: 0.59875357-(best 0.59875357)\n",
      "\n",
      "Detailed Loss: recon 0.47062206-0.23531103 | disen 0.71627814-0.35813907 | temporal 0.67044741-0.00000000 | total 0.59345007\n",
      "Epoch 6, loss: 0.59345007-(best 0.59345007)\n",
      "\n",
      "Detailed Loss: recon 0.44570264-0.22285132 | disen 0.71915132-0.35957566 | temporal 0.67033023-0.00000000 | total 0.58242697\n",
      "Epoch 7, loss: 0.58242697-(best 0.58242697)\n",
      "\n",
      "Detailed Loss: recon 0.42826527-0.21413264 | disen 0.71724826-0.35862413 | temporal 0.66979915-0.00000000 | total 0.57275677\n",
      "Epoch 8, loss: 0.57275677-(best 0.57275677)\n",
      "\n",
      "Detailed Loss: recon 0.41360801-0.20680401 | disen 0.71871364-0.35935682 | temporal 0.66902524-0.00000000 | total 0.56616080\n",
      "Epoch 9, loss: 0.56616080-(best 0.56616080)\n",
      "\n",
      "Detailed Loss: recon 0.41275239-0.20637619 | disen 0.71078563-0.35539281 | temporal 0.66830140-0.00000000 | total 0.56176901\n",
      "Epoch 10, loss: 0.56176901-(best 0.56176901)\n",
      "\n",
      "Detailed Loss: recon 0.40861377-0.20430689 | disen 0.71363795-0.35681897 | temporal 0.66779178-0.00000000 | total 0.56112587\n",
      "Epoch 11, loss: 0.56112587-(best 0.56112587)\n",
      "\n",
      "Detailed Loss: recon 0.41256532-0.20628266 | disen 0.71412969-0.35706484 | temporal 0.66763800-0.00000000 | total 0.56334752\n",
      "Epoch 12, loss: 0.56334752-(best 0.56112587)\n",
      "\n",
      "Detailed Loss: recon 0.41587171-0.20793585 | disen 0.71038091-0.35519046 | temporal 0.66753501-0.00000000 | total 0.56312633\n",
      "Epoch 13, loss: 0.56312633-(best 0.56112587)\n",
      "\n",
      "Detailed Loss: recon 0.40211841-0.20105921 | disen 0.71055275-0.35527638 | temporal 0.66776431-0.00000000 | total 0.55633557\n",
      "Epoch 14, loss: 0.55633557-(best 0.55633557)\n",
      "\n",
      "Detailed Loss: recon 0.40033242-0.20016621 | disen 0.70871270-0.35435635 | temporal 0.66812629-0.00000000 | total 0.55452257\n",
      "Epoch 15, loss: 0.55452257-(best 0.55452257)\n",
      "\n",
      "Detailed Loss: recon 0.38753271-0.19376636 | disen 0.70155394-0.35077697 | temporal 0.66895622-0.00000000 | total 0.54454333\n",
      "Epoch 16, loss: 0.54454333-(best 0.54454333)\n",
      "\n",
      "Detailed Loss: recon 0.38923079-0.19461539 | disen 0.70530963-0.35265481 | temporal 0.66943526-0.00000000 | total 0.54727018\n",
      "Epoch 17, loss: 0.54727018-(best 0.54454333)\n",
      "\n",
      "Detailed Loss: recon 0.38744181-0.19372091 | disen 0.70159560-0.35079780 | temporal 0.66978890-0.00000000 | total 0.54451871\n",
      "Epoch 18, loss: 0.54451871-(best 0.54451871)\n",
      "\n",
      "Detailed Loss: recon 0.37925118-0.18962559 | disen 0.69692874-0.34846437 | temporal 0.67039716-0.00000000 | total 0.53808999\n",
      "Epoch 19, loss: 0.53808999-(best 0.53808999)\n",
      "\n",
      "Detailed Loss: recon 0.37356445-0.18678223 | disen 0.69275141-0.34637570 | temporal 0.67059720-0.00000000 | total 0.53315794\n",
      "Epoch 20, loss: 0.53315794-(best 0.53315794)\n",
      "\n",
      "Detailed Loss: recon 0.37221074-0.18610537 | disen 0.68872237-0.34436119 | temporal 0.67056060-0.00000000 | total 0.53046656\n",
      "Epoch 21, loss: 0.53046656-(best 0.53046656)\n",
      "\n",
      "Detailed Loss: recon 0.36421844-0.18210922 | disen 0.68728030-0.34364015 | temporal 0.67073143-0.00000000 | total 0.52574939\n",
      "Epoch 22, loss: 0.52574939-(best 0.52574939)\n",
      "\n",
      "Detailed Loss: recon 0.36101949-0.18050975 | disen 0.68543410-0.34271705 | temporal 0.67070961-0.00000000 | total 0.52322680\n",
      "Epoch 23, loss: 0.52322680-(best 0.52322680)\n",
      "\n",
      "Detailed Loss: recon 0.36263126-0.18131563 | disen 0.68324566-0.34162283 | temporal 0.67087191-0.00000000 | total 0.52293849\n",
      "Epoch 24, loss: 0.52293849-(best 0.52293849)\n",
      "\n",
      "Detailed Loss: recon 0.35871944-0.17935972 | disen 0.67914891-0.33957446 | temporal 0.67107934-0.00000000 | total 0.51893419\n",
      "Epoch 25, loss: 0.51893419-(best 0.51893419)\n",
      "\n",
      "Detailed Loss: recon 0.35798562-0.17899281 | disen 0.67283607-0.33641803 | temporal 0.67138147-0.00000000 | total 0.51541084\n",
      "Epoch 26, loss: 0.51541084-(best 0.51541084)\n",
      "\n",
      "Detailed Loss: recon 0.35312390-0.17656195 | disen 0.67878211-0.33939105 | temporal 0.67143774-0.00000000 | total 0.51595300\n",
      "Epoch 27, loss: 0.51595300-(best 0.51541084)\n",
      "\n",
      "Detailed Loss: recon 0.35285759-0.17642879 | disen 0.67584819-0.33792409 | temporal 0.67158610-0.00000000 | total 0.51435292\n",
      "Epoch 28, loss: 0.51435292-(best 0.51435292)\n",
      "\n",
      "Detailed Loss: recon 0.34649262-0.17324631 | disen 0.66475409-0.33237705 | temporal 0.67155963-0.00000000 | total 0.50562334\n",
      "Epoch 29, loss: 0.50562334-(best 0.50562334)\n",
      "\n",
      "Detailed Loss: recon 0.34733412-0.17366706 | disen 0.66358030-0.33179015 | temporal 0.67183656-0.00000000 | total 0.50545722\n",
      "Epoch 30, loss: 0.50545722-(best 0.50545722)\n",
      "\n",
      "Detailed Loss: recon 0.34425855-0.17212927 | disen 0.66150731-0.33075365 | temporal 0.67190540-0.00000000 | total 0.50288296\n",
      "Epoch 31, loss: 0.50288296-(best 0.50288296)\n",
      "\n",
      "Detailed Loss: recon 0.34185681-0.17092840 | disen 0.64950085-0.32475042 | temporal 0.67189753-0.00000000 | total 0.49567884\n",
      "Epoch 32, loss: 0.49567884-(best 0.49567884)\n",
      "\n",
      "Detailed Loss: recon 0.34051639-0.17025819 | disen 0.64927661-0.32463831 | temporal 0.67202705-0.00000000 | total 0.49489650\n",
      "Epoch 33, loss: 0.49489650-(best 0.49489650)\n",
      "\n",
      "Detailed Loss: recon 0.34132600-0.17066300 | disen 0.64845014-0.32422507 | temporal 0.67210484-0.00000000 | total 0.49488807\n",
      "Epoch 34, loss: 0.49488807-(best 0.49488807)\n",
      "\n",
      "Detailed Loss: recon 0.33782524-0.16891262 | disen 0.64440650-0.32220325 | temporal 0.67184597-0.00000000 | total 0.49111587\n",
      "Epoch 35, loss: 0.49111587-(best 0.49111587)\n",
      "\n",
      "Detailed Loss: recon 0.33749050-0.16874525 | disen 0.63300836-0.31650418 | temporal 0.67172354-0.00000000 | total 0.48524943\n",
      "Epoch 36, loss: 0.48524943-(best 0.48524943)\n",
      "\n",
      "Detailed Loss: recon 0.33550739-0.16775370 | disen 0.62292236-0.31146118 | temporal 0.67178321-0.00000000 | total 0.47921488\n",
      "Epoch 37, loss: 0.47921488-(best 0.47921488)\n",
      "\n",
      "Detailed Loss: recon 0.33426720-0.16713360 | disen 0.62081087-0.31040543 | temporal 0.67174369-0.00000000 | total 0.47753903\n",
      "Epoch 38, loss: 0.47753903-(best 0.47753903)\n",
      "\n",
      "Detailed Loss: recon 0.33650595-0.16825297 | disen 0.62024045-0.31012022 | temporal 0.67168540-0.00000000 | total 0.47837320\n",
      "Epoch 39, loss: 0.47837320-(best 0.47753903)\n",
      "\n",
      "Detailed Loss: recon 0.33631068-0.16815534 | disen 0.61942649-0.30971324 | temporal 0.67139179-0.00000000 | total 0.47786859\n",
      "Epoch 40, loss: 0.47786859-(best 0.47753903)\n",
      "\n",
      "Detailed Loss: recon 0.33506745-0.16753373 | disen 0.61674017-0.30837008 | temporal 0.67153817-0.00000000 | total 0.47590381\n",
      "Epoch 41, loss: 0.47590381-(best 0.47590381)\n",
      "\n",
      "Detailed Loss: recon 0.33571014-0.16785507 | disen 0.60473233-0.30236617 | temporal 0.67113608-0.00000000 | total 0.47022122\n",
      "Epoch 42, loss: 0.47022122-(best 0.47022122)\n",
      "\n",
      "Detailed Loss: recon 0.33264440-0.16632220 | disen 0.59954101-0.29977050 | temporal 0.67079931-0.00000000 | total 0.46609271\n",
      "Epoch 43, loss: 0.46609271-(best 0.46609271)\n",
      "\n",
      "Detailed Loss: recon 0.33154285-0.16577142 | disen 0.59481144-0.29740572 | temporal 0.67117703-0.00000000 | total 0.46317714\n",
      "Epoch 44, loss: 0.46317714-(best 0.46317714)\n",
      "\n",
      "Detailed Loss: recon 0.33477122-0.16738561 | disen 0.58625054-0.29312527 | temporal 0.67133540-0.00000000 | total 0.46051088\n",
      "Epoch 45, loss: 0.46051088-(best 0.46051088)\n",
      "\n",
      "Detailed Loss: recon 0.32908666-0.16454333 | disen 0.58538067-0.29269034 | temporal 0.67122477-0.00000000 | total 0.45723367\n",
      "Epoch 46, loss: 0.45723367-(best 0.45723367)\n",
      "\n",
      "Detailed Loss: recon 0.32998091-0.16499045 | disen 0.57269371-0.28634685 | temporal 0.67050380-0.00000000 | total 0.45133731\n",
      "Epoch 47, loss: 0.45133731-(best 0.45133731)\n",
      "\n",
      "Detailed Loss: recon 0.33239356-0.16619678 | disen 0.57044578-0.28522289 | temporal 0.67072368-0.00000000 | total 0.45141965\n",
      "Epoch 48, loss: 0.45141965-(best 0.45133731)\n",
      "\n",
      "Detailed Loss: recon 0.33310658-0.16655329 | disen 0.57318258-0.28659129 | temporal 0.67051852-0.00000000 | total 0.45314458\n",
      "Epoch 49, loss: 0.45314458-(best 0.45133731)\n",
      "\n",
      "Detailed Loss: recon 0.33170900-0.16585450 | disen 0.56859940-0.28429970 | temporal 0.67029208-0.00000000 | total 0.45015419\n",
      "Epoch 50, loss: 0.45015419-(best 0.45015419)\n",
      "\n",
      "Detailed Loss: recon 0.33669040-0.16834520 | disen 0.56309593-0.28154796 | temporal 0.67012429-0.00000000 | total 0.44989318\n",
      "Epoch 51, loss: 0.44989318-(best 0.44989318)\n",
      "\n",
      "Detailed Loss: recon 0.33316946-0.16658473 | disen 0.56266737-0.28133368 | temporal 0.67065412-0.00000000 | total 0.44791842\n",
      "Epoch 52, loss: 0.44791842-(best 0.44791842)\n",
      "\n",
      "Detailed Loss: recon 0.33591223-0.16795611 | disen 0.55601943-0.27800971 | temporal 0.67038381-0.00000000 | total 0.44596583\n",
      "Epoch 53, loss: 0.44596583-(best 0.44596583)\n",
      "\n",
      "Detailed Loss: recon 0.33031076-0.16515538 | disen 0.55439866-0.27719933 | temporal 0.67042363-0.00000000 | total 0.44235471\n",
      "Epoch 54, loss: 0.44235471-(best 0.44235471)\n",
      "\n",
      "Detailed Loss: recon 0.33226717-0.16613358 | disen 0.55822659-0.27911329 | temporal 0.67053390-0.00000000 | total 0.44524688\n",
      "Epoch 55, loss: 0.44524688-(best 0.44235471)\n",
      "\n",
      "Detailed Loss: recon 0.33065975-0.16532987 | disen 0.54807413-0.27403706 | temporal 0.67047119-0.00000000 | total 0.43936694\n",
      "Epoch 56, loss: 0.43936694-(best 0.43936694)\n",
      "\n",
      "Detailed Loss: recon 0.33395886-0.16697943 | disen 0.53991580-0.26995790 | temporal 0.66960675-0.00000000 | total 0.43693733\n",
      "Epoch 57, loss: 0.43693733-(best 0.43693733)\n",
      "\n",
      "Detailed Loss: recon 0.32866192-0.16433096 | disen 0.53755021-0.26877511 | temporal 0.66930902-0.00000000 | total 0.43310606\n",
      "Epoch 58, loss: 0.43310606-(best 0.43310606)\n",
      "\n",
      "Detailed Loss: recon 0.33190465-0.16595232 | disen 0.52856576-0.26428288 | temporal 0.66925037-0.00000000 | total 0.43023521\n",
      "Epoch 59, loss: 0.43023521-(best 0.43023521)\n",
      "\n",
      "Detailed Loss: recon 0.33030534-0.16515267 | disen 0.53103238-0.26551619 | temporal 0.66903740-0.00000000 | total 0.43066886\n",
      "Epoch 60, loss: 0.43066886-(best 0.43023521)\n",
      "\n",
      "Detailed Loss: recon 0.33107287-0.16553643 | disen 0.52200961-0.26100481 | temporal 0.66903424-0.00000000 | total 0.42654124\n",
      "Epoch 61, loss: 0.42654124-(best 0.42654124)\n",
      "\n",
      "Detailed Loss: recon 0.32960635-0.16480318 | disen 0.52327514-0.26163757 | temporal 0.66964042-0.00000000 | total 0.42644075\n",
      "Epoch 62, loss: 0.42644075-(best 0.42644075)\n",
      "\n",
      "Detailed Loss: recon 0.32966739-0.16483369 | disen 0.51801383-0.25900692 | temporal 0.66962469-0.00000000 | total 0.42384061\n",
      "Epoch 63, loss: 0.42384061-(best 0.42384061)\n",
      "\n",
      "Detailed Loss: recon 0.32929978-0.16464989 | disen 0.51473701-0.25736851 | temporal 0.66917032-0.00000000 | total 0.42201841\n",
      "Epoch 64, loss: 0.42201841-(best 0.42201841)\n",
      "\n",
      "Detailed Loss: recon 0.32786617-0.16393308 | disen 0.50997466-0.25498733 | temporal 0.66950816-0.00000000 | total 0.41892040\n",
      "Epoch 65, loss: 0.41892040-(best 0.41892040)\n",
      "\n",
      "Detailed Loss: recon 0.32994148-0.16497074 | disen 0.50863767-0.25431883 | temporal 0.66929156-0.00000000 | total 0.41928959\n",
      "Epoch 66, loss: 0.41928959-(best 0.41892040)\n",
      "\n",
      "Detailed Loss: recon 0.33165196-0.16582598 | disen 0.50185418-0.25092709 | temporal 0.66853851-0.00000000 | total 0.41675305\n",
      "Epoch 67, loss: 0.41675305-(best 0.41675305)\n",
      "\n",
      "Detailed Loss: recon 0.33047804-0.16523902 | disen 0.49390101-0.24695051 | temporal 0.66864866-0.00000000 | total 0.41218954\n",
      "Epoch 68, loss: 0.41218954-(best 0.41218954)\n",
      "\n",
      "Detailed Loss: recon 0.33510217-0.16755109 | disen 0.49361998-0.24680999 | temporal 0.66830581-0.00000000 | total 0.41436106\n",
      "Epoch 69, loss: 0.41436106-(best 0.41218954)\n",
      "\n",
      "Detailed Loss: recon 0.33583260-0.16791630 | disen 0.48268294-0.24134147 | temporal 0.66848260-0.00000000 | total 0.40925777\n",
      "Epoch 70, loss: 0.40925777-(best 0.40925777)\n",
      "\n",
      "Detailed Loss: recon 0.33738178-0.16869089 | disen 0.49032384-0.24516192 | temporal 0.66772270-0.00000000 | total 0.41385281\n",
      "Epoch 71, loss: 0.41385281-(best 0.40925777)\n",
      "\n",
      "Detailed Loss: recon 0.33651873-0.16825937 | disen 0.48063844-0.24031922 | temporal 0.66802037-0.00000000 | total 0.40857857\n",
      "Epoch 72, loss: 0.40857857-(best 0.40857857)\n",
      "\n",
      "Detailed Loss: recon 0.33456618-0.16728309 | disen 0.47417772-0.23708886 | temporal 0.66745293-0.00000000 | total 0.40437195\n",
      "Epoch 73, loss: 0.40437195-(best 0.40437195)\n",
      "\n",
      "Detailed Loss: recon 0.33554804-0.16777402 | disen 0.47400504-0.23700252 | temporal 0.66726714-0.00000000 | total 0.40477654\n",
      "Epoch 74, loss: 0.40477654-(best 0.40437195)\n",
      "\n",
      "Detailed Loss: recon 0.33427379-0.16713689 | disen 0.47614014-0.23807007 | temporal 0.66785002-0.00000000 | total 0.40520698\n",
      "Epoch 75, loss: 0.40520698-(best 0.40437195)\n",
      "\n",
      "Detailed Loss: recon 0.33711982-0.16855991 | disen 0.47084141-0.23542070 | temporal 0.66784155-0.00000000 | total 0.40398061\n",
      "Epoch 76, loss: 0.40398061-(best 0.40398061)\n",
      "\n",
      "Detailed Loss: recon 0.33929431-0.16964716 | disen 0.46598798-0.23299399 | temporal 0.66757721-0.00000000 | total 0.40264115\n",
      "Epoch 77, loss: 0.40264115-(best 0.40264115)\n",
      "\n",
      "Detailed Loss: recon 0.33890566-0.16945283 | disen 0.45910186-0.22955093 | temporal 0.66701621-0.00000000 | total 0.39900374\n",
      "Epoch 78, loss: 0.39900374-(best 0.39900374)\n",
      "\n",
      "Detailed Loss: recon 0.33578980-0.16789490 | disen 0.45745659-0.22872829 | temporal 0.66715515-0.00000000 | total 0.39662319\n",
      "Epoch 79, loss: 0.39662319-(best 0.39662319)\n",
      "\n",
      "Detailed Loss: recon 0.33710393-0.16855197 | disen 0.46256387-0.23128194 | temporal 0.66699803-0.00000000 | total 0.39983392\n",
      "Epoch 80, loss: 0.39983392-(best 0.39662319)\n",
      "\n",
      "Detailed Loss: recon 0.33367312-0.16683656 | disen 0.46253014-0.23126507 | temporal 0.66752380-0.00000000 | total 0.39810163\n",
      "Epoch 81, loss: 0.39810163-(best 0.39662319)\n",
      "\n",
      "Detailed Loss: recon 0.34332669-0.17166334 | disen 0.45454150-0.22727075 | temporal 0.66616082-0.00000000 | total 0.39893410\n",
      "Epoch 82, loss: 0.39893410-(best 0.39662319)\n",
      "\n",
      "Detailed Loss: recon 0.33310068-0.16655034 | disen 0.45462745-0.22731373 | temporal 0.66749132-0.00000000 | total 0.39386407\n",
      "Epoch 83, loss: 0.39386407-(best 0.39386407)\n",
      "\n",
      "Detailed Loss: recon 0.34165835-0.17082918 | disen 0.44355732-0.22177866 | temporal 0.66553628-0.00000000 | total 0.39260784\n",
      "Epoch 84, loss: 0.39260784-(best 0.39260784)\n",
      "\n",
      "Detailed Loss: recon 0.33682689-0.16841345 | disen 0.44609749-0.22304875 | temporal 0.66630089-0.00000000 | total 0.39146221\n",
      "Epoch 85, loss: 0.39146221-(best 0.39146221)\n",
      "\n",
      "Detailed Loss: recon 0.33306608-0.16653304 | disen 0.44592851-0.22296426 | temporal 0.66733587-0.00000000 | total 0.38949728\n",
      "Epoch 86, loss: 0.38949728-(best 0.38949728)\n",
      "\n",
      "Detailed Loss: recon 0.34458351-0.17229176 | disen 0.44276947-0.22138473 | temporal 0.66528493-0.00000000 | total 0.39367649\n",
      "Epoch 87, loss: 0.39367649-(best 0.38949728)\n",
      "\n",
      "Detailed Loss: recon 0.33901137-0.16950569 | disen 0.43651879-0.21825939 | temporal 0.66660935-0.00000000 | total 0.38776508\n",
      "Epoch 88, loss: 0.38776508-(best 0.38776508)\n",
      "\n",
      "Detailed Loss: recon 0.33677667-0.16838834 | disen 0.43256825-0.21628413 | temporal 0.66537452-0.00000000 | total 0.38467246\n",
      "Epoch 89, loss: 0.38467246-(best 0.38467246)\n",
      "\n",
      "Detailed Loss: recon 0.34366012-0.17183006 | disen 0.42908096-0.21454048 | temporal 0.66430354-0.00000000 | total 0.38637054\n",
      "Epoch 90, loss: 0.38637054-(best 0.38467246)\n",
      "\n",
      "Detailed Loss: recon 0.33429915-0.16714957 | disen 0.43813115-0.21906558 | temporal 0.66661859-0.00000000 | total 0.38621515\n",
      "Epoch 91, loss: 0.38621515-(best 0.38467246)\n",
      "\n",
      "Detailed Loss: recon 0.34305727-0.17152864 | disen 0.42727309-0.21363655 | temporal 0.66411471-0.00000000 | total 0.38516518\n",
      "Epoch 92, loss: 0.38516518-(best 0.38467246)\n",
      "\n",
      "Detailed Loss: recon 0.34338006-0.17169003 | disen 0.42829061-0.21414530 | temporal 0.66490299-0.00000000 | total 0.38583535\n",
      "Epoch 93, loss: 0.38583535-(best 0.38467246)\n",
      "\n",
      "Detailed Loss: recon 0.34101480-0.17050740 | disen 0.43101996-0.21550998 | temporal 0.66594213-0.00000000 | total 0.38601738\n",
      "Epoch 94, loss: 0.38601738-(best 0.38467246)\n",
      "\n",
      "Detailed Loss: recon 0.34809288-0.17404644 | disen 0.42608112-0.21304056 | temporal 0.66404969-0.00000000 | total 0.38708699\n",
      "Epoch 95, loss: 0.38708699-(best 0.38467246)\n",
      "\n",
      "Detailed Loss: recon 0.32792825-0.16396412 | disen 0.43541849-0.21770924 | temporal 0.66656488-0.00000000 | total 0.38167337\n",
      "Epoch 96, loss: 0.38167337-(best 0.38167337)\n",
      "\n",
      "Detailed Loss: recon 0.32750988-0.16375494 | disen 0.44066042-0.22033021 | temporal 0.66690141-0.00000000 | total 0.38408515\n",
      "Epoch 97, loss: 0.38408515-(best 0.38167337)\n",
      "\n",
      "Detailed Loss: recon 0.34215185-0.17107593 | disen 0.42122781-0.21061391 | temporal 0.66460401-0.00000000 | total 0.38168985\n",
      "Epoch 98, loss: 0.38168985-(best 0.38167337)\n",
      "\n",
      "Detailed Loss: recon 0.33619437-0.16809718 | disen 0.41963780-0.20981890 | temporal 0.66597587-0.00000000 | total 0.37791610\n",
      "Epoch 99, loss: 0.37791610-(best 0.37791610)\n",
      "\n",
      "Detailed Loss: recon 0.33775538-0.16887769 | disen 0.41441476-0.20720738 | temporal 0.66525036-0.00000000 | total 0.37608507\n",
      "Epoch 100, loss: 0.37608507-(best 0.37608507)\n",
      "\n",
      ">> Reached final epoch. Loading best model...\n",
      "TRAINING XGB PREDICTOR\n",
      ">> Ending training!\n",
      "\n",
      "-----------------------\n",
      "TIME COST: 6.486300706863403 s\n",
      "Best Val: REC 50.19 PRE 44.66 MF1 65.62 AUC 77.55 TP 259 FP 321 TN 1515 FN 257\n",
      "-----------------------\n",
      "\n",
      "ASSIGNED GRAPH FOR EVALUATION: 11758 {1: 2566, 0: 9192} Nodes, 1049758 Edges\n",
      "\n",
      "-------------------\n",
      "STARTING EVALUATION\n",
      "-------------------\n",
      "\n",
      "Dataset - Overall: REC 48.87 PRE 47.68 MF1 66.80 AUC 78.43 TP 1254 FP 1376 TN 7816 FN 1312 | 11758 {1: 2566, 0: 9192}\n",
      "Dataset - Train: REC 58.47 PRE 51.48 MF1 70.46 AUC 82.64 TP 452 FP 426 TN 2328 FN 321 | 3527 {0: 2754, 1: 773}\n",
      "Dataset - Val: REC 43.99 PRE 42.59 MF1 63.50 AUC 75.27 TP 227 FP 306 TN 1530 FN 289 | 2352 {1: 516, 0: 1836}\n",
      "Dataset - Test: REC 45.03 PRE 47.17 MF1 65.77 AUC 77.48 TP 575 FP 644 TN 3958 FN 702 | 5879 {0: 4602, 1: 1277}\n",
      "PREDICTION STATUS - {'1-True': 1864, '0-False': 3958, '0-True': 5234, '1-False': 702}\n",
      "    >> 702 positive nodes left unpredicted...\n",
      "    >> 702 OG positive nodes left unpredicted...\n",
      "Selecting data as budgeted ground truth for next round...\n",
      "  > Starting round 1...\n",
      "\n",
      "+++++++++++++++++\n",
      "STARTING ROUND 1!\n",
      "+++++++++++++++++\n",
      "        \n",
      "ASSIGNED GRAPH FOR TRAINING: 11758 {1: 2566, 0: 9192} Nodes, 1049758 Edges\n",
      "\n",
      ">> Splitting to train-validation set...\n",
      "Alotting train-val-test split for round 1\n",
      "Initial pool: 5879, Prediction pool: 2630, Budget pool: 0, Full pool: 7098\n",
      "Full graph size: 11758, Training: 4258, Val: 2840, Test: 11758\n",
      "    >> INITIAL DATA SPLIT: 4258 train rows ({1: 1118, 0: 3140}) | 2840 val rows ({0: 2094, 1: 746}) | 4660 test rows ({0: 3958, 1: 702})\n",
      "    >> AUGMENTED DATA SPLIT: 4258 train rows ({1: 1118, 0: 3140}) | 2840 val rows ({0: 2094, 1: 746}) | 4660 test rows ({0: 3958, 1: 702})\n",
      "    >> Updated cross-entropy weight to 2.808586762075134...\n",
      "\n",
      "-----------------------\n",
      "STARTING MODEL TRAININGps_l\n",
      "-----------------------\n",
      "\n",
      "TRAINING NODE EMBEDDER\n",
      "Detailed Loss: recon 0.32508209-0.16254105 | disen 0.41264671-0.20632336 | temporal 0.56336790-0.00000000 | total 0.36886442\n",
      "Epoch 1, loss: 0.36886442-(best 0.36886442)\n",
      "\n",
      "Detailed Loss: recon 0.54581487-0.27290744 | disen 0.38658977-0.19329488 | temporal 0.56242770-0.00000000 | total 0.46620232\n",
      "Epoch 2, loss: 0.46620232-(best 0.36886442)\n",
      "\n",
      "Detailed Loss: recon 0.33039686-0.16519843 | disen 0.41306090-0.20653045 | temporal 0.56536561-0.00000000 | total 0.37172890\n",
      "Epoch 3, loss: 0.37172890-(best 0.36886442)\n",
      "\n",
      "Detailed Loss: recon 0.38420829-0.19210415 | disen 0.44027579-0.22013789 | temporal 0.56619930-0.00000000 | total 0.41224205\n",
      "Epoch 4, loss: 0.41224205-(best 0.36886442)\n",
      "\n",
      "Detailed Loss: recon 0.34439218-0.17219609 | disen 0.42683792-0.21341896 | temporal 0.56591946-0.00000000 | total 0.38561505\n",
      "Epoch 5, loss: 0.38561505-(best 0.36886442)\n",
      "\n",
      "Detailed Loss: recon 0.32543468-0.16271734 | disen 0.40729451-0.20364726 | temporal 0.56536585-0.00000000 | total 0.36636460\n",
      "Epoch 6, loss: 0.36636460-(best 0.36636460)\n",
      "\n",
      "Detailed Loss: recon 0.35978839-0.17989419 | disen 0.39991778-0.19995889 | temporal 0.56500286-0.00000000 | total 0.37985307\n",
      "Epoch 7, loss: 0.37985307-(best 0.36636460)\n",
      "\n",
      "Detailed Loss: recon 0.35713100-0.17856550 | disen 0.39578182-0.19789091 | temporal 0.56509209-0.00000000 | total 0.37645641\n",
      "Epoch 8, loss: 0.37645641-(best 0.36636460)\n",
      "\n",
      "Detailed Loss: recon 0.34118354-0.17059177 | disen 0.39078522-0.19539261 | temporal 0.56522095-0.00000000 | total 0.36598438\n",
      "Epoch 9, loss: 0.36598438-(best 0.36598438)\n",
      "\n",
      "Detailed Loss: recon 0.33388466-0.16694233 | disen 0.39603436-0.19801718 | temporal 0.56559169-0.00000000 | total 0.36495951\n",
      "Epoch 10, loss: 0.36495951-(best 0.36495951)\n",
      "\n",
      "Detailed Loss: recon 0.32979545-0.16489772 | disen 0.39528680-0.19764340 | temporal 0.56585562-0.00000000 | total 0.36254114\n",
      "Epoch 11, loss: 0.36254114-(best 0.36254114)\n",
      "\n",
      "Detailed Loss: recon 0.32833534-0.16416767 | disen 0.39649272-0.19824636 | temporal 0.56595534-0.00000000 | total 0.36241403\n",
      "Epoch 12, loss: 0.36241403-(best 0.36241403)\n",
      "\n",
      "Detailed Loss: recon 0.32729080-0.16364540 | disen 0.39196748-0.19598374 | temporal 0.56589371-0.00000000 | total 0.35962915\n",
      "Epoch 13, loss: 0.35962915-(best 0.35962915)\n",
      "\n",
      "Detailed Loss: recon 0.32758969-0.16379485 | disen 0.39016598-0.19508299 | temporal 0.56578583-0.00000000 | total 0.35887784\n",
      "Epoch 14, loss: 0.35887784-(best 0.35887784)\n",
      "\n",
      "Detailed Loss: recon 0.32469589-0.16234794 | disen 0.39156228-0.19578114 | temporal 0.56571883-0.00000000 | total 0.35812908\n",
      "Epoch 15, loss: 0.35812908-(best 0.35812908)\n",
      "\n",
      "Detailed Loss: recon 0.31951067-0.15975533 | disen 0.38581294-0.19290647 | temporal 0.56544411-0.00000000 | total 0.35266179\n",
      "Epoch 16, loss: 0.35266179-(best 0.35266179)\n",
      "\n",
      "Detailed Loss: recon 0.31420475-0.15710238 | disen 0.38703346-0.19351673 | temporal 0.56522828-0.00000000 | total 0.35061911\n",
      "Epoch 17, loss: 0.35061911-(best 0.35061911)\n",
      "\n",
      "Detailed Loss: recon 0.31260318-0.15630159 | disen 0.38276076-0.19138038 | temporal 0.56498271-0.00000000 | total 0.34768197\n",
      "Epoch 18, loss: 0.34768197-(best 0.34768197)\n",
      "\n",
      "Detailed Loss: recon 0.31325102-0.15662551 | disen 0.38016218-0.19008109 | temporal 0.56476575-0.00000000 | total 0.34670660\n",
      "Epoch 19, loss: 0.34670660-(best 0.34670660)\n",
      "\n",
      "Detailed Loss: recon 0.31353748-0.15676874 | disen 0.37890708-0.18945354 | temporal 0.56460315-0.00000000 | total 0.34622228\n",
      "Epoch 20, loss: 0.34622228-(best 0.34622228)\n",
      "\n",
      "Detailed Loss: recon 0.31489524-0.15744762 | disen 0.37333184-0.18666592 | temporal 0.56442350-0.00000000 | total 0.34411353\n",
      "Epoch 21, loss: 0.34411353-(best 0.34411353)\n",
      "\n",
      "Detailed Loss: recon 0.31314832-0.15657416 | disen 0.36964053-0.18482026 | temporal 0.56435400-0.00000000 | total 0.34139442\n",
      "Epoch 22, loss: 0.34139442-(best 0.34139442)\n",
      "\n",
      "Detailed Loss: recon 0.31232673-0.15616336 | disen 0.36482817-0.18241408 | temporal 0.56432962-0.00000000 | total 0.33857745\n",
      "Epoch 23, loss: 0.33857745-(best 0.33857745)\n",
      "\n",
      "Detailed Loss: recon 0.30718881-0.15359440 | disen 0.36175543-0.18087772 | temporal 0.56433195-0.00000000 | total 0.33447212\n",
      "Epoch 24, loss: 0.33447212-(best 0.33447212)\n",
      "\n",
      "Detailed Loss: recon 0.30851641-0.15425821 | disen 0.36045116-0.18022558 | temporal 0.56444967-0.00000000 | total 0.33448380\n",
      "Epoch 25, loss: 0.33448380-(best 0.33447212)\n",
      "\n",
      "Detailed Loss: recon 0.31097350-0.15548675 | disen 0.36011052-0.18005526 | temporal 0.56453073-0.00000000 | total 0.33554202\n",
      "Epoch 26, loss: 0.33554202-(best 0.33447212)\n",
      "\n",
      "Detailed Loss: recon 0.31146464-0.15573232 | disen 0.35742861-0.17871431 | temporal 0.56448936-0.00000000 | total 0.33444661\n",
      "Epoch 27, loss: 0.33444661-(best 0.33444661)\n",
      "\n",
      "Detailed Loss: recon 0.30975378-0.15487689 | disen 0.35169506-0.17584753 | temporal 0.56438744-0.00000000 | total 0.33072442\n",
      "Epoch 28, loss: 0.33072442-(best 0.33072442)\n",
      "\n",
      "Detailed Loss: recon 0.31062454-0.15531227 | disen 0.34887671-0.17443836 | temporal 0.56429100-0.00000000 | total 0.32975063\n",
      "Epoch 29, loss: 0.32975063-(best 0.32975063)\n",
      "\n",
      "Detailed Loss: recon 0.30776614-0.15388307 | disen 0.34730160-0.17365080 | temporal 0.56427288-0.00000000 | total 0.32753387\n",
      "Epoch 30, loss: 0.32753387-(best 0.32753387)\n",
      "\n",
      "Detailed Loss: recon 0.31012785-0.15506393 | disen 0.34459817-0.17229909 | temporal 0.56418556-0.00000000 | total 0.32736301\n",
      "Epoch 31, loss: 0.32736301-(best 0.32736301)\n",
      "\n",
      "Detailed Loss: recon 0.30919254-0.15459627 | disen 0.34283495-0.17141747 | temporal 0.56405270-0.00000000 | total 0.32601374\n",
      "Epoch 32, loss: 0.32601374-(best 0.32601374)\n",
      "\n",
      "Detailed Loss: recon 0.30800825-0.15400413 | disen 0.34312004-0.17156002 | temporal 0.56401086-0.00000000 | total 0.32556415\n",
      "Epoch 33, loss: 0.32556415-(best 0.32556415)\n",
      "\n",
      "Detailed Loss: recon 0.30838695-0.15419348 | disen 0.34035259-0.17017630 | temporal 0.56384534-0.00000000 | total 0.32436979\n",
      "Epoch 34, loss: 0.32436979-(best 0.32436979)\n",
      "\n",
      "Detailed Loss: recon 0.31085896-0.15542948 | disen 0.33646947-0.16823474 | temporal 0.56363082-0.00000000 | total 0.32366422\n",
      "Epoch 35, loss: 0.32366422-(best 0.32366422)\n",
      "\n",
      "Detailed Loss: recon 0.31056818-0.15528409 | disen 0.33363801-0.16681901 | temporal 0.56356144-0.00000000 | total 0.32210308\n",
      "Epoch 36, loss: 0.32210308-(best 0.32210308)\n",
      "\n",
      "Detailed Loss: recon 0.30940792-0.15470396 | disen 0.33326757-0.16663378 | temporal 0.56360614-0.00000000 | total 0.32133776\n",
      "Epoch 37, loss: 0.32133776-(best 0.32133776)\n",
      "\n",
      "Detailed Loss: recon 0.30377972-0.15188986 | disen 0.33388847-0.16694424 | temporal 0.56364954-0.00000000 | total 0.31883410\n",
      "Epoch 38, loss: 0.31883410-(best 0.31883410)\n",
      "\n",
      "Detailed Loss: recon 0.30848607-0.15424304 | disen 0.33279216-0.16639608 | temporal 0.56353807-0.00000000 | total 0.32063913\n",
      "Epoch 39, loss: 0.32063913-(best 0.31883410)\n",
      "\n",
      "Detailed Loss: recon 0.30483910-0.15241955 | disen 0.33270311-0.16635156 | temporal 0.56352746-0.00000000 | total 0.31877112\n",
      "Epoch 40, loss: 0.31877112-(best 0.31877112)\n",
      "\n",
      "Detailed Loss: recon 0.30168170-0.15084085 | disen 0.33418131-0.16709065 | temporal 0.56360143-0.00000000 | total 0.31793150\n",
      "Epoch 41, loss: 0.31793150-(best 0.31793150)\n",
      "\n",
      "Detailed Loss: recon 0.30347216-0.15173608 | disen 0.33308673-0.16654336 | temporal 0.56357688-0.00000000 | total 0.31827945\n",
      "Epoch 42, loss: 0.31827945-(best 0.31793150)\n",
      "\n",
      "Detailed Loss: recon 0.30112141-0.15056071 | disen 0.33052695-0.16526347 | temporal 0.56336564-0.00000000 | total 0.31582418\n",
      "Epoch 43, loss: 0.31582418-(best 0.31582418)\n",
      "\n",
      "Detailed Loss: recon 0.30330610-0.15165305 | disen 0.32874316-0.16437158 | temporal 0.56328034-0.00000000 | total 0.31602463\n",
      "Epoch 44, loss: 0.31602463-(best 0.31582418)\n",
      "\n",
      "Detailed Loss: recon 0.29987118-0.14993559 | disen 0.32933170-0.16466585 | temporal 0.56349647-0.00000000 | total 0.31460142\n",
      "Epoch 45, loss: 0.31460142-(best 0.31460142)\n",
      "\n",
      "Detailed Loss: recon 0.29919806-0.14959903 | disen 0.33017564-0.16508782 | temporal 0.56353390-0.00000000 | total 0.31468683\n",
      "Epoch 46, loss: 0.31468683-(best 0.31460142)\n",
      "\n",
      "Detailed Loss: recon 0.30095521-0.15047760 | disen 0.32654089-0.16327044 | temporal 0.56334823-0.00000000 | total 0.31374806\n",
      "Epoch 47, loss: 0.31374806-(best 0.31374806)\n",
      "\n",
      "Detailed Loss: recon 0.30002379-0.15001190 | disen 0.32589054-0.16294527 | temporal 0.56334066-0.00000000 | total 0.31295717\n",
      "Epoch 48, loss: 0.31295717-(best 0.31295717)\n",
      "\n",
      "Detailed Loss: recon 0.29856071-0.14928035 | disen 0.32563818-0.16281909 | temporal 0.56339359-0.00000000 | total 0.31209946\n",
      "Epoch 49, loss: 0.31209946-(best 0.31209946)\n",
      "\n",
      "Detailed Loss: recon 0.29890233-0.14945117 | disen 0.32718420-0.16359210 | temporal 0.56340474-0.00000000 | total 0.31304327\n",
      "Epoch 50, loss: 0.31304327-(best 0.31209946)\n",
      "\n",
      ">> Reached final epoch. Loading best model...\n",
      "TRAINING XGB PREDICTOR\n",
      ">> Ending training!\n",
      "\n",
      "-----------------------\n",
      "TIME COST: 5.860335826873779 s\n",
      "Best Val: REC 57.51 PRE 52.64 MF1 68.95 AUC 79.87 TP 429 FP 386 TN 1708 FN 317\n",
      "-----------------------\n",
      "Generating additional positive data from adversary...\n",
      "Generating additional negative data by duplicating random nodes...\n",
      "\n",
      "ASSIGNED GRAPH FOR EVALUATION: 12345 {1: 2694, 0: 9651} Nodes, 1155466 Edges\n",
      "\n",
      "-------------------\n",
      "STARTING EVALUATION\n",
      "-------------------\n",
      "\n",
      "Dataset - Overall: REC 57.65 PRE 51.70 MF1 70.43 AUC 82.29 TP 1553 FP 1451 TN 8200 FN 1141 | 12345 {1: 2694, 0: 9651}\n",
      "Dataset - Train: REC 75.22 PRE 62.25 MF1 77.56 AUC 89.02 TP 841 FP 510 TN 2630 FN 277 | 4258 {1: 1118, 0: 3140}\n",
      "Dataset - Val: REC 57.77 PRE 48.70 MF1 66.93 AUC 78.24 TP 431 FP 454 TN 1640 FN 315 | 2840 {0: 2094, 1: 746}\n",
      "Dataset - Test: REC 33.86 PRE 36.59 MF1 61.76 AUC 76.07 TP 281 FP 487 TN 3930 FN 549 | 5247 {0: 4417, 1: 830}\n",
      "PREDICTION STATUS - {'1-True': 2145, '0-False': 3930, '0-True': 5721, '1-False': 549}\n",
      "    >> 549 positive nodes left unpredicted...\n",
      "    >> 456 OG positive nodes left unpredicted...\n",
      "-----------------------\n",
      "PREDICTION RESULT - ROUNDS\n",
      "Dataset - Round 0: REC 54.35 PRE 51.41 MF1 69.63 AUC 80.89 TP 694 FP 656 TN 3946 FN 583 | 5879 {0: 4602, 1: 1277}\n",
      "Dataset - Round 1: REC 27.34 PRE 34.65 MF1 56.87 AUC 71.17 TP 35 FP 66 TN 393 FN 93 | 587 {1: 128, 0: 459}\n",
      "-----------------------\n",
      "PREDICTION RESULT - SEEDS\n",
      "Seeds - Current: REC 35.16 PRE 39.13 MF1 60.30 AUC 72.09 TP 45 FP 70 TN 389 FN 83 | 587 {1: 128, 0: 459}\n",
      "Seeds - Prev: REC 0.00 PRE 0.00 MF1 39.98 AUC 72.09 TP 0 FP 68 TN 391 FN 128 | 587 {1: 128, 0: 459}\n",
      "Selecting data as budgeted ground truth for next round...\n",
      "  > Starting round 2...\n",
      "\n",
      "+++++++++++++++++\n",
      "STARTING ROUND 2!\n",
      "+++++++++++++++++\n",
      "        \n",
      "ASSIGNED GRAPH FOR TRAINING: 12345 {1: 2694, 0: 9651} Nodes, 1155466 Edges\n",
      "\n",
      ">> Splitting to train-validation set...\n",
      "Alotting train-val-test split for round 2\n",
      "Initial pool: 5879, Prediction pool: 5634, Budget pool: 0, Full pool: 7866\n",
      "Full graph size: 12345, Training: 4719, Val: 3147, Test: 12345\n",
      "    >> INITIAL DATA SPLIT: 4719 train rows ({1: 1287, 0: 3432}) | 3147 val rows ({0: 2289, 1: 858}) | 4479 test rows ({0: 3930, 1: 549})\n",
      "    >> AUGMENTED DATA SPLIT: 4719 train rows ({1: 1287, 0: 3432}) | 3147 val rows ({0: 2289, 1: 858}) | 4479 test rows ({0: 3930, 1: 549})\n",
      "    >> Updated cross-entropy weight to 2.6666666666666665...\n",
      "\n",
      "-----------------------\n",
      "STARTING MODEL TRAININGps_l\n",
      "-----------------------\n",
      "\n",
      "TRAINING NODE EMBEDDER\n",
      "Detailed Loss: recon 0.30481452-0.15240726 | disen 0.34041440-0.17020720 | temporal 0.58855486-0.00000000 | total 0.32261446\n",
      "Epoch 1, loss: 0.32261446-(best 0.32261446)\n",
      "\n",
      "Detailed Loss: recon 0.39228976-0.19614488 | disen 0.36552197-0.18276098 | temporal 0.58872873-0.00000000 | total 0.37890586\n",
      "Epoch 2, loss: 0.37890586-(best 0.32261446)\n",
      "\n",
      "Detailed Loss: recon 0.36150193-0.18075097 | disen 0.35530281-0.17765141 | temporal 0.58958387-0.00000000 | total 0.35840237\n",
      "Epoch 3, loss: 0.35840237-(best 0.32261446)\n",
      "\n",
      "Detailed Loss: recon 0.33382535-0.16691267 | disen 0.35222232-0.17611116 | temporal 0.59002090-0.00000000 | total 0.34302384\n",
      "Epoch 4, loss: 0.34302384-(best 0.32261446)\n",
      "\n",
      "Detailed Loss: recon 0.33017221-0.16508611 | disen 0.35507089-0.17753544 | temporal 0.58940369-0.00000000 | total 0.34262156\n",
      "Epoch 5, loss: 0.34262156-(best 0.32261446)\n",
      "\n",
      "Detailed Loss: recon 0.32502112-0.16251056 | disen 0.35633600-0.17816800 | temporal 0.58931971-0.00000000 | total 0.34067857\n",
      "Epoch 6, loss: 0.34067857-(best 0.32261446)\n",
      "\n",
      "Detailed Loss: recon 0.30444938-0.15222469 | disen 0.36203015-0.18101507 | temporal 0.58953106-0.00000000 | total 0.33323976\n",
      "Epoch 7, loss: 0.33323976-(best 0.32261446)\n",
      "\n",
      "Detailed Loss: recon 0.30107796-0.15053898 | disen 0.36943573-0.18471786 | temporal 0.58971834-0.00000000 | total 0.33525684\n",
      "Epoch 8, loss: 0.33525684-(best 0.32261446)\n",
      "\n",
      "Detailed Loss: recon 0.30087361-0.15043680 | disen 0.36644089-0.18322045 | temporal 0.58961326-0.00000000 | total 0.33365726\n",
      "Epoch 9, loss: 0.33365726-(best 0.32261446)\n",
      "\n",
      "Detailed Loss: recon 0.30166298-0.15083149 | disen 0.35713512-0.17856756 | temporal 0.58929849-0.00000000 | total 0.32939905\n",
      "Epoch 10, loss: 0.32939905-(best 0.32261446)\n",
      "\n",
      "Detailed Loss: recon 0.30872080-0.15436040 | disen 0.35147101-0.17573550 | temporal 0.58905506-0.00000000 | total 0.33009589\n",
      "Epoch 11, loss: 0.33009589-(best 0.32261446)\n",
      "\n",
      "Detailed Loss: recon 0.30375683-0.15187842 | disen 0.35267729-0.17633864 | temporal 0.58913791-0.00000000 | total 0.32821706\n",
      "Epoch 12, loss: 0.32821706-(best 0.32261446)\n",
      "\n",
      "Detailed Loss: recon 0.30215222-0.15107611 | disen 0.34859258-0.17429629 | temporal 0.58920342-0.00000000 | total 0.32537240\n",
      "Epoch 13, loss: 0.32537240-(best 0.32261446)\n",
      "\n",
      "Detailed Loss: recon 0.30165699-0.15082850 | disen 0.34830290-0.17415145 | temporal 0.58927739-0.00000000 | total 0.32497996\n",
      "Epoch 14, loss: 0.32497996-(best 0.32261446)\n",
      "\n",
      "Detailed Loss: recon 0.30283946-0.15141973 | disen 0.34722918-0.17361459 | temporal 0.58922529-0.00000000 | total 0.32503432\n",
      "Epoch 15, loss: 0.32503432-(best 0.32261446)\n",
      "\n",
      "Detailed Loss: recon 0.30250314-0.15125157 | disen 0.34677815-0.17338908 | temporal 0.58919430-0.00000000 | total 0.32464063\n",
      "Epoch 16, loss: 0.32464063-(best 0.32261446)\n",
      "\n",
      "Detailed Loss: recon 0.30064502-0.15032251 | disen 0.34427303-0.17213652 | temporal 0.58902413-0.00000000 | total 0.32245904\n",
      "Epoch 17, loss: 0.32245904-(best 0.32245904)\n",
      "\n",
      "Detailed Loss: recon 0.30233359-0.15116680 | disen 0.33999670-0.16999835 | temporal 0.58884406-0.00000000 | total 0.32116514\n",
      "Epoch 18, loss: 0.32116514-(best 0.32116514)\n",
      "\n",
      "Detailed Loss: recon 0.30056164-0.15028082 | disen 0.34206462-0.17103231 | temporal 0.58897680-0.00000000 | total 0.32131314\n",
      "Epoch 19, loss: 0.32131314-(best 0.32116514)\n",
      "\n",
      "Detailed Loss: recon 0.30061096-0.15030548 | disen 0.33935928-0.16967964 | temporal 0.58887637-0.00000000 | total 0.31998512\n",
      "Epoch 20, loss: 0.31998512-(best 0.31998512)\n",
      "\n",
      "Detailed Loss: recon 0.30224892-0.15112446 | disen 0.33843601-0.16921800 | temporal 0.58885109-0.00000000 | total 0.32034248\n",
      "Epoch 21, loss: 0.32034248-(best 0.31998512)\n",
      "\n",
      "Detailed Loss: recon 0.30087519-0.15043759 | disen 0.33434069-0.16717035 | temporal 0.58877993-0.00000000 | total 0.31760794\n",
      "Epoch 22, loss: 0.31760794-(best 0.31760794)\n",
      "\n",
      "Detailed Loss: recon 0.30106810-0.15053405 | disen 0.33324754-0.16662377 | temporal 0.58875632-0.00000000 | total 0.31715780\n",
      "Epoch 23, loss: 0.31715780-(best 0.31715780)\n",
      "\n",
      "Detailed Loss: recon 0.30079719-0.15039860 | disen 0.33225667-0.16612834 | temporal 0.58871955-0.00000000 | total 0.31652695\n",
      "Epoch 24, loss: 0.31652695-(best 0.31652695)\n",
      "\n",
      "Detailed Loss: recon 0.30017707-0.15008853 | disen 0.32985157-0.16492578 | temporal 0.58869374-0.00000000 | total 0.31501430\n",
      "Epoch 25, loss: 0.31501430-(best 0.31501430)\n",
      "\n",
      "Detailed Loss: recon 0.30132613-0.15066306 | disen 0.33012366-0.16506183 | temporal 0.58862764-0.00000000 | total 0.31572491\n",
      "Epoch 26, loss: 0.31572491-(best 0.31501430)\n",
      "\n",
      "Detailed Loss: recon 0.30144745-0.15072373 | disen 0.32865793-0.16432896 | temporal 0.58861548-0.00000000 | total 0.31505269\n",
      "Epoch 27, loss: 0.31505269-(best 0.31501430)\n",
      "\n",
      "Detailed Loss: recon 0.29963109-0.14981554 | disen 0.32602817-0.16301408 | temporal 0.58856308-0.00000000 | total 0.31282961\n",
      "Epoch 28, loss: 0.31282961-(best 0.31282961)\n",
      "\n",
      "Detailed Loss: recon 0.29902661-0.14951330 | disen 0.32633859-0.16316929 | temporal 0.58866233-0.00000000 | total 0.31268260\n",
      "Epoch 29, loss: 0.31268260-(best 0.31268260)\n",
      "\n",
      "Detailed Loss: recon 0.30032969-0.15016484 | disen 0.32617557-0.16308779 | temporal 0.58871800-0.00000000 | total 0.31325263\n",
      "Epoch 30, loss: 0.31325263-(best 0.31268260)\n",
      "\n",
      "Detailed Loss: recon 0.29935458-0.14967729 | disen 0.32494378-0.16247189 | temporal 0.58874726-0.00000000 | total 0.31214917\n",
      "Epoch 31, loss: 0.31214917-(best 0.31214917)\n",
      "\n",
      "Detailed Loss: recon 0.30112463-0.15056232 | disen 0.32503939-0.16251969 | temporal 0.58893502-0.00000000 | total 0.31308201\n",
      "Epoch 32, loss: 0.31308201-(best 0.31214917)\n",
      "\n",
      "Detailed Loss: recon 0.30449384-0.15224692 | disen 0.32071280-0.16035640 | temporal 0.58877021-0.00000000 | total 0.31260332\n",
      "Epoch 33, loss: 0.31260332-(best 0.31214917)\n",
      "\n",
      "Detailed Loss: recon 0.30648071-0.15324035 | disen 0.31978351-0.15989175 | temporal 0.58872455-0.00000000 | total 0.31313211\n",
      "Epoch 34, loss: 0.31313211-(best 0.31214917)\n",
      "\n",
      "Detailed Loss: recon 0.30522111-0.15261056 | disen 0.31772143-0.15886071 | temporal 0.58867353-0.00000000 | total 0.31147128\n",
      "Epoch 35, loss: 0.31147128-(best 0.31147128)\n",
      "\n",
      "Detailed Loss: recon 0.30112118-0.15056059 | disen 0.31754750-0.15877375 | temporal 0.58873278-0.00000000 | total 0.30933434\n",
      "Epoch 36, loss: 0.30933434-(best 0.30933434)\n",
      "\n",
      "Detailed Loss: recon 0.29924017-0.14962009 | disen 0.31966507-0.15983254 | temporal 0.58875841-0.00000000 | total 0.30945262\n",
      "Epoch 37, loss: 0.30945262-(best 0.30933434)\n",
      "\n",
      "Detailed Loss: recon 0.30086046-0.15043023 | disen 0.32032830-0.16016415 | temporal 0.58890527-0.00000000 | total 0.31059438\n",
      "Epoch 38, loss: 0.31059438-(best 0.30933434)\n",
      "\n",
      "Detailed Loss: recon 0.29814300-0.14907150 | disen 0.32050967-0.16025484 | temporal 0.58894569-0.00000000 | total 0.30932635\n",
      "Epoch 39, loss: 0.30932635-(best 0.30932635)\n",
      "\n",
      "Detailed Loss: recon 0.29867649-0.14933825 | disen 0.32087904-0.16043952 | temporal 0.58894759-0.00000000 | total 0.30977777\n",
      "Epoch 40, loss: 0.30977777-(best 0.30932635)\n",
      "\n",
      "Detailed Loss: recon 0.30252829-0.15126415 | disen 0.31861591-0.15930796 | temporal 0.58886683-0.00000000 | total 0.31057209\n",
      "Epoch 41, loss: 0.31057209-(best 0.30932635)\n",
      "\n",
      "Detailed Loss: recon 0.30168468-0.15084234 | disen 0.31703162-0.15851581 | temporal 0.58871555-0.00000000 | total 0.30935815\n",
      "Epoch 42, loss: 0.30935815-(best 0.30932635)\n",
      "\n",
      "Detailed Loss: recon 0.29813379-0.14906690 | disen 0.31588936-0.15794468 | temporal 0.58865350-0.00000000 | total 0.30701157\n",
      "Epoch 43, loss: 0.30701157-(best 0.30701157)\n",
      "\n",
      "Detailed Loss: recon 0.29788613-0.14894307 | disen 0.31552738-0.15776369 | temporal 0.58878547-0.00000000 | total 0.30670676\n",
      "Epoch 44, loss: 0.30670676-(best 0.30670676)\n",
      "\n",
      "Detailed Loss: recon 0.29536426-0.14768213 | disen 0.31591696-0.15795848 | temporal 0.58883411-0.00000000 | total 0.30564061\n",
      "Epoch 45, loss: 0.30564061-(best 0.30564061)\n",
      "\n",
      "Detailed Loss: recon 0.29597279-0.14798640 | disen 0.31614065-0.15807033 | temporal 0.58894444-0.00000000 | total 0.30605674\n",
      "Epoch 46, loss: 0.30605674-(best 0.30564061)\n",
      "\n",
      "Detailed Loss: recon 0.29679531-0.14839765 | disen 0.31316644-0.15658322 | temporal 0.58890259-0.00000000 | total 0.30498087\n",
      "Epoch 47, loss: 0.30498087-(best 0.30498087)\n",
      "\n",
      "Detailed Loss: recon 0.29913592-0.14956796 | disen 0.31118518-0.15559259 | temporal 0.58885467-0.00000000 | total 0.30516055\n",
      "Epoch 48, loss: 0.30516055-(best 0.30498087)\n",
      "\n",
      "Detailed Loss: recon 0.29973325-0.14986663 | disen 0.31170237-0.15585119 | temporal 0.58891082-0.00000000 | total 0.30571783\n",
      "Epoch 49, loss: 0.30571783-(best 0.30498087)\n",
      "\n",
      "Detailed Loss: recon 0.29551888-0.14775944 | disen 0.31096166-0.15548083 | temporal 0.58889741-0.00000000 | total 0.30324027\n",
      "Epoch 50, loss: 0.30324027-(best 0.30324027)\n",
      "\n",
      ">> Reached final epoch. Loading best model...\n",
      "TRAINING XGB PREDICTOR\n",
      ">> Ending training!\n",
      "\n",
      "-----------------------\n",
      "TIME COST: 7.463793516159058 s\n",
      "Best Val: REC 64.22 PRE 52.73 MF1 69.83 AUC 78.95 TP 551 FP 494 TN 1795 FN 307\n",
      "-----------------------\n",
      "Generating additional positive data from adversary...\n",
      "Generating additional negative data by duplicating random nodes...\n",
      "\n",
      "ASSIGNED GRAPH FOR EVALUATION: 12932 {1: 2822, 0: 10110} Nodes, 1268968 Edges\n",
      "\n",
      "-------------------\n",
      "STARTING EVALUATION\n",
      "-------------------\n",
      "\n",
      "Dataset - Overall: REC 59.57 PRE 51.02 MF1 70.51 AUC 82.14 TP 1681 FP 1614 TN 8496 FN 1141 | 12932 {1: 2822, 0: 10110}\n",
      "Dataset - Train: REC 74.59 PRE 59.63 MF1 75.67 AUC 86.71 TP 960 FP 650 TN 2782 FN 327 | 4719 {1: 1287, 0: 3432}\n",
      "Dataset - Val: REC 62.00 PRE 47.25 MF1 66.14 AUC 76.49 TP 532 FP 594 TN 1695 FN 326 | 3147 {0: 2289, 1: 858}\n",
      "Dataset - Test: REC 27.92 PRE 33.81 MF1 60.47 AUC 77.21 TP 189 FP 370 TN 4019 FN 488 | 5066 {0: 4389, 1: 677}\n",
      "PREDICTION STATUS - {'1-True': 2334, '0-False': 4019, '0-True': 6091, '1-False': 488}\n",
      "    >> 488 positive nodes left unpredicted...\n",
      "    >> 332 OG positive nodes left unpredicted...\n",
      "-----------------------\n",
      "PREDICTION RESULT - ROUNDS\n",
      "Dataset - Round 0: REC 57.32 PRE 51.26 MF1 70.21 AUC 82.00 TP 732 FP 696 TN 3906 FN 545 | 5879 {0: 4602, 1: 1277}\n",
      "Dataset - Round 1: REC 40.62 PRE 49.06 MF1 65.31 AUC 78.07 TP 52 FP 54 TN 405 FN 76 | 587 {1: 128, 0: 459}\n",
      "Dataset - Round 2: REC 29.69 PRE 46.34 MF1 61.15 AUC 73.48 TP 38 FP 44 TN 415 FN 90 | 587 {1: 128, 0: 459}\n",
      "-----------------------\n",
      "PREDICTION RESULT - SEEDS\n",
      "Seeds - Current: REC 37.50 PRE 40.34 MF1 61.29 AUC 71.60 TP 48 FP 71 TN 388 FN 80 | 587 {1: 128, 0: 459}\n",
      "Seeds - Prev: REC 0.00 PRE 0.00 MF1 40.41 AUC 71.60 TP 0 FP 61 TN 398 FN 128 | 587 {1: 128, 0: 459}\n",
      "Selecting data as budgeted ground truth for next round...\n",
      "The file ../checkpoint/working_model_file_1730091793.422833_epoch.py does not exist\n",
      "The file ../checkpoint/working_model_file_1730091793.422833_round-M.json does not exist\n",
      "The file ../checkpoint/working_model_file_1730091793.422833_round-T.json does not exist\n",
      "The file ../checkpoint/working_model_file_1730091793.422833_round.pt does not exist\n",
      "Experiment ended, experienced 0 failures\n",
      "Elapsed experiment time 68.41516161s\n",
      "================\n",
      "++++++++++++++++\n",
      "TRIAL NUMBER 0\n",
      "++++++++++++++++\n",
      "================\n",
      "  > Rereading graph data...\n",
      "  > Initializing multiround object...\n",
      "11758 1049758\n",
      "\n",
      "=========================\n",
      "STARTING ADVER EXPERIMENT\n",
      "=========================\n",
      "MAIN CONFIG: [('device', 'cuda:0'), ('exp_type', 'ADVER'), ('full_oracle', False), ('node_expiration_time', 1), ('round_budget_neg', 0), ('round_budget_pos', 0), ('round_new_neg', 459), ('round_new_pos', 128), ('round_num', 3), ('save_df', True), ('save_embedding', False), ('save_temp_sne', False), ('setting_type', 'INDUCTIVE'), ('task_type', 'NODE'), ('verbose', 4)]\n",
      "MODEL CONFIG: [('act_name', 'LeakyReLU'), ('addon_internal_agg', 'OR'), ('addon_name', 'NONE'), ('addon_perc', 0.05), ('addon_round_window', 7), ('alpha', 1), ('att_heads', 2), ('beta', 1), ('boost_agg_backbone', <class 'models.benchmarks_supervised.booster.GIN_noparam'>), ('boost_metric', <function average_precision_score at 0x000001C8520A5760>), ('boost_predictor', <class 'xgboost.sklearn.XGBClassifier'>), ('device', 'cuda:0'), ('drop_rate', 0), ('dropout_rate', 0.1), ('early_stopping', 75), ('embed_type', 'temporal'), ('etypes', ['none']), ('gamma', 1), ('h_feats', 64), ('k', 1), ('lambda1', 1), ('lambda2', 1), ('lambda_', 1), ('loss_sample', True), ('loss_sample_ratio', 0.1), ('loss_type', 'ndist'), ('mlp_feats', 64), ('mlp_layers', 2), ('model_name', 'XGB-SP'), ('n_estimator', 500), ('norm_name', 'layer'), ('num_epoch', 100), ('num_layers', 2), ('num_round_epoch', 50), ('round_window', 7), ('temporal_agg', 'mean_final'), ('tloss_type', 'normal'), ('training_type', 'round'), ('verbose', 4)]\n",
      "STRAT CONFIG: [('augment_name', 'REAGE'), ('augment_neg_ratio', 0.09772070079945569), ('augment_pos_ratio', 0.02727929920054431), ('augment_prob', 0.5), ('augment_round_split', 8), ('device', 'cuda:0'), ('pseudo_name', 'NONE'), ('pseudo_new_pos_only', False), ('pseudo_skip_initial', False), ('verbose', 4)]\n",
      "TRAIN CONFIG: [('ce_weight', 2.6666666666666665), ('device', 'cuda:0'), ('dset_name', 'tolokers_bid'), ('early_stopping', 75), ('learning_rate', 0.01), ('loss', <function cross_entropy at 0x000001C84AF6F880>), ('loss_gamma', 5), ('num_epoch', 100), ('num_round_epoch', 50), ('optimizer', <class 'torch.optim.adam.Adam'>), ('random_state', 7777777), ('ratio_initial', 0.5), ('ratio_test', 0.2), ('ratio_train', 0.6), ('ratio_val', 0.2), ('round_reset_model', False), ('round_train_list', []), ('stuck_stopping', 50), ('stuck_threshold', 0), ('train_max_retry', 10), ('verbose', 4)]\n",
      "ADVER CONFIG: [('adver_choose_name', 'GREEDY'), ('adver_conn_coef', 0.1), ('adver_feat_coef', 1), ('adver_mod_name', 'REPLAY'), ('device', 'cuda:0'), ('verbose', 4)]\n",
      "\n",
      "INITIAL GRAPH: 11758 {1: 2566, 0: 9192} Nodes, 1049758 Edges\n",
      "=========================\n",
      "        \n",
      ">> Initializing graph...\n",
      "MAIN GRAPH: 11758 {1: 2566, 0: 9192} Nodes, 1049758 Edges\n",
      "\n",
      ">> Initializing experiment type-specific modules:\n",
      "  >> Initializing adversarial modules...\n",
      ">> Initializing task-specific modules:\n",
      ">> Initializing classifier model...\n",
      ">> Initializing other strategies...\n",
      "\n",
      "FINISH INITIALIZATION\n",
      "=====================\n",
      "        \n",
      "  > Starting round 0...\n",
      "\n",
      "+++++++++++++++++\n",
      "STARTING ROUND 0!\n",
      "+++++++++++++++++\n",
      "        \n",
      "ASSIGNED GRAPH FOR TRAINING: 5879 {0: 4585, 1: 1294} Nodes, 255241 Edges\n",
      "\n",
      ">> Splitting to train-validation set...\n",
      "Alotting train-val-test split for round 0\n",
      "Initial pool: 0, Prediction pool: 0, Budget pool: 0, Full pool: 5879\n",
      "Full graph size: 5879, Training: 3527, Val: 2352, Test: 5879\n",
      "    >> INITIAL DATA SPLIT: 3527 train rows ({0: 2751, 1: 776}) | 2352 val rows ({0: 1834, 1: 518}) | 0 test rows ({})\n",
      "    >> AUGMENTED DATA SPLIT: 3527 train rows ({0: 2751, 1: 776}) | 2352 val rows ({0: 1834, 1: 518}) | 0 test rows ({})\n",
      "    >> Updated cross-entropy weight to 3.545103092783505...\n",
      "\n",
      "-----------------------\n",
      "STARTING MODEL TRAININGps_l\n",
      "-----------------------\n",
      "\n",
      "TRAINING NODE EMBEDDER\n",
      "Detailed Loss: recon 0.71818459-0.23939486 | disen 0.71033394-0.23677798 | temporal 0.64982456-0.21660819 | total 0.69278109\n",
      "Epoch 1, loss: 0.69278109-(best 0.69278109)\n",
      "\n",
      "Detailed Loss: recon 0.43542212-0.14514071 | disen 0.72741127-0.24247042 | temporal 0.66096276-0.22032092 | total 0.60793209\n",
      "Epoch 2, loss: 0.60793209-(best 0.60793209)\n",
      "\n",
      "Detailed Loss: recon 0.37433368-0.12477789 | disen 0.72831237-0.24277079 | temporal 0.66521734-0.22173911 | total 0.58928782\n",
      "Epoch 3, loss: 0.58928782-(best 0.58928782)\n",
      "\n",
      "Detailed Loss: recon 0.35770702-0.11923567 | disen 0.73239219-0.24413073 | temporal 0.66769570-0.22256523 | total 0.58593166\n",
      "Epoch 4, loss: 0.58593166-(best 0.58593166)\n",
      "\n",
      "Detailed Loss: recon 0.36916190-0.12305397 | disen 0.73272330-0.24424110 | temporal 0.66919750-0.22306583 | total 0.59036094\n",
      "Epoch 5, loss: 0.59036094-(best 0.58593166)\n",
      "\n",
      "Detailed Loss: recon 0.36377460-0.12125820 | disen 0.72520936-0.24173645 | temporal 0.66982925-0.22327642 | total 0.58627111\n",
      "Epoch 6, loss: 0.58627111-(best 0.58593166)\n",
      "\n",
      "Detailed Loss: recon 0.35562909-0.11854303 | disen 0.72656643-0.24218881 | temporal 0.67010075-0.22336692 | total 0.58409876\n",
      "Epoch 7, loss: 0.58409876-(best 0.58409876)\n",
      "\n",
      "Detailed Loss: recon 0.33402070-0.11134023 | disen 0.72002161-0.24000720 | temporal 0.66970706-0.22323569 | total 0.57458317\n",
      "Epoch 8, loss: 0.57458317-(best 0.57458317)\n",
      "\n",
      "Detailed Loss: recon 0.32000101-0.10666700 | disen 0.71991622-0.23997207 | temporal 0.66901124-0.22300375 | total 0.56964284\n",
      "Epoch 9, loss: 0.56964284-(best 0.56964284)\n",
      "\n",
      "Detailed Loss: recon 0.30078727-0.10026242 | disen 0.71684742-0.23894914 | temporal 0.66804761-0.22268254 | total 0.56189412\n",
      "Epoch 10, loss: 0.56189412-(best 0.56189412)\n",
      "\n",
      "Detailed Loss: recon 0.29263261-0.09754420 | disen 0.71390682-0.23796894 | temporal 0.66724437-0.22241479 | total 0.55792797\n",
      "Epoch 11, loss: 0.55792797-(best 0.55792797)\n",
      "\n",
      "Detailed Loss: recon 0.29119831-0.09706610 | disen 0.71102571-0.23700857 | temporal 0.66655117-0.22218372 | total 0.55625844\n",
      "Epoch 12, loss: 0.55625844-(best 0.55625844)\n",
      "\n",
      "Detailed Loss: recon 0.29393876-0.09797959 | disen 0.71211624-0.23737208 | temporal 0.66618508-0.22206169 | total 0.55741334\n",
      "Epoch 13, loss: 0.55741334-(best 0.55625844)\n",
      "\n",
      "Detailed Loss: recon 0.29958344-0.09986115 | disen 0.70104563-0.23368188 | temporal 0.66619736-0.22206579 | total 0.55560881\n",
      "Epoch 14, loss: 0.55560881-(best 0.55560881)\n",
      "\n",
      "Detailed Loss: recon 0.28581294-0.09527098 | disen 0.70223415-0.23407805 | temporal 0.66643649-0.22214550 | total 0.55149454\n",
      "Epoch 15, loss: 0.55149454-(best 0.55149454)\n",
      "\n",
      "Detailed Loss: recon 0.27621463-0.09207154 | disen 0.69656652-0.23218884 | temporal 0.66697609-0.22232536 | total 0.54658580\n",
      "Epoch 16, loss: 0.54658580-(best 0.54658580)\n",
      "\n",
      "Detailed Loss: recon 0.26850218-0.08950073 | disen 0.68802470-0.22934157 | temporal 0.66759664-0.22253221 | total 0.54137450\n",
      "Epoch 17, loss: 0.54137450-(best 0.54137450)\n",
      "\n",
      "Detailed Loss: recon 0.26348245-0.08782748 | disen 0.69735575-0.23245192 | temporal 0.66821533-0.22273844 | total 0.54301786\n",
      "Epoch 18, loss: 0.54301786-(best 0.54137450)\n",
      "\n",
      "Detailed Loss: recon 0.26745892-0.08915297 | disen 0.68924671-0.22974890 | temporal 0.66870338-0.22290113 | total 0.54180300\n",
      "Epoch 19, loss: 0.54180300-(best 0.54137450)\n",
      "\n",
      "Detailed Loss: recon 0.25933391-0.08644464 | disen 0.67845106-0.22615035 | temporal 0.66892469-0.22297490 | total 0.53556991\n",
      "Epoch 20, loss: 0.53556991-(best 0.53556991)\n",
      "\n",
      "Detailed Loss: recon 0.25805983-0.08601994 | disen 0.68218338-0.22739446 | temporal 0.66926587-0.22308862 | total 0.53650302\n",
      "Epoch 21, loss: 0.53650302-(best 0.53556991)\n",
      "\n",
      "Detailed Loss: recon 0.25552753-0.08517584 | disen 0.67772686-0.22590895 | temporal 0.66949016-0.22316339 | total 0.53424823\n",
      "Epoch 22, loss: 0.53424823-(best 0.53424823)\n",
      "\n",
      "Detailed Loss: recon 0.25046450-0.08348817 | disen 0.66721052-0.22240351 | temporal 0.66961968-0.22320656 | total 0.52909827\n",
      "Epoch 23, loss: 0.52909827-(best 0.52909827)\n",
      "\n",
      "Detailed Loss: recon 0.24753140-0.08251047 | disen 0.66781998-0.22260666 | temporal 0.66994411-0.22331470 | total 0.52843183\n",
      "Epoch 24, loss: 0.52843183-(best 0.52843183)\n",
      "\n",
      "Detailed Loss: recon 0.25300282-0.08433427 | disen 0.64654773-0.21551591 | temporal 0.66961998-0.22320666 | total 0.52305686\n",
      "Epoch 25, loss: 0.52305686-(best 0.52305686)\n",
      "\n",
      "Detailed Loss: recon 0.24679804-0.08226601 | disen 0.65607524-0.21869175 | temporal 0.66970652-0.22323551 | total 0.52419329\n",
      "Epoch 26, loss: 0.52419329-(best 0.52305686)\n",
      "\n",
      "Detailed Loss: recon 0.24273595-0.08091198 | disen 0.65004569-0.21668190 | temporal 0.67010015-0.22336672 | total 0.52096063\n",
      "Epoch 27, loss: 0.52096063-(best 0.52096063)\n",
      "\n",
      "Detailed Loss: recon 0.24410307-0.08136769 | disen 0.64447278-0.21482426 | temporal 0.66979516-0.22326505 | total 0.51945698\n",
      "Epoch 28, loss: 0.51945698-(best 0.51945698)\n",
      "\n",
      "Detailed Loss: recon 0.24385676-0.08128559 | disen 0.63445932-0.21148644 | temporal 0.66991889-0.22330630 | total 0.51607835\n",
      "Epoch 29, loss: 0.51607835-(best 0.51607835)\n",
      "\n",
      "Detailed Loss: recon 0.23665623-0.07888541 | disen 0.64263469-0.21421156 | temporal 0.67048460-0.22349487 | total 0.51659185\n",
      "Epoch 30, loss: 0.51659185-(best 0.51607835)\n",
      "\n",
      "Detailed Loss: recon 0.23644774-0.07881591 | disen 0.63874424-0.21291475 | temporal 0.67065221-0.22355074 | total 0.51528144\n",
      "Epoch 31, loss: 0.51528144-(best 0.51528144)\n",
      "\n",
      "Detailed Loss: recon 0.24063675-0.08021225 | disen 0.61728656-0.20576219 | temporal 0.67036313-0.22345438 | total 0.50942886\n",
      "Epoch 32, loss: 0.50942886-(best 0.50942886)\n",
      "\n",
      "Detailed Loss: recon 0.23613881-0.07871294 | disen 0.63263237-0.21087746 | temporal 0.67036653-0.22345551 | total 0.51304591\n",
      "Epoch 33, loss: 0.51304591-(best 0.50942886)\n",
      "\n",
      "Detailed Loss: recon 0.23262706-0.07754235 | disen 0.61134541-0.20378180 | temporal 0.67092979-0.22364326 | total 0.50496745\n",
      "Epoch 34, loss: 0.50496745-(best 0.50496745)\n",
      "\n",
      "Detailed Loss: recon 0.23508888-0.07836296 | disen 0.61260104-0.20420035 | temporal 0.67102033-0.22367344 | total 0.50623679\n",
      "Epoch 35, loss: 0.50623679-(best 0.50496745)\n",
      "\n",
      "Detailed Loss: recon 0.23630242-0.07876747 | disen 0.60299349-0.20099783 | temporal 0.67085522-0.22361841 | total 0.50338376\n",
      "Epoch 36, loss: 0.50338376-(best 0.50338376)\n",
      "\n",
      "Detailed Loss: recon 0.23331468-0.07777156 | disen 0.60481137-0.20160379 | temporal 0.67115718-0.22371906 | total 0.50309443\n",
      "Epoch 37, loss: 0.50309443-(best 0.50309443)\n",
      "\n",
      "Detailed Loss: recon 0.22754526-0.07584842 | disen 0.59036827-0.19678942 | temporal 0.67160320-0.22386773 | total 0.49650559\n",
      "Epoch 38, loss: 0.49650559-(best 0.49650559)\n",
      "\n",
      "Detailed Loss: recon 0.23011856-0.07670619 | disen 0.60474205-0.20158068 | temporal 0.67161816-0.22387272 | total 0.50215960\n",
      "Epoch 39, loss: 0.50215960-(best 0.49650559)\n",
      "\n",
      "Detailed Loss: recon 0.23183420-0.07727807 | disen 0.58981526-0.19660509 | temporal 0.67151123-0.22383708 | total 0.49772024\n",
      "Epoch 40, loss: 0.49772024-(best 0.49650559)\n",
      "\n",
      "Detailed Loss: recon 0.23144323-0.07714774 | disen 0.57240105-0.19080035 | temporal 0.67167228-0.22389076 | total 0.49183887\n",
      "Epoch 41, loss: 0.49183887-(best 0.49183887)\n",
      "\n",
      "Detailed Loss: recon 0.23034608-0.07678203 | disen 0.57657468-0.19219156 | temporal 0.67188686-0.22396229 | total 0.49293590\n",
      "Epoch 42, loss: 0.49293590-(best 0.49183887)\n",
      "\n",
      "Detailed Loss: recon 0.23349571-0.07783190 | disen 0.57727450-0.19242483 | temporal 0.67181712-0.22393904 | total 0.49419582\n",
      "Epoch 43, loss: 0.49419582-(best 0.49183887)\n",
      "\n",
      "Detailed Loss: recon 0.23453277-0.07817759 | disen 0.57407975-0.19135992 | temporal 0.67187285-0.22395762 | total 0.49349514\n",
      "Epoch 44, loss: 0.49349514-(best 0.49183887)\n",
      "\n",
      "Detailed Loss: recon 0.23097943-0.07699314 | disen 0.56093377-0.18697792 | temporal 0.67184597-0.22394866 | total 0.48791972\n",
      "Epoch 45, loss: 0.48791972-(best 0.48791972)\n",
      "\n",
      "Detailed Loss: recon 0.23253995-0.07751332 | disen 0.55112910-0.18370970 | temporal 0.67185807-0.22395269 | total 0.48517573\n",
      "Epoch 46, loss: 0.48517573-(best 0.48517573)\n",
      "\n",
      "Detailed Loss: recon 0.23341027-0.07780342 | disen 0.55416501-0.18472167 | temporal 0.67167592-0.22389197 | total 0.48641708\n",
      "Epoch 47, loss: 0.48641708-(best 0.48517573)\n",
      "\n",
      "Detailed Loss: recon 0.23028043-0.07676014 | disen 0.55349886-0.18449962 | temporal 0.67138451-0.22379484 | total 0.48505461\n",
      "Epoch 48, loss: 0.48505461-(best 0.48505461)\n",
      "\n",
      "Detailed Loss: recon 0.23012815-0.07670938 | disen 0.55106223-0.18368741 | temporal 0.67148107-0.22382702 | total 0.48422384\n",
      "Epoch 49, loss: 0.48422384-(best 0.48422384)\n",
      "\n",
      "Detailed Loss: recon 0.22409475-0.07469825 | disen 0.55297410-0.18432470 | temporal 0.67172450-0.22390817 | total 0.48293114\n",
      "Epoch 50, loss: 0.48293114-(best 0.48293114)\n",
      "\n",
      "Detailed Loss: recon 0.22660337-0.07553446 | disen 0.54585850-0.18195283 | temporal 0.67157793-0.22385931 | total 0.48134661\n",
      "Epoch 51, loss: 0.48134661-(best 0.48134661)\n",
      "\n",
      "Detailed Loss: recon 0.23094589-0.07698196 | disen 0.52241009-0.17413670 | temporal 0.67112410-0.22370803 | total 0.47482669\n",
      "Epoch 52, loss: 0.47482669-(best 0.47482669)\n",
      "\n",
      "Detailed Loss: recon 0.22807273-0.07602424 | disen 0.53361595-0.17787198 | temporal 0.67153096-0.22384365 | total 0.47773990\n",
      "Epoch 53, loss: 0.47773990-(best 0.47482669)\n",
      "\n",
      "Detailed Loss: recon 0.23328140-0.07776047 | disen 0.52829313-0.17609771 | temporal 0.67132407-0.22377469 | total 0.47763291\n",
      "Epoch 54, loss: 0.47763291-(best 0.47482669)\n",
      "\n",
      "Detailed Loss: recon 0.23390441-0.07796814 | disen 0.52463377-0.17487792 | temporal 0.67098784-0.22366261 | total 0.47650868\n",
      "Epoch 55, loss: 0.47650868-(best 0.47482669)\n",
      "\n",
      "Detailed Loss: recon 0.23280025-0.07760008 | disen 0.52115953-0.17371984 | temporal 0.67125779-0.22375260 | total 0.47507256\n",
      "Epoch 56, loss: 0.47507256-(best 0.47482669)\n",
      "\n",
      "Detailed Loss: recon 0.23769021-0.07923007 | disen 0.51807797-0.17269266 | temporal 0.67055446-0.22351815 | total 0.47544089\n",
      "Epoch 57, loss: 0.47544089-(best 0.47482669)\n",
      "\n",
      "Detailed Loss: recon 0.23437099-0.07812366 | disen 0.51257718-0.17085906 | temporal 0.67045420-0.22348473 | total 0.47246748\n",
      "Epoch 58, loss: 0.47246748-(best 0.47246748)\n",
      "\n",
      "Detailed Loss: recon 0.23583876-0.07861292 | disen 0.51029819-0.17009940 | temporal 0.67084306-0.22361435 | total 0.47232670\n",
      "Epoch 59, loss: 0.47232670-(best 0.47232670)\n",
      "\n",
      "Detailed Loss: recon 0.22892201-0.07630734 | disen 0.51033902-0.17011301 | temporal 0.67072779-0.22357593 | total 0.46999627\n",
      "Epoch 60, loss: 0.46999627-(best 0.46999627)\n",
      "\n",
      "Detailed Loss: recon 0.22764575-0.07588192 | disen 0.51814842-0.17271614 | temporal 0.67063779-0.22354593 | total 0.47214401\n",
      "Epoch 61, loss: 0.47214401-(best 0.46999627)\n",
      "\n",
      "Detailed Loss: recon 0.23056071-0.07685357 | disen 0.49816680-0.16605560 | temporal 0.66971201-0.22323734 | total 0.46614653\n",
      "Epoch 62, loss: 0.46614653-(best 0.46614653)\n",
      "\n",
      "Detailed Loss: recon 0.23226890-0.07742297 | disen 0.50077838-0.16692613 | temporal 0.67003703-0.22334568 | total 0.46769476\n",
      "Epoch 63, loss: 0.46769476-(best 0.46614653)\n",
      "\n",
      "Detailed Loss: recon 0.23149155-0.07716385 | disen 0.50354761-0.16784920 | temporal 0.67106563-0.22368854 | total 0.46870160\n",
      "Epoch 64, loss: 0.46870160-(best 0.46614653)\n",
      "\n",
      "Detailed Loss: recon 0.23253100-0.07751033 | disen 0.48940343-0.16313448 | temporal 0.67011100-0.22337033 | total 0.46401516\n",
      "Epoch 65, loss: 0.46401516-(best 0.46401516)\n",
      "\n",
      "Detailed Loss: recon 0.24177149-0.08059050 | disen 0.48642397-0.16214132 | temporal 0.66917485-0.22305828 | total 0.46579009\n",
      "Epoch 66, loss: 0.46579009-(best 0.46401516)\n",
      "\n",
      "Detailed Loss: recon 0.23666100-0.07888700 | disen 0.49270409-0.16423470 | temporal 0.67001081-0.22333694 | total 0.46645862\n",
      "Epoch 67, loss: 0.46645862-(best 0.46401516)\n",
      "\n",
      "Detailed Loss: recon 0.23045203-0.07681734 | disen 0.49174088-0.16391363 | temporal 0.67067039-0.22355680 | total 0.46428779\n",
      "Epoch 68, loss: 0.46428779-(best 0.46401516)\n",
      "\n",
      "Detailed Loss: recon 0.23835590-0.07945197 | disen 0.47695786-0.15898595 | temporal 0.66936123-0.22312041 | total 0.46155834\n",
      "Epoch 69, loss: 0.46155834-(best 0.46155834)\n",
      "\n",
      "Detailed Loss: recon 0.24005896-0.08001965 | disen 0.48038828-0.16012943 | temporal 0.66900891-0.22300297 | total 0.46315205\n",
      "Epoch 70, loss: 0.46315205-(best 0.46155834)\n",
      "\n",
      "Detailed Loss: recon 0.23060100-0.07686700 | disen 0.48713225-0.16237742 | temporal 0.66999668-0.22333223 | total 0.46257663\n",
      "Epoch 71, loss: 0.46257663-(best 0.46155834)\n",
      "\n",
      "Detailed Loss: recon 0.23106478-0.07702159 | disen 0.48897195-0.16299065 | temporal 0.67009211-0.22336404 | total 0.46337628\n",
      "Epoch 72, loss: 0.46337628-(best 0.46155834)\n",
      "\n",
      "Detailed Loss: recon 0.23331699-0.07777233 | disen 0.47672784-0.15890928 | temporal 0.66931558-0.22310519 | total 0.45978683\n",
      "Epoch 73, loss: 0.45978683-(best 0.45978683)\n",
      "\n",
      "Detailed Loss: recon 0.23324531-0.07774844 | disen 0.46551526-0.15517175 | temporal 0.66911280-0.22303760 | total 0.45595780\n",
      "Epoch 74, loss: 0.45595780-(best 0.45595780)\n",
      "\n",
      "Detailed Loss: recon 0.23132741-0.07710914 | disen 0.47423738-0.15807913 | temporal 0.66945684-0.22315228 | total 0.45834056\n",
      "Epoch 75, loss: 0.45834056-(best 0.45595780)\n",
      "\n",
      "Detailed Loss: recon 0.23061523-0.07687174 | disen 0.47473884-0.15824628 | temporal 0.66902834-0.22300945 | total 0.45812750\n",
      "Epoch 76, loss: 0.45812750-(best 0.45595780)\n",
      "\n",
      "Detailed Loss: recon 0.23146288-0.07715429 | disen 0.46079981-0.15359994 | temporal 0.66873312-0.22291104 | total 0.45366529\n",
      "Epoch 77, loss: 0.45366529-(best 0.45366529)\n",
      "\n",
      "Detailed Loss: recon 0.23898381-0.07966127 | disen 0.46105134-0.15368378 | temporal 0.66814053-0.22271351 | total 0.45605856\n",
      "Epoch 78, loss: 0.45605856-(best 0.45366529)\n",
      "\n",
      "Detailed Loss: recon 0.23203713-0.07734571 | disen 0.46360397-0.15453466 | temporal 0.66913456-0.22304485 | total 0.45492524\n",
      "Epoch 79, loss: 0.45492524-(best 0.45366529)\n",
      "\n",
      "Detailed Loss: recon 0.23032080-0.07677360 | disen 0.46243495-0.15414498 | temporal 0.66909546-0.22303182 | total 0.45395041\n",
      "Epoch 80, loss: 0.45395041-(best 0.45366529)\n",
      "\n",
      "Detailed Loss: recon 0.24813323-0.08271108 | disen 0.44730508-0.14910169 | temporal 0.66706222-0.22235407 | total 0.45416686\n",
      "Epoch 81, loss: 0.45416686-(best 0.45366529)\n",
      "\n",
      "Detailed Loss: recon 0.23351023-0.07783674 | disen 0.45014453-0.15004818 | temporal 0.66808116-0.22269372 | total 0.45057863\n",
      "Epoch 82, loss: 0.45057863-(best 0.45057863)\n",
      "\n",
      "Detailed Loss: recon 0.22935623-0.07645208 | disen 0.46409440-0.15469813 | temporal 0.66851318-0.22283773 | total 0.45398796\n",
      "Epoch 83, loss: 0.45398796-(best 0.45057863)\n",
      "\n",
      "Detailed Loss: recon 0.23822765-0.07940922 | disen 0.44212687-0.14737562 | temporal 0.66625369-0.22208456 | total 0.44886941\n",
      "Epoch 84, loss: 0.44886941-(best 0.44886941)\n",
      "\n",
      "Detailed Loss: recon 0.24080762-0.08026921 | disen 0.43940538-0.14646846 | temporal 0.66675270-0.22225090 | total 0.44898859\n",
      "Epoch 85, loss: 0.44898859-(best 0.44886941)\n",
      "\n",
      "Detailed Loss: recon 0.23484761-0.07828254 | disen 0.45579386-0.15193129 | temporal 0.66803867-0.22267956 | total 0.45289338\n",
      "Epoch 86, loss: 0.45289338-(best 0.44886941)\n",
      "\n",
      "Detailed Loss: recon 0.23554687-0.07851562 | disen 0.44992137-0.14997379 | temporal 0.66761190-0.22253730 | total 0.45102674\n",
      "Epoch 87, loss: 0.45102674-(best 0.44886941)\n",
      "\n",
      "Detailed Loss: recon 0.23888861-0.07962954 | disen 0.43783277-0.14594426 | temporal 0.66683233-0.22227744 | total 0.44785124\n",
      "Epoch 88, loss: 0.44785124-(best 0.44785124)\n",
      "\n",
      "Detailed Loss: recon 0.23231596-0.07743865 | disen 0.44481349-0.14827116 | temporal 0.66667891-0.22222630 | total 0.44793612\n",
      "Epoch 89, loss: 0.44793612-(best 0.44785124)\n",
      "\n",
      "Detailed Loss: recon 0.23386829-0.07795610 | disen 0.44251531-0.14750510 | temporal 0.66632330-0.22210777 | total 0.44756895\n",
      "Epoch 90, loss: 0.44756895-(best 0.44756895)\n",
      "\n",
      "Detailed Loss: recon 0.24015781-0.08005260 | disen 0.44025731-0.14675244 | temporal 0.66607183-0.22202394 | total 0.44882900\n",
      "Epoch 91, loss: 0.44882900-(best 0.44756895)\n",
      "\n",
      "Detailed Loss: recon 0.22857188-0.07619063 | disen 0.43911511-0.14637170 | temporal 0.66693223-0.22231074 | total 0.44487309\n",
      "Epoch 92, loss: 0.44487309-(best 0.44487309)\n",
      "\n",
      "Detailed Loss: recon 0.23334520-0.07778173 | disen 0.43954247-0.14651416 | temporal 0.66688621-0.22229540 | total 0.44659132\n",
      "Epoch 93, loss: 0.44659132-(best 0.44487309)\n",
      "\n",
      "Detailed Loss: recon 0.23627220-0.07875740 | disen 0.43838120-0.14612707 | temporal 0.66645288-0.22215096 | total 0.44703543\n",
      "Epoch 94, loss: 0.44703543-(best 0.44487309)\n",
      "\n",
      "Detailed Loss: recon 0.23802833-0.07934278 | disen 0.44364840-0.14788280 | temporal 0.66603827-0.22201276 | total 0.44923836\n",
      "Epoch 95, loss: 0.44923836-(best 0.44487309)\n",
      "\n",
      "Detailed Loss: recon 0.23639803-0.07879934 | disen 0.43312371-0.14437457 | temporal 0.66545594-0.22181865 | total 0.44499257\n",
      "Epoch 96, loss: 0.44499257-(best 0.44487309)\n",
      "\n",
      "Detailed Loss: recon 0.23841438-0.07947146 | disen 0.44088501-0.14696167 | temporal 0.66583240-0.22194413 | total 0.44837725\n",
      "Epoch 97, loss: 0.44837725-(best 0.44487309)\n",
      "\n",
      "Detailed Loss: recon 0.23328714-0.07776238 | disen 0.43503231-0.14501077 | temporal 0.66574818-0.22191606 | total 0.44468921\n",
      "Epoch 98, loss: 0.44468921-(best 0.44468921)\n",
      "\n",
      "Detailed Loss: recon 0.23019318-0.07673106 | disen 0.43210673-0.14403558 | temporal 0.66528159-0.22176053 | total 0.44252717\n",
      "Epoch 99, loss: 0.44252717-(best 0.44252717)\n",
      "\n",
      "Detailed Loss: recon 0.23436534-0.07812178 | disen 0.44066864-0.14688955 | temporal 0.66595155-0.22198385 | total 0.44699520\n",
      "Epoch 100, loss: 0.44699520-(best 0.44252717)\n",
      "\n",
      ">> Reached final epoch. Loading best model...\n",
      "TRAINING XGB PREDICTOR\n",
      ">> Ending training!\n",
      "\n",
      "-----------------------\n",
      "TIME COST: 7.292841672897339 s\n",
      "Best Val: REC 55.79 PRE 48.09 MF1 68.28 AUC 80.65 TP 289 FP 312 TN 1522 FN 229\n",
      "-----------------------\n",
      "\n",
      "ASSIGNED GRAPH FOR EVALUATION: 11758 {1: 2566, 0: 9192} Nodes, 1049758 Edges\n",
      "\n",
      "-------------------\n",
      "STARTING EVALUATION\n",
      "-------------------\n",
      "\n",
      "Dataset - Overall: REC 55.26 PRE 47.13 MF1 67.80 AUC 80.33 TP 1418 FP 1591 TN 7601 FN 1148 | 11758 {1: 2566, 0: 9192}\n",
      "Dataset - Train: REC 60.18 PRE 51.09 MF1 70.59 AUC 83.75 TP 467 FP 447 TN 2304 FN 309 | 3527 {0: 2751, 1: 776}\n",
      "Dataset - Val: REC 55.21 PRE 48.07 MF1 68.16 AUC 80.54 TP 286 FP 309 TN 1525 FN 232 | 2352 {0: 1834, 1: 518}\n",
      "Dataset - Test: REC 52.28 PRE 44.33 MF1 65.97 AUC 78.24 TP 665 FP 835 TN 3772 FN 607 | 5879 {1: 1272, 0: 4607}\n",
      "PREDICTION STATUS - {'1-True': 1959, '0-False': 3772, '0-True': 5420, '1-False': 607}\n",
      "    >> 607 positive nodes left unpredicted...\n",
      "    >> 607 OG positive nodes left unpredicted...\n",
      "Selecting data as budgeted ground truth for next round...\n",
      "  > Starting round 1...\n",
      "\n",
      "+++++++++++++++++\n",
      "STARTING ROUND 1!\n",
      "+++++++++++++++++\n",
      "        \n",
      "ASSIGNED GRAPH FOR TRAINING: 11758 {1: 2566, 0: 9192} Nodes, 1049758 Edges\n",
      "\n",
      ">> Splitting to train-validation set...\n",
      "Alotting train-val-test split for round 1\n",
      "Initial pool: 5879, Prediction pool: 3009, Budget pool: 0, Full pool: 7379\n",
      "Full graph size: 11758, Training: 4427, Val: 2952, Test: 11758\n",
      "    >> INITIAL DATA SPLIT: 4427 train rows ({1: 1175, 0: 3252}) | 2952 val rows ({0: 2168, 1: 784}) | 4379 test rows ({0: 3772, 1: 607})\n",
      "    >> AUGMENTED DATA SPLIT: 4427 train rows ({1: 1175, 0: 3252}) | 2952 val rows ({0: 2168, 1: 784}) | 4379 test rows ({0: 3772, 1: 607})\n",
      "    >> Updated cross-entropy weight to 2.767659574468085...\n",
      "\n",
      "-----------------------\n",
      "STARTING MODEL TRAININGps_l\n",
      "-----------------------\n",
      "\n",
      "TRAINING NODE EMBEDDER\n",
      "Detailed Loss: recon 0.21854655-0.07284885 | disen 0.35710090-0.11903363 | temporal 0.56206846-0.18735615 | total 0.37923867\n",
      "Epoch 1, loss: 0.37923867-(best 0.37923867)\n",
      "\n",
      "Detailed Loss: recon 0.42693424-0.14231141 | disen 0.47048581-0.15682860 | temporal 0.56633198-0.18877733 | total 0.48791736\n",
      "Epoch 2, loss: 0.48791736-(best 0.37923867)\n",
      "\n",
      "Detailed Loss: recon 0.32945520-0.10981840 | disen 0.51697838-0.17232613 | temporal 0.57092643-0.19030881 | total 0.47245336\n",
      "Epoch 3, loss: 0.47245336-(best 0.37923867)\n",
      "\n",
      "Detailed Loss: recon 0.30033571-0.10011190 | disen 0.51510417-0.17170139 | temporal 0.57069385-0.19023128 | total 0.46204460\n",
      "Epoch 4, loss: 0.46204460-(best 0.37923867)\n",
      "\n",
      "Detailed Loss: recon 0.27234215-0.09078072 | disen 0.47987908-0.15995969 | temporal 0.56953591-0.18984530 | total 0.44058573\n",
      "Epoch 5, loss: 0.44058573-(best 0.37923867)\n",
      "\n",
      "Detailed Loss: recon 0.28835523-0.09611841 | disen 0.43957198-0.14652399 | temporal 0.56766635-0.18922212 | total 0.43186453\n",
      "Epoch 6, loss: 0.43186453-(best 0.37923867)\n",
      "\n",
      "Detailed Loss: recon 0.31382853-0.10460951 | disen 0.41019940-0.13673313 | temporal 0.56632876-0.18877625 | total 0.43011892\n",
      "Epoch 7, loss: 0.43011892-(best 0.37923867)\n",
      "\n",
      "Detailed Loss: recon 0.28141218-0.09380406 | disen 0.40262669-0.13420890 | temporal 0.56623709-0.18874570 | total 0.41675866\n",
      "Epoch 8, loss: 0.41675866-(best 0.37923867)\n",
      "\n",
      "Detailed Loss: recon 0.24091244-0.08030415 | disen 0.40395027-0.13465009 | temporal 0.56688666-0.18896222 | total 0.40391648\n",
      "Epoch 9, loss: 0.40391648-(best 0.37923867)\n",
      "\n",
      "Detailed Loss: recon 0.25238937-0.08412979 | disen 0.41366643-0.13788881 | temporal 0.56735986-0.18911995 | total 0.41113859\n",
      "Epoch 10, loss: 0.41113859-(best 0.37923867)\n",
      "\n",
      "Detailed Loss: recon 0.23709154-0.07903051 | disen 0.41442585-0.13814195 | temporal 0.56732064-0.18910688 | total 0.40627936\n",
      "Epoch 11, loss: 0.40627936-(best 0.37923867)\n",
      "\n",
      "Detailed Loss: recon 0.21797538-0.07265846 | disen 0.41144127-0.13714709 | temporal 0.56710762-0.18903587 | total 0.39884144\n",
      "Epoch 12, loss: 0.39884144-(best 0.37923867)\n",
      "\n",
      "Detailed Loss: recon 0.21930379-0.07310126 | disen 0.40044129-0.13348043 | temporal 0.56678766-0.18892922 | total 0.39551091\n",
      "Epoch 13, loss: 0.39551091-(best 0.37923867)\n",
      "\n",
      "Detailed Loss: recon 0.22004437-0.07334812 | disen 0.39981478-0.13327159 | temporal 0.56659174-0.18886391 | total 0.39548367\n",
      "Epoch 14, loss: 0.39548367-(best 0.37923867)\n",
      "\n",
      "Detailed Loss: recon 0.22757857-0.07585952 | disen 0.39403713-0.13134571 | temporal 0.56637549-0.18879183 | total 0.39599708\n",
      "Epoch 15, loss: 0.39599708-(best 0.37923867)\n",
      "\n",
      "Detailed Loss: recon 0.22090654-0.07363551 | disen 0.38892418-0.12964139 | temporal 0.56620586-0.18873529 | total 0.39201221\n",
      "Epoch 16, loss: 0.39201221-(best 0.37923867)\n",
      "\n",
      "Detailed Loss: recon 0.21619384-0.07206461 | disen 0.38935465-0.12978488 | temporal 0.56619394-0.18873131 | total 0.39058080\n",
      "Epoch 17, loss: 0.39058080-(best 0.37923867)\n",
      "\n",
      "Detailed Loss: recon 0.21253017-0.07084339 | disen 0.38896090-0.12965363 | temporal 0.56618750-0.18872917 | total 0.38922620\n",
      "Epoch 18, loss: 0.38922620-(best 0.37923867)\n",
      "\n",
      "Detailed Loss: recon 0.21284863-0.07094954 | disen 0.38331264-0.12777088 | temporal 0.56603962-0.18867987 | total 0.38740030\n",
      "Epoch 19, loss: 0.38740030-(best 0.37923867)\n",
      "\n",
      "Detailed Loss: recon 0.21071689-0.07023896 | disen 0.38118351-0.12706117 | temporal 0.56601626-0.18867209 | total 0.38597223\n",
      "Epoch 20, loss: 0.38597223-(best 0.37923867)\n",
      "\n",
      "Detailed Loss: recon 0.21435653-0.07145218 | disen 0.37496984-0.12498995 | temporal 0.56589943-0.18863314 | total 0.38507527\n",
      "Epoch 21, loss: 0.38507527-(best 0.37923867)\n",
      "\n",
      "Detailed Loss: recon 0.21075308-0.07025103 | disen 0.37332296-0.12444099 | temporal 0.56588846-0.18862949 | total 0.38332152\n",
      "Epoch 22, loss: 0.38332152-(best 0.37923867)\n",
      "\n",
      "Detailed Loss: recon 0.21400869-0.07133623 | disen 0.36623561-0.12207854 | temporal 0.56573141-0.18857714 | total 0.38199192\n",
      "Epoch 23, loss: 0.38199192-(best 0.37923867)\n",
      "\n",
      "Detailed Loss: recon 0.20996454-0.06998818 | disen 0.36221778-0.12073926 | temporal 0.56560957-0.18853652 | total 0.37926397\n",
      "Epoch 24, loss: 0.37926397-(best 0.37923867)\n",
      "\n",
      "Detailed Loss: recon 0.20856863-0.06952288 | disen 0.36216879-0.12072293 | temporal 0.56550056-0.18850019 | total 0.37874600\n",
      "Epoch 25, loss: 0.37874600-(best 0.37874600)\n",
      "\n",
      "Detailed Loss: recon 0.20844392-0.06948131 | disen 0.35629261-0.11876420 | temporal 0.56528550-0.18842850 | total 0.37667403\n",
      "Epoch 26, loss: 0.37667403-(best 0.37667403)\n",
      "\n",
      "Detailed Loss: recon 0.20980164-0.06993388 | disen 0.35327739-0.11775913 | temporal 0.56516999-0.18839000 | total 0.37608302\n",
      "Epoch 27, loss: 0.37608302-(best 0.37608302)\n",
      "\n",
      "Detailed Loss: recon 0.20651734-0.06883911 | disen 0.34940130-0.11646710 | temporal 0.56509715-0.18836572 | total 0.37367195\n",
      "Epoch 28, loss: 0.37367195-(best 0.37367195)\n",
      "\n",
      "Detailed Loss: recon 0.20548579-0.06849526 | disen 0.35104990-0.11701663 | temporal 0.56522137-0.18840712 | total 0.37391901\n",
      "Epoch 29, loss: 0.37391901-(best 0.37367195)\n",
      "\n",
      "Detailed Loss: recon 0.20284647-0.06761549 | disen 0.34988624-0.11662875 | temporal 0.56527311-0.18842437 | total 0.37266862\n",
      "Epoch 30, loss: 0.37266862-(best 0.37266862)\n",
      "\n",
      "Detailed Loss: recon 0.20053469-0.06684490 | disen 0.34534574-0.11511525 | temporal 0.56525803-0.18841934 | total 0.37037948\n",
      "Epoch 31, loss: 0.37037948-(best 0.37037948)\n",
      "\n",
      "Detailed Loss: recon 0.20157814-0.06719271 | disen 0.34025866-0.11341955 | temporal 0.56516778-0.18838926 | total 0.36900154\n",
      "Epoch 32, loss: 0.36900154-(best 0.36900154)\n",
      "\n",
      "Detailed Loss: recon 0.20291990-0.06763997 | disen 0.33627886-0.11209295 | temporal 0.56509238-0.18836413 | total 0.36809707\n",
      "Epoch 33, loss: 0.36809707-(best 0.36809707)\n",
      "\n",
      "Detailed Loss: recon 0.20251559-0.06750520 | disen 0.33527237-0.11175746 | temporal 0.56505734-0.18835245 | total 0.36761510\n",
      "Epoch 34, loss: 0.36761510-(best 0.36761510)\n",
      "\n",
      "Detailed Loss: recon 0.20137493-0.06712498 | disen 0.33328402-0.11109467 | temporal 0.56496096-0.18832032 | total 0.36653996\n",
      "Epoch 35, loss: 0.36653996-(best 0.36653996)\n",
      "\n",
      "Detailed Loss: recon 0.19943240-0.06647747 | disen 0.33219212-0.11073071 | temporal 0.56487590-0.18829197 | total 0.36550015\n",
      "Epoch 36, loss: 0.36550015-(best 0.36550015)\n",
      "\n",
      "Detailed Loss: recon 0.20174217-0.06724739 | disen 0.32791358-0.10930453 | temporal 0.56461889-0.18820630 | total 0.36475822\n",
      "Epoch 37, loss: 0.36475822-(best 0.36475822)\n",
      "\n",
      "Detailed Loss: recon 0.20018911-0.06672970 | disen 0.32477140-0.10825713 | temporal 0.56443477-0.18814492 | total 0.36313176\n",
      "Epoch 38, loss: 0.36313176-(best 0.36313176)\n",
      "\n",
      "Detailed Loss: recon 0.19909075-0.06636358 | disen 0.32446700-0.10815567 | temporal 0.56444663-0.18814888 | total 0.36266816\n",
      "Epoch 39, loss: 0.36266816-(best 0.36266816)\n",
      "\n",
      "Detailed Loss: recon 0.19988456-0.06662819 | disen 0.32237333-0.10745778 | temporal 0.56443954-0.18814651 | total 0.36223251\n",
      "Epoch 40, loss: 0.36223251-(best 0.36223251)\n",
      "\n",
      "Detailed Loss: recon 0.20278411-0.06759470 | disen 0.32079834-0.10693278 | temporal 0.56436908-0.18812303 | total 0.36265051\n",
      "Epoch 41, loss: 0.36265051-(best 0.36223251)\n",
      "\n",
      "Detailed Loss: recon 0.20224375-0.06741458 | disen 0.31602061-0.10534020 | temporal 0.56408691-0.18802897 | total 0.36078376\n",
      "Epoch 42, loss: 0.36078376-(best 0.36078376)\n",
      "\n",
      "Detailed Loss: recon 0.20399925-0.06799975 | disen 0.31450629-0.10483543 | temporal 0.56408489-0.18802830 | total 0.36086351\n",
      "Epoch 43, loss: 0.36086351-(best 0.36078376)\n",
      "\n",
      "Detailed Loss: recon 0.20402047-0.06800682 | disen 0.31294501-0.10431500 | temporal 0.56394917-0.18798306 | total 0.36030489\n",
      "Epoch 44, loss: 0.36030489-(best 0.36030489)\n",
      "\n",
      "Detailed Loss: recon 0.20252024-0.06750675 | disen 0.31177646-0.10392549 | temporal 0.56391639-0.18797213 | total 0.35940436\n",
      "Epoch 45, loss: 0.35940436-(best 0.35940436)\n",
      "\n",
      "Detailed Loss: recon 0.20327950-0.06775983 | disen 0.30884469-0.10294823 | temporal 0.56376094-0.18792031 | total 0.35862839\n",
      "Epoch 46, loss: 0.35862839-(best 0.35862839)\n",
      "\n",
      "Detailed Loss: recon 0.19966725-0.06655575 | disen 0.30913955-0.10304652 | temporal 0.56379396-0.18793132 | total 0.35753360\n",
      "Epoch 47, loss: 0.35753360-(best 0.35753360)\n",
      "\n",
      "Detailed Loss: recon 0.19723722-0.06574574 | disen 0.30937290-0.10312430 | temporal 0.56376845-0.18792282 | total 0.35679287\n",
      "Epoch 48, loss: 0.35679287-(best 0.35679287)\n",
      "\n",
      "Detailed Loss: recon 0.19729339-0.06576446 | disen 0.30777627-0.10259209 | temporal 0.56362242-0.18787414 | total 0.35623071\n",
      "Epoch 49, loss: 0.35623071-(best 0.35623071)\n",
      "\n",
      "Detailed Loss: recon 0.19863008-0.06621003 | disen 0.30582303-0.10194101 | temporal 0.56354153-0.18784718 | total 0.35599822\n",
      "Epoch 50, loss: 0.35599822-(best 0.35599822)\n",
      "\n",
      ">> Reached final epoch. Loading best model...\n",
      "TRAINING XGB PREDICTOR\n",
      ">> Ending training!\n",
      "\n",
      "-----------------------\n",
      "TIME COST: 5.609437942504883 s\n",
      "Best Val: REC 61.48 PRE 48.10 MF1 67.01 AUC 77.55 TP 482 FP 520 TN 1648 FN 302\n",
      "-----------------------\n",
      "Generating additional positive data from adversary...\n",
      "Generating additional negative data by duplicating random nodes...\n",
      "\n",
      "ASSIGNED GRAPH FOR EVALUATION: 12345 {1: 2694, 0: 9651} Nodes, 1153086 Edges\n",
      "\n",
      "-------------------\n",
      "STARTING EVALUATION\n",
      "-------------------\n",
      "\n",
      "Dataset - Overall: REC 55.57 PRE 52.80 MF1 70.46 AUC 82.61 TP 1497 FP 1338 TN 8313 FN 1197 | 12345 {1: 2694, 0: 9651}\n",
      "Dataset - Train: REC 72.00 PRE 64.68 MF1 77.86 AUC 89.20 TP 846 FP 462 TN 2790 FN 329 | 4427 {1: 1175, 0: 3252}\n",
      "Dataset - Val: REC 53.44 PRE 48.61 MF1 65.97 AUC 76.28 TP 419 FP 443 TN 1725 FN 365 | 2952 {0: 2168, 1: 784}\n",
      "Dataset - Test: REC 31.56 PRE 34.89 MF1 61.09 AUC 76.97 TP 232 FP 433 TN 3798 FN 503 | 4966 {0: 4231, 1: 735}\n",
      "PREDICTION STATUS - {'1-True': 2191, '0-False': 3798, '0-True': 5853, '1-False': 503}\n",
      "    >> 503 positive nodes left unpredicted...\n",
      "    >> 409 OG positive nodes left unpredicted...\n",
      "-----------------------\n",
      "PREDICTION RESULT - ROUNDS\n",
      "Dataset - Round 0: REC 55.27 PRE 48.89 MF1 68.73 AUC 80.78 TP 703 FP 735 TN 3872 FN 569 | 5879 {1: 1272, 0: 4607}\n",
      "Dataset - Round 1: REC 26.56 PRE 40.48 MF1 58.55 AUC 73.81 TP 34 FP 50 TN 409 FN 94 | 587 {1: 128, 0: 459}\n",
      "-----------------------\n",
      "PREDICTION RESULT - SEEDS\n",
      "Seeds - Current: REC 28.12 PRE 37.50 MF1 58.07 AUC 71.90 TP 36 FP 60 TN 399 FN 92 | 587 {1: 128, 0: 459}\n",
      "Seeds - Prev: REC 0.00 PRE 0.00 MF1 39.61 AUC 71.90 TP 0 FP 74 TN 385 FN 128 | 587 {1: 128, 0: 459}\n",
      "Selecting data as budgeted ground truth for next round...\n",
      "  > Starting round 2...\n",
      "\n",
      "+++++++++++++++++\n",
      "STARTING ROUND 2!\n",
      "+++++++++++++++++\n",
      "        \n",
      "ASSIGNED GRAPH FOR TRAINING: 12345 {1: 2694, 0: 9651} Nodes, 1153086 Edges\n",
      "\n",
      ">> Splitting to train-validation set...\n",
      "Alotting train-val-test split for round 2\n",
      "Initial pool: 5879, Prediction pool: 5844, Budget pool: 0, Full pool: 8044\n",
      "Full graph size: 12345, Training: 4826, Val: 3218, Test: 12345\n",
      "    >> INITIAL DATA SPLIT: 4826 train rows ({1: 1314, 0: 3512}) | 3218 val rows ({0: 2341, 1: 877}) | 4301 test rows ({0: 3798, 1: 503})\n",
      "    >> AUGMENTED DATA SPLIT: 4826 train rows ({1: 1314, 0: 3512}) | 3218 val rows ({0: 2341, 1: 877}) | 4301 test rows ({0: 3798, 1: 503})\n",
      "    >> Updated cross-entropy weight to 2.6727549467275495...\n",
      "\n",
      "-----------------------\n",
      "STARTING MODEL TRAININGps_l\n",
      "-----------------------\n",
      "\n",
      "TRAINING NODE EMBEDDER\n",
      "Detailed Loss: recon 0.20176691-0.06725564 | disen 0.35417801-0.11805934 | temporal 0.58751863-0.19583954 | total 0.38115454\n",
      "Epoch 1, loss: 0.38115454-(best 0.38115454)\n",
      "\n",
      "Detailed Loss: recon 0.24290016-0.08096672 | disen 0.36785293-0.12261764 | temporal 0.58858633-0.19619544 | total 0.39977983\n",
      "Epoch 2, loss: 0.39977983-(best 0.38115454)\n",
      "\n",
      "Detailed Loss: recon 0.21502057-0.07167352 | disen 0.36144572-0.12048191 | temporal 0.58824342-0.19608114 | total 0.38823658\n",
      "Epoch 3, loss: 0.38823658-(best 0.38115454)\n",
      "\n",
      "Detailed Loss: recon 0.24094996-0.08031665 | disen 0.35693395-0.11897798 | temporal 0.58720022-0.19573341 | total 0.39502805\n",
      "Epoch 4, loss: 0.39502805-(best 0.38115454)\n",
      "\n",
      "Detailed Loss: recon 0.20437740-0.06812580 | disen 0.37179857-0.12393286 | temporal 0.58792591-0.19597530 | total 0.38803396\n",
      "Epoch 5, loss: 0.38803396-(best 0.38115454)\n",
      "\n",
      "Detailed Loss: recon 0.19135123-0.06378374 | disen 0.37584513-0.12528171 | temporal 0.58845514-0.19615171 | total 0.38521719\n",
      "Epoch 6, loss: 0.38521719-(best 0.38115454)\n",
      "\n",
      "Detailed Loss: recon 0.19679753-0.06559918 | disen 0.37488759-0.12496253 | temporal 0.58875901-0.19625300 | total 0.38681471\n",
      "Epoch 7, loss: 0.38681471-(best 0.38115454)\n",
      "\n",
      "Detailed Loss: recon 0.20051762-0.06683921 | disen 0.37080348-0.12360116 | temporal 0.58868319-0.19622773 | total 0.38666809\n",
      "Epoch 8, loss: 0.38666809-(best 0.38115454)\n",
      "\n",
      "Detailed Loss: recon 0.20257884-0.06752628 | disen 0.36451513-0.12150504 | temporal 0.58833849-0.19611283 | total 0.38514417\n",
      "Epoch 9, loss: 0.38514417-(best 0.38115454)\n",
      "\n",
      "Detailed Loss: recon 0.20202558-0.06734186 | disen 0.35725039-0.11908346 | temporal 0.58805323-0.19601774 | total 0.38244307\n",
      "Epoch 10, loss: 0.38244307-(best 0.38115454)\n",
      "\n",
      "Detailed Loss: recon 0.19573233-0.06524411 | disen 0.35771221-0.11923740 | temporal 0.58796203-0.19598734 | total 0.38046885\n",
      "Epoch 11, loss: 0.38046885-(best 0.38046885)\n",
      "\n",
      "Detailed Loss: recon 0.19545239-0.06515080 | disen 0.35381681-0.11793894 | temporal 0.58771193-0.19590398 | total 0.37899372\n",
      "Epoch 12, loss: 0.37899372-(best 0.37899372)\n",
      "\n",
      "Detailed Loss: recon 0.19773303-0.06591101 | disen 0.35336816-0.11778939 | temporal 0.58780915-0.19593638 | total 0.37963676\n",
      "Epoch 13, loss: 0.37963676-(best 0.37899372)\n",
      "\n",
      "Detailed Loss: recon 0.19514643-0.06504881 | disen 0.35241383-0.11747128 | temporal 0.58781976-0.19593992 | total 0.37846002\n",
      "Epoch 14, loss: 0.37846002-(best 0.37846002)\n",
      "\n",
      "Detailed Loss: recon 0.19803467-0.06601156 | disen 0.34913599-0.11637866 | temporal 0.58770758-0.19590253 | total 0.37829274\n",
      "Epoch 15, loss: 0.37829274-(best 0.37829274)\n",
      "\n",
      "Detailed Loss: recon 0.19786775-0.06595592 | disen 0.34583962-0.11527987 | temporal 0.58757442-0.19585814 | total 0.37709394\n",
      "Epoch 16, loss: 0.37709394-(best 0.37709394)\n",
      "\n",
      "Detailed Loss: recon 0.19576108-0.06525369 | disen 0.34486705-0.11495568 | temporal 0.58749455-0.19583152 | total 0.37604091\n",
      "Epoch 17, loss: 0.37604091-(best 0.37604091)\n",
      "\n",
      "Detailed Loss: recon 0.19717626-0.06572542 | disen 0.34323728-0.11441243 | temporal 0.58741277-0.19580426 | total 0.37594211\n",
      "Epoch 18, loss: 0.37594211-(best 0.37594211)\n",
      "\n",
      "Detailed Loss: recon 0.19869789-0.06623263 | disen 0.33573115-0.11191038 | temporal 0.58723855-0.19574618 | total 0.37388921\n",
      "Epoch 19, loss: 0.37388921-(best 0.37388921)\n",
      "\n",
      "Detailed Loss: recon 0.19934064-0.06644688 | disen 0.33332938-0.11110979 | temporal 0.58709508-0.19569836 | total 0.37325504\n",
      "Epoch 20, loss: 0.37325504-(best 0.37325504)\n",
      "\n",
      "Detailed Loss: recon 0.19950554-0.06650185 | disen 0.33347285-0.11115762 | temporal 0.58705395-0.19568465 | total 0.37334412\n",
      "Epoch 21, loss: 0.37334412-(best 0.37325504)\n",
      "\n",
      "Detailed Loss: recon 0.19509554-0.06503185 | disen 0.33318514-0.11106171 | temporal 0.58697313-0.19565771 | total 0.37175128\n",
      "Epoch 22, loss: 0.37175128-(best 0.37175128)\n",
      "\n",
      "Detailed Loss: recon 0.19461915-0.06487305 | disen 0.33135426-0.11045142 | temporal 0.58687860-0.19562620 | total 0.37095067\n",
      "Epoch 23, loss: 0.37095067-(best 0.37095067)\n",
      "\n",
      "Detailed Loss: recon 0.19577181-0.06525727 | disen 0.33048421-0.11016140 | temporal 0.58680987-0.19560329 | total 0.37102199\n",
      "Epoch 24, loss: 0.37102199-(best 0.37095067)\n",
      "\n",
      "Detailed Loss: recon 0.19500485-0.06500162 | disen 0.32897323-0.10965774 | temporal 0.58678806-0.19559602 | total 0.37025541\n",
      "Epoch 25, loss: 0.37025541-(best 0.37025541)\n",
      "\n",
      "Detailed Loss: recon 0.19507656-0.06502552 | disen 0.32588309-0.10862770 | temporal 0.58682668-0.19560889 | total 0.36926210\n",
      "Epoch 26, loss: 0.36926210-(best 0.36926210)\n",
      "\n",
      "Detailed Loss: recon 0.19345036-0.06448345 | disen 0.32745951-0.10915317 | temporal 0.58693659-0.19564553 | total 0.36928219\n",
      "Epoch 27, loss: 0.36928219-(best 0.36926210)\n",
      "\n",
      "Detailed Loss: recon 0.19199365-0.06399788 | disen 0.32347554-0.10782518 | temporal 0.58688599-0.19562866 | total 0.36745173\n",
      "Epoch 28, loss: 0.36745173-(best 0.36745173)\n",
      "\n",
      "Detailed Loss: recon 0.19557002-0.06519001 | disen 0.31948549-0.10649516 | temporal 0.58665222-0.19555074 | total 0.36723590\n",
      "Epoch 29, loss: 0.36723590-(best 0.36723590)\n",
      "\n",
      "Detailed Loss: recon 0.19677912-0.06559304 | disen 0.31981039-0.10660346 | temporal 0.58677065-0.19559022 | total 0.36778674\n",
      "Epoch 30, loss: 0.36778674-(best 0.36723590)\n",
      "\n",
      "Detailed Loss: recon 0.19347166-0.06449055 | disen 0.32046247-0.10682082 | temporal 0.58675379-0.19558460 | total 0.36689597\n",
      "Epoch 31, loss: 0.36689597-(best 0.36689597)\n",
      "\n",
      "Detailed Loss: recon 0.19174752-0.06391584 | disen 0.32009304-0.10669768 | temporal 0.58674216-0.19558072 | total 0.36619425\n",
      "Epoch 32, loss: 0.36619425-(best 0.36619425)\n",
      "\n",
      "Detailed Loss: recon 0.19405238-0.06468413 | disen 0.31796819-0.10598940 | temporal 0.58674324-0.19558108 | total 0.36625460\n",
      "Epoch 33, loss: 0.36625460-(best 0.36619425)\n",
      "\n",
      "Detailed Loss: recon 0.19429336-0.06476445 | disen 0.31552529-0.10517510 | temporal 0.58671802-0.19557267 | total 0.36551222\n",
      "Epoch 34, loss: 0.36551222-(best 0.36551222)\n",
      "\n",
      "Detailed Loss: recon 0.19383389-0.06461130 | disen 0.31391084-0.10463695 | temporal 0.58667630-0.19555877 | total 0.36480701\n",
      "Epoch 35, loss: 0.36480701-(best 0.36480701)\n",
      "\n",
      "Detailed Loss: recon 0.19132251-0.06377417 | disen 0.31524050-0.10508017 | temporal 0.58679211-0.19559737 | total 0.36445171\n",
      "Epoch 36, loss: 0.36445171-(best 0.36445171)\n",
      "\n",
      "Detailed Loss: recon 0.18943742-0.06314581 | disen 0.31394619-0.10464873 | temporal 0.58680141-0.19560047 | total 0.36339504\n",
      "Epoch 37, loss: 0.36339504-(best 0.36339504)\n",
      "\n",
      "Detailed Loss: recon 0.18994938-0.06331646 | disen 0.31511563-0.10503854 | temporal 0.58683753-0.19561251 | total 0.36396754\n",
      "Epoch 38, loss: 0.36396754-(best 0.36339504)\n",
      "\n",
      "Detailed Loss: recon 0.18929666-0.06309889 | disen 0.31119949-0.10373316 | temporal 0.58675933-0.19558644 | total 0.36241850\n",
      "Epoch 39, loss: 0.36241850-(best 0.36241850)\n",
      "\n",
      "Detailed Loss: recon 0.19066757-0.06355586 | disen 0.31016934-0.10338978 | temporal 0.58671212-0.19557071 | total 0.36251634\n",
      "Epoch 40, loss: 0.36251634-(best 0.36241850)\n",
      "\n",
      "Detailed Loss: recon 0.19018160-0.06339387 | disen 0.31044871-0.10348290 | temporal 0.58676225-0.19558742 | total 0.36246419\n",
      "Epoch 41, loss: 0.36246419-(best 0.36241850)\n",
      "\n",
      "Detailed Loss: recon 0.19309568-0.06436523 | disen 0.30762720-0.10254240 | temporal 0.58658528-0.19552843 | total 0.36243606\n",
      "Epoch 42, loss: 0.36243606-(best 0.36241850)\n",
      "\n",
      "Detailed Loss: recon 0.19093928-0.06364643 | disen 0.30658537-0.10219512 | temporal 0.58638394-0.19546131 | total 0.36130285\n",
      "Epoch 43, loss: 0.36130285-(best 0.36130285)\n",
      "\n",
      "Detailed Loss: recon 0.18942605-0.06314202 | disen 0.30629587-0.10209862 | temporal 0.58641630-0.19547210 | total 0.36071277\n",
      "Epoch 44, loss: 0.36071277-(best 0.36071277)\n",
      "\n",
      "Detailed Loss: recon 0.19104844-0.06368281 | disen 0.30556601-0.10185534 | temporal 0.58654195-0.19551398 | total 0.36105216\n",
      "Epoch 45, loss: 0.36105216-(best 0.36071277)\n",
      "\n",
      "Detailed Loss: recon 0.19060898-0.06353633 | disen 0.30587590-0.10195863 | temporal 0.58655155-0.19551718 | total 0.36101216\n",
      "Epoch 46, loss: 0.36101216-(best 0.36071277)\n",
      "\n",
      "Detailed Loss: recon 0.18813205-0.06271068 | disen 0.30410033-0.10136678 | temporal 0.58644730-0.19548243 | total 0.35955989\n",
      "Epoch 47, loss: 0.35955989-(best 0.35955989)\n",
      "\n",
      "Detailed Loss: recon 0.19162288-0.06387429 | disen 0.30318254-0.10106085 | temporal 0.58660424-0.19553475 | total 0.36046988\n",
      "Epoch 48, loss: 0.36046988-(best 0.35955989)\n",
      "\n",
      "Detailed Loss: recon 0.18989085-0.06329695 | disen 0.30464602-0.10154867 | temporal 0.58669639-0.19556546 | total 0.36041108\n",
      "Epoch 49, loss: 0.36041108-(best 0.35955989)\n",
      "\n",
      "Detailed Loss: recon 0.18733691-0.06244564 | disen 0.30366093-0.10122031 | temporal 0.58664417-0.19554806 | total 0.35921401\n",
      "Epoch 50, loss: 0.35921401-(best 0.35921401)\n",
      "\n",
      ">> Reached final epoch. Loading best model...\n",
      "TRAINING XGB PREDICTOR\n",
      ">> Ending training!\n",
      "\n",
      "-----------------------\n",
      "TIME COST: 5.82973575592041 s\n",
      "Best Val: REC 49.49 PRE 51.36 MF1 66.15 AUC 76.55 TP 434 FP 411 TN 1930 FN 443\n",
      "-----------------------\n",
      "Generating additional positive data from adversary...\n",
      "Generating additional negative data by duplicating random nodes...\n",
      "\n",
      "ASSIGNED GRAPH FOR EVALUATION: 12932 {1: 2822, 0: 10110} Nodes, 1248078 Edges\n",
      "\n",
      "-------------------\n",
      "STARTING EVALUATION\n",
      "-------------------\n",
      "\n",
      "Dataset - Overall: REC 61.73 PRE 48.56 MF1 69.66 AUC 80.50 TP 1742 FP 1845 TN 8265 FN 1080 | 12932 {1: 2822, 0: 10110}\n",
      "Dataset - Train: REC 76.26 PRE 58.19 MF1 75.20 AUC 85.68 TP 1002 FP 720 TN 2792 FN 312 | 4826 {1: 1314, 0: 3512}\n",
      "Dataset - Val: REC 61.23 PRE 46.17 MF1 65.34 AUC 74.82 TP 537 FP 626 TN 1715 FN 340 | 3218 {0: 2341, 1: 877}\n",
      "Dataset - Test: REC 32.17 PRE 28.92 MF1 59.74 AUC 73.56 TP 203 FP 499 TN 3758 FN 428 | 4888 {0: 4257, 1: 631}\n",
      "PREDICTION STATUS - {'1-True': 2394, '0-False': 3758, '0-True': 6352, '1-False': 428}\n",
      "    >> 428 positive nodes left unpredicted...\n",
      "    >> 280 OG positive nodes left unpredicted...\n",
      "-----------------------\n",
      "PREDICTION RESULT - ROUNDS\n",
      "Dataset - Round 0: REC 62.11 PRE 47.22 MF1 69.08 AUC 80.04 TP 790 FP 883 TN 3724 FN 482 | 5879 {1: 1272, 0: 4607}\n",
      "Dataset - Round 1: REC 44.53 PRE 42.86 MF1 63.79 AUC 71.91 TP 57 FP 76 TN 383 FN 71 | 587 {1: 128, 0: 459}\n",
      "Dataset - Round 2: REC 35.16 PRE 36.59 MF1 59.21 AUC 68.18 TP 45 FP 78 TN 381 FN 83 | 587 {1: 128, 0: 459}\n",
      "-----------------------\n",
      "PREDICTION RESULT - SEEDS\n",
      "Seeds - Current: REC 36.72 PRE 36.43 MF1 59.40 AUC 67.86 TP 47 FP 82 TN 377 FN 81 | 587 {1: 128, 0: 459}\n",
      "Seeds - Prev: REC 0.00 PRE 0.00 MF1 39.92 AUC 67.86 TP 0 FP 69 TN 390 FN 128 | 587 {1: 128, 0: 459}\n",
      "Selecting data as budgeted ground truth for next round...\n",
      "The file ../checkpoint/working_model_file_1730091816.4589097_epoch.py does not exist\n",
      "The file ../checkpoint/working_model_file_1730091816.4589097_round-M.json does not exist\n",
      "The file ../checkpoint/working_model_file_1730091816.4589097_round-T.json does not exist\n",
      "The file ../checkpoint/working_model_file_1730091816.4589097_round.pt does not exist\n",
      "================\n",
      "++++++++++++++++\n",
      "TRIAL NUMBER 1\n",
      "++++++++++++++++\n",
      "================\n",
      "  > Rereading graph data...\n",
      "  > Initializing multiround object...\n",
      "11758 1049758\n",
      "\n",
      "=========================\n",
      "STARTING ADVER EXPERIMENT\n",
      "=========================\n",
      "MAIN CONFIG: [('device', 'cuda:0'), ('exp_type', 'ADVER'), ('full_oracle', False), ('node_expiration_time', 1), ('round_budget_neg', 0), ('round_budget_pos', 0), ('round_new_neg', 459), ('round_new_pos', 128), ('round_num', 3), ('save_df', True), ('save_embedding', False), ('save_temp_sne', False), ('setting_type', 'INDUCTIVE'), ('task_type', 'NODE'), ('verbose', 4)]\n",
      "MODEL CONFIG: [('act_name', 'LeakyReLU'), ('addon_internal_agg', 'OR'), ('addon_name', 'NONE'), ('addon_perc', 0.05), ('addon_round_window', 7), ('alpha', 1), ('att_heads', 2), ('beta', 1), ('boost_agg_backbone', <class 'models.benchmarks_supervised.booster.GIN_noparam'>), ('boost_metric', <function average_precision_score at 0x000001C8520A5760>), ('boost_predictor', <class 'xgboost.sklearn.XGBClassifier'>), ('device', 'cuda:0'), ('drop_rate', 0), ('dropout_rate', 0.1), ('early_stopping', 75), ('embed_type', 'temporal'), ('etypes', ['none']), ('gamma', 1), ('h_feats', 64), ('k', 1), ('lambda1', 1), ('lambda2', 1), ('lambda_', 1), ('loss_sample', True), ('loss_sample_ratio', 0.1), ('loss_type', 'ndist'), ('mlp_feats', 64), ('mlp_layers', 2), ('model_name', 'XGB-SP'), ('n_estimator', 500), ('norm_name', 'layer'), ('num_epoch', 100), ('num_layers', 2), ('num_round_epoch', 50), ('round_window', 7), ('temporal_agg', 'mean_final'), ('tloss_type', 'normal'), ('training_type', 'round'), ('verbose', 4)]\n",
      "STRAT CONFIG: [('augment_name', 'REAGE'), ('augment_neg_ratio', 0.09772070079945569), ('augment_pos_ratio', 0.02727929920054431), ('augment_prob', 0.5), ('augment_round_split', 8), ('device', 'cuda:0'), ('pseudo_name', 'NONE'), ('pseudo_new_pos_only', False), ('pseudo_skip_initial', False), ('verbose', 4)]\n",
      "TRAIN CONFIG: [('ce_weight', 2.6727549467275495), ('device', 'cuda:0'), ('dset_name', 'tolokers_bid'), ('early_stopping', 75), ('learning_rate', 0.01), ('loss', <function cross_entropy at 0x000001C84AF6F880>), ('loss_gamma', 5), ('num_epoch', 100), ('num_round_epoch', 50), ('optimizer', <class 'torch.optim.adam.Adam'>), ('random_state', 7777777), ('ratio_initial', 0.5), ('ratio_test', 0.2), ('ratio_train', 0.6), ('ratio_val', 0.2), ('round_reset_model', False), ('round_train_list', []), ('stuck_stopping', 50), ('stuck_threshold', 0), ('train_max_retry', 10), ('verbose', 4)]\n",
      "ADVER CONFIG: [('adver_choose_name', 'GREEDY'), ('adver_conn_coef', 0.1), ('adver_feat_coef', 1), ('adver_mod_name', 'REPLAY'), ('device', 'cuda:0'), ('verbose', 4)]\n",
      "\n",
      "INITIAL GRAPH: 11758 {1: 2566, 0: 9192} Nodes, 1049758 Edges\n",
      "=========================\n",
      "        \n",
      ">> Initializing graph...\n",
      "MAIN GRAPH: 11758 {1: 2566, 0: 9192} Nodes, 1049758 Edges\n",
      "\n",
      ">> Initializing experiment type-specific modules:\n",
      "  >> Initializing adversarial modules...\n",
      ">> Initializing task-specific modules:\n",
      ">> Initializing classifier model...\n",
      ">> Initializing other strategies...\n",
      "\n",
      "FINISH INITIALIZATION\n",
      "=====================\n",
      "        \n",
      "  > Starting round 0...\n",
      "\n",
      "+++++++++++++++++\n",
      "STARTING ROUND 0!\n",
      "+++++++++++++++++\n",
      "        \n",
      "ASSIGNED GRAPH FOR TRAINING: 5879 {0: 4617, 1: 1262} Nodes, 276633 Edges\n",
      "\n",
      ">> Splitting to train-validation set...\n",
      "Alotting train-val-test split for round 0\n",
      "Initial pool: 0, Prediction pool: 0, Budget pool: 0, Full pool: 5879\n",
      "Full graph size: 5879, Training: 3527, Val: 2352, Test: 5879\n",
      "    >> INITIAL DATA SPLIT: 3527 train rows ({0: 2770, 1: 757}) | 2352 val rows ({0: 1847, 1: 505}) | 0 test rows ({})\n",
      "    >> AUGMENTED DATA SPLIT: 3527 train rows ({0: 2770, 1: 757}) | 2352 val rows ({0: 1847, 1: 505}) | 0 test rows ({})\n",
      "    >> Updated cross-entropy weight to 3.6591809775429325...\n",
      "\n",
      "-----------------------\n",
      "STARTING MODEL TRAININGps_l\n",
      "-----------------------\n",
      "\n",
      "TRAINING NODE EMBEDDER\n",
      "Detailed Loss: recon 0.78419799-0.26139933 | disen 0.70550942-0.23516981 | temporal 0.64477301-0.21492434 | total 0.71149349\n",
      "Epoch 1, loss: 0.71149349-(best 0.71149349)\n",
      "\n",
      "Detailed Loss: recon 0.47934109-0.15978036 | disen 0.72152334-0.24050778 | temporal 0.65918899-0.21972966 | total 0.62001783\n",
      "Epoch 2, loss: 0.62001783-(best 0.62001783)\n",
      "\n",
      "Detailed Loss: recon 0.41445744-0.13815248 | disen 0.72637224-0.24212408 | temporal 0.66392511-0.22130837 | total 0.60158491\n",
      "Epoch 3, loss: 0.60158491-(best 0.60158491)\n",
      "\n",
      "Detailed Loss: recon 0.41140684-0.13713561 | disen 0.72276187-0.24092062 | temporal 0.66555673-0.22185224 | total 0.59990847\n",
      "Epoch 4, loss: 0.59990847-(best 0.59990847)\n",
      "\n",
      "Detailed Loss: recon 0.39212665-0.13070888 | disen 0.72147858-0.24049286 | temporal 0.66713238-0.22237746 | total 0.59357923\n",
      "Epoch 5, loss: 0.59357923-(best 0.59357923)\n",
      "\n",
      "Detailed Loss: recon 0.37762687-0.12587562 | disen 0.72009146-0.24003049 | temporal 0.66704273-0.22234758 | total 0.58825374\n",
      "Epoch 6, loss: 0.58825374-(best 0.58825374)\n",
      "\n",
      "Detailed Loss: recon 0.36413142-0.12137714 | disen 0.71635449-0.23878483 | temporal 0.66703463-0.22234488 | total 0.58250684\n",
      "Epoch 7, loss: 0.58250684-(best 0.58250684)\n",
      "\n",
      "Detailed Loss: recon 0.35285330-0.11761777 | disen 0.71251321-0.23750440 | temporal 0.66590965-0.22196988 | total 0.57709205\n",
      "Epoch 8, loss: 0.57709205-(best 0.57709205)\n",
      "\n",
      "Detailed Loss: recon 0.35467932-0.11822644 | disen 0.70924342-0.23641447 | temporal 0.66536069-0.22178690 | total 0.57642782\n",
      "Epoch 9, loss: 0.57642782-(best 0.57642782)\n",
      "\n",
      "Detailed Loss: recon 0.35016081-0.11672027 | disen 0.70049465-0.23349822 | temporal 0.66500157-0.22166719 | total 0.57188570\n",
      "Epoch 10, loss: 0.57188570-(best 0.57188570)\n",
      "\n",
      "Detailed Loss: recon 0.34563217-0.11521072 | disen 0.70239335-0.23413112 | temporal 0.66519850-0.22173283 | total 0.57107472\n",
      "Epoch 11, loss: 0.57107472-(best 0.57107472)\n",
      "\n",
      "Detailed Loss: recon 0.33412093-0.11137364 | disen 0.69536024-0.23178675 | temporal 0.66641772-0.22213924 | total 0.56529963\n",
      "Epoch 12, loss: 0.56529963-(best 0.56529963)\n",
      "\n",
      "Detailed Loss: recon 0.33012578-0.11004193 | disen 0.69275779-0.23091926 | temporal 0.66706157-0.22235386 | total 0.56331509\n",
      "Epoch 13, loss: 0.56331509-(best 0.56331509)\n",
      "\n",
      "Detailed Loss: recon 0.31885049-0.10628350 | disen 0.69496071-0.23165357 | temporal 0.66778123-0.22259374 | total 0.56053078\n",
      "Epoch 14, loss: 0.56053078-(best 0.56053078)\n",
      "\n",
      "Detailed Loss: recon 0.30714735-0.10238245 | disen 0.68762541-0.22920847 | temporal 0.66823560-0.22274520 | total 0.55433613\n",
      "Epoch 15, loss: 0.55433613-(best 0.55433613)\n",
      "\n",
      "Detailed Loss: recon 0.30774111-0.10258037 | disen 0.67506295-0.22502098 | temporal 0.66851634-0.22283878 | total 0.55044019\n",
      "Epoch 16, loss: 0.55044019-(best 0.55044019)\n",
      "\n",
      "Detailed Loss: recon 0.30618531-0.10206177 | disen 0.67626023-0.22542008 | temporal 0.66879892-0.22293297 | total 0.55041486\n",
      "Epoch 17, loss: 0.55041486-(best 0.55041486)\n",
      "\n",
      "Detailed Loss: recon 0.30260810-0.10086937 | disen 0.67170620-0.22390207 | temporal 0.66896188-0.22298729 | total 0.54775876\n",
      "Epoch 18, loss: 0.54775876-(best 0.54775876)\n",
      "\n",
      "Detailed Loss: recon 0.29992974-0.09997658 | disen 0.66942888-0.22314296 | temporal 0.66943896-0.22314632 | total 0.54626584\n",
      "Epoch 19, loss: 0.54626584-(best 0.54626584)\n",
      "\n",
      "Detailed Loss: recon 0.29563475-0.09854492 | disen 0.66636264-0.22212088 | temporal 0.66981018-0.22327006 | total 0.54393589\n",
      "Epoch 20, loss: 0.54393589-(best 0.54393589)\n",
      "\n",
      "Detailed Loss: recon 0.29409003-0.09803001 | disen 0.65104932-0.21701644 | temporal 0.67027164-0.22342388 | total 0.53847033\n",
      "Epoch 21, loss: 0.53847033-(best 0.53847033)\n",
      "\n",
      "Detailed Loss: recon 0.29532319-0.09844106 | disen 0.65603411-0.21867804 | temporal 0.67041087-0.22347029 | total 0.54058945\n",
      "Epoch 22, loss: 0.54058945-(best 0.53847033)\n",
      "\n",
      "Detailed Loss: recon 0.28705740-0.09568580 | disen 0.64688993-0.21562998 | temporal 0.67037803-0.22345934 | total 0.53477514\n",
      "Epoch 23, loss: 0.53477514-(best 0.53477514)\n",
      "\n",
      "Detailed Loss: recon 0.28472656-0.09490885 | disen 0.63453406-0.21151135 | temporal 0.67073810-0.22357937 | total 0.52999961\n",
      "Epoch 24, loss: 0.52999961-(best 0.52999961)\n",
      "\n",
      "Detailed Loss: recon 0.28731582-0.09577194 | disen 0.62695885-0.20898628 | temporal 0.67112935-0.22370978 | total 0.52846801\n",
      "Epoch 25, loss: 0.52846801-(best 0.52846801)\n",
      "\n",
      "Detailed Loss: recon 0.29104504-0.09701501 | disen 0.63529539-0.21176513 | temporal 0.67123008-0.22374336 | total 0.53252351\n",
      "Epoch 26, loss: 0.53252351-(best 0.52846801)\n",
      "\n",
      "Detailed Loss: recon 0.28483689-0.09494563 | disen 0.63092732-0.21030911 | temporal 0.67174923-0.22391641 | total 0.52917117\n",
      "Epoch 27, loss: 0.52917117-(best 0.52846801)\n",
      "\n",
      "Detailed Loss: recon 0.27981830-0.09327277 | disen 0.62909555-0.20969852 | temporal 0.67175728-0.22391909 | total 0.52689040\n",
      "Epoch 28, loss: 0.52689040-(best 0.52689040)\n",
      "\n",
      "Detailed Loss: recon 0.27872851-0.09290950 | disen 0.60932088-0.20310696 | temporal 0.67155069-0.22385023 | total 0.51986670\n",
      "Epoch 29, loss: 0.51986670-(best 0.51986670)\n",
      "\n",
      "Detailed Loss: recon 0.28212795-0.09404265 | disen 0.61164725-0.20388242 | temporal 0.67162180-0.22387393 | total 0.52179903\n",
      "Epoch 30, loss: 0.52179903-(best 0.51986670)\n",
      "\n",
      "Detailed Loss: recon 0.27722308-0.09240769 | disen 0.61967742-0.20655914 | temporal 0.67181265-0.22393755 | total 0.52290440\n",
      "Epoch 31, loss: 0.52290440-(best 0.51986670)\n",
      "\n",
      "Detailed Loss: recon 0.27605337-0.09201779 | disen 0.61140144-0.20380048 | temporal 0.67197573-0.22399191 | total 0.51981020\n",
      "Epoch 32, loss: 0.51981020-(best 0.51981020)\n",
      "\n",
      "Detailed Loss: recon 0.27400792-0.09133597 | disen 0.60960662-0.20320221 | temporal 0.67224872-0.22408291 | total 0.51862109\n",
      "Epoch 33, loss: 0.51862109-(best 0.51862109)\n",
      "\n",
      "Detailed Loss: recon 0.27132332-0.09044111 | disen 0.60653806-0.20217935 | temporal 0.67247194-0.22415731 | total 0.51677781\n",
      "Epoch 34, loss: 0.51677781-(best 0.51677781)\n",
      "\n",
      "Detailed Loss: recon 0.27174515-0.09058172 | disen 0.59886372-0.19962124 | temporal 0.67243618-0.22414539 | total 0.51434839\n",
      "Epoch 35, loss: 0.51434839-(best 0.51434839)\n",
      "\n",
      "Detailed Loss: recon 0.27274281-0.09091427 | disen 0.58166063-0.19388688 | temporal 0.67249727-0.22416576 | total 0.50896692\n",
      "Epoch 36, loss: 0.50896692-(best 0.50896692)\n",
      "\n",
      "Detailed Loss: recon 0.27297562-0.09099187 | disen 0.58316374-0.19438791 | temporal 0.67243856-0.22414619 | total 0.50952601\n",
      "Epoch 37, loss: 0.50952601-(best 0.50896692)\n",
      "\n",
      "Detailed Loss: recon 0.27691761-0.09230587 | disen 0.58914238-0.19638079 | temporal 0.67256927-0.22418976 | total 0.51287639\n",
      "Epoch 38, loss: 0.51287639-(best 0.50896692)\n",
      "\n",
      "Detailed Loss: recon 0.27010188-0.09003396 | disen 0.58034134-0.19344711 | temporal 0.67243069-0.22414356 | total 0.50762463\n",
      "Epoch 39, loss: 0.50762463-(best 0.50762463)\n",
      "\n",
      "Detailed Loss: recon 0.27296627-0.09098876 | disen 0.57922506-0.19307502 | temporal 0.67257971-0.22419324 | total 0.50825703\n",
      "Epoch 40, loss: 0.50825703-(best 0.50762463)\n",
      "\n",
      "Detailed Loss: recon 0.27199858-0.09066619 | disen 0.57444149-0.19148050 | temporal 0.67234927-0.22411642 | total 0.50626314\n",
      "Epoch 41, loss: 0.50626314-(best 0.50626314)\n",
      "\n",
      "Detailed Loss: recon 0.26935121-0.08978374 | disen 0.56612760-0.18870920 | temporal 0.67209685-0.22403228 | total 0.50252521\n",
      "Epoch 42, loss: 0.50252521-(best 0.50252521)\n",
      "\n",
      "Detailed Loss: recon 0.26867115-0.08955705 | disen 0.56597757-0.18865919 | temporal 0.67201865-0.22400622 | total 0.50222248\n",
      "Epoch 43, loss: 0.50222248-(best 0.50222248)\n",
      "\n",
      "Detailed Loss: recon 0.27041453-0.09013818 | disen 0.56268144-0.18756048 | temporal 0.67178720-0.22392907 | total 0.50162774\n",
      "Epoch 44, loss: 0.50162774-(best 0.50162774)\n",
      "\n",
      "Detailed Loss: recon 0.27216002-0.09072001 | disen 0.55895799-0.18631933 | temporal 0.67203987-0.22401329 | total 0.50105262\n",
      "Epoch 45, loss: 0.50105262-(best 0.50105262)\n",
      "\n",
      "Detailed Loss: recon 0.27452645-0.09150882 | disen 0.56015921-0.18671974 | temporal 0.67242086-0.22414029 | total 0.50236887\n",
      "Epoch 46, loss: 0.50236887-(best 0.50105262)\n",
      "\n",
      "Detailed Loss: recon 0.26850140-0.08950047 | disen 0.54857302-0.18285767 | temporal 0.67224145-0.22408048 | total 0.49643862\n",
      "Epoch 47, loss: 0.49643862-(best 0.49643862)\n",
      "\n",
      "Detailed Loss: recon 0.27382556-0.09127519 | disen 0.56223994-0.18741331 | temporal 0.67218077-0.22406026 | total 0.50274873\n",
      "Epoch 48, loss: 0.50274873-(best 0.49643862)\n",
      "\n",
      "Detailed Loss: recon 0.26921949-0.08973983 | disen 0.54542476-0.18180825 | temporal 0.67230648-0.22410216 | total 0.49565026\n",
      "Epoch 49, loss: 0.49565026-(best 0.49565026)\n",
      "\n",
      "Detailed Loss: recon 0.26874954-0.08958318 | disen 0.54482502-0.18160834 | temporal 0.67237842-0.22412614 | total 0.49531770\n",
      "Epoch 50, loss: 0.49531770-(best 0.49531770)\n",
      "\n",
      "Detailed Loss: recon 0.27068871-0.09022957 | disen 0.53892028-0.17964009 | temporal 0.67253572-0.22417857 | total 0.49404827\n",
      "Epoch 51, loss: 0.49404827-(best 0.49404827)\n",
      "\n",
      "Detailed Loss: recon 0.27168405-0.09056135 | disen 0.53539872-0.17846624 | temporal 0.67219961-0.22406654 | total 0.49309415\n",
      "Epoch 52, loss: 0.49309415-(best 0.49309415)\n",
      "\n",
      "Detailed Loss: recon 0.27039272-0.09013091 | disen 0.53872466-0.17957489 | temporal 0.67229009-0.22409670 | total 0.49380249\n",
      "Epoch 53, loss: 0.49380249-(best 0.49309415)\n",
      "\n",
      "Detailed Loss: recon 0.27183396-0.09061132 | disen 0.52507716-0.17502572 | temporal 0.67198348-0.22399449 | total 0.48963153\n",
      "Epoch 54, loss: 0.48963153-(best 0.48963153)\n",
      "\n",
      "Detailed Loss: recon 0.27268565-0.09089522 | disen 0.52732921-0.17577640 | temporal 0.67183983-0.22394661 | total 0.49061823\n",
      "Epoch 55, loss: 0.49061823-(best 0.48963153)\n",
      "\n",
      "Detailed Loss: recon 0.27048835-0.09016278 | disen 0.52811086-0.17603695 | temporal 0.67198879-0.22399626 | total 0.49019599\n",
      "Epoch 56, loss: 0.49019599-(best 0.48963153)\n",
      "\n",
      "Detailed Loss: recon 0.27245042-0.09081681 | disen 0.51724654-0.17241551 | temporal 0.67160141-0.22386714 | total 0.48709950\n",
      "Epoch 57, loss: 0.48709950-(best 0.48709950)\n",
      "\n",
      "Detailed Loss: recon 0.26923832-0.08974611 | disen 0.51889139-0.17296380 | temporal 0.67152762-0.22384254 | total 0.48655248\n",
      "Epoch 58, loss: 0.48655248-(best 0.48655248)\n",
      "\n",
      "Detailed Loss: recon 0.27264717-0.09088239 | disen 0.50180995-0.16726998 | temporal 0.67122298-0.22374099 | total 0.48189336\n",
      "Epoch 59, loss: 0.48189336-(best 0.48189336)\n",
      "\n",
      "Detailed Loss: recon 0.27515897-0.09171966 | disen 0.50681108-0.16893703 | temporal 0.67154813-0.22384938 | total 0.48450607\n",
      "Epoch 60, loss: 0.48450607-(best 0.48189336)\n",
      "\n",
      "Detailed Loss: recon 0.27806360-0.09268787 | disen 0.49416560-0.16472187 | temporal 0.67130953-0.22376984 | total 0.48117959\n",
      "Epoch 61, loss: 0.48117959-(best 0.48117959)\n",
      "\n",
      "Detailed Loss: recon 0.28012601-0.09337534 | disen 0.49636996-0.16545665 | temporal 0.67136335-0.22378778 | total 0.48261976\n",
      "Epoch 62, loss: 0.48261976-(best 0.48117959)\n",
      "\n",
      "Detailed Loss: recon 0.27461779-0.09153926 | disen 0.48858505-0.16286168 | temporal 0.67106014-0.22368671 | total 0.47808769\n",
      "Epoch 63, loss: 0.47808769-(best 0.47808769)\n",
      "\n",
      "Detailed Loss: recon 0.27167305-0.09055768 | disen 0.49628466-0.16542822 | temporal 0.67063373-0.22354458 | total 0.47953051\n",
      "Epoch 64, loss: 0.47953051-(best 0.47808769)\n",
      "\n",
      "Detailed Loss: recon 0.27506632-0.09168877 | disen 0.48694420-0.16231473 | temporal 0.67077470-0.22359157 | total 0.47759509\n",
      "Epoch 65, loss: 0.47759509-(best 0.47759509)\n",
      "\n",
      "Detailed Loss: recon 0.27582359-0.09194120 | disen 0.48710465-0.16236822 | temporal 0.67103225-0.22367742 | total 0.47798684\n",
      "Epoch 66, loss: 0.47798684-(best 0.47759509)\n",
      "\n",
      "Detailed Loss: recon 0.28234959-0.09411653 | disen 0.47761220-0.15920407 | temporal 0.67044735-0.22348245 | total 0.47680306\n",
      "Epoch 67, loss: 0.47680306-(best 0.47680306)\n",
      "\n",
      "Detailed Loss: recon 0.28188336-0.09396112 | disen 0.47904086-0.15968029 | temporal 0.66999859-0.22333286 | total 0.47697431\n",
      "Epoch 68, loss: 0.47697431-(best 0.47680306)\n",
      "\n",
      "Detailed Loss: recon 0.27855647-0.09285216 | disen 0.48020172-0.16006724 | temporal 0.67039609-0.22346536 | total 0.47638476\n",
      "Epoch 69, loss: 0.47638476-(best 0.47638476)\n",
      "\n",
      "Detailed Loss: recon 0.27662262-0.09220754 | disen 0.46829683-0.15609894 | temporal 0.66982293-0.22327431 | total 0.47158080\n",
      "Epoch 70, loss: 0.47158080-(best 0.47158080)\n",
      "\n",
      "Detailed Loss: recon 0.27984083-0.09328028 | disen 0.46410620-0.15470207 | temporal 0.66973162-0.22324387 | total 0.47122622\n",
      "Epoch 71, loss: 0.47122622-(best 0.47122622)\n",
      "\n",
      "Detailed Loss: recon 0.27810657-0.09270219 | disen 0.46337646-0.15445882 | temporal 0.66947955-0.22315985 | total 0.47032088\n",
      "Epoch 72, loss: 0.47032088-(best 0.47032088)\n",
      "\n",
      "Detailed Loss: recon 0.27767894-0.09255965 | disen 0.46688360-0.15562787 | temporal 0.66995680-0.22331893 | total 0.47150648\n",
      "Epoch 73, loss: 0.47150648-(best 0.47032088)\n",
      "\n",
      "Detailed Loss: recon 0.28115523-0.09371841 | disen 0.46690333-0.15563444 | temporal 0.66969657-0.22323219 | total 0.47258505\n",
      "Epoch 74, loss: 0.47258505-(best 0.47032088)\n",
      "\n",
      "Detailed Loss: recon 0.27977332-0.09325777 | disen 0.46825475-0.15608492 | temporal 0.66938609-0.22312870 | total 0.47247142\n",
      "Epoch 75, loss: 0.47247142-(best 0.47032088)\n",
      "\n",
      "Detailed Loss: recon 0.27670878-0.09223626 | disen 0.46472919-0.15490973 | temporal 0.66960245-0.22320082 | total 0.47034681\n",
      "Epoch 76, loss: 0.47034681-(best 0.47032088)\n",
      "\n",
      "Detailed Loss: recon 0.27573496-0.09191165 | disen 0.46946007-0.15648669 | temporal 0.66985965-0.22328655 | total 0.47168490\n",
      "Epoch 77, loss: 0.47168490-(best 0.47032088)\n",
      "\n",
      "Detailed Loss: recon 0.27808225-0.09269408 | disen 0.45802921-0.15267640 | temporal 0.66893607-0.22297869 | total 0.46834919\n",
      "Epoch 78, loss: 0.46834919-(best 0.46834919)\n",
      "\n",
      "Detailed Loss: recon 0.28268272-0.09422757 | disen 0.45159501-0.15053167 | temporal 0.66865504-0.22288501 | total 0.46764427\n",
      "Epoch 79, loss: 0.46764427-(best 0.46764427)\n",
      "\n",
      "Detailed Loss: recon 0.27708513-0.09236171 | disen 0.46034336-0.15344779 | temporal 0.66953021-0.22317674 | total 0.46898624\n",
      "Epoch 80, loss: 0.46898624-(best 0.46764427)\n",
      "\n",
      "Detailed Loss: recon 0.28402573-0.09467524 | disen 0.44434917-0.14811639 | temporal 0.66852623-0.22284208 | total 0.46563372\n",
      "Epoch 81, loss: 0.46563372-(best 0.46563372)\n",
      "\n",
      "Detailed Loss: recon 0.28340089-0.09446696 | disen 0.44032770-0.14677590 | temporal 0.66828918-0.22276306 | total 0.46400595\n",
      "Epoch 82, loss: 0.46400595-(best 0.46400595)\n",
      "\n",
      "Detailed Loss: recon 0.28342873-0.09447624 | disen 0.44622982-0.14874327 | temporal 0.66902751-0.22300917 | total 0.46622869\n",
      "Epoch 83, loss: 0.46622869-(best 0.46400595)\n",
      "\n",
      "Detailed Loss: recon 0.28505233-0.09501744 | disen 0.43884915-0.14628305 | temporal 0.66801214-0.22267071 | total 0.46397123\n",
      "Epoch 84, loss: 0.46397123-(best 0.46397123)\n",
      "\n",
      "Detailed Loss: recon 0.28730306-0.09576769 | disen 0.43296462-0.14432154 | temporal 0.66743112-0.22247704 | total 0.46256629\n",
      "Epoch 85, loss: 0.46256629-(best 0.46256629)\n",
      "\n",
      "Detailed Loss: recon 0.27493411-0.09164470 | disen 0.44179189-0.14726396 | temporal 0.66845608-0.22281869 | total 0.46172738\n",
      "Epoch 86, loss: 0.46172738-(best 0.46172738)\n",
      "\n",
      "Detailed Loss: recon 0.28570709-0.09523570 | disen 0.43382287-0.14460762 | temporal 0.66735762-0.22245254 | total 0.46229589\n",
      "Epoch 87, loss: 0.46229589-(best 0.46172738)\n",
      "\n",
      "Detailed Loss: recon 0.28392878-0.09464293 | disen 0.43303543-0.14434514 | temporal 0.66738778-0.22246259 | total 0.46145067\n",
      "Epoch 88, loss: 0.46145067-(best 0.46145067)\n",
      "\n",
      "Detailed Loss: recon 0.28838757-0.09612919 | disen 0.43236256-0.14412085 | temporal 0.66761512-0.22253837 | total 0.46278843\n",
      "Epoch 89, loss: 0.46278843-(best 0.46145067)\n",
      "\n",
      "Detailed Loss: recon 0.28525960-0.09508653 | disen 0.42868096-0.14289365 | temporal 0.66768533-0.22256178 | total 0.46054196\n",
      "Epoch 90, loss: 0.46054196-(best 0.46054196)\n",
      "\n",
      "Detailed Loss: recon 0.28600287-0.09533429 | disen 0.42916960-0.14305653 | temporal 0.66686875-0.22228958 | total 0.46068043\n",
      "Epoch 91, loss: 0.46068043-(best 0.46054196)\n",
      "\n",
      "Detailed Loss: recon 0.28150833-0.09383611 | disen 0.43573284-0.14524428 | temporal 0.66745853-0.22248618 | total 0.46156657\n",
      "Epoch 92, loss: 0.46156657-(best 0.46054196)\n",
      "\n",
      "Detailed Loss: recon 0.28659636-0.09553212 | disen 0.42957598-0.14319199 | temporal 0.66737878-0.22245959 | total 0.46118373\n",
      "Epoch 93, loss: 0.46118373-(best 0.46054196)\n",
      "\n",
      "Detailed Loss: recon 0.28322408-0.09440803 | disen 0.42294317-0.14098106 | temporal 0.66712445-0.22237482 | total 0.45776391\n",
      "Epoch 94, loss: 0.45776391-(best 0.45776391)\n",
      "\n",
      "Detailed Loss: recon 0.29232377-0.09744126 | disen 0.42423499-0.14141166 | temporal 0.66620600-0.22206867 | total 0.46092159\n",
      "Epoch 95, loss: 0.46092159-(best 0.45776391)\n",
      "\n",
      "Detailed Loss: recon 0.27369696-0.09123232 | disen 0.43980461-0.14660154 | temporal 0.66769487-0.22256496 | total 0.46039882\n",
      "Epoch 96, loss: 0.46039882-(best 0.45776391)\n",
      "\n",
      "Detailed Loss: recon 0.29789990-0.09929997 | disen 0.42652756-0.14217585 | temporal 0.66539454-0.22179818 | total 0.46327400\n",
      "Epoch 97, loss: 0.46327400-(best 0.45776391)\n",
      "\n",
      "Detailed Loss: recon 0.27280515-0.09093505 | disen 0.43779939-0.14593313 | temporal 0.66794276-0.22264759 | total 0.45951578\n",
      "Epoch 98, loss: 0.45951578-(best 0.45776391)\n",
      "\n",
      "Detailed Loss: recon 0.28191283-0.09397094 | disen 0.42521864-0.14173955 | temporal 0.66732496-0.22244165 | total 0.45815217\n",
      "Epoch 99, loss: 0.45815217-(best 0.45776391)\n",
      "\n",
      "Detailed Loss: recon 0.29282963-0.09760988 | disen 0.42057359-0.14019120 | temporal 0.66572702-0.22190901 | total 0.45971009\n",
      "Epoch 100, loss: 0.45971009-(best 0.45776391)\n",
      "\n",
      ">> Reached final epoch. Loading best model...\n",
      "TRAINING XGB PREDICTOR\n",
      ">> Ending training!\n",
      "\n",
      "-----------------------\n",
      "TIME COST: 6.275032997131348 s\n",
      "Best Val: REC 50.10 PRE 50.50 MF1 68.39 AUC 80.54 TP 253 FP 248 TN 1599 FN 252\n",
      "-----------------------\n",
      "\n",
      "ASSIGNED GRAPH FOR EVALUATION: 11758 {1: 2566, 0: 9192} Nodes, 1049758 Edges\n",
      "\n",
      "-------------------\n",
      "STARTING EVALUATION\n",
      "-------------------\n",
      "\n",
      "Dataset - Overall: REC 62.67 PRE 47.02 MF1 68.96 AUC 81.21 TP 1608 FP 1812 TN 7380 FN 958 | 11758 {1: 2566, 0: 9192}\n",
      "Dataset - Train: REC 71.60 PRE 52.22 MF1 73.44 AUC 86.33 TP 542 FP 496 TN 2274 FN 215 | 3527 {0: 2770, 1: 757}\n",
      "Dataset - Val: REC 58.61 PRE 43.98 MF1 66.82 AUC 79.83 TP 296 FP 377 TN 1470 FN 209 | 2352 {0: 1847, 1: 505}\n",
      "Dataset - Test: REC 59.05 PRE 45.06 MF1 67.13 AUC 78.93 TP 770 FP 939 TN 3636 FN 534 | 5879 {1: 1304, 0: 4575}\n",
      "PREDICTION STATUS - {'1-True': 2032, '0-True': 5556, '0-False': 3636, '1-False': 534}\n",
      "    >> 534 positive nodes left unpredicted...\n",
      "    >> 534 OG positive nodes left unpredicted...\n",
      "Selecting data as budgeted ground truth for next round...\n",
      "  > Starting round 1...\n",
      "\n",
      "+++++++++++++++++\n",
      "STARTING ROUND 1!\n",
      "+++++++++++++++++\n",
      "        \n",
      "ASSIGNED GRAPH FOR TRAINING: 11758 {1: 2566, 0: 9192} Nodes, 1049758 Edges\n",
      "\n",
      ">> Splitting to train-validation set...\n",
      "Alotting train-val-test split for round 1\n",
      "Initial pool: 5879, Prediction pool: 3420, Budget pool: 0, Full pool: 7588\n",
      "Full graph size: 11758, Training: 4552, Val: 3036, Test: 11758\n",
      "    >> INITIAL DATA SPLIT: 4552 train rows ({1: 1219, 0: 3333}) | 3036 val rows ({1: 813, 0: 2223}) | 4170 test rows ({0: 3636, 1: 534})\n",
      "    >> AUGMENTED DATA SPLIT: 4552 train rows ({1: 1219, 0: 3333}) | 3036 val rows ({1: 813, 0: 2223}) | 4170 test rows ({0: 3636, 1: 534})\n",
      "    >> Updated cross-entropy weight to 2.734208367514356...\n",
      "\n",
      "-----------------------\n",
      "STARTING MODEL TRAININGps_l\n",
      "-----------------------\n",
      "\n",
      "TRAINING NODE EMBEDDER\n",
      "Detailed Loss: recon 0.28388676-0.09462892 | disen 0.39809668-0.13269889 | temporal 0.56141919-0.18713973 | total 0.41446757\n",
      "Epoch 1, loss: 0.41446757-(best 0.41446757)\n",
      "\n",
      "Detailed Loss: recon 0.44826674-0.14942225 | disen 0.40484709-0.13494903 | temporal 0.56245935-0.18748645 | total 0.47185773\n",
      "Epoch 2, loss: 0.47185773-(best 0.41446757)\n",
      "\n",
      "Detailed Loss: recon 0.28783023-0.09594341 | disen 0.41538692-0.13846231 | temporal 0.56397933-0.18799311 | total 0.42239884\n",
      "Epoch 3, loss: 0.42239884-(best 0.41446757)\n",
      "\n",
      "Detailed Loss: recon 0.26833588-0.08944529 | disen 0.42267191-0.14089064 | temporal 0.56446111-0.18815370 | total 0.41848963\n",
      "Epoch 4, loss: 0.41848963-(best 0.41446757)\n",
      "\n",
      "Detailed Loss: recon 0.26052910-0.08684303 | disen 0.41137397-0.13712466 | temporal 0.56427181-0.18809060 | total 0.41205829\n",
      "Epoch 5, loss: 0.41205829-(best 0.41205829)\n",
      "\n",
      "Detailed Loss: recon 0.27307242-0.09102414 | disen 0.40300035-0.13433345 | temporal 0.56406647-0.18802216 | total 0.41337979\n",
      "Epoch 6, loss: 0.41337979-(best 0.41205829)\n",
      "\n",
      "Detailed Loss: recon 0.27664226-0.09221409 | disen 0.39797378-0.13265793 | temporal 0.56377000-0.18792333 | total 0.41279536\n",
      "Epoch 7, loss: 0.41279536-(best 0.41205829)\n",
      "\n",
      "Detailed Loss: recon 0.26858091-0.08952697 | disen 0.39088398-0.13029466 | temporal 0.56368500-0.18789500 | total 0.40771663\n",
      "Epoch 8, loss: 0.40771663-(best 0.40771663)\n",
      "\n",
      "Detailed Loss: recon 0.26655445-0.08885148 | disen 0.38777614-0.12925871 | temporal 0.56368786-0.18789595 | total 0.40600616\n",
      "Epoch 9, loss: 0.40600616-(best 0.40600616)\n",
      "\n",
      "Detailed Loss: recon 0.26356670-0.08785557 | disen 0.38533139-0.12844380 | temporal 0.56374192-0.18791397 | total 0.40421337\n",
      "Epoch 10, loss: 0.40421337-(best 0.40421337)\n",
      "\n",
      "Detailed Loss: recon 0.26350150-0.08783383 | disen 0.38325477-0.12775159 | temporal 0.56375867-0.18791956 | total 0.40350497\n",
      "Epoch 11, loss: 0.40350497-(best 0.40350497)\n",
      "\n",
      "Detailed Loss: recon 0.26164347-0.08721449 | disen 0.37787730-0.12595910 | temporal 0.56365943-0.18788648 | total 0.40106007\n",
      "Epoch 12, loss: 0.40106007-(best 0.40106007)\n",
      "\n",
      "Detailed Loss: recon 0.26373371-0.08791124 | disen 0.37374187-0.12458062 | temporal 0.56347454-0.18782485 | total 0.40031672\n",
      "Epoch 13, loss: 0.40031672-(best 0.40031672)\n",
      "\n",
      "Detailed Loss: recon 0.26003805-0.08667935 | disen 0.36821008-0.12273669 | temporal 0.56330687-0.18776896 | total 0.39718503\n",
      "Epoch 14, loss: 0.39718503-(best 0.39718503)\n",
      "\n",
      "Detailed Loss: recon 0.25731039-0.08577013 | disen 0.36545473-0.12181824 | temporal 0.56311762-0.18770587 | total 0.39529425\n",
      "Epoch 15, loss: 0.39529425-(best 0.39529425)\n",
      "\n",
      "Detailed Loss: recon 0.25975490-0.08658497 | disen 0.36492747-0.12164249 | temporal 0.56315851-0.18771950 | total 0.39594698\n",
      "Epoch 16, loss: 0.39594698-(best 0.39529425)\n",
      "\n",
      "Detailed Loss: recon 0.25711605-0.08570535 | disen 0.36283773-0.12094591 | temporal 0.56324172-0.18774724 | total 0.39439851\n",
      "Epoch 17, loss: 0.39439851-(best 0.39439851)\n",
      "\n",
      "Detailed Loss: recon 0.25120214-0.08373405 | disen 0.36170614-0.12056871 | temporal 0.56328404-0.18776135 | total 0.39206409\n",
      "Epoch 18, loss: 0.39206409-(best 0.39206409)\n",
      "\n",
      "Detailed Loss: recon 0.25277704-0.08425901 | disen 0.35907042-0.11969014 | temporal 0.56335068-0.18778356 | total 0.39173272\n",
      "Epoch 19, loss: 0.39173272-(best 0.39173272)\n",
      "\n",
      "Detailed Loss: recon 0.25566483-0.08522161 | disen 0.35619229-0.11873076 | temporal 0.56335092-0.18778364 | total 0.39173603\n",
      "Epoch 20, loss: 0.39173603-(best 0.39173272)\n",
      "\n",
      "Detailed Loss: recon 0.25529096-0.08509699 | disen 0.35456175-0.11818725 | temporal 0.56334877-0.18778292 | total 0.39106715\n",
      "Epoch 21, loss: 0.39106715-(best 0.39106715)\n",
      "\n",
      "Detailed Loss: recon 0.25329515-0.08443172 | disen 0.35066140-0.11688713 | temporal 0.56325740-0.18775247 | total 0.38907135\n",
      "Epoch 22, loss: 0.38907135-(best 0.38907135)\n",
      "\n",
      "Detailed Loss: recon 0.25563276-0.08521092 | disen 0.34925663-0.11641888 | temporal 0.56318128-0.18772709 | total 0.38935691\n",
      "Epoch 23, loss: 0.38935691-(best 0.38907135)\n",
      "\n",
      "Detailed Loss: recon 0.25200653-0.08400218 | disen 0.34727263-0.11575754 | temporal 0.56316763-0.18772254 | total 0.38748229\n",
      "Epoch 24, loss: 0.38748229-(best 0.38748229)\n",
      "\n",
      "Detailed Loss: recon 0.24995613-0.08331871 | disen 0.34550613-0.11516871 | temporal 0.56306791-0.18768930 | total 0.38617674\n",
      "Epoch 25, loss: 0.38617674-(best 0.38617674)\n",
      "\n",
      "Detailed Loss: recon 0.24934362-0.08311454 | disen 0.34388518-0.11462839 | temporal 0.56298077-0.18766026 | total 0.38540322\n",
      "Epoch 26, loss: 0.38540322-(best 0.38540322)\n",
      "\n",
      "Detailed Loss: recon 0.25237477-0.08412492 | disen 0.34176338-0.11392113 | temporal 0.56301308-0.18767103 | total 0.38571709\n",
      "Epoch 27, loss: 0.38571709-(best 0.38540322)\n",
      "\n",
      "Detailed Loss: recon 0.24964905-0.08321635 | disen 0.33943671-0.11314557 | temporal 0.56294847-0.18764949 | total 0.38401142\n",
      "Epoch 28, loss: 0.38401142-(best 0.38401142)\n",
      "\n",
      "Detailed Loss: recon 0.25121725-0.08373908 | disen 0.33505815-0.11168605 | temporal 0.56279600-0.18759867 | total 0.38302380\n",
      "Epoch 29, loss: 0.38302380-(best 0.38302380)\n",
      "\n",
      "Detailed Loss: recon 0.25114620-0.08371540 | disen 0.33430797-0.11143599 | temporal 0.56270897-0.18756966 | total 0.38272107\n",
      "Epoch 30, loss: 0.38272107-(best 0.38272107)\n",
      "\n",
      "Detailed Loss: recon 0.25098199-0.08366066 | disen 0.33070976-0.11023659 | temporal 0.56248820-0.18749607 | total 0.38139331\n",
      "Epoch 31, loss: 0.38139331-(best 0.38139331)\n",
      "\n",
      "Detailed Loss: recon 0.24808434-0.08269478 | disen 0.33173597-0.11057866 | temporal 0.56250352-0.18750117 | total 0.38077462\n",
      "Epoch 32, loss: 0.38077462-(best 0.38077462)\n",
      "\n",
      "Detailed Loss: recon 0.24758826-0.08252942 | disen 0.33007061-0.11002354 | temporal 0.56247723-0.18749241 | total 0.38004538\n",
      "Epoch 33, loss: 0.38004538-(best 0.38004538)\n",
      "\n",
      "Detailed Loss: recon 0.24374352-0.08124784 | disen 0.32839251-0.10946417 | temporal 0.56244528-0.18748176 | total 0.37819377\n",
      "Epoch 34, loss: 0.37819377-(best 0.37819377)\n",
      "\n",
      "Detailed Loss: recon 0.24449247-0.08149749 | disen 0.32826000-0.10942000 | temporal 0.56241751-0.18747250 | total 0.37839001\n",
      "Epoch 35, loss: 0.37839001-(best 0.37819377)\n",
      "\n",
      "Detailed Loss: recon 0.24580345-0.08193448 | disen 0.32715040-0.10905013 | temporal 0.56231856-0.18743952 | total 0.37842417\n",
      "Epoch 36, loss: 0.37842417-(best 0.37819377)\n",
      "\n",
      "Detailed Loss: recon 0.24739107-0.08246369 | disen 0.32237792-0.10745931 | temporal 0.56213534-0.18737845 | total 0.37730145\n",
      "Epoch 37, loss: 0.37730145-(best 0.37730145)\n",
      "\n",
      "Detailed Loss: recon 0.24816304-0.08272101 | disen 0.31978703-0.10659568 | temporal 0.56200933-0.18733644 | total 0.37665313\n",
      "Epoch 38, loss: 0.37665313-(best 0.37665313)\n",
      "\n",
      "Detailed Loss: recon 0.25025007-0.08341669 | disen 0.31909180-0.10636393 | temporal 0.56205857-0.18735286 | total 0.37713349\n",
      "Epoch 39, loss: 0.37713349-(best 0.37665313)\n",
      "\n",
      "Detailed Loss: recon 0.24878785-0.08292928 | disen 0.31874347-0.10624782 | temporal 0.56208861-0.18736287 | total 0.37654001\n",
      "Epoch 40, loss: 0.37654001-(best 0.37654001)\n",
      "\n",
      "Detailed Loss: recon 0.24755296-0.08251765 | disen 0.31576782-0.10525594 | temporal 0.56211227-0.18737076 | total 0.37514436\n",
      "Epoch 41, loss: 0.37514436-(best 0.37514436)\n",
      "\n",
      "Detailed Loss: recon 0.24867766-0.08289255 | disen 0.31393284-0.10464428 | temporal 0.56217307-0.18739102 | total 0.37492788\n",
      "Epoch 42, loss: 0.37492788-(best 0.37492788)\n",
      "\n",
      "Detailed Loss: recon 0.24535434-0.08178478 | disen 0.31582940-0.10527647 | temporal 0.56231058-0.18743686 | total 0.37449813\n",
      "Epoch 43, loss: 0.37449813-(best 0.37449813)\n",
      "\n",
      "Detailed Loss: recon 0.24510387-0.08170129 | disen 0.31357896-0.10452632 | temporal 0.56221658-0.18740553 | total 0.37363315\n",
      "Epoch 44, loss: 0.37363315-(best 0.37363315)\n",
      "\n",
      "Detailed Loss: recon 0.24525720-0.08175240 | disen 0.31224793-0.10408264 | temporal 0.56222320-0.18740773 | total 0.37324280\n",
      "Epoch 45, loss: 0.37324280-(best 0.37324280)\n",
      "\n",
      "Detailed Loss: recon 0.24662265-0.08220755 | disen 0.31212831-0.10404277 | temporal 0.56223005-0.18741002 | total 0.37366036\n",
      "Epoch 46, loss: 0.37366036-(best 0.37324280)\n",
      "\n",
      "Detailed Loss: recon 0.24328852-0.08109617 | disen 0.31254482-0.10418161 | temporal 0.56225580-0.18741860 | total 0.37269640\n",
      "Epoch 47, loss: 0.37269640-(best 0.37269640)\n",
      "\n",
      "Detailed Loss: recon 0.24331516-0.08110505 | disen 0.30931741-0.10310580 | temporal 0.56206417-0.18735472 | total 0.37156558\n",
      "Epoch 48, loss: 0.37156558-(best 0.37156558)\n",
      "\n",
      "Detailed Loss: recon 0.24560869-0.08186956 | disen 0.31074303-0.10358101 | temporal 0.56204969-0.18734990 | total 0.37280047\n",
      "Epoch 49, loss: 0.37280047-(best 0.37156558)\n",
      "\n",
      "Detailed Loss: recon 0.24416301-0.08138767 | disen 0.31011599-0.10337200 | temporal 0.56199944-0.18733315 | total 0.37209284\n",
      "Epoch 50, loss: 0.37209284-(best 0.37156558)\n",
      "\n",
      ">> Reached final epoch. Loading best model...\n",
      "TRAINING XGB PREDICTOR\n",
      ">> Ending training!\n",
      "\n",
      "-----------------------\n",
      "TIME COST: 5.302275896072388 s\n",
      "Best Val: REC 56.58 PRE 52.81 MF1 68.61 AUC 79.79 TP 460 FP 411 TN 1812 FN 353\n",
      "-----------------------\n",
      "Generating additional positive data from adversary...\n",
      "Generating additional negative data by duplicating random nodes...\n",
      "\n",
      "ASSIGNED GRAPH FOR EVALUATION: 12345 {1: 2694, 0: 9651} Nodes, 1152022 Edges\n",
      "\n",
      "-------------------\n",
      "STARTING EVALUATION\n",
      "-------------------\n",
      "\n",
      "Dataset - Overall: REC 58.61 PRE 57.00 MF1 72.90 AUC 84.31 TP 1579 FP 1191 TN 8460 FN 1115 | 12345 {1: 2694, 0: 9651}\n",
      "Dataset - Train: REC 77.03 PRE 67.65 MF1 80.41 AUC 90.99 TP 939 FP 449 TN 2884 FN 280 | 4552 {1: 1219, 0: 3333}\n",
      "Dataset - Val: REC 57.81 PRE 51.93 MF1 68.42 AUC 79.07 TP 470 FP 435 TN 1788 FN 343 | 3036 {1: 813, 0: 2223}\n",
      "Dataset - Test: REC 25.68 PRE 35.64 MF1 60.16 AUC 77.63 TP 170 FP 307 TN 3788 FN 492 | 4757 {0: 4095, 1: 662}\n",
      "PREDICTION STATUS - {'1-True': 2202, '0-True': 5863, '0-False': 3788, '1-False': 492}\n",
      "    >> 492 positive nodes left unpredicted...\n",
      "    >> 396 OG positive nodes left unpredicted...\n",
      "-----------------------\n",
      "PREDICTION RESULT - ROUNDS\n",
      "Dataset - Round 0: REC 55.44 PRE 56.57 MF1 71.81 AUC 83.10 TP 723 FP 555 TN 4020 FN 581 | 5879 {1: 1304, 0: 4575}\n",
      "Dataset - Round 1: REC 25.00 PRE 34.41 MF1 56.24 AUC 69.54 TP 32 FP 61 TN 398 FN 96 | 587 {1: 128, 0: 459}\n",
      "-----------------------\n",
      "PREDICTION RESULT - SEEDS\n",
      "Seeds - Current: REC 26.56 PRE 36.56 MF1 57.36 AUC 67.26 TP 34 FP 59 TN 400 FN 94 | 587 {1: 128, 0: 459}\n",
      "Seeds - Prev: REC 0.00 PRE 0.00 MF1 37.82 AUC 67.26 TP 0 FP 102 TN 357 FN 128 | 587 {1: 128, 0: 459}\n",
      "Selecting data as budgeted ground truth for next round...\n",
      "  > Starting round 2...\n",
      "\n",
      "+++++++++++++++++\n",
      "STARTING ROUND 2!\n",
      "+++++++++++++++++\n",
      "        \n",
      "ASSIGNED GRAPH FOR TRAINING: 12345 {1: 2694, 0: 9651} Nodes, 1152022 Edges\n",
      "\n",
      ">> Splitting to train-validation set...\n",
      "Alotting train-val-test split for round 2\n",
      "Initial pool: 5879, Prediction pool: 6190, Budget pool: 0, Full pool: 8065\n",
      "Full graph size: 12345, Training: 4839, Val: 3226, Test: 12345\n",
      "    >> INITIAL DATA SPLIT: 4839 train rows ({1: 1321, 0: 3518}) | 3226 val rows ({0: 2345, 1: 881}) | 4280 test rows ({0: 3788, 1: 492})\n",
      "    >> AUGMENTED DATA SPLIT: 4839 train rows ({1: 1321, 0: 3518}) | 3226 val rows ({0: 2345, 1: 881}) | 4280 test rows ({0: 3788, 1: 492})\n",
      "    >> Updated cross-entropy weight to 2.663133989401968...\n",
      "\n",
      "-----------------------\n",
      "STARTING MODEL TRAININGps_l\n",
      "-----------------------\n",
      "\n",
      "TRAINING NODE EMBEDDER\n",
      "Detailed Loss: recon 0.24552934-0.08184311 | disen 0.33672553-0.11224184 | temporal 0.58201939-0.19400646 | total 0.38809144\n",
      "Epoch 1, loss: 0.38809144-(best 0.38809144)\n",
      "\n",
      "Detailed Loss: recon 0.31807399-0.10602466 | disen 0.36737645-0.12245882 | temporal 0.58363968-0.19454656 | total 0.42303005\n",
      "Epoch 2, loss: 0.42303005-(best 0.38809144)\n",
      "\n",
      "Detailed Loss: recon 0.28664732-0.09554911 | disen 0.36013865-0.12004622 | temporal 0.58367968-0.19455989 | total 0.41015524\n",
      "Epoch 3, loss: 0.41015524-(best 0.38809144)\n",
      "\n",
      "Detailed Loss: recon 0.26625842-0.08875281 | disen 0.36571962-0.12190654 | temporal 0.58404392-0.19468131 | total 0.40534067\n",
      "Epoch 4, loss: 0.40534067-(best 0.38809144)\n",
      "\n",
      "Detailed Loss: recon 0.24160866-0.08053622 | disen 0.36792690-0.12264230 | temporal 0.58362824-0.19454275 | total 0.39772129\n",
      "Epoch 5, loss: 0.39772129-(best 0.38809144)\n",
      "\n",
      "Detailed Loss: recon 0.25866422-0.08622141 | disen 0.36625820-0.12208607 | temporal 0.58327883-0.19442628 | total 0.40273374\n",
      "Epoch 6, loss: 0.40273374-(best 0.38809144)\n",
      "\n",
      "Detailed Loss: recon 0.25193763-0.08397921 | disen 0.36808169-0.12269390 | temporal 0.58321446-0.19440482 | total 0.40107793\n",
      "Epoch 7, loss: 0.40107793-(best 0.38809144)\n",
      "\n",
      "Detailed Loss: recon 0.24322633-0.08107544 | disen 0.35795385-0.11931795 | temporal 0.58319879-0.19439960 | total 0.39479297\n",
      "Epoch 8, loss: 0.39479297-(best 0.38809144)\n",
      "\n",
      "Detailed Loss: recon 0.24578431-0.08192810 | disen 0.35496372-0.11832124 | temporal 0.58334941-0.19444980 | total 0.39469916\n",
      "Epoch 9, loss: 0.39469916-(best 0.38809144)\n",
      "\n",
      "Detailed Loss: recon 0.24633770-0.08211257 | disen 0.34950805-0.11650268 | temporal 0.58336794-0.19445598 | total 0.39307123\n",
      "Epoch 10, loss: 0.39307123-(best 0.38809144)\n",
      "\n",
      "Detailed Loss: recon 0.25226352-0.08408784 | disen 0.34810722-0.11603574 | temporal 0.58344066-0.19448022 | total 0.39460379\n",
      "Epoch 11, loss: 0.39460379-(best 0.38809144)\n",
      "\n",
      "Detailed Loss: recon 0.24777089-0.08259030 | disen 0.34946817-0.11648939 | temporal 0.58358210-0.19452737 | total 0.39360708\n",
      "Epoch 12, loss: 0.39360708-(best 0.38809144)\n",
      "\n",
      "Detailed Loss: recon 0.24736041-0.08245347 | disen 0.34905356-0.11635119 | temporal 0.58346713-0.19448904 | total 0.39329371\n",
      "Epoch 13, loss: 0.39329371-(best 0.38809144)\n",
      "\n",
      "Detailed Loss: recon 0.24517255-0.08172418 | disen 0.34968281-0.11656094 | temporal 0.58340609-0.19446870 | total 0.39275384\n",
      "Epoch 14, loss: 0.39275384-(best 0.38809144)\n",
      "\n",
      "Detailed Loss: recon 0.24425232-0.08141744 | disen 0.34293836-0.11431279 | temporal 0.58328187-0.19442729 | total 0.39015752\n",
      "Epoch 15, loss: 0.39015752-(best 0.38809144)\n",
      "\n",
      "Detailed Loss: recon 0.24448815-0.08149605 | disen 0.34220725-0.11406908 | temporal 0.58316040-0.19438680 | total 0.38995194\n",
      "Epoch 16, loss: 0.38995194-(best 0.38809144)\n",
      "\n",
      "Detailed Loss: recon 0.24268374-0.08089458 | disen 0.33615369-0.11205123 | temporal 0.58300364-0.19433455 | total 0.38728034\n",
      "Epoch 17, loss: 0.38728034-(best 0.38728034)\n",
      "\n",
      "Detailed Loss: recon 0.24107811-0.08035937 | disen 0.33438528-0.11146176 | temporal 0.58292186-0.19430729 | total 0.38612843\n",
      "Epoch 18, loss: 0.38612843-(best 0.38612843)\n",
      "\n",
      "Detailed Loss: recon 0.24282895-0.08094298 | disen 0.33404297-0.11134766 | temporal 0.58284354-0.19428118 | total 0.38657182\n",
      "Epoch 19, loss: 0.38657182-(best 0.38612843)\n",
      "\n",
      "Detailed Loss: recon 0.24140887-0.08046962 | disen 0.32760835-0.10920278 | temporal 0.58259124-0.19419708 | total 0.38386950\n",
      "Epoch 20, loss: 0.38386950-(best 0.38386950)\n",
      "\n",
      "Detailed Loss: recon 0.24109694-0.08036565 | disen 0.32761467-0.10920489 | temporal 0.58262467-0.19420822 | total 0.38377878\n",
      "Epoch 21, loss: 0.38377878-(best 0.38377878)\n",
      "\n",
      "Detailed Loss: recon 0.24149907-0.08049969 | disen 0.32931322-0.10977107 | temporal 0.58261395-0.19420465 | total 0.38447541\n",
      "Epoch 22, loss: 0.38447541-(best 0.38377878)\n",
      "\n",
      "Detailed Loss: recon 0.23827839-0.07942613 | disen 0.32653725-0.10884575 | temporal 0.58259833-0.19419944 | total 0.38247132\n",
      "Epoch 23, loss: 0.38247132-(best 0.38247132)\n",
      "\n",
      "Detailed Loss: recon 0.24122052-0.08040684 | disen 0.32276160-0.10758720 | temporal 0.58244979-0.19414993 | total 0.38214397\n",
      "Epoch 24, loss: 0.38214397-(best 0.38214397)\n",
      "\n",
      "Detailed Loss: recon 0.24220578-0.08073526 | disen 0.32111049-0.10703683 | temporal 0.58242673-0.19414224 | total 0.38191435\n",
      "Epoch 25, loss: 0.38191435-(best 0.38191435)\n",
      "\n",
      "Detailed Loss: recon 0.24157815-0.08052605 | disen 0.32021886-0.10673962 | temporal 0.58257645-0.19419215 | total 0.38145784\n",
      "Epoch 26, loss: 0.38145784-(best 0.38145784)\n",
      "\n",
      "Detailed Loss: recon 0.23875760-0.07958587 | disen 0.31988591-0.10662864 | temporal 0.58266896-0.19422299 | total 0.38043749\n",
      "Epoch 27, loss: 0.38043749-(best 0.38043749)\n",
      "\n",
      "Detailed Loss: recon 0.23688760-0.07896253 | disen 0.32210970-0.10736990 | temporal 0.58289844-0.19429948 | total 0.38063192\n",
      "Epoch 28, loss: 0.38063192-(best 0.38043749)\n",
      "\n",
      "Detailed Loss: recon 0.24004468-0.08001489 | disen 0.31747061-0.10582354 | temporal 0.58289397-0.19429799 | total 0.38013643\n",
      "Epoch 29, loss: 0.38013643-(best 0.38013643)\n",
      "\n",
      "Detailed Loss: recon 0.23722193-0.07907398 | disen 0.31539494-0.10513165 | temporal 0.58280116-0.19426705 | total 0.37847269\n",
      "Epoch 30, loss: 0.37847269-(best 0.37847269)\n",
      "\n",
      "Detailed Loss: recon 0.23620096-0.07873365 | disen 0.31583089-0.10527696 | temporal 0.58288866-0.19429622 | total 0.37830687\n",
      "Epoch 31, loss: 0.37830687-(best 0.37830687)\n",
      "\n",
      "Detailed Loss: recon 0.23733285-0.07911095 | disen 0.31467825-0.10489275 | temporal 0.58291495-0.19430498 | total 0.37830871\n",
      "Epoch 32, loss: 0.37830871-(best 0.37830687)\n",
      "\n",
      "Detailed Loss: recon 0.23706304-0.07902101 | disen 0.31465840-0.10488613 | temporal 0.58295071-0.19431690 | total 0.37822407\n",
      "Epoch 33, loss: 0.37822407-(best 0.37822407)\n",
      "\n",
      "Detailed Loss: recon 0.23355198-0.07785066 | disen 0.31533015-0.10511005 | temporal 0.58297688-0.19432563 | total 0.37728634\n",
      "Epoch 34, loss: 0.37728634-(best 0.37728634)\n",
      "\n",
      "Detailed Loss: recon 0.23790392-0.07930131 | disen 0.31337321-0.10445774 | temporal 0.58291608-0.19430536 | total 0.37806439\n",
      "Epoch 35, loss: 0.37806439-(best 0.37728634)\n",
      "\n",
      "Detailed Loss: recon 0.23579210-0.07859737 | disen 0.31147003-0.10382334 | temporal 0.58285475-0.19428492 | total 0.37670565\n",
      "Epoch 36, loss: 0.37670565-(best 0.37670565)\n",
      "\n",
      "Detailed Loss: recon 0.23452698-0.07817566 | disen 0.31249845-0.10416615 | temporal 0.58290994-0.19430331 | total 0.37664515\n",
      "Epoch 37, loss: 0.37664515-(best 0.37664515)\n",
      "\n",
      "Detailed Loss: recon 0.23233163-0.07744388 | disen 0.30987269-0.10329090 | temporal 0.58287346-0.19429115 | total 0.37502593\n",
      "Epoch 38, loss: 0.37502593-(best 0.37502593)\n",
      "\n",
      "Detailed Loss: recon 0.23347202-0.07782401 | disen 0.30843854-0.10281285 | temporal 0.58280301-0.19426767 | total 0.37490451\n",
      "Epoch 39, loss: 0.37490451-(best 0.37490451)\n",
      "\n",
      "Detailed Loss: recon 0.23255232-0.07751744 | disen 0.30748922-0.10249641 | temporal 0.58275151-0.19425050 | total 0.37426436\n",
      "Epoch 40, loss: 0.37426436-(best 0.37426436)\n",
      "\n",
      "Detailed Loss: recon 0.23198508-0.07732836 | disen 0.30640721-0.10213574 | temporal 0.58275056-0.19425019 | total 0.37371430\n",
      "Epoch 41, loss: 0.37371430-(best 0.37371430)\n",
      "\n",
      "Detailed Loss: recon 0.22969052-0.07656351 | disen 0.30722356-0.10240785 | temporal 0.58285654-0.19428551 | total 0.37325686\n",
      "Epoch 42, loss: 0.37325686-(best 0.37325686)\n",
      "\n",
      "Detailed Loss: recon 0.23113158-0.07704386 | disen 0.30641699-0.10213900 | temporal 0.58290482-0.19430161 | total 0.37348446\n",
      "Epoch 43, loss: 0.37348446-(best 0.37325686)\n",
      "\n",
      "Detailed Loss: recon 0.22950435-0.07650145 | disen 0.30702025-0.10234008 | temporal 0.58285952-0.19428651 | total 0.37312806\n",
      "Epoch 44, loss: 0.37312806-(best 0.37312806)\n",
      "\n",
      "Detailed Loss: recon 0.22965382-0.07655127 | disen 0.30520654-0.10173551 | temporal 0.58275378-0.19425126 | total 0.37253806\n",
      "Epoch 45, loss: 0.37253806-(best 0.37253806)\n",
      "\n",
      "Detailed Loss: recon 0.22997281-0.07665760 | disen 0.30539358-0.10179786 | temporal 0.58277637-0.19425879 | total 0.37271428\n",
      "Epoch 46, loss: 0.37271428-(best 0.37253806)\n",
      "\n",
      "Detailed Loss: recon 0.23022215-0.07674072 | disen 0.30294412-0.10098137 | temporal 0.58261836-0.19420612 | total 0.37192822\n",
      "Epoch 47, loss: 0.37192822-(best 0.37192822)\n",
      "\n",
      "Detailed Loss: recon 0.23394547-0.07798182 | disen 0.30240178-0.10080059 | temporal 0.58244461-0.19414820 | total 0.37293065\n",
      "Epoch 48, loss: 0.37293065-(best 0.37192822)\n",
      "\n",
      "Detailed Loss: recon 0.22923051-0.07641017 | disen 0.30510062-0.10170021 | temporal 0.58261520-0.19420507 | total 0.37231547\n",
      "Epoch 49, loss: 0.37231547-(best 0.37192822)\n",
      "\n",
      "Detailed Loss: recon 0.22769937-0.07589979 | disen 0.30725962-0.10241987 | temporal 0.58266813-0.19422271 | total 0.37254238\n",
      "Epoch 50, loss: 0.37254238-(best 0.37192822)\n",
      "\n",
      ">> Reached final epoch. Loading best model...\n",
      "TRAINING XGB PREDICTOR\n",
      ">> Ending training!\n",
      "\n",
      "-----------------------\n",
      "TIME COST: 5.913353204727173 s\n",
      "Best Val: REC 58.34 PRE 49.42 MF1 66.90 AUC 76.59 TP 514 FP 526 TN 1819 FN 367\n",
      "-----------------------\n",
      "Generating additional positive data from adversary...\n",
      "Generating additional negative data by duplicating random nodes...\n",
      "\n",
      "ASSIGNED GRAPH FOR EVALUATION: 12932 {1: 2822, 0: 10110} Nodes, 1244028 Edges\n",
      "\n",
      "-------------------\n",
      "STARTING EVALUATION\n",
      "-------------------\n",
      "\n",
      "Dataset - Overall: REC 61.94 PRE 52.56 MF1 71.71 AUC 82.88 TP 1748 FP 1578 TN 8532 FN 1074 | 12932 {1: 2822, 0: 10110}\n",
      "Dataset - Train: REC 79.11 PRE 63.29 MF1 78.59 AUC 88.77 TP 1045 FP 606 TN 2912 FN 276 | 4839 {1: 1321, 0: 3518}\n",
      "Dataset - Val: REC 62.66 PRE 47.10 MF1 66.10 AUC 76.26 TP 552 FP 620 TN 1725 FN 329 | 3226 {0: 2345, 1: 881}\n",
      "Dataset - Test: REC 24.35 PRE 30.02 MF1 58.68 AUC 76.34 TP 151 FP 352 TN 3895 FN 469 | 4867 {0: 4247, 1: 620}\n",
      "PREDICTION STATUS - {'1-True': 2353, '0-True': 6215, '0-False': 3895, '1-False': 469}\n",
      "    >> 469 positive nodes left unpredicted...\n",
      "    >> 302 OG positive nodes left unpredicted...\n",
      "-----------------------\n",
      "PREDICTION RESULT - ROUNDS\n",
      "Dataset - Round 0: REC 61.43 PRE 53.79 MF1 72.04 AUC 82.90 TP 801 FP 688 TN 3887 FN 503 | 5879 {1: 1304, 0: 4575}\n",
      "Dataset - Round 1: REC 36.72 PRE 38.84 MF1 60.50 AUC 69.64 TP 47 FP 74 TN 385 FN 81 | 587 {1: 128, 0: 459}\n",
      "Dataset - Round 2: REC 24.22 PRE 32.29 MF1 55.31 AUC 70.62 TP 31 FP 65 TN 394 FN 97 | 587 {1: 128, 0: 459}\n",
      "-----------------------\n",
      "PREDICTION RESULT - SEEDS\n",
      "Seeds - Current: REC 26.56 PRE 31.78 MF1 55.58 AUC 67.53 TP 34 FP 73 TN 386 FN 94 | 587 {1: 128, 0: 459}\n",
      "Seeds - Prev: REC 0.00 PRE 0.00 MF1 41.53 AUC 67.53 TP 0 FP 42 TN 417 FN 128 | 587 {1: 128, 0: 459}\n",
      "Selecting data as budgeted ground truth for next round...\n",
      "The file ../checkpoint/working_model_file_1730091838.1091218_epoch.py does not exist\n",
      "The file ../checkpoint/working_model_file_1730091838.1091218_round-M.json does not exist\n",
      "The file ../checkpoint/working_model_file_1730091838.1091218_round-T.json does not exist\n",
      "The file ../checkpoint/working_model_file_1730091838.1091218_round.pt does not exist\n",
      "================\n",
      "++++++++++++++++\n",
      "TRIAL NUMBER 2\n",
      "++++++++++++++++\n",
      "================\n",
      "  > Rereading graph data...\n",
      "  > Initializing multiround object...\n",
      "11758 1049758\n",
      "\n",
      "=========================\n",
      "STARTING ADVER EXPERIMENT\n",
      "=========================\n",
      "MAIN CONFIG: [('device', 'cuda:0'), ('exp_type', 'ADVER'), ('full_oracle', False), ('node_expiration_time', 1), ('round_budget_neg', 0), ('round_budget_pos', 0), ('round_new_neg', 459), ('round_new_pos', 128), ('round_num', 3), ('save_df', True), ('save_embedding', False), ('save_temp_sne', False), ('setting_type', 'INDUCTIVE'), ('task_type', 'NODE'), ('verbose', 4)]\n",
      "MODEL CONFIG: [('act_name', 'LeakyReLU'), ('addon_internal_agg', 'OR'), ('addon_name', 'NONE'), ('addon_perc', 0.05), ('addon_round_window', 7), ('alpha', 1), ('att_heads', 2), ('beta', 1), ('boost_agg_backbone', <class 'models.benchmarks_supervised.booster.GIN_noparam'>), ('boost_metric', <function average_precision_score at 0x000001C8520A5760>), ('boost_predictor', <class 'xgboost.sklearn.XGBClassifier'>), ('device', 'cuda:0'), ('drop_rate', 0), ('dropout_rate', 0.1), ('early_stopping', 75), ('embed_type', 'temporal'), ('etypes', ['none']), ('gamma', 1), ('h_feats', 64), ('k', 1), ('lambda1', 1), ('lambda2', 1), ('lambda_', 1), ('loss_sample', True), ('loss_sample_ratio', 0.1), ('loss_type', 'ndist'), ('mlp_feats', 64), ('mlp_layers', 2), ('model_name', 'XGB-SP'), ('n_estimator', 500), ('norm_name', 'layer'), ('num_epoch', 100), ('num_layers', 2), ('num_round_epoch', 50), ('round_window', 7), ('temporal_agg', 'mean_final'), ('tloss_type', 'normal'), ('training_type', 'round'), ('verbose', 4)]\n",
      "STRAT CONFIG: [('augment_name', 'REAGE'), ('augment_neg_ratio', 0.09772070079945569), ('augment_pos_ratio', 0.02727929920054431), ('augment_prob', 0.5), ('augment_round_split', 8), ('device', 'cuda:0'), ('pseudo_name', 'NONE'), ('pseudo_new_pos_only', False), ('pseudo_skip_initial', False), ('verbose', 4)]\n",
      "TRAIN CONFIG: [('ce_weight', 2.663133989401968), ('device', 'cuda:0'), ('dset_name', 'tolokers_bid'), ('early_stopping', 75), ('learning_rate', 0.01), ('loss', <function cross_entropy at 0x000001C84AF6F880>), ('loss_gamma', 5), ('num_epoch', 100), ('num_round_epoch', 50), ('optimizer', <class 'torch.optim.adam.Adam'>), ('random_state', 7777777), ('ratio_initial', 0.5), ('ratio_test', 0.2), ('ratio_train', 0.6), ('ratio_val', 0.2), ('round_reset_model', False), ('round_train_list', []), ('stuck_stopping', 50), ('stuck_threshold', 0), ('train_max_retry', 10), ('verbose', 4)]\n",
      "ADVER CONFIG: [('adver_choose_name', 'GREEDY'), ('adver_conn_coef', 0.1), ('adver_feat_coef', 1), ('adver_mod_name', 'REPLAY'), ('device', 'cuda:0'), ('verbose', 4)]\n",
      "\n",
      "INITIAL GRAPH: 11758 {1: 2566, 0: 9192} Nodes, 1049758 Edges\n",
      "=========================\n",
      "        \n",
      ">> Initializing graph...\n",
      "MAIN GRAPH: 11758 {1: 2566, 0: 9192} Nodes, 1049758 Edges\n",
      "\n",
      ">> Initializing experiment type-specific modules:\n",
      "  >> Initializing adversarial modules...\n",
      ">> Initializing task-specific modules:\n",
      ">> Initializing classifier model...\n",
      ">> Initializing other strategies...\n",
      "\n",
      "FINISH INITIALIZATION\n",
      "=====================\n",
      "        \n",
      "  > Starting round 0...\n",
      "\n",
      "+++++++++++++++++\n",
      "STARTING ROUND 0!\n",
      "+++++++++++++++++\n",
      "        \n",
      "ASSIGNED GRAPH FOR TRAINING: 5879 {1: 1301, 0: 4578} Nodes, 252575 Edges\n",
      "\n",
      ">> Splitting to train-validation set...\n",
      "Alotting train-val-test split for round 0\n",
      "Initial pool: 0, Prediction pool: 0, Budget pool: 0, Full pool: 5879\n",
      "Full graph size: 5879, Training: 3527, Val: 2352, Test: 5879\n",
      "    >> INITIAL DATA SPLIT: 3527 train rows ({0: 2746, 1: 781}) | 2352 val rows ({1: 520, 0: 1832}) | 0 test rows ({})\n",
      "    >> AUGMENTED DATA SPLIT: 3527 train rows ({0: 2746, 1: 781}) | 2352 val rows ({1: 520, 0: 1832}) | 0 test rows ({})\n",
      "    >> Updated cross-entropy weight to 3.5160051216389245...\n",
      "\n",
      "-----------------------\n",
      "STARTING MODEL TRAININGps_l\n",
      "-----------------------\n",
      "\n",
      "TRAINING NODE EMBEDDER\n",
      "Detailed Loss: recon 0.77764815-0.25921605 | disen 0.71264613-0.23754871 | temporal 0.64380509-0.21460170 | total 0.71136647\n",
      "Epoch 1, loss: 0.71136647-(best 0.71136647)\n",
      "\n",
      "Detailed Loss: recon 0.44815776-0.14938592 | disen 0.72116721-0.24038907 | temporal 0.65787077-0.21929026 | total 0.60906529\n",
      "Epoch 2, loss: 0.60906529-(best 0.60906529)\n",
      "\n",
      "Detailed Loss: recon 0.39328575-0.13109525 | disen 0.72243655-0.24081218 | temporal 0.66263759-0.22087920 | total 0.59278667\n",
      "Epoch 3, loss: 0.59278667-(best 0.59278667)\n",
      "\n",
      "Detailed Loss: recon 0.39239281-0.13079760 | disen 0.72558218-0.24186073 | temporal 0.66477263-0.22159088 | total 0.59424925\n",
      "Epoch 4, loss: 0.59424925-(best 0.59278667)\n",
      "\n",
      "Detailed Loss: recon 0.38113064-0.12704355 | disen 0.72525501-0.24175167 | temporal 0.66606653-0.22202218 | total 0.59081739\n",
      "Epoch 5, loss: 0.59081739-(best 0.59081739)\n",
      "\n",
      "Detailed Loss: recon 0.37133762-0.12377921 | disen 0.72346878-0.24115626 | temporal 0.66642237-0.22214079 | total 0.58707625\n",
      "Epoch 6, loss: 0.58707625-(best 0.58707625)\n",
      "\n",
      "Detailed Loss: recon 0.35297513-0.11765838 | disen 0.72728729-0.24242910 | temporal 0.66687262-0.22229087 | total 0.58237839\n",
      "Epoch 7, loss: 0.58237839-(best 0.58237839)\n",
      "\n",
      "Detailed Loss: recon 0.35257280-0.11752427 | disen 0.71901000-0.23967000 | temporal 0.66665614-0.22221871 | total 0.57941300\n",
      "Epoch 8, loss: 0.57941300-(best 0.57941300)\n",
      "\n",
      "Detailed Loss: recon 0.33174551-0.11058184 | disen 0.71567690-0.23855897 | temporal 0.66583627-0.22194542 | total 0.57108629\n",
      "Epoch 9, loss: 0.57108629-(best 0.57108629)\n",
      "\n",
      "Detailed Loss: recon 0.31970119-0.10656706 | disen 0.70967388-0.23655796 | temporal 0.66494513-0.22164838 | total 0.56477344\n",
      "Epoch 10, loss: 0.56477344-(best 0.56477344)\n",
      "\n",
      "Detailed Loss: recon 0.32084817-0.10694939 | disen 0.71250415-0.23750138 | temporal 0.66429144-0.22143048 | total 0.56588125\n",
      "Epoch 11, loss: 0.56588125-(best 0.56477344)\n",
      "\n",
      "Detailed Loss: recon 0.32423601-0.10807867 | disen 0.71196008-0.23732003 | temporal 0.66394252-0.22131417 | total 0.56671292\n",
      "Epoch 12, loss: 0.56671292-(best 0.56477344)\n",
      "\n",
      "Detailed Loss: recon 0.31778166-0.10592722 | disen 0.70403612-0.23467871 | temporal 0.66404706-0.22134902 | total 0.56195498\n",
      "Epoch 13, loss: 0.56195498-(best 0.56195498)\n",
      "\n",
      "Detailed Loss: recon 0.30862027-0.10287342 | disen 0.70257604-0.23419201 | temporal 0.66450816-0.22150272 | total 0.55856818\n",
      "Epoch 14, loss: 0.55856818-(best 0.55856818)\n",
      "\n",
      "Detailed Loss: recon 0.30328977-0.10109659 | disen 0.69922531-0.23307510 | temporal 0.66496992-0.22165664 | total 0.55582833\n",
      "Epoch 15, loss: 0.55582833-(best 0.55582833)\n",
      "\n",
      "Detailed Loss: recon 0.29460424-0.09820141 | disen 0.70146996-0.23382332 | temporal 0.66574746-0.22191582 | total 0.55394059\n",
      "Epoch 16, loss: 0.55394059-(best 0.55394059)\n",
      "\n",
      "Detailed Loss: recon 0.29426911-0.09808970 | disen 0.69962525-0.23320842 | temporal 0.66632164-0.22210721 | total 0.55340534\n",
      "Epoch 17, loss: 0.55340534-(best 0.55340534)\n",
      "\n",
      "Detailed Loss: recon 0.28575727-0.09525242 | disen 0.68937838-0.22979279 | temporal 0.66709888-0.22236629 | total 0.54741156\n",
      "Epoch 18, loss: 0.54741156-(best 0.54741156)\n",
      "\n",
      "Detailed Loss: recon 0.29365307-0.09788436 | disen 0.68479866-0.22826622 | temporal 0.66718125-0.22239375 | total 0.54854435\n",
      "Epoch 19, loss: 0.54854435-(best 0.54741156)\n",
      "\n",
      "Detailed Loss: recon 0.28714737-0.09571579 | disen 0.69381928-0.23127309 | temporal 0.66778600-0.22259533 | total 0.54958421\n",
      "Epoch 20, loss: 0.54958421-(best 0.54741156)\n",
      "\n",
      "Detailed Loss: recon 0.28332233-0.09444078 | disen 0.68283087-0.22761029 | temporal 0.66778851-0.22259617 | total 0.54464722\n",
      "Epoch 21, loss: 0.54464722-(best 0.54464722)\n",
      "\n",
      "Detailed Loss: recon 0.28358477-0.09452826 | disen 0.67547637-0.22515879 | temporal 0.66770911-0.22256970 | total 0.54225677\n",
      "Epoch 22, loss: 0.54225677-(best 0.54225677)\n",
      "\n",
      "Detailed Loss: recon 0.28361315-0.09453772 | disen 0.68439889-0.22813296 | temporal 0.66766977-0.22255659 | total 0.54522729\n",
      "Epoch 23, loss: 0.54522729-(best 0.54225677)\n",
      "\n",
      "Detailed Loss: recon 0.28124145-0.09374715 | disen 0.67346412-0.22448804 | temporal 0.66759008-0.22253003 | total 0.54076529\n",
      "Epoch 24, loss: 0.54076529-(best 0.54076529)\n",
      "\n",
      "Detailed Loss: recon 0.27943262-0.09314421 | disen 0.67595994-0.22531998 | temporal 0.66748029-0.22249343 | total 0.54095763\n",
      "Epoch 25, loss: 0.54095763-(best 0.54076529)\n",
      "\n",
      "Detailed Loss: recon 0.27757692-0.09252564 | disen 0.65727746-0.21909249 | temporal 0.66742748-0.22247583 | total 0.53409398\n",
      "Epoch 26, loss: 0.53409398-(best 0.53409398)\n",
      "\n",
      "Detailed Loss: recon 0.27281076-0.09093692 | disen 0.66803867-0.22267956 | temporal 0.66749173-0.22249724 | total 0.53611374\n",
      "Epoch 27, loss: 0.53611374-(best 0.53409398)\n",
      "\n",
      "Detailed Loss: recon 0.27024162-0.09008054 | disen 0.66196829-0.22065610 | temporal 0.66778588-0.22259529 | total 0.53333199\n",
      "Epoch 28, loss: 0.53333199-(best 0.53333199)\n",
      "\n",
      "Detailed Loss: recon 0.27340361-0.09113454 | disen 0.66120899-0.22040300 | temporal 0.66819739-0.22273246 | total 0.53426999\n",
      "Epoch 29, loss: 0.53426999-(best 0.53333199)\n",
      "\n",
      "Detailed Loss: recon 0.27013856-0.09004619 | disen 0.64563304-0.21521101 | temporal 0.66827029-0.22275676 | total 0.52801394\n",
      "Epoch 30, loss: 0.52801394-(best 0.52801394)\n",
      "\n",
      "Detailed Loss: recon 0.26383126-0.08794375 | disen 0.64618516-0.21539505 | temporal 0.66866696-0.22288899 | total 0.52622783\n",
      "Epoch 31, loss: 0.52622783-(best 0.52622783)\n",
      "\n",
      "Detailed Loss: recon 0.26882955-0.08960985 | disen 0.64000458-0.21333486 | temporal 0.66855383-0.22285128 | total 0.52579600\n",
      "Epoch 32, loss: 0.52579600-(best 0.52579600)\n",
      "\n",
      "Detailed Loss: recon 0.26778090-0.08926030 | disen 0.64576221-0.21525407 | temporal 0.66848004-0.22282668 | total 0.52734107\n",
      "Epoch 33, loss: 0.52734107-(best 0.52579600)\n",
      "\n",
      "Detailed Loss: recon 0.26769295-0.08923098 | disen 0.63037968-0.21012656 | temporal 0.66845208-0.22281736 | total 0.52217489\n",
      "Epoch 34, loss: 0.52217489-(best 0.52217489)\n",
      "\n",
      "Detailed Loss: recon 0.26827607-0.08942536 | disen 0.63885784-0.21295261 | temporal 0.66852373-0.22284124 | total 0.52521920\n",
      "Epoch 35, loss: 0.52521920-(best 0.52217489)\n",
      "\n",
      "Detailed Loss: recon 0.25857762-0.08619254 | disen 0.63306522-0.21102174 | temporal 0.66859490-0.22286497 | total 0.52007926\n",
      "Epoch 36, loss: 0.52007926-(best 0.52007926)\n",
      "\n",
      "Detailed Loss: recon 0.26009333-0.08669778 | disen 0.63329661-0.21109887 | temporal 0.66884774-0.22294925 | total 0.52074593\n",
      "Epoch 37, loss: 0.52074593-(best 0.52007926)\n",
      "\n",
      "Detailed Loss: recon 0.26164496-0.08721499 | disen 0.63012576-0.21004192 | temporal 0.66859156-0.22286385 | total 0.52012074\n",
      "Epoch 38, loss: 0.52012074-(best 0.52007926)\n",
      "\n",
      "Detailed Loss: recon 0.26036263-0.08678754 | disen 0.61516762-0.20505587 | temporal 0.66874903-0.22291634 | total 0.51475978\n",
      "Epoch 39, loss: 0.51475978-(best 0.51475978)\n",
      "\n",
      "Detailed Loss: recon 0.25744271-0.08581424 | disen 0.62459052-0.20819684 | temporal 0.66917825-0.22305942 | total 0.51707053\n",
      "Epoch 40, loss: 0.51707053-(best 0.51475978)\n",
      "\n",
      "Detailed Loss: recon 0.25953573-0.08651191 | disen 0.61431366-0.20477122 | temporal 0.66906995-0.22302332 | total 0.51430643\n",
      "Epoch 41, loss: 0.51430643-(best 0.51430643)\n",
      "\n",
      "Detailed Loss: recon 0.25556043-0.08518681 | disen 0.61299956-0.20433319 | temporal 0.66935360-0.22311787 | total 0.51263785\n",
      "Epoch 42, loss: 0.51263785-(best 0.51263785)\n",
      "\n",
      "Detailed Loss: recon 0.25895384-0.08631795 | disen 0.59889090-0.19963030 | temporal 0.66935205-0.22311735 | total 0.50906563\n",
      "Epoch 43, loss: 0.50906563-(best 0.50906563)\n",
      "\n",
      "Detailed Loss: recon 0.25639307-0.08546436 | disen 0.60022461-0.20007487 | temporal 0.66904652-0.22301551 | total 0.50855476\n",
      "Epoch 44, loss: 0.50855476-(best 0.50855476)\n",
      "\n",
      "Detailed Loss: recon 0.25748739-0.08582913 | disen 0.60124624-0.20041541 | temporal 0.66876376-0.22292125 | total 0.50916576\n",
      "Epoch 45, loss: 0.50916576-(best 0.50855476)\n",
      "\n",
      "Detailed Loss: recon 0.25266540-0.08422180 | disen 0.59777135-0.19925712 | temporal 0.66897893-0.22299298 | total 0.50647187\n",
      "Epoch 46, loss: 0.50647187-(best 0.50647187)\n",
      "\n",
      "Detailed Loss: recon 0.25498375-0.08499458 | disen 0.58639097-0.19546366 | temporal 0.66895276-0.22298425 | total 0.50344253\n",
      "Epoch 47, loss: 0.50344253-(best 0.50344253)\n",
      "\n",
      "Detailed Loss: recon 0.25339109-0.08446370 | disen 0.58499646-0.19499882 | temporal 0.66873920-0.22291307 | total 0.50237560\n",
      "Epoch 48, loss: 0.50237560-(best 0.50237560)\n",
      "\n",
      "Detailed Loss: recon 0.25226009-0.08408670 | disen 0.57543182-0.19181061 | temporal 0.66868472-0.22289491 | total 0.49879223\n",
      "Epoch 49, loss: 0.49879223-(best 0.49879223)\n",
      "\n",
      "Detailed Loss: recon 0.25199169-0.08399723 | disen 0.57515228-0.19171743 | temporal 0.66865003-0.22288334 | total 0.49859801\n",
      "Epoch 50, loss: 0.49859801-(best 0.49859801)\n",
      "\n",
      "Detailed Loss: recon 0.25147003-0.08382334 | disen 0.57513517-0.19171172 | temporal 0.66865677-0.22288559 | total 0.49842066\n",
      "Epoch 51, loss: 0.49842066-(best 0.49842066)\n",
      "\n",
      "Detailed Loss: recon 0.25377318-0.08459106 | disen 0.56619775-0.18873258 | temporal 0.66843641-0.22281214 | total 0.49613580\n",
      "Epoch 52, loss: 0.49613580-(best 0.49613580)\n",
      "\n",
      "Detailed Loss: recon 0.25363696-0.08454565 | disen 0.55986214-0.18662071 | temporal 0.66852337-0.22284112 | total 0.49400753\n",
      "Epoch 53, loss: 0.49400753-(best 0.49400753)\n",
      "\n",
      "Detailed Loss: recon 0.25145835-0.08381945 | disen 0.56291580-0.18763860 | temporal 0.66868442-0.22289481 | total 0.49435288\n",
      "Epoch 54, loss: 0.49435288-(best 0.49400753)\n",
      "\n",
      "Detailed Loss: recon 0.25330558-0.08443519 | disen 0.56123829-0.18707943 | temporal 0.66823894-0.22274631 | total 0.49426094\n",
      "Epoch 55, loss: 0.49426094-(best 0.49400753)\n",
      "\n",
      "Detailed Loss: recon 0.25343090-0.08447697 | disen 0.54286540-0.18095513 | temporal 0.66830677-0.22276892 | total 0.48820105\n",
      "Epoch 56, loss: 0.48820105-(best 0.48820105)\n",
      "\n",
      "Detailed Loss: recon 0.25401425-0.08467142 | disen 0.54952717-0.18317572 | temporal 0.66816676-0.22272225 | total 0.49056941\n",
      "Epoch 57, loss: 0.49056941-(best 0.48820105)\n",
      "\n",
      "Detailed Loss: recon 0.25111189-0.08370396 | disen 0.55034721-0.18344907 | temporal 0.66804236-0.22268079 | total 0.48983383\n",
      "Epoch 58, loss: 0.48983383-(best 0.48820105)\n",
      "\n",
      "Detailed Loss: recon 0.25984579-0.08661526 | disen 0.53196239-0.17732080 | temporal 0.66778558-0.22259519 | total 0.48653126\n",
      "Epoch 59, loss: 0.48653126-(best 0.48653126)\n",
      "\n",
      "Detailed Loss: recon 0.25983071-0.08661024 | disen 0.52728552-0.17576184 | temporal 0.66778356-0.22259452 | total 0.48496661\n",
      "Epoch 60, loss: 0.48496661-(best 0.48496661)\n",
      "\n",
      "Detailed Loss: recon 0.26501173-0.08833724 | disen 0.52458906-0.17486302 | temporal 0.66740167-0.22246722 | total 0.48566753\n",
      "Epoch 61, loss: 0.48566753-(best 0.48496661)\n",
      "\n",
      "Detailed Loss: recon 0.25997949-0.08665983 | disen 0.52268589-0.17422863 | temporal 0.66745412-0.22248471 | total 0.48337317\n",
      "Epoch 62, loss: 0.48337317-(best 0.48337317)\n",
      "\n",
      "Detailed Loss: recon 0.26100427-0.08700142 | disen 0.52220374-0.17406791 | temporal 0.66717726-0.22239242 | total 0.48346180\n",
      "Epoch 63, loss: 0.48346180-(best 0.48337317)\n",
      "\n",
      "Detailed Loss: recon 0.26024371-0.08674790 | disen 0.51393306-0.17131102 | temporal 0.66732395-0.22244132 | total 0.48050025\n",
      "Epoch 64, loss: 0.48050025-(best 0.48050025)\n",
      "\n",
      "Detailed Loss: recon 0.26008666-0.08669555 | disen 0.52123117-0.17374372 | temporal 0.66749585-0.22249862 | total 0.48293790\n",
      "Epoch 65, loss: 0.48293790-(best 0.48050025)\n",
      "\n",
      "Detailed Loss: recon 0.25840101-0.08613367 | disen 0.50830156-0.16943385 | temporal 0.66759485-0.22253162 | total 0.47809917\n",
      "Epoch 66, loss: 0.47809917-(best 0.47809917)\n",
      "\n",
      "Detailed Loss: recon 0.26080257-0.08693419 | disen 0.50523931-0.16841310 | temporal 0.66693079-0.22231026 | total 0.47765759\n",
      "Epoch 67, loss: 0.47765759-(best 0.47765759)\n",
      "\n",
      "Detailed Loss: recon 0.26699436-0.08899812 | disen 0.50245214-0.16748405 | temporal 0.66634661-0.22211554 | total 0.47859773\n",
      "Epoch 68, loss: 0.47859773-(best 0.47765759)\n",
      "\n",
      "Detailed Loss: recon 0.25512585-0.08504195 | disen 0.50018895-0.16672965 | temporal 0.66688699-0.22229566 | total 0.47406730\n",
      "Epoch 69, loss: 0.47406730-(best 0.47406730)\n",
      "\n",
      "Detailed Loss: recon 0.25617844-0.08539281 | disen 0.50082290-0.16694097 | temporal 0.66689551-0.22229850 | total 0.47463229\n",
      "Epoch 70, loss: 0.47463229-(best 0.47406730)\n",
      "\n",
      "Detailed Loss: recon 0.26001596-0.08667199 | disen 0.49180305-0.16393435 | temporal 0.66591477-0.22197159 | total 0.47257796\n",
      "Epoch 71, loss: 0.47257796-(best 0.47257796)\n",
      "\n",
      "Detailed Loss: recon 0.26120514-0.08706838 | disen 0.48928785-0.16309595 | temporal 0.66618979-0.22206326 | total 0.47222760\n",
      "Epoch 72, loss: 0.47222760-(best 0.47222760)\n",
      "\n",
      "Detailed Loss: recon 0.25731927-0.08577309 | disen 0.50031984-0.16677328 | temporal 0.66665959-0.22221986 | total 0.47476625\n",
      "Epoch 73, loss: 0.47476625-(best 0.47222760)\n",
      "\n",
      "Detailed Loss: recon 0.25962245-0.08654082 | disen 0.49529397-0.16509799 | temporal 0.66622168-0.22207389 | total 0.47371274\n",
      "Epoch 74, loss: 0.47371274-(best 0.47222760)\n",
      "\n",
      "Detailed Loss: recon 0.25913826-0.08637942 | disen 0.48105597-0.16035199 | temporal 0.66614246-0.22204749 | total 0.46877891\n",
      "Epoch 75, loss: 0.46877891-(best 0.46877891)\n",
      "\n",
      "Detailed Loss: recon 0.26074335-0.08691445 | disen 0.47710520-0.15903507 | temporal 0.66530550-0.22176850 | total 0.46771801\n",
      "Epoch 76, loss: 0.46771801-(best 0.46771801)\n",
      "\n",
      "Detailed Loss: recon 0.25569260-0.08523087 | disen 0.49019235-0.16339745 | temporal 0.66623515-0.22207838 | total 0.47070670\n",
      "Epoch 77, loss: 0.47070670-(best 0.46771801)\n",
      "\n",
      "Detailed Loss: recon 0.25468245-0.08489415 | disen 0.48175442-0.16058481 | temporal 0.66626608-0.22208869 | total 0.46756765\n",
      "Epoch 78, loss: 0.46756765-(best 0.46756765)\n",
      "\n",
      "Detailed Loss: recon 0.26009530-0.08669843 | disen 0.47741520-0.15913840 | temporal 0.66494906-0.22164969 | total 0.46748656\n",
      "Epoch 79, loss: 0.46748656-(best 0.46748656)\n",
      "\n",
      "Detailed Loss: recon 0.26601145-0.08867048 | disen 0.46574008-0.15524669 | temporal 0.66450894-0.22150298 | total 0.46542019\n",
      "Epoch 80, loss: 0.46542019-(best 0.46542019)\n",
      "\n",
      "Detailed Loss: recon 0.26119408-0.08706469 | disen 0.47492450-0.15830817 | temporal 0.66526252-0.22175417 | total 0.46712705\n",
      "Epoch 81, loss: 0.46712705-(best 0.46542019)\n",
      "\n",
      "Detailed Loss: recon 0.25620896-0.08540299 | disen 0.46485418-0.15495139 | temporal 0.66497457-0.22165819 | total 0.46201259\n",
      "Epoch 82, loss: 0.46201259-(best 0.46201259)\n",
      "\n",
      "Detailed Loss: recon 0.25924200-0.08641400 | disen 0.45927715-0.15309238 | temporal 0.66492075-0.22164025 | total 0.46114665\n",
      "Epoch 83, loss: 0.46114665-(best 0.46114665)\n",
      "\n",
      "Detailed Loss: recon 0.26898974-0.08966325 | disen 0.45582920-0.15194307 | temporal 0.66456300-0.22152100 | total 0.46312732\n",
      "Epoch 84, loss: 0.46312732-(best 0.46114665)\n",
      "\n",
      "Detailed Loss: recon 0.26005870-0.08668623 | disen 0.45428187-0.15142729 | temporal 0.66479266-0.22159755 | total 0.45971107\n",
      "Epoch 85, loss: 0.45971107-(best 0.45971107)\n",
      "\n",
      "Detailed Loss: recon 0.25702557-0.08567519 | disen 0.45549870-0.15183290 | temporal 0.66492593-0.22164198 | total 0.45915008\n",
      "Epoch 86, loss: 0.45915008-(best 0.45915008)\n",
      "\n",
      "Detailed Loss: recon 0.25715858-0.08571953 | disen 0.45708638-0.15236213 | temporal 0.66492409-0.22164136 | total 0.45972303\n",
      "Epoch 87, loss: 0.45972303-(best 0.45915008)\n",
      "\n",
      "Detailed Loss: recon 0.25982207-0.08660736 | disen 0.45824212-0.15274737 | temporal 0.66416216-0.22138739 | total 0.46074212\n",
      "Epoch 88, loss: 0.46074212-(best 0.45915008)\n",
      "\n",
      "Detailed Loss: recon 0.26106831-0.08702277 | disen 0.45040578-0.15013526 | temporal 0.66512638-0.22170879 | total 0.45886683\n",
      "Epoch 89, loss: 0.45886683-(best 0.45886683)\n",
      "\n",
      "Detailed Loss: recon 0.26446623-0.08815541 | disen 0.44431609-0.14810536 | temporal 0.66468865-0.22156288 | total 0.45782366\n",
      "Epoch 90, loss: 0.45782366-(best 0.45782366)\n",
      "\n",
      "Detailed Loss: recon 0.25732243-0.08577414 | disen 0.44311661-0.14770554 | temporal 0.66463107-0.22154369 | total 0.45502338\n",
      "Epoch 91, loss: 0.45502338-(best 0.45502338)\n",
      "\n",
      "Detailed Loss: recon 0.26445094-0.08815031 | disen 0.44843769-0.14947923 | temporal 0.66500145-0.22166715 | total 0.45929670\n",
      "Epoch 92, loss: 0.45929670-(best 0.45502338)\n",
      "\n",
      "Detailed Loss: recon 0.26417583-0.08805861 | disen 0.44220591-0.14740197 | temporal 0.66493315-0.22164438 | total 0.45710498\n",
      "Epoch 93, loss: 0.45710498-(best 0.45502338)\n",
      "\n",
      "Detailed Loss: recon 0.26181418-0.08727139 | disen 0.44125390-0.14708463 | temporal 0.66423887-0.22141296 | total 0.45576900\n",
      "Epoch 94, loss: 0.45576900-(best 0.45502338)\n",
      "\n",
      "Detailed Loss: recon 0.26604381-0.08868127 | disen 0.44460857-0.14820286 | temporal 0.66297215-0.22099072 | total 0.45787486\n",
      "Epoch 95, loss: 0.45787486-(best 0.45502338)\n",
      "\n",
      "Detailed Loss: recon 0.25666767-0.08555589 | disen 0.44468433-0.14822811 | temporal 0.66482800-0.22160933 | total 0.45539334\n",
      "Epoch 96, loss: 0.45539334-(best 0.45502338)\n",
      "\n",
      "Detailed Loss: recon 0.26278293-0.08759431 | disen 0.43687344-0.14562448 | temporal 0.66400534-0.22133511 | total 0.45455390\n",
      "Epoch 97, loss: 0.45455390-(best 0.45455390)\n",
      "\n",
      "Detailed Loss: recon 0.26905441-0.08968480 | disen 0.43436217-0.14478739 | temporal 0.66242939-0.22080980 | total 0.45528203\n",
      "Epoch 98, loss: 0.45528203-(best 0.45455390)\n",
      "\n",
      "Detailed Loss: recon 0.25425556-0.08475185 | disen 0.44486332-0.14828777 | temporal 0.66416740-0.22138913 | total 0.45442879\n",
      "Epoch 99, loss: 0.45442879-(best 0.45442879)\n",
      "\n",
      "Detailed Loss: recon 0.25481889-0.08493963 | disen 0.43924439-0.14641480 | temporal 0.66407609-0.22135870 | total 0.45271313\n",
      "Epoch 100, loss: 0.45271313-(best 0.45271313)\n",
      "\n",
      ">> Reached final epoch. Loading best model...\n",
      "TRAINING XGB PREDICTOR\n",
      ">> Ending training!\n",
      "\n",
      "-----------------------\n",
      "TIME COST: 6.072216987609863 s\n",
      "Best Val: REC 61.73 PRE 44.09 MF1 66.95 AUC 79.69 TP 321 FP 407 TN 1425 FN 199\n",
      "-----------------------\n",
      "\n",
      "ASSIGNED GRAPH FOR EVALUATION: 11758 {1: 2566, 0: 9192} Nodes, 1049758 Edges\n",
      "\n",
      "-------------------\n",
      "STARTING EVALUATION\n",
      "-------------------\n",
      "\n",
      "Dataset - Overall: REC 65.00 PRE 44.29 MF1 67.62 AUC 80.31 TP 1668 FP 2098 TN 7094 FN 898 | 11758 {1: 2566, 0: 9192}\n",
      "Dataset - Train: REC 75.80 PRE 48.45 MF1 71.45 AUC 84.53 TP 592 FP 630 TN 2116 FN 189 | 3527 {0: 2746, 1: 781}\n",
      "Dataset - Val: REC 65.77 PRE 41.71 MF1 65.77 AUC 78.66 TP 342 FP 478 TN 1354 FN 178 | 2352 {1: 520, 0: 1832}\n",
      "Dataset - Test: REC 58.02 PRE 42.58 MF1 65.88 AUC 78.42 TP 734 FP 990 TN 3624 FN 531 | 5879 {0: 4614, 1: 1265}\n",
      "PREDICTION STATUS - {'1-True': 2035, '0-False': 3624, '0-True': 5568, '1-False': 531}\n",
      "    >> 531 positive nodes left unpredicted...\n",
      "    >> 531 OG positive nodes left unpredicted...\n",
      "Selecting data as budgeted ground truth for next round...\n",
      "  > Starting round 1...\n",
      "\n",
      "+++++++++++++++++\n",
      "STARTING ROUND 1!\n",
      "+++++++++++++++++\n",
      "        \n",
      "ASSIGNED GRAPH FOR TRAINING: 11758 {1: 2566, 0: 9192} Nodes, 1049758 Edges\n",
      "\n",
      ">> Splitting to train-validation set...\n",
      "Alotting train-val-test split for round 1\n",
      "Initial pool: 5879, Prediction pool: 3766, Budget pool: 0, Full pool: 7603\n",
      "Full graph size: 11758, Training: 4561, Val: 3042, Test: 11758\n",
      "    >> INITIAL DATA SPLIT: 4561 train rows ({1: 1221, 0: 3340}) | 3042 val rows ({1: 814, 0: 2228}) | 4155 test rows ({0: 3624, 1: 531})\n",
      "    >> AUGMENTED DATA SPLIT: 4561 train rows ({1: 1221, 0: 3340}) | 3042 val rows ({1: 814, 0: 2228}) | 4155 test rows ({0: 3624, 1: 531})\n",
      "    >> Updated cross-entropy weight to 2.7354627354627357...\n",
      "\n",
      "-----------------------\n",
      "STARTING MODEL TRAININGps_l\n",
      "-----------------------\n",
      "\n",
      "TRAINING NODE EMBEDDER\n",
      "Detailed Loss: recon 0.23860997-0.07953666 | disen 0.40302813-0.13434271 | temporal 0.55989879-0.18663293 | total 0.40051231\n",
      "Epoch 1, loss: 0.40051231-(best 0.40051231)\n",
      "\n",
      "Detailed Loss: recon 0.31621775-0.10540592 | disen 0.39692998-0.13230999 | temporal 0.56132889-0.18710963 | total 0.42482555\n",
      "Epoch 2, loss: 0.42482555-(best 0.40051231)\n",
      "\n",
      "Detailed Loss: recon 0.31520289-0.10506763 | disen 0.39606529-0.13202176 | temporal 0.56290698-0.18763566 | total 0.42472506\n",
      "Epoch 3, loss: 0.42472506-(best 0.40051231)\n",
      "\n",
      "Detailed Loss: recon 0.27269143-0.09089714 | disen 0.39586401-0.13195467 | temporal 0.56300670-0.18766890 | total 0.41052073\n",
      "Epoch 4, loss: 0.41052073-(best 0.40051231)\n",
      "\n",
      "Detailed Loss: recon 0.26438177-0.08812726 | disen 0.40073121-0.13357707 | temporal 0.56293958-0.18764653 | total 0.40935087\n",
      "Epoch 5, loss: 0.40935087-(best 0.40051231)\n",
      "\n",
      "Detailed Loss: recon 0.25103280-0.08367760 | disen 0.40088063-0.13362688 | temporal 0.56277990-0.18759330 | total 0.40489781\n",
      "Epoch 6, loss: 0.40489781-(best 0.40051231)\n",
      "\n",
      "Detailed Loss: recon 0.23439240-0.07813080 | disen 0.40283269-0.13427756 | temporal 0.56261820-0.18753940 | total 0.39994776\n",
      "Epoch 7, loss: 0.39994776-(best 0.39994776)\n",
      "\n",
      "Detailed Loss: recon 0.23307696-0.07769232 | disen 0.40341604-0.13447201 | temporal 0.56251597-0.18750532 | total 0.39966968\n",
      "Epoch 8, loss: 0.39966968-(best 0.39966968)\n",
      "\n",
      "Detailed Loss: recon 0.22786388-0.07595463 | disen 0.40339017-0.13446339 | temporal 0.56238294-0.18746098 | total 0.39787900\n",
      "Epoch 9, loss: 0.39787900-(best 0.39787900)\n",
      "\n",
      "Detailed Loss: recon 0.22414467-0.07471489 | disen 0.40114319-0.13371440 | temporal 0.56230557-0.18743519 | total 0.39586449\n",
      "Epoch 10, loss: 0.39586449-(best 0.39586449)\n",
      "\n",
      "Detailed Loss: recon 0.22852068-0.07617356 | disen 0.39480209-0.13160070 | temporal 0.56224573-0.18741524 | total 0.39518952\n",
      "Epoch 11, loss: 0.39518952-(best 0.39518952)\n",
      "\n",
      "Detailed Loss: recon 0.22301674-0.07433891 | disen 0.39089131-0.13029710 | temporal 0.56223661-0.18741220 | total 0.39204824\n",
      "Epoch 12, loss: 0.39204824-(best 0.39204824)\n",
      "\n",
      "Detailed Loss: recon 0.22187530-0.07395843 | disen 0.39088255-0.13029418 | temporal 0.56228411-0.18742804 | total 0.39168066\n",
      "Epoch 13, loss: 0.39168066-(best 0.39168066)\n",
      "\n",
      "Detailed Loss: recon 0.21892247-0.07297416 | disen 0.38794720-0.12931573 | temporal 0.56231081-0.18743694 | total 0.38972682\n",
      "Epoch 14, loss: 0.38972682-(best 0.38972682)\n",
      "\n",
      "Detailed Loss: recon 0.22114073-0.07371358 | disen 0.38075614-0.12691871 | temporal 0.56217664-0.18739221 | total 0.38802451\n",
      "Epoch 15, loss: 0.38802451-(best 0.38802451)\n",
      "\n",
      "Detailed Loss: recon 0.21996570-0.07332190 | disen 0.37528014-0.12509338 | temporal 0.56201237-0.18733746 | total 0.38575274\n",
      "Epoch 16, loss: 0.38575274-(best 0.38575274)\n",
      "\n",
      "Detailed Loss: recon 0.21890740-0.07296913 | disen 0.37277865-0.12425955 | temporal 0.56184369-0.18728123 | total 0.38450992\n",
      "Epoch 17, loss: 0.38450992-(best 0.38450992)\n",
      "\n",
      "Detailed Loss: recon 0.22086278-0.07362093 | disen 0.36872011-0.12290670 | temporal 0.56175339-0.18725113 | total 0.38377875\n",
      "Epoch 18, loss: 0.38377875-(best 0.38377875)\n",
      "\n",
      "Detailed Loss: recon 0.21805081-0.07268360 | disen 0.36852348-0.12284116 | temporal 0.56169558-0.18723186 | total 0.38275665\n",
      "Epoch 19, loss: 0.38275665-(best 0.38275665)\n",
      "\n",
      "Detailed Loss: recon 0.21927229-0.07309076 | disen 0.36223042-0.12074347 | temporal 0.56155074-0.18718358 | total 0.38101783\n",
      "Epoch 20, loss: 0.38101783-(best 0.38101783)\n",
      "\n",
      "Detailed Loss: recon 0.21554565-0.07184855 | disen 0.35915148-0.11971716 | temporal 0.56150693-0.18716898 | total 0.37873471\n",
      "Epoch 21, loss: 0.37873471-(best 0.37873471)\n",
      "\n",
      "Detailed Loss: recon 0.21449898-0.07149966 | disen 0.35803878-0.11934626 | temporal 0.56168884-0.18722961 | total 0.37807554\n",
      "Epoch 22, loss: 0.37807554-(best 0.37807554)\n",
      "\n",
      "Detailed Loss: recon 0.21446797-0.07148932 | disen 0.35702962-0.11900987 | temporal 0.56171501-0.18723834 | total 0.37773752\n",
      "Epoch 23, loss: 0.37773752-(best 0.37773752)\n",
      "\n",
      "Detailed Loss: recon 0.21525446-0.07175149 | disen 0.35621876-0.11873959 | temporal 0.56173831-0.18724610 | total 0.37773719\n",
      "Epoch 24, loss: 0.37773719-(best 0.37773719)\n",
      "\n",
      "Detailed Loss: recon 0.21772063-0.07257354 | disen 0.35166180-0.11722060 | temporal 0.56158698-0.18719566 | total 0.37698981\n",
      "Epoch 25, loss: 0.37698981-(best 0.37698981)\n",
      "\n",
      "Detailed Loss: recon 0.21354561-0.07118187 | disen 0.35029483-0.11676494 | temporal 0.56174308-0.18724769 | total 0.37519452\n",
      "Epoch 26, loss: 0.37519452-(best 0.37519452)\n",
      "\n",
      "Detailed Loss: recon 0.21260887-0.07086962 | disen 0.34828013-0.11609338 | temporal 0.56169593-0.18723198 | total 0.37419498\n",
      "Epoch 27, loss: 0.37419498-(best 0.37419498)\n",
      "\n",
      "Detailed Loss: recon 0.21289787-0.07096596 | disen 0.34511387-0.11503796 | temporal 0.56165934-0.18721978 | total 0.37322372\n",
      "Epoch 28, loss: 0.37322372-(best 0.37322372)\n",
      "\n",
      "Detailed Loss: recon 0.21210256-0.07070085 | disen 0.34341073-0.11447024 | temporal 0.56168401-0.18722800 | total 0.37239909\n",
      "Epoch 29, loss: 0.37239909-(best 0.37239909)\n",
      "\n",
      "Detailed Loss: recon 0.21200345-0.07066782 | disen 0.34359503-0.11453168 | temporal 0.56172985-0.18724328 | total 0.37244278\n",
      "Epoch 30, loss: 0.37244278-(best 0.37239909)\n",
      "\n",
      "Detailed Loss: recon 0.21178511-0.07059504 | disen 0.34019464-0.11339821 | temporal 0.56154698-0.18718233 | total 0.37117559\n",
      "Epoch 31, loss: 0.37117559-(best 0.37117559)\n",
      "\n",
      "Detailed Loss: recon 0.21487877-0.07162626 | disen 0.33827704-0.11275901 | temporal 0.56130117-0.18710039 | total 0.37148565\n",
      "Epoch 32, loss: 0.37148565-(best 0.37117559)\n",
      "\n",
      "Detailed Loss: recon 0.20999661-0.06999887 | disen 0.33884352-0.11294784 | temporal 0.56136364-0.18712121 | total 0.37006792\n",
      "Epoch 33, loss: 0.37006792-(best 0.37006792)\n",
      "\n",
      "Detailed Loss: recon 0.21011749-0.07003916 | disen 0.33725566-0.11241855 | temporal 0.56130105-0.18710035 | total 0.36955807\n",
      "Epoch 34, loss: 0.36955807-(best 0.36955807)\n",
      "\n",
      "Detailed Loss: recon 0.21035653-0.07011884 | disen 0.33401626-0.11133875 | temporal 0.56114215-0.18704738 | total 0.36850500\n",
      "Epoch 35, loss: 0.36850500-(best 0.36850500)\n",
      "\n",
      "Detailed Loss: recon 0.21368477-0.07122826 | disen 0.33392471-0.11130824 | temporal 0.56110954-0.18703651 | total 0.36957300\n",
      "Epoch 36, loss: 0.36957300-(best 0.36850500)\n",
      "\n",
      "Detailed Loss: recon 0.21082383-0.07027461 | disen 0.33252370-0.11084123 | temporal 0.56113791-0.18704597 | total 0.36816183\n",
      "Epoch 37, loss: 0.36816183-(best 0.36816183)\n",
      "\n",
      "Detailed Loss: recon 0.21036980-0.07012327 | disen 0.33229661-0.11076554 | temporal 0.56103039-0.18701013 | total 0.36789894\n",
      "Epoch 38, loss: 0.36789894-(best 0.36789894)\n",
      "\n",
      "Detailed Loss: recon 0.20856157-0.06952052 | disen 0.32887030-0.10962343 | temporal 0.56084275-0.18694758 | total 0.36609155\n",
      "Epoch 39, loss: 0.36609155-(best 0.36609155)\n",
      "\n",
      "Detailed Loss: recon 0.21072404-0.07024135 | disen 0.32853079-0.10951026 | temporal 0.56081653-0.18693884 | total 0.36669046\n",
      "Epoch 40, loss: 0.36669046-(best 0.36609155)\n",
      "\n",
      "Detailed Loss: recon 0.20927382-0.06975794 | disen 0.32811433-0.10937144 | temporal 0.56086695-0.18695565 | total 0.36608505\n",
      "Epoch 41, loss: 0.36608505-(best 0.36608505)\n",
      "\n",
      "Detailed Loss: recon 0.20923162-0.06974387 | disen 0.32799292-0.10933097 | temporal 0.56079471-0.18693157 | total 0.36600643\n",
      "Epoch 42, loss: 0.36600643-(best 0.36600643)\n",
      "\n",
      "Detailed Loss: recon 0.20806497-0.06935499 | disen 0.32521492-0.10840497 | temporal 0.56088221-0.18696074 | total 0.36472070\n",
      "Epoch 43, loss: 0.36472070-(best 0.36472070)\n",
      "\n",
      "Detailed Loss: recon 0.20798181-0.06932727 | disen 0.32418919-0.10806306 | temporal 0.56089002-0.18696334 | total 0.36435369\n",
      "Epoch 44, loss: 0.36435369-(best 0.36435369)\n",
      "\n",
      "Detailed Loss: recon 0.20902172-0.06967391 | disen 0.32388824-0.10796275 | temporal 0.56084538-0.18694846 | total 0.36458510\n",
      "Epoch 45, loss: 0.36458510-(best 0.36435369)\n",
      "\n",
      "Detailed Loss: recon 0.20688641-0.06896214 | disen 0.32243472-0.10747824 | temporal 0.56082684-0.18694228 | total 0.36338267\n",
      "Epoch 46, loss: 0.36338267-(best 0.36338267)\n",
      "\n",
      "Detailed Loss: recon 0.20710747-0.06903582 | disen 0.32108337-0.10702779 | temporal 0.56067824-0.18689275 | total 0.36295637\n",
      "Epoch 47, loss: 0.36295637-(best 0.36295637)\n",
      "\n",
      "Detailed Loss: recon 0.20742783-0.06914261 | disen 0.32044888-0.10681629 | temporal 0.56059486-0.18686495 | total 0.36282384\n",
      "Epoch 48, loss: 0.36282384-(best 0.36282384)\n",
      "\n",
      "Detailed Loss: recon 0.21146600-0.07048867 | disen 0.31797409-0.10599136 | temporal 0.56047773-0.18682591 | total 0.36330593\n",
      "Epoch 49, loss: 0.36330593-(best 0.36282384)\n",
      "\n",
      "Detailed Loss: recon 0.20962787-0.06987596 | disen 0.31931955-0.10643985 | temporal 0.56064171-0.18688057 | total 0.36319637\n",
      "Epoch 50, loss: 0.36319637-(best 0.36282384)\n",
      "\n",
      ">> Reached final epoch. Loading best model...\n",
      "TRAINING XGB PREDICTOR\n",
      ">> Ending training!\n",
      "\n",
      "-----------------------\n",
      "TIME COST: 7.8864662647247314 s\n",
      "Best Val: REC 56.88 PRE 44.69 MF1 64.12 AUC 71.82 TP 463 FP 573 TN 1655 FN 351\n",
      "-----------------------\n",
      "Generating additional positive data from adversary...\n",
      "Generating additional negative data by duplicating random nodes...\n",
      "\n",
      "ASSIGNED GRAPH FOR EVALUATION: 12345 {1: 2694, 0: 9651} Nodes, 1168874 Edges\n",
      "\n",
      "-------------------\n",
      "STARTING EVALUATION\n",
      "-------------------\n",
      "\n",
      "Dataset - Overall: REC 45.77 PRE 46.06 MF1 65.44 AUC 73.14 TP 1233 FP 1444 TN 8207 FN 1461 | 12345 {1: 2694, 0: 9651}\n",
      "Dataset - Train: REC 54.87 PRE 55.69 MF1 69.55 AUC 77.52 TP 670 FP 533 TN 2807 FN 551 | 4561 {1: 1221, 0: 3340}\n",
      "Dataset - Val: REC 46.93 PRE 46.76 MF1 63.69 AUC 69.70 TP 382 FP 435 TN 1793 FN 432 | 3042 {1: 814, 0: 2228}\n",
      "Dataset - Test: REC 27.47 PRE 27.55 MF1 57.91 AUC 66.24 TP 181 FP 476 TN 3607 FN 478 | 4742 {0: 4083, 1: 659}\n",
      "PREDICTION STATUS - {'1-True': 2216, '0-False': 3607, '0-True': 6044, '1-False': 478}\n",
      "    >> 478 positive nodes left unpredicted...\n",
      "    >> 383 OG positive nodes left unpredicted...\n",
      "-----------------------\n",
      "PREDICTION RESULT - ROUNDS\n",
      "Dataset - Round 0: REC 47.59 PRE 45.03 MF1 65.50 AUC 72.66 TP 602 FP 735 TN 3879 FN 663 | 5879 {0: 4614, 1: 1265}\n",
      "Dataset - Round 1: REC 25.78 PRE 30.84 MF1 55.04 AUC 61.37 TP 33 FP 74 TN 385 FN 95 | 587 {1: 128, 0: 459}\n",
      "-----------------------\n",
      "PREDICTION RESULT - SEEDS\n",
      "Seeds - Current: REC 28.91 PRE 38.14 MF1 58.49 AUC 60.92 TP 37 FP 60 TN 399 FN 91 | 587 {1: 128, 0: 459}\n",
      "Seeds - Prev: REC 0.00 PRE 0.00 MF1 38.15 AUC 60.92 TP 0 FP 97 TN 362 FN 128 | 587 {1: 128, 0: 459}\n",
      "Selecting data as budgeted ground truth for next round...\n",
      "  > Starting round 2...\n",
      "\n",
      "+++++++++++++++++\n",
      "STARTING ROUND 2!\n",
      "+++++++++++++++++\n",
      "        \n",
      "ASSIGNED GRAPH FOR TRAINING: 12345 {1: 2694, 0: 9651} Nodes, 1168874 Edges\n",
      "\n",
      ">> Splitting to train-validation set...\n",
      "Alotting train-val-test split for round 2\n",
      "Initial pool: 5879, Prediction pool: 6443, Budget pool: 0, Full pool: 8260\n",
      "Full graph size: 12345, Training: 4956, Val: 3304, Test: 12345\n",
      "    >> INITIAL DATA SPLIT: 4956 train rows ({1: 1330, 0: 3626}) | 3304 val rows ({0: 2418, 1: 886}) | 4085 test rows ({0: 3607, 1: 478})\n",
      "    >> AUGMENTED DATA SPLIT: 4956 train rows ({1: 1330, 0: 3626}) | 3304 val rows ({0: 2418, 1: 886}) | 4085 test rows ({0: 3607, 1: 478})\n",
      "    >> Updated cross-entropy weight to 2.7263157894736842...\n",
      "\n",
      "-----------------------\n",
      "STARTING MODEL TRAININGps_l\n",
      "-----------------------\n",
      "\n",
      "TRAINING NODE EMBEDDER\n",
      "Detailed Loss: recon 0.21569887-0.07189962 | disen 0.32755035-0.10918345 | temporal 0.58198988-0.19399663 | total 0.37507972\n",
      "Epoch 1, loss: 0.37507972-(best 0.37507972)\n",
      "\n",
      "Detailed Loss: recon 0.26655191-0.08885064 | disen 0.34744310-0.11581437 | temporal 0.58337826-0.19445942 | total 0.39912444\n",
      "Epoch 2, loss: 0.39912444-(best 0.37507972)\n",
      "\n",
      "Detailed Loss: recon 0.27627024-0.09209008 | disen 0.37916559-0.12638853 | temporal 0.58405900-0.19468633 | total 0.41316497\n",
      "Epoch 3, loss: 0.41316497-(best 0.37507972)\n",
      "\n",
      "Detailed Loss: recon 0.25978664-0.08659555 | disen 0.37777627-0.12592542 | temporal 0.58434159-0.19478053 | total 0.40730149\n",
      "Epoch 4, loss: 0.40730149-(best 0.37507972)\n",
      "\n",
      "Detailed Loss: recon 0.24139822-0.08046607 | disen 0.36705112-0.12235037 | temporal 0.58448899-0.19482966 | total 0.39764613\n",
      "Epoch 5, loss: 0.39764613-(best 0.37507972)\n",
      "\n",
      "Detailed Loss: recon 0.23068461-0.07689487 | disen 0.35901272-0.11967091 | temporal 0.58442885-0.19480962 | total 0.39137539\n",
      "Epoch 6, loss: 0.39137539-(best 0.37507972)\n",
      "\n",
      "Detailed Loss: recon 0.22525975-0.07508658 | disen 0.36132431-0.12044144 | temporal 0.58443785-0.19481262 | total 0.39034066\n",
      "Epoch 7, loss: 0.39034066-(best 0.37507972)\n",
      "\n",
      "Detailed Loss: recon 0.21777138-0.07259046 | disen 0.36692649-0.12230883 | temporal 0.58436406-0.19478802 | total 0.38968730\n",
      "Epoch 8, loss: 0.38968730-(best 0.37507972)\n",
      "\n",
      "Detailed Loss: recon 0.21225229-0.07075076 | disen 0.36736995-0.12245665 | temporal 0.58397257-0.19465752 | total 0.38786495\n",
      "Epoch 9, loss: 0.38786495-(best 0.37507972)\n",
      "\n",
      "Detailed Loss: recon 0.22013047-0.07337682 | disen 0.36212385-0.12070795 | temporal 0.58360571-0.19453524 | total 0.38862002\n",
      "Epoch 10, loss: 0.38862002-(best 0.37507972)\n",
      "\n",
      "Detailed Loss: recon 0.22343698-0.07447899 | disen 0.35733074-0.11911025 | temporal 0.58344841-0.19448280 | total 0.38807204\n",
      "Epoch 11, loss: 0.38807204-(best 0.37507972)\n",
      "\n",
      "Detailed Loss: recon 0.21480525-0.07160175 | disen 0.35292459-0.11764153 | temporal 0.58356428-0.19452143 | total 0.38376471\n",
      "Epoch 12, loss: 0.38376471-(best 0.37507972)\n",
      "\n",
      "Detailed Loss: recon 0.20957415-0.06985805 | disen 0.35159987-0.11719996 | temporal 0.58377868-0.19459289 | total 0.38165089\n",
      "Epoch 13, loss: 0.38165089-(best 0.37507972)\n",
      "\n",
      "Detailed Loss: recon 0.21434878-0.07144959 | disen 0.34893388-0.11631129 | temporal 0.58399624-0.19466541 | total 0.38242632\n",
      "Epoch 14, loss: 0.38242632-(best 0.37507972)\n",
      "\n",
      "Detailed Loss: recon 0.21234357-0.07078119 | disen 0.34618109-0.11539370 | temporal 0.58398128-0.19466043 | total 0.38083532\n",
      "Epoch 15, loss: 0.38083532-(best 0.37507972)\n",
      "\n",
      "Detailed Loss: recon 0.21008788-0.07002929 | disen 0.34370154-0.11456718 | temporal 0.58387411-0.19462470 | total 0.37922120\n",
      "Epoch 16, loss: 0.37922120-(best 0.37507972)\n",
      "\n",
      "Detailed Loss: recon 0.21497010-0.07165670 | disen 0.34219342-0.11406447 | temporal 0.58372873-0.19457624 | total 0.38029742\n",
      "Epoch 17, loss: 0.38029742-(best 0.37507972)\n",
      "\n",
      "Detailed Loss: recon 0.21352927-0.07117642 | disen 0.33484691-0.11161564 | temporal 0.58354682-0.19451561 | total 0.37730768\n",
      "Epoch 18, loss: 0.37730768-(best 0.37507972)\n",
      "\n",
      "Detailed Loss: recon 0.21261375-0.07087125 | disen 0.33741575-0.11247192 | temporal 0.58359790-0.19453263 | total 0.37787580\n",
      "Epoch 19, loss: 0.37787580-(best 0.37507972)\n",
      "\n",
      "Detailed Loss: recon 0.21215121-0.07071707 | disen 0.33529204-0.11176401 | temporal 0.58351469-0.19450490 | total 0.37698597\n",
      "Epoch 20, loss: 0.37698597-(best 0.37507972)\n",
      "\n",
      "Detailed Loss: recon 0.20962398-0.06987466 | disen 0.33502316-0.11167439 | temporal 0.58349967-0.19449989 | total 0.37604892\n",
      "Epoch 21, loss: 0.37604892-(best 0.37507972)\n",
      "\n",
      "Detailed Loss: recon 0.20912677-0.06970892 | disen 0.33386290-0.11128763 | temporal 0.58335465-0.19445155 | total 0.37544811\n",
      "Epoch 22, loss: 0.37544811-(best 0.37507972)\n",
      "\n",
      "Detailed Loss: recon 0.20988846-0.06996282 | disen 0.33034325-0.11011442 | temporal 0.58301944-0.19433981 | total 0.37441707\n",
      "Epoch 23, loss: 0.37441707-(best 0.37441707)\n",
      "\n",
      "Detailed Loss: recon 0.21029013-0.07009671 | disen 0.33025503-0.11008501 | temporal 0.58288968-0.19429656 | total 0.37447828\n",
      "Epoch 24, loss: 0.37447828-(best 0.37441707)\n",
      "\n",
      "Detailed Loss: recon 0.20986378-0.06995459 | disen 0.32654768-0.10884923 | temporal 0.58280861-0.19426954 | total 0.37307337\n",
      "Epoch 25, loss: 0.37307337-(best 0.37307337)\n",
      "\n",
      "Detailed Loss: recon 0.20723033-0.06907678 | disen 0.32544106-0.10848035 | temporal 0.58280367-0.19426789 | total 0.37182504\n",
      "Epoch 26, loss: 0.37182504-(best 0.37182504)\n",
      "\n",
      "Detailed Loss: recon 0.20803981-0.06934660 | disen 0.32524687-0.10841562 | temporal 0.58274472-0.19424824 | total 0.37201047\n",
      "Epoch 27, loss: 0.37201047-(best 0.37182504)\n",
      "\n",
      "Detailed Loss: recon 0.20780769-0.06926923 | disen 0.32248622-0.10749541 | temporal 0.58257586-0.19419195 | total 0.37095660\n",
      "Epoch 28, loss: 0.37095660-(best 0.37095660)\n",
      "\n",
      "Detailed Loss: recon 0.20893911-0.06964637 | disen 0.32271999-0.10757333 | temporal 0.58255988-0.19418663 | total 0.37140632\n",
      "Epoch 29, loss: 0.37140632-(best 0.37095660)\n",
      "\n",
      "Detailed Loss: recon 0.20710745-0.06903582 | disen 0.32150304-0.10716768 | temporal 0.58274120-0.19424707 | total 0.37045056\n",
      "Epoch 30, loss: 0.37045056-(best 0.37045056)\n",
      "\n",
      "Detailed Loss: recon 0.20769280-0.06923093 | disen 0.32033557-0.10677852 | temporal 0.58279061-0.19426354 | total 0.37027299\n",
      "Epoch 31, loss: 0.37027299-(best 0.37027299)\n",
      "\n",
      "Detailed Loss: recon 0.20544001-0.06848000 | disen 0.31832832-0.10610944 | temporal 0.58264643-0.19421548 | total 0.36880493\n",
      "Epoch 32, loss: 0.36880493-(best 0.36880493)\n",
      "\n",
      "Detailed Loss: recon 0.20971712-0.06990571 | disen 0.31740320-0.10580107 | temporal 0.58248878-0.19416293 | total 0.36986971\n",
      "Epoch 33, loss: 0.36986971-(best 0.36880493)\n",
      "\n",
      "Detailed Loss: recon 0.20813549-0.06937850 | disen 0.31544822-0.10514941 | temporal 0.58242637-0.19414212 | total 0.36867005\n",
      "Epoch 34, loss: 0.36867005-(best 0.36867005)\n",
      "\n",
      "Detailed Loss: recon 0.20760541-0.06920180 | disen 0.31497407-0.10499136 | temporal 0.58242661-0.19414220 | total 0.36833537\n",
      "Epoch 35, loss: 0.36833537-(best 0.36833537)\n",
      "\n",
      "Detailed Loss: recon 0.20522240-0.06840747 | disen 0.31398809-0.10466270 | temporal 0.58234590-0.19411530 | total 0.36718547\n",
      "Epoch 36, loss: 0.36718547-(best 0.36718547)\n",
      "\n",
      "Detailed Loss: recon 0.20791787-0.06930596 | disen 0.31187660-0.10395887 | temporal 0.58204919-0.19401640 | total 0.36728123\n",
      "Epoch 37, loss: 0.36728123-(best 0.36718547)\n",
      "\n",
      "Detailed Loss: recon 0.21131825-0.07043942 | disen 0.31136370-0.10378790 | temporal 0.58193624-0.19397875 | total 0.36820608\n",
      "Epoch 38, loss: 0.36820608-(best 0.36718547)\n",
      "\n",
      "Detailed Loss: recon 0.20926259-0.06975420 | disen 0.30995870-0.10331957 | temporal 0.58194882-0.19398294 | total 0.36705673\n",
      "Epoch 39, loss: 0.36705673-(best 0.36705673)\n",
      "\n",
      "Detailed Loss: recon 0.20661300-0.06887100 | disen 0.30902714-0.10300905 | temporal 0.58203709-0.19401236 | total 0.36589241\n",
      "Epoch 40, loss: 0.36589241-(best 0.36589241)\n",
      "\n",
      "Detailed Loss: recon 0.20952533-0.06984178 | disen 0.30820042-0.10273347 | temporal 0.58189291-0.19396430 | total 0.36653957\n",
      "Epoch 41, loss: 0.36653957-(best 0.36589241)\n",
      "\n",
      "Detailed Loss: recon 0.20543437-0.06847812 | disen 0.30852860-0.10284287 | temporal 0.58195418-0.19398473 | total 0.36530572\n",
      "Epoch 42, loss: 0.36530572-(best 0.36530572)\n",
      "\n",
      "Detailed Loss: recon 0.20396507-0.06798836 | disen 0.30892360-0.10297453 | temporal 0.58198810-0.19399603 | total 0.36495894\n",
      "Epoch 43, loss: 0.36495894-(best 0.36495894)\n",
      "\n",
      "Detailed Loss: recon 0.20485358-0.06828453 | disen 0.30877948-0.10292649 | temporal 0.58194256-0.19398085 | total 0.36519188\n",
      "Epoch 44, loss: 0.36519188-(best 0.36495894)\n",
      "\n",
      "Detailed Loss: recon 0.20628861-0.06876287 | disen 0.30638397-0.10212799 | temporal 0.58166867-0.19388956 | total 0.36478043\n",
      "Epoch 45, loss: 0.36478043-(best 0.36478043)\n",
      "\n",
      "Detailed Loss: recon 0.20576495-0.06858832 | disen 0.30509818-0.10169939 | temporal 0.58154708-0.19384903 | total 0.36413676\n",
      "Epoch 46, loss: 0.36413676-(best 0.36413676)\n",
      "\n",
      "Detailed Loss: recon 0.20658837-0.06886279 | disen 0.30500233-0.10166744 | temporal 0.58152121-0.19384040 | total 0.36437064\n",
      "Epoch 47, loss: 0.36437064-(best 0.36413676)\n",
      "\n",
      "Detailed Loss: recon 0.20517038-0.06839013 | disen 0.30507112-0.10169037 | temporal 0.58158016-0.19386005 | total 0.36394057\n",
      "Epoch 48, loss: 0.36394057-(best 0.36394057)\n",
      "\n",
      "Detailed Loss: recon 0.20283888-0.06761296 | disen 0.30435807-0.10145269 | temporal 0.58152574-0.19384191 | total 0.36290759\n",
      "Epoch 49, loss: 0.36290759-(best 0.36290759)\n",
      "\n",
      "Detailed Loss: recon 0.20526442-0.06842147 | disen 0.30284035-0.10094678 | temporal 0.58148950-0.19382983 | total 0.36319810\n",
      "Epoch 50, loss: 0.36319810-(best 0.36290759)\n",
      "\n",
      ">> Reached final epoch. Loading best model...\n",
      "TRAINING XGB PREDICTOR\n",
      ">> Ending training!\n",
      "\n",
      "-----------------------\n",
      "TIME COST: 8.024938106536865 s\n",
      "Best Val: REC 48.08 PRE 48.46 MF1 64.71 AUC 72.49 TP 426 FP 453 TN 1965 FN 460\n",
      "-----------------------\n",
      "Generating additional positive data from adversary...\n",
      "Generating additional negative data by duplicating random nodes...\n",
      "\n",
      "ASSIGNED GRAPH FOR EVALUATION: 12932 {1: 2822, 0: 10110} Nodes, 1280900 Edges\n",
      "\n",
      "-------------------\n",
      "STARTING EVALUATION\n",
      "-------------------\n",
      "\n",
      "Dataset - Overall: REC 36.78 PRE 49.33 MF1 64.27 AUC 71.84 TP 1038 FP 1066 TN 9044 FN 1784 | 12932 {1: 2822, 0: 10110}\n",
      "Dataset - Train: REC 44.96 PRE 56.63 MF1 67.16 AUC 75.24 TP 598 FP 458 TN 3168 FN 732 | 4956 {1: 1330, 0: 3626}\n",
      "Dataset - Val: REC 37.13 PRE 48.53 MF1 62.05 AUC 69.34 TP 329 FP 349 TN 2069 FN 557 | 3304 {0: 2418, 1: 886}\n",
      "Dataset - Test: REC 18.32 PRE 30.00 MF1 56.87 AUC 62.55 TP 111 FP 259 TN 3807 FN 495 | 4672 {0: 4066, 1: 606}\n",
      "PREDICTION STATUS - {'1-True': 2327, '0-False': 3807, '0-True': 6303, '1-False': 495}\n",
      "    >> 495 positive nodes left unpredicted...\n",
      "    >> 305 OG positive nodes left unpredicted...\n",
      "-----------------------\n",
      "PREDICTION RESULT - ROUNDS\n",
      "Dataset - Round 0: REC 38.89 PRE 48.47 MF1 64.74 AUC 71.36 TP 492 FP 523 TN 4091 FN 773 | 5879 {0: 4614, 1: 1265}\n",
      "Dataset - Round 1: REC 22.66 PRE 35.37 MF1 55.93 AUC 61.44 TP 29 FP 53 TN 406 FN 99 | 587 {1: 128, 0: 459}\n",
      "Dataset - Round 2: REC 11.72 PRE 26.32 MF1 50.27 AUC 56.25 TP 15 FP 42 TN 417 FN 113 | 587 {1: 128, 0: 459}\n",
      "-----------------------\n",
      "PREDICTION RESULT - SEEDS\n",
      "Seeds - Current: REC 24.22 PRE 37.80 MF1 57.09 AUC 59.18 TP 31 FP 51 TN 408 FN 97 | 587 {1: 128, 0: 459}\n",
      "Seeds - Prev: REC 0.00 PRE 0.00 MF1 39.67 AUC 59.18 TP 0 FP 73 TN 386 FN 128 | 587 {1: 128, 0: 459}\n",
      "Selecting data as budgeted ground truth for next round...\n",
      "The file ../checkpoint/working_model_file_1730091858.4208176_epoch.py does not exist\n",
      "The file ../checkpoint/working_model_file_1730091858.4208176_round-M.json does not exist\n",
      "The file ../checkpoint/working_model_file_1730091858.4208176_round-T.json does not exist\n",
      "The file ../checkpoint/working_model_file_1730091858.4208176_round.pt does not exist\n",
      "Experiment ended, experienced 0 failures\n",
      "Elapsed experiment time 66.97445893s\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import datetime\n",
    "import itertools\n",
    "import re\n",
    "import gc\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import dgl\n",
    "import pandas as pd\n",
    "\n",
    "from time import time\n",
    "from experiment.supervised_multi import MultiroundExperiment\n",
    "from utils.utils_const import DEFAULT_MAIN_CONFIG, DEFAULT_TRAIN_CONFIG, DEFAULT_ADVER_CONFIG, DEFAULT_MODEL_CONFIG,  DEFAULT_STRAT_CONFIG\n",
    "from utils.utils_const import LOSS_DICT, BACKBONE_DICT\n",
    "\n",
    "main_config = DEFAULT_MAIN_CONFIG.copy()\n",
    "train_config = DEFAULT_TRAIN_CONFIG.copy()\n",
    "model_config = DEFAULT_MODEL_CONFIG.copy()\n",
    "strat_config = DEFAULT_STRAT_CONFIG.copy()\n",
    "adver_config = DEFAULT_ADVER_CONFIG.copy()\n",
    "\n",
    "def main():\n",
    "    # HARD CONFIG\n",
    "    TRIAL_NUM = 1\n",
    "    FAILURE_LIMIT = 10\n",
    "\n",
    "    # EXPERIMENT PARAMETER GRID - Full List on utils/utils_const.py\n",
    "    EXPERIMENT_DESC = \"Example experiment - this description will be written to the log .txt file\"\n",
    "    LIST_DSET = ['tolokers_bid'] # Array of datasets\n",
    "    LIST_TRAIN_DSET = ['NONE'] # Array of datasets for training set on round #0; should be set to 'NONE' except for 'tcdataset_all' (which should be set to 'tcdataset_tr')\n",
    "    EXP_DICT = {\n",
    "      'device': ['cuda:0'],\n",
    "      'exp_type': ['ADVER'],\n",
    "      'round_num': [5],\n",
    "\n",
    "      'model_name': ['XGB-SP'],\n",
    "      'num_epoch': [100],\n",
    "      'num_round_epoch': [50],\n",
    "      'round_reset_model': [False],\n",
    "      \n",
    "      'h_feats': [64],\n",
    "      'num_layers': [2],\n",
    "      'round_window': [7],\n",
    "      'norm_name': ['layer'],\n",
    "      'temporal_agg': ['mean_final'],\n",
    "\n",
    "      'alpha': [0, 1],\n",
    "      'beta': [0, 1],\n",
    "\n",
    "      'augment_name': ['REAGE'],\n",
    "    }\n",
    "\n",
    "    outer_dfs = []\n",
    "    ts = datetime.datetime.now().strftime(\"%y%m%d%H%M%S\")\n",
    "\n",
    "    # WRITE LOG FIRST\n",
    "    if not os.path.exists(f'../result/{ts}'):\n",
    "        os.mkdir(f'../result/{ts}')\n",
    "\n",
    "    f = open(f'../result/{ts}/meta.txt', \"a\")\n",
    "    f.write(f'{EXPERIMENT_DESC}\\n\\n')\n",
    "    f.write(f'MAIN CONFIG\\n{str(main_config)}\\n\\n')\n",
    "    f.write(f'TRAIN CONFIG\\n{str(train_config)}\\n\\n')\n",
    "    f.write(f'MODEL CONFIG\\n{str(model_config)}\\n\\n')\n",
    "    f.write(f'STRAT CONFIG\\n{str(strat_config)}\\n\\n')\n",
    "    f.write(f'ADVER CONFIG\\n{str(adver_config)}\\n\\n')\n",
    "    f.write(f'EXP DICT\\n{str(EXP_DICT)}\\n\\n')\n",
    "  \n",
    "    f.close()\n",
    "\n",
    "    ### DATASET STUFF\n",
    "    for idx, dset in enumerate(LIST_DSET):\n",
    "      dataset, _ = dgl.load_graphs(f'../dataset/{dset}')\n",
    "      graph = dataset[0].long()\n",
    "    \n",
    "      if len(graph.ndata['label'].shape) > 1:\n",
    "          graph.ndata['label'] = graph.ndata['label'].argmax(1)\n",
    "          graph.ndata['label'] = graph.ndata['label'].long().squeeze(-1)\n",
    "          \n",
    "      graph.ndata['feature'] = graph.ndata['feature'].float()\n",
    "      train_config['dset_name'] = dset\n",
    "    \n",
    "      if LIST_TRAIN_DSET[idx] != 'NONE':\n",
    "        dataset, _ = dgl.load_graphs(f'../dataset/{LIST_TRAIN_DSET[idx]}')\n",
    "        train_graph = dataset[0].long()\n",
    "    \n",
    "        if len(train_graph.ndata['label'].shape) > 1:\n",
    "            train_graph.ndata['label'] = train_graph.ndata['label'].argmax(1)\n",
    "            train_graph.ndata['label'] = train_graph.ndata['label'].long().squeeze(-1)\n",
    "        train_graph.ndata['feature'] = train_graph.ndata['feature'].float()\n",
    "      else:\n",
    "        train_graph = None\n",
    "    \n",
    "      ### ADJUST BUDGETS\n",
    "      pos = (graph.ndata['label'] == 1).sum().item()\n",
    "      neg = (graph.ndata['label'] == 0).sum().item()\n",
    "    \n",
    "      main_config['round_new_pos'] = int(0.05 * pos)\n",
    "      main_config['round_new_neg'] = int(0.05 * neg)\n",
    "      main_config['round_budget_pos'] = 0\n",
    "      main_config['round_budget_neg'] = 0\n",
    "    \n",
    "      EXP_DICT['round_budget'] = [0]\n",
    "      EXP_DICT['augment_pos_ratio'] = [pos / (pos + neg) * 0.125]\n",
    "      EXP_DICT['augment_neg_ratio'] = [neg / (pos + neg) * 0.125]\n",
    "    \n",
    "      for key in ['num_epoch', 'num_round_epoch', 'early_stopping']:\n",
    "        model_config[key] = train_config[key]\n",
    "    \n",
    "      ### MAIN LOOP\n",
    "      keywords = EXP_DICT.keys()\n",
    "      combinations = list(itertools.product(*EXP_DICT.values()))\n",
    "    \n",
    "      for combi in combinations:\n",
    "        for key, val in (zip(keywords, combi)):\n",
    "          # Special cases\n",
    "          if key == 'round_budget':\n",
    "            main_config['round_budget_pos'] = int(val * pos)\n",
    "            main_config['round_budget_neg'] = int(val * neg)\n",
    "          if key == 'loss':\n",
    "            train_config['loss'] = LOSS_DICT[val]\n",
    "          if key == 'boost_agg_backbone_name':\n",
    "            model_config['boost_agg_backbone'] = BACKBONE_DICT[val]\n",
    "          if key == 'round_window':\n",
    "            model_config['round_window'] = val\n",
    "            strat_config['augment_round_split'] = val + 1\n",
    "          if key == 'dropedge_prob':\n",
    "            strat_config['dropedge_prob'] = val\n",
    "          \n",
    "          # Else just copy\n",
    "          else:\n",
    "            for cnfg in [main_config, strat_config, train_config, model_config, adver_config]:\n",
    "                if key in cnfg.keys():\n",
    "                    cnfg[key] = val\n",
    "    \n",
    "            # Other hardcoded settings\n",
    "          model_config['mlp_feats'] = model_config['h_feats']\n",
    "    \n",
    "        # Counter and container   \n",
    "        dfs, exp, dataset, graph = [], [], [], []\n",
    "        trial_counter, failure_counter = 0, 0\n",
    "    \n",
    "        start = time()\n",
    "        while trial_counter < TRIAL_NUM:\n",
    "          print(f\"================\")\n",
    "          print(f\"++++++++++++++++\")\n",
    "          print(f\"TRIAL NUMBER {trial_counter}\")\n",
    "          print(f\"++++++++++++++++\")\n",
    "          print(f\"================\")\n",
    "\n",
    "          del dataset \n",
    "          del graph\n",
    "          del train_graph\n",
    "          del exp\n",
    "          gc.collect()\n",
    "            \n",
    "          # REREAD GRAPH DATA\n",
    "          print(f\"  > Rereading graph data...\")\n",
    "          dataset, _ = dgl.load_graphs(f'../dataset/{dset}')\n",
    "          graph = dataset[0].long()\n",
    "    \n",
    "          if len(graph.ndata['label'].shape) > 1:\n",
    "              graph.ndata['label'] = graph.ndata['label'].argmax(1)\n",
    "              graph.ndata['label'] = graph.ndata['label'].long().squeeze(-1)\n",
    "          graph.ndata['feature'] = graph.ndata['feature'].float()\n",
    "          train_config['dset_name'] = dset\n",
    "    \n",
    "          if LIST_TRAIN_DSET[idx] != 'NONE':\n",
    "            dataset, _ = dgl.load_graphs(f'../dataset/{LIST_TRAIN_DSET[idx]}')\n",
    "            train_graph = dataset[0].long()\n",
    "    \n",
    "            if len(train_graph.ndata['label'].shape) > 1:\n",
    "                train_graph.ndata['label'] = train_graph.ndata['label'].argmax(1)\n",
    "                train_graph.ndata['label'] = train_graph.ndata['label'].long().squeeze(-1)\n",
    "            train_graph.ndata['feature'] = train_graph.ndata['feature'].float()\n",
    "          else:\n",
    "            train_graph = None\n",
    "    \n",
    "          print(f\"  > Initializing multiround object...\")\n",
    "          print(graph.num_nodes(), graph.num_edges())\n",
    "          \n",
    "          exp = MultiroundExperiment(\n",
    "            graph, train_graph=train_graph,\n",
    "            main_config=main_config, model_config=model_config, strat_config=strat_config, \n",
    "            adver_config=adver_config, train_config=train_config\n",
    "          )\n",
    "          \n",
    "          # Adversarial Round\n",
    "          round_counter = 0\n",
    "          round_flag = True\n",
    "          while (round_counter < main_config['round_num']) and (round_flag):\n",
    "            print(f\"  > Starting round {round_counter}...\")\n",
    "            round_flag = exp.one_round_node(round_counter)\n",
    "            round_counter = round_counter + 1\n",
    "    \n",
    "          # Check if round successful or need hard reset\n",
    "          if round_flag:\n",
    "            eval_df = pd.DataFrame(sum([r['log_single_eval'] for r in exp.rounds], []), columns=['round', 'eval_type', 'time', 'rec', 'prec', 'f1', 'auc', 'tp', 'fp', 'tn', 'fn']) # Evaluation log\n",
    "            trainlog_df = pd.DataFrame([r['log_round'] for r in exp.rounds]) # Round training log\n",
    "            log_df = pd.merge(left=eval_df, right=trainlog_df, on='round', how='left')\n",
    "            \n",
    "            # Other global log            \n",
    "            log_df['trial'] = trial_counter\n",
    "            log_df['num_nodes'] = graph.num_nodes()\n",
    "            log_df['num_edges'] = graph.num_edges()\n",
    "            \n",
    "            dfs.append(log_df)\n",
    "            trial_counter = trial_counter + 1\n",
    "          else:\n",
    "            failure_counter = failure_counter + 1\n",
    "    \n",
    "          if failure_counter > FAILURE_LIMIT:\n",
    "            raise Exception('Too many failed experiments!')\n",
    "          \n",
    "          # Remove checkpoints\n",
    "          exp.clean_temp_files()\n",
    "    \n",
    "        print(f'Experiment ended, experienced {failure_counter} failures')\n",
    "        print(f'Elapsed experiment time {time() - start:.8f}s')\n",
    "        \n",
    "        # Save artifacts per config setting\n",
    "        if main_config['save_df']:\n",
    "          final_df = pd.concat(dfs)\n",
    "          \n",
    "          for cnfg in [main_config, strat_config, model_config, train_config, adver_config]:\n",
    "            for key, value in cnfg.items():\n",
    "                if key not in ['verbose']:\n",
    "                    final_df[key] = str(value)\n",
    "    \n",
    "          final_df['timestamp'] = ts\n",
    "          \n",
    "          stripped = [re.sub(r\"\\W+\", \"\", str(val))[:6] for val in combi]\n",
    "          suffix = '-'.join(stripped)\n",
    "          final_df.to_csv(f'../result/{ts}/{dset}-{suffix}-E.csv')\n",
    "          outer_dfs.append(final_df)\n",
    "        \n",
    "        if main_config['save_embedding']:\n",
    "          exp.model.set_graph(exp.dset['graph'], round_num=(main_config['round_num']-1), device=main_config['device'])\n",
    "          embedding = exp.model.embed_nodes(exp.model.graph, exp.model.graph.ndata['feature'])\n",
    "          torch.save(torch.linalg.vector_norm(embedding[:,int(embedding.shape[1] / 2):], ord=2, dim=1), f'../result/{ts}/{dset}-{suffix}-TEMP.pt')\n",
    "          torch.save(torch.linalg.vector_norm(embedding[:,:int(embedding.shape[1] / 2)], ord=2, dim=1), f'../result/{ts}/{dset}-{suffix}-NONTEMP.pt')\n",
    "    \n",
    "    # Save overall artifacts\n",
    "    if main_config['save_df']:\n",
    "      final_outer_df = pd.concat(outer_dfs)\n",
    "      final_outer_df.to_csv(f'../result/{ts}/combined_result.csv')\n",
    "\n",
    "if __name__ == \"__main__\":    \n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dgl_pyg_cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
