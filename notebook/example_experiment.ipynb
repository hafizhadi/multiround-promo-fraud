{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================\n",
      "++++++++++++++++\n",
      "TRIAL NUMBER 0\n",
      "++++++++++++++++\n",
      "================\n",
      "  > Rereading graph data...\n",
      "  > Initializing multiround object...\n",
      "11758 1049758\n",
      "\n",
      "=========================\n",
      "STARTING ADVER EXPERIMENT\n",
      "=========================\n",
      "MAIN CONFIG: [('device', 'cuda:0'), ('exp_type', 'ADVER'), ('full_oracle', False), ('node_expiration_time', 1), ('round_budget_neg', 0), ('round_budget_pos', 0), ('round_new_neg', 459), ('round_new_pos', 128), ('round_num', 5), ('save_df', True), ('save_embedding', False), ('save_temp_sne', False), ('setting_type', 'INDUCTIVE'), ('task_type', 'NODE'), ('verbose', 4)]\n",
      "MODEL CONFIG: [('act_name', 'LeakyReLU'), ('addon_internal_agg', 'OR'), ('addon_name', 'NONE'), ('addon_perc', 0.05), ('addon_round_window', 7), ('alpha', 0), ('att_heads', 2), ('beta', 0), ('boost_agg_backbone', <class 'models.benchmarks_supervised.booster.GIN_noparam'>), ('boost_metric', <function average_precision_score at 0x000001FC8CFC5620>), ('boost_predictor', <class 'xgboost.sklearn.XGBClassifier'>), ('device', 'cuda:0'), ('drop_rate', 0), ('dropout_rate', 0.1), ('early_stopping', 75), ('embed_type', 'temporal'), ('etypes', ['none']), ('gamma', 1), ('h_feats', 64), ('k', 1), ('lambda1', 1), ('lambda2', 1), ('lambda_', 1), ('loss_sample', True), ('loss_sample_ratio', 0.1), ('loss_type', 'ndist'), ('mlp_feats', 64), ('mlp_layers', 2), ('model_name', 'XGB-SP'), ('n_estimator', 500), ('norm_name', 'layer'), ('num_epoch', 100), ('num_layers', 2), ('num_round_epoch', 50), ('round_window', 7), ('temporal_agg', 'mean_final'), ('tloss_type', 'normal'), ('training_type', 'round'), ('verbose', 4)]\n",
      "STRAT CONFIG: [('augment_name', 'REAGE'), ('augment_neg_ratio', 0.09772070079945569), ('augment_pos_ratio', 0.02727929920054431), ('augment_prob', 0.5), ('augment_round_split', 8), ('device', 'cuda:0'), ('pseudo_name', 'NONE'), ('pseudo_new_pos_only', False), ('pseudo_skip_initial', False), ('verbose', 4)]\n",
      "TRAIN CONFIG: [('device', 'cuda:0'), ('dset_name', 'tolokers_bid'), ('early_stopping', 75), ('learning_rate', 0.01), ('loss', <function cross_entropy at 0x000001FC85EA3740>), ('loss_gamma', 5), ('num_epoch', 100), ('num_round_epoch', 50), ('optimizer', <class 'torch.optim.adam.Adam'>), ('random_state', 7777777), ('ratio_initial', 0.5), ('ratio_test', 0.2), ('ratio_train', 0.6), ('ratio_val', 0.2), ('round_reset_model', False), ('round_train_list', []), ('stuck_stopping', 50), ('stuck_threshold', 0), ('train_max_retry', 10), ('verbose', 4)]\n",
      "ADVER CONFIG: [('adver_choose_name', 'GREEDY'), ('adver_conn_coef', 0.1), ('adver_feat_coef', 1), ('adver_mod_name', 'REPLAY'), ('device', 'cuda:0'), ('verbose', 4)]\n",
      "\n",
      "INITIAL GRAPH: 11758 {1: 2566, 0: 9192} Nodes, 1049758 Edges\n",
      "=========================\n",
      "        \n",
      ">> Initializing graph...\n",
      "MAIN GRAPH: 11758 {1: 2566, 0: 9192} Nodes, 1049758 Edges\n",
      "\n",
      ">> Initializing experiment type-specific modules:\n",
      "  >> Initializing adversarial modules...\n",
      ">> Initializing task-specific modules:\n",
      ">> Initializing classifier model...\n",
      ">> Initializing other strategies...\n",
      "\n",
      "FINISH INITIALIZATION\n",
      "=====================\n",
      "        \n",
      "  > Starting round 0...\n",
      "\n",
      "+++++++++++++++++\n",
      "STARTING ROUND 0!\n",
      "+++++++++++++++++\n",
      "        \n",
      "ASSIGNED GRAPH FOR TRAINING: 5879 {1: 1291, 0: 4588} Nodes, 258133 Edges\n",
      "\n",
      ">> Splitting to train-validation set...\n",
      "Alotting train-val-test split for round 0\n",
      "Initial pool: 0, Prediction pool: 0, Budget pool: 0, Full pool: 5879\n",
      "Full graph size: 5879, Training: 3527, Val: 2352, Test: 5879\n",
      "    >> INITIAL DATA SPLIT: 3527 train rows ({0: 2752, 1: 775}) | 2352 val rows ({1: 516, 0: 1836}) | 0 test rows ({})\n",
      "    >> AUGMENTED DATA SPLIT: 3527 train rows ({0: 2752, 1: 775}) | 2352 val rows ({1: 516, 0: 1836}) | 0 test rows ({})\n",
      "    >> Updated cross-entropy weight to 3.5509677419354837...\n",
      "\n",
      "-----------------------\n",
      "STARTING MODEL TRAININGps_l\n",
      "-----------------------\n",
      "\n",
      "TRAINING NODE EMBEDDER\n",
      "Detailed Loss: recon 0.70859420-0.70859420 | disen 0.70136678-0.00000000 | temporal 0.64510143-0.00000000 | total 0.70859420\n",
      "Epoch 1, loss: 0.70859420-(best 0.70859420)\n",
      "\n",
      "Detailed Loss: recon 0.40843225-0.40843225 | disen 0.72141010-0.00000000 | temporal 0.66265345-0.00000000 | total 0.40843225\n",
      "Epoch 2, loss: 0.40843225-(best 0.40843225)\n",
      "\n",
      "Detailed Loss: recon 0.35109770-0.35109770 | disen 0.73644358-0.00000000 | temporal 0.67010427-0.00000000 | total 0.35109770\n",
      "Epoch 3, loss: 0.35109770-(best 0.35109770)\n",
      "\n",
      "Detailed Loss: recon 0.34897178-0.34897178 | disen 0.73275733-0.00000000 | temporal 0.67346907-0.00000000 | total 0.34897178\n",
      "Epoch 4, loss: 0.34897178-(best 0.34897178)\n",
      "\n",
      "Detailed Loss: recon 0.34522662-0.34522662 | disen 0.73901582-0.00000000 | temporal 0.67440081-0.00000000 | total 0.34522662\n",
      "Epoch 5, loss: 0.34522662-(best 0.34522662)\n",
      "\n",
      "Detailed Loss: recon 0.33351645-0.33351645 | disen 0.74881375-0.00000000 | temporal 0.67476141-0.00000000 | total 0.33351645\n",
      "Epoch 6, loss: 0.33351645-(best 0.33351645)\n",
      "\n",
      "Detailed Loss: recon 0.31339699-0.31339699 | disen 0.73753262-0.00000000 | temporal 0.67423064-0.00000000 | total 0.31339699\n",
      "Epoch 7, loss: 0.31339699-(best 0.31339699)\n",
      "\n",
      "Detailed Loss: recon 0.29999149-0.29999149 | disen 0.74365985-0.00000000 | temporal 0.67375231-0.00000000 | total 0.29999149\n",
      "Epoch 8, loss: 0.29999149-(best 0.29999149)\n",
      "\n",
      "Detailed Loss: recon 0.27782801-0.27782801 | disen 0.74076319-0.00000000 | temporal 0.67312747-0.00000000 | total 0.27782801\n",
      "Epoch 9, loss: 0.27782801-(best 0.27782801)\n",
      "\n",
      "Detailed Loss: recon 0.27071148-0.27071148 | disen 0.74254596-0.00000000 | temporal 0.67229778-0.00000000 | total 0.27071148\n",
      "Epoch 10, loss: 0.27071148-(best 0.27071148)\n",
      "\n",
      "Detailed Loss: recon 0.27332196-0.27332196 | disen 0.74323970-0.00000000 | temporal 0.67178416-0.00000000 | total 0.27332196\n",
      "Epoch 11, loss: 0.27332196-(best 0.27071148)\n",
      "\n",
      "Detailed Loss: recon 0.26629674-0.26629674 | disen 0.74057817-0.00000000 | temporal 0.67161411-0.00000000 | total 0.26629674\n",
      "Epoch 12, loss: 0.26629674-(best 0.26629674)\n",
      "\n",
      "Detailed Loss: recon 0.26428989-0.26428989 | disen 0.74077737-0.00000000 | temporal 0.67147523-0.00000000 | total 0.26428989\n",
      "Epoch 13, loss: 0.26428989-(best 0.26428989)\n",
      "\n",
      "Detailed Loss: recon 0.25688776-0.25688776 | disen 0.74251509-0.00000000 | temporal 0.67204857-0.00000000 | total 0.25688776\n",
      "Epoch 14, loss: 0.25688776-(best 0.25688776)\n",
      "\n",
      "Detailed Loss: recon 0.25407875-0.25407875 | disen 0.74245346-0.00000000 | temporal 0.67251974-0.00000000 | total 0.25407875\n",
      "Epoch 15, loss: 0.25407875-(best 0.25407875)\n",
      "\n",
      "Detailed Loss: recon 0.24584246-0.24584246 | disen 0.73903966-0.00000000 | temporal 0.67310870-0.00000000 | total 0.24584246\n",
      "Epoch 16, loss: 0.24584246-(best 0.24584246)\n",
      "\n",
      "Detailed Loss: recon 0.24551122-0.24551122 | disen 0.74147999-0.00000000 | temporal 0.67363387-0.00000000 | total 0.24551122\n",
      "Epoch 17, loss: 0.24551122-(best 0.24551122)\n",
      "\n",
      "Detailed Loss: recon 0.24055927-0.24055927 | disen 0.74103898-0.00000000 | temporal 0.67408597-0.00000000 | total 0.24055927\n",
      "Epoch 18, loss: 0.24055927-(best 0.24055927)\n",
      "\n",
      "Detailed Loss: recon 0.24050218-0.24050218 | disen 0.74161738-0.00000000 | temporal 0.67426956-0.00000000 | total 0.24050218\n",
      "Epoch 19, loss: 0.24050218-(best 0.24050218)\n",
      "\n",
      "Detailed Loss: recon 0.23590040-0.23590040 | disen 0.74075401-0.00000000 | temporal 0.67474145-0.00000000 | total 0.23590040\n",
      "Epoch 20, loss: 0.23590040-(best 0.23590040)\n",
      "\n",
      "Detailed Loss: recon 0.23959079-0.23959079 | disen 0.74016535-0.00000000 | temporal 0.67483497-0.00000000 | total 0.23959079\n",
      "Epoch 21, loss: 0.23959079-(best 0.23590040)\n",
      "\n",
      "Detailed Loss: recon 0.23255718-0.23255718 | disen 0.74249446-0.00000000 | temporal 0.67514133-0.00000000 | total 0.23255718\n",
      "Epoch 22, loss: 0.23255718-(best 0.23255718)\n",
      "\n",
      "Detailed Loss: recon 0.23231976-0.23231976 | disen 0.73945141-0.00000000 | temporal 0.67530775-0.00000000 | total 0.23231976\n",
      "Epoch 23, loss: 0.23231976-(best 0.23231976)\n",
      "\n",
      "Detailed Loss: recon 0.22740561-0.22740561 | disen 0.73955834-0.00000000 | temporal 0.67530221-0.00000000 | total 0.22740561\n",
      "Epoch 24, loss: 0.22740561-(best 0.22740561)\n",
      "\n",
      "Detailed Loss: recon 0.22927281-0.22927281 | disen 0.73809880-0.00000000 | temporal 0.67564213-0.00000000 | total 0.22927281\n",
      "Epoch 25, loss: 0.22927281-(best 0.22740561)\n",
      "\n",
      "Detailed Loss: recon 0.22803757-0.22803757 | disen 0.73935902-0.00000000 | temporal 0.67573804-0.00000000 | total 0.22803757\n",
      "Epoch 26, loss: 0.22803757-(best 0.22740561)\n",
      "\n",
      "Detailed Loss: recon 0.22060286-0.22060286 | disen 0.73821461-0.00000000 | temporal 0.67603654-0.00000000 | total 0.22060286\n",
      "Epoch 27, loss: 0.22060286-(best 0.22060286)\n",
      "\n",
      "Detailed Loss: recon 0.22077630-0.22077630 | disen 0.73882699-0.00000000 | temporal 0.67602479-0.00000000 | total 0.22077630\n",
      "Epoch 28, loss: 0.22077630-(best 0.22060286)\n",
      "\n",
      "Detailed Loss: recon 0.22122674-0.22122674 | disen 0.74062467-0.00000000 | temporal 0.67621619-0.00000000 | total 0.22122674\n",
      "Epoch 29, loss: 0.22122674-(best 0.22060286)\n",
      "\n",
      "Detailed Loss: recon 0.22130613-0.22130613 | disen 0.74041694-0.00000000 | temporal 0.67633009-0.00000000 | total 0.22130613\n",
      "Epoch 30, loss: 0.22130613-(best 0.22060286)\n",
      "\n",
      "Detailed Loss: recon 0.21933734-0.21933734 | disen 0.74192894-0.00000000 | temporal 0.67645043-0.00000000 | total 0.21933734\n",
      "Epoch 31, loss: 0.21933734-(best 0.21933734)\n",
      "\n",
      "Detailed Loss: recon 0.21407346-0.21407346 | disen 0.73960400-0.00000000 | temporal 0.67642283-0.00000000 | total 0.21407346\n",
      "Epoch 32, loss: 0.21407346-(best 0.21407346)\n",
      "\n",
      "Detailed Loss: recon 0.21468757-0.21468757 | disen 0.73938578-0.00000000 | temporal 0.67655200-0.00000000 | total 0.21468757\n",
      "Epoch 33, loss: 0.21468757-(best 0.21407346)\n",
      "\n",
      "Detailed Loss: recon 0.21223204-0.21223204 | disen 0.74195290-0.00000000 | temporal 0.67654747-0.00000000 | total 0.21223204\n",
      "Epoch 34, loss: 0.21223204-(best 0.21223204)\n",
      "\n",
      "Detailed Loss: recon 0.21016906-0.21016906 | disen 0.73970115-0.00000000 | temporal 0.67683327-0.00000000 | total 0.21016906\n",
      "Epoch 35, loss: 0.21016906-(best 0.21016906)\n",
      "\n",
      "Detailed Loss: recon 0.21201721-0.21201721 | disen 0.74165529-0.00000000 | temporal 0.67694956-0.00000000 | total 0.21201721\n",
      "Epoch 36, loss: 0.21201721-(best 0.21016906)\n",
      "\n",
      "Detailed Loss: recon 0.21245316-0.21245316 | disen 0.73789740-0.00000000 | temporal 0.67674506-0.00000000 | total 0.21245316\n",
      "Epoch 37, loss: 0.21245316-(best 0.21016906)\n",
      "\n",
      "Detailed Loss: recon 0.21342608-0.21342608 | disen 0.73975152-0.00000000 | temporal 0.67676133-0.00000000 | total 0.21342608\n",
      "Epoch 38, loss: 0.21342608-(best 0.21016906)\n",
      "\n",
      "Detailed Loss: recon 0.20811284-0.20811284 | disen 0.73939836-0.00000000 | temporal 0.67686015-0.00000000 | total 0.20811284\n",
      "Epoch 39, loss: 0.20811284-(best 0.20811284)\n",
      "\n",
      "Detailed Loss: recon 0.20695159-0.20695159 | disen 0.73894703-0.00000000 | temporal 0.67692316-0.00000000 | total 0.20695159\n",
      "Epoch 40, loss: 0.20695159-(best 0.20695159)\n",
      "\n",
      "Detailed Loss: recon 0.20600484-0.20600484 | disen 0.74097240-0.00000000 | temporal 0.67704868-0.00000000 | total 0.20600484\n",
      "Epoch 41, loss: 0.20600484-(best 0.20600484)\n",
      "\n",
      "Detailed Loss: recon 0.20771699-0.20771699 | disen 0.73954546-0.00000000 | temporal 0.67716181-0.00000000 | total 0.20771699\n",
      "Epoch 42, loss: 0.20771699-(best 0.20600484)\n",
      "\n",
      "Detailed Loss: recon 0.20599872-0.20599872 | disen 0.74029940-0.00000000 | temporal 0.67720926-0.00000000 | total 0.20599872\n",
      "Epoch 43, loss: 0.20599872-(best 0.20599872)\n",
      "\n",
      "Detailed Loss: recon 0.20455334-0.20455334 | disen 0.74041796-0.00000000 | temporal 0.67735803-0.00000000 | total 0.20455334\n",
      "Epoch 44, loss: 0.20455334-(best 0.20455334)\n",
      "\n",
      "Detailed Loss: recon 0.20262086-0.20262086 | disen 0.74040067-0.00000000 | temporal 0.67746818-0.00000000 | total 0.20262086\n",
      "Epoch 45, loss: 0.20262086-(best 0.20262086)\n",
      "\n",
      "Detailed Loss: recon 0.20317158-0.20317158 | disen 0.74186289-0.00000000 | temporal 0.67735177-0.00000000 | total 0.20317158\n",
      "Epoch 46, loss: 0.20317158-(best 0.20262086)\n",
      "\n",
      "Detailed Loss: recon 0.20382366-0.20382366 | disen 0.74063325-0.00000000 | temporal 0.67723089-0.00000000 | total 0.20382366\n",
      "Epoch 47, loss: 0.20382366-(best 0.20262086)\n",
      "\n",
      "Detailed Loss: recon 0.20197146-0.20197146 | disen 0.74110043-0.00000000 | temporal 0.67723888-0.00000000 | total 0.20197146\n",
      "Epoch 48, loss: 0.20197146-(best 0.20197146)\n",
      "\n",
      "Detailed Loss: recon 0.20106372-0.20106372 | disen 0.74198705-0.00000000 | temporal 0.67728966-0.00000000 | total 0.20106372\n",
      "Epoch 49, loss: 0.20106372-(best 0.20106372)\n",
      "\n",
      "Detailed Loss: recon 0.20200400-0.20200400 | disen 0.73839140-0.00000000 | temporal 0.67728293-0.00000000 | total 0.20200400\n",
      "Epoch 50, loss: 0.20200400-(best 0.20106372)\n",
      "\n",
      "Detailed Loss: recon 0.19831967-0.19831967 | disen 0.73791158-0.00000000 | temporal 0.67730069-0.00000000 | total 0.19831967\n",
      "Epoch 51, loss: 0.19831967-(best 0.19831967)\n",
      "\n",
      "Detailed Loss: recon 0.19923058-0.19923058 | disen 0.74010539-0.00000000 | temporal 0.67729908-0.00000000 | total 0.19923058\n",
      "Epoch 52, loss: 0.19923058-(best 0.19831967)\n",
      "\n",
      "Detailed Loss: recon 0.19630706-0.19630706 | disen 0.73935181-0.00000000 | temporal 0.67711872-0.00000000 | total 0.19630706\n",
      "Epoch 53, loss: 0.19630706-(best 0.19630706)\n",
      "\n",
      "Detailed Loss: recon 0.19799714-0.19799714 | disen 0.73790085-0.00000000 | temporal 0.67735845-0.00000000 | total 0.19799714\n",
      "Epoch 54, loss: 0.19799714-(best 0.19630706)\n",
      "\n",
      "Detailed Loss: recon 0.19581306-0.19581306 | disen 0.74112403-0.00000000 | temporal 0.67742693-0.00000000 | total 0.19581306\n",
      "Epoch 55, loss: 0.19581306-(best 0.19581306)\n",
      "\n",
      "Detailed Loss: recon 0.19429770-0.19429770 | disen 0.73630476-0.00000000 | temporal 0.67752749-0.00000000 | total 0.19429770\n",
      "Epoch 56, loss: 0.19429770-(best 0.19429770)\n",
      "\n",
      "Detailed Loss: recon 0.19528274-0.19528274 | disen 0.73720849-0.00000000 | temporal 0.67734861-0.00000000 | total 0.19528274\n",
      "Epoch 57, loss: 0.19528274-(best 0.19429770)\n",
      "\n",
      "Detailed Loss: recon 0.19426975-0.19426975 | disen 0.73961687-0.00000000 | temporal 0.67758846-0.00000000 | total 0.19426975\n",
      "Epoch 58, loss: 0.19426975-(best 0.19426975)\n",
      "\n",
      "Detailed Loss: recon 0.19635431-0.19635431 | disen 0.74028695-0.00000000 | temporal 0.67754179-0.00000000 | total 0.19635431\n",
      "Epoch 59, loss: 0.19635431-(best 0.19426975)\n",
      "\n",
      "Detailed Loss: recon 0.19494250-0.19494250 | disen 0.73953235-0.00000000 | temporal 0.67736501-0.00000000 | total 0.19494250\n",
      "Epoch 60, loss: 0.19494250-(best 0.19426975)\n",
      "\n",
      "Detailed Loss: recon 0.19381879-0.19381879 | disen 0.73871505-0.00000000 | temporal 0.67726231-0.00000000 | total 0.19381879\n",
      "Epoch 61, loss: 0.19381879-(best 0.19381879)\n",
      "\n",
      "Detailed Loss: recon 0.19378917-0.19378917 | disen 0.74095273-0.00000000 | temporal 0.67709959-0.00000000 | total 0.19378917\n",
      "Epoch 62, loss: 0.19378917-(best 0.19378917)\n",
      "\n",
      "Detailed Loss: recon 0.19396405-0.19396405 | disen 0.73886311-0.00000000 | temporal 0.67742950-0.00000000 | total 0.19396405\n",
      "Epoch 63, loss: 0.19396405-(best 0.19378917)\n",
      "\n",
      "Detailed Loss: recon 0.19328938-0.19328938 | disen 0.74075532-0.00000000 | temporal 0.67720830-0.00000000 | total 0.19328938\n",
      "Epoch 64, loss: 0.19328938-(best 0.19328938)\n",
      "\n",
      "Detailed Loss: recon 0.19233696-0.19233696 | disen 0.73772037-0.00000000 | temporal 0.67708719-0.00000000 | total 0.19233696\n",
      "Epoch 65, loss: 0.19233696-(best 0.19233696)\n",
      "\n",
      "Detailed Loss: recon 0.19240174-0.19240174 | disen 0.73962307-0.00000000 | temporal 0.67742795-0.00000000 | total 0.19240174\n",
      "Epoch 66, loss: 0.19240174-(best 0.19233696)\n",
      "\n",
      "Detailed Loss: recon 0.19296527-0.19296527 | disen 0.73523945-0.00000000 | temporal 0.67765605-0.00000000 | total 0.19296527\n",
      "Epoch 67, loss: 0.19296527-(best 0.19233696)\n",
      "\n",
      "Detailed Loss: recon 0.19001006-0.19001006 | disen 0.74438745-0.00000000 | temporal 0.67752361-0.00000000 | total 0.19001006\n",
      "Epoch 68, loss: 0.19001006-(best 0.19001006)\n",
      "\n",
      "Detailed Loss: recon 0.19161835-0.19161835 | disen 0.74033308-0.00000000 | temporal 0.67752874-0.00000000 | total 0.19161835\n",
      "Epoch 69, loss: 0.19161835-(best 0.19001006)\n",
      "\n",
      "Detailed Loss: recon 0.19104597-0.19104597 | disen 0.74002749-0.00000000 | temporal 0.67745197-0.00000000 | total 0.19104597\n",
      "Epoch 70, loss: 0.19104597-(best 0.19001006)\n",
      "\n",
      "Detailed Loss: recon 0.19035056-0.19035056 | disen 0.73874056-0.00000000 | temporal 0.67747921-0.00000000 | total 0.19035056\n",
      "Epoch 71, loss: 0.19035056-(best 0.19001006)\n",
      "\n",
      "Detailed Loss: recon 0.19159186-0.19159186 | disen 0.73886377-0.00000000 | temporal 0.67740524-0.00000000 | total 0.19159186\n",
      "Epoch 72, loss: 0.19159186-(best 0.19001006)\n",
      "\n",
      "Detailed Loss: recon 0.19063416-0.19063416 | disen 0.73792207-0.00000000 | temporal 0.67719054-0.00000000 | total 0.19063416\n",
      "Epoch 73, loss: 0.19063416-(best 0.19001006)\n",
      "\n",
      "Detailed Loss: recon 0.19038588-0.19038588 | disen 0.74076796-0.00000000 | temporal 0.67722702-0.00000000 | total 0.19038588\n",
      "Epoch 74, loss: 0.19038588-(best 0.19001006)\n",
      "\n",
      "Detailed Loss: recon 0.19121766-0.19121766 | disen 0.74091721-0.00000000 | temporal 0.67736059-0.00000000 | total 0.19121766\n",
      "Epoch 75, loss: 0.19121766-(best 0.19001006)\n",
      "\n",
      "Detailed Loss: recon 0.18801121-0.18801121 | disen 0.73965621-0.00000000 | temporal 0.67709428-0.00000000 | total 0.18801121\n",
      "Epoch 76, loss: 0.18801121-(best 0.18801121)\n",
      "\n",
      "Detailed Loss: recon 0.18921642-0.18921642 | disen 0.73618245-0.00000000 | temporal 0.67701447-0.00000000 | total 0.18921642\n",
      "Epoch 77, loss: 0.18921642-(best 0.18801121)\n",
      "\n",
      "Detailed Loss: recon 0.18938760-0.18938760 | disen 0.73853147-0.00000000 | temporal 0.67723006-0.00000000 | total 0.18938760\n",
      "Epoch 78, loss: 0.18938760-(best 0.18801121)\n",
      "\n",
      "Detailed Loss: recon 0.18890493-0.18890493 | disen 0.73759520-0.00000000 | temporal 0.67726439-0.00000000 | total 0.18890493\n",
      "Epoch 79, loss: 0.18890493-(best 0.18801121)\n",
      "\n",
      "Detailed Loss: recon 0.19089454-0.19089454 | disen 0.73920393-0.00000000 | temporal 0.67690122-0.00000000 | total 0.19089454\n",
      "Epoch 80, loss: 0.19089454-(best 0.18801121)\n",
      "\n",
      "Detailed Loss: recon 0.18703090-0.18703090 | disen 0.74124396-0.00000000 | temporal 0.67709088-0.00000000 | total 0.18703090\n",
      "Epoch 81, loss: 0.18703090-(best 0.18703090)\n",
      "\n",
      "Detailed Loss: recon 0.18785197-0.18785197 | disen 0.73724568-0.00000000 | temporal 0.67722446-0.00000000 | total 0.18785197\n",
      "Epoch 82, loss: 0.18785197-(best 0.18703090)\n",
      "\n",
      "Detailed Loss: recon 0.18877041-0.18877041 | disen 0.73875970-0.00000000 | temporal 0.67701066-0.00000000 | total 0.18877041\n",
      "Epoch 83, loss: 0.18877041-(best 0.18703090)\n",
      "\n",
      "Detailed Loss: recon 0.18895870-0.18895870 | disen 0.74023336-0.00000000 | temporal 0.67687201-0.00000000 | total 0.18895870\n",
      "Epoch 84, loss: 0.18895870-(best 0.18703090)\n",
      "\n",
      "Detailed Loss: recon 0.18809395-0.18809395 | disen 0.74051899-0.00000000 | temporal 0.67702264-0.00000000 | total 0.18809395\n",
      "Epoch 85, loss: 0.18809395-(best 0.18703090)\n",
      "\n",
      "Detailed Loss: recon 0.18797320-0.18797320 | disen 0.73788792-0.00000000 | temporal 0.67718285-0.00000000 | total 0.18797320\n",
      "Epoch 86, loss: 0.18797320-(best 0.18703090)\n",
      "\n",
      "Detailed Loss: recon 0.18589489-0.18589489 | disen 0.74016935-0.00000000 | temporal 0.67713779-0.00000000 | total 0.18589489\n",
      "Epoch 87, loss: 0.18589489-(best 0.18589489)\n",
      "\n",
      "Detailed Loss: recon 0.18632977-0.18632977 | disen 0.73801303-0.00000000 | temporal 0.67716473-0.00000000 | total 0.18632977\n",
      "Epoch 88, loss: 0.18632977-(best 0.18589489)\n",
      "\n",
      "Detailed Loss: recon 0.18830258-0.18830258 | disen 0.73831433-0.00000000 | temporal 0.67710209-0.00000000 | total 0.18830258\n",
      "Epoch 89, loss: 0.18830258-(best 0.18589489)\n",
      "\n",
      "Detailed Loss: recon 0.18825495-0.18825495 | disen 0.73956954-0.00000000 | temporal 0.67688203-0.00000000 | total 0.18825495\n",
      "Epoch 90, loss: 0.18825495-(best 0.18589489)\n",
      "\n",
      "Detailed Loss: recon 0.18916504-0.18916504 | disen 0.73648506-0.00000000 | temporal 0.67688459-0.00000000 | total 0.18916504\n",
      "Epoch 91, loss: 0.18916504-(best 0.18589489)\n",
      "\n",
      "Detailed Loss: recon 0.18574023-0.18574023 | disen 0.73746663-0.00000000 | temporal 0.67716950-0.00000000 | total 0.18574023\n",
      "Epoch 92, loss: 0.18574023-(best 0.18574023)\n",
      "\n",
      "Detailed Loss: recon 0.18622336-0.18622336 | disen 0.73988771-0.00000000 | temporal 0.67728698-0.00000000 | total 0.18622336\n",
      "Epoch 93, loss: 0.18622336-(best 0.18574023)\n",
      "\n",
      "Detailed Loss: recon 0.18953851-0.18953851 | disen 0.73924136-0.00000000 | temporal 0.67712897-0.00000000 | total 0.18953851\n",
      "Epoch 94, loss: 0.18953851-(best 0.18574023)\n",
      "\n",
      "Detailed Loss: recon 0.18679915-0.18679915 | disen 0.73980618-0.00000000 | temporal 0.67734188-0.00000000 | total 0.18679915\n",
      "Epoch 95, loss: 0.18679915-(best 0.18574023)\n",
      "\n",
      "Detailed Loss: recon 0.18663704-0.18663704 | disen 0.73921728-0.00000000 | temporal 0.67763036-0.00000000 | total 0.18663704\n",
      "Epoch 96, loss: 0.18663704-(best 0.18574023)\n",
      "\n",
      "Detailed Loss: recon 0.18479082-0.18479082 | disen 0.73955548-0.00000000 | temporal 0.67724049-0.00000000 | total 0.18479082\n",
      "Epoch 97, loss: 0.18479082-(best 0.18479082)\n",
      "\n",
      "Detailed Loss: recon 0.18443292-0.18443292 | disen 0.73655701-0.00000000 | temporal 0.67730951-0.00000000 | total 0.18443292\n",
      "Epoch 98, loss: 0.18443292-(best 0.18443292)\n",
      "\n",
      "Detailed Loss: recon 0.18532634-0.18532634 | disen 0.74098665-0.00000000 | temporal 0.67746550-0.00000000 | total 0.18532634\n",
      "Epoch 99, loss: 0.18532634-(best 0.18443292)\n",
      "\n",
      "Detailed Loss: recon 0.18418853-0.18418853 | disen 0.74002218-0.00000000 | temporal 0.67750818-0.00000000 | total 0.18418853\n",
      "Epoch 100, loss: 0.18418853-(best 0.18418853)\n",
      "\n",
      ">> Reached final epoch. Loading best model...\n",
      "TRAINING XGB PREDICTOR\n",
      ">> Ending training!\n",
      "\n",
      "-----------------------\n",
      "TIME COST: 7.777412414550781 s\n",
      "Best Val: REC 54.84 PRE 47.88 MF1 68.04 AUC 80.45 TP 283 FP 308 TN 1528 FN 233\n",
      "-----------------------\n",
      "\n",
      "ASSIGNED GRAPH FOR EVALUATION: 11758 {1: 2566, 0: 9192} Nodes, 1049758 Edges\n",
      "\n",
      "-------------------\n",
      "STARTING EVALUATION\n",
      "-------------------\n",
      "\n",
      "Dataset - Overall: REC 66.84 PRE 46.35 MF1 69.15 AUC 81.73 TP 1715 FP 1985 TN 7207 FN 851 | 11758 {1: 2566, 0: 9192}\n",
      "Dataset - Train: REC 69.68 PRE 50.70 MF1 72.06 AUC 84.35 TP 540 FP 525 TN 2227 FN 235 | 3527 {0: 2752, 1: 775}\n",
      "Dataset - Val: REC 63.95 PRE 45.77 MF1 68.36 AUC 80.72 TP 330 FP 391 TN 1445 FN 186 | 2352 {1: 516, 0: 1836}\n",
      "Dataset - Test: REC 66.27 PRE 44.15 MF1 67.75 AUC 80.51 TP 845 FP 1069 TN 3535 FN 430 | 5879 {0: 4604, 1: 1275}\n",
      "PREDICTION STATUS - {'1-True': 2136, '0-False': 3535, '0-True': 5657, '1-False': 430}\n",
      "    >> 430 positive nodes left unpredicted...\n",
      "    >> 430 OG positive nodes left unpredicted...\n",
      "Selecting data as budgeted ground truth for next round...\n",
      "  > Starting round 1...\n",
      "\n",
      "+++++++++++++++++\n",
      "STARTING ROUND 1!\n",
      "+++++++++++++++++\n",
      "        \n",
      "ASSIGNED GRAPH FOR TRAINING: 11758 {1: 2566, 0: 9192} Nodes, 1049758 Edges\n",
      "\n",
      ">> Splitting to train-validation set...\n",
      "Alotting train-val-test split for round 1\n",
      "Initial pool: 5879, Prediction pool: 3700, Budget pool: 0, Full pool: 7793\n",
      "Full graph size: 11758, Training: 4675, Val: 3118, Test: 11758\n",
      "    >> INITIAL DATA SPLIT: 4675 train rows ({1: 1281, 0: 3394}) | 3118 val rows ({0: 2263, 1: 855}) | 3965 test rows ({0: 3535, 1: 430})\n",
      "    >> AUGMENTED DATA SPLIT: 4675 train rows ({1: 1281, 0: 3394}) | 3118 val rows ({0: 2263, 1: 855}) | 3965 test rows ({0: 3535, 1: 430})\n",
      "    >> Updated cross-entropy weight to 2.6494925839188133...\n",
      "\n",
      "-----------------------\n",
      "STARTING MODEL TRAININGps_l\n",
      "-----------------------\n",
      "\n",
      "TRAINING NODE EMBEDDER\n",
      "Detailed Loss: recon 0.17145333-0.17145333 | disen 0.59028447-0.00000000 | temporal 0.57008737-0.00000000 | total 0.17145333\n",
      "Epoch 1, loss: 0.17145333-(best 0.17145333)\n",
      "\n",
      "Detailed Loss: recon 0.41429049-0.41429049 | disen 0.58815080-0.00000000 | temporal 0.57081431-0.00000000 | total 0.41429049\n",
      "Epoch 2, loss: 0.41429049-(best 0.17145333)\n",
      "\n",
      "Detailed Loss: recon 0.17950451-0.17950451 | disen 0.58787352-0.00000000 | temporal 0.56974542-0.00000000 | total 0.17950451\n",
      "Epoch 3, loss: 0.17950451-(best 0.17145333)\n",
      "\n",
      "Detailed Loss: recon 0.23591271-0.23591271 | disen 0.58874422-0.00000000 | temporal 0.56866693-0.00000000 | total 0.23591271\n",
      "Epoch 4, loss: 0.23591271-(best 0.17145333)\n",
      "\n",
      "Detailed Loss: recon 0.27421686-0.27421686 | disen 0.58859855-0.00000000 | temporal 0.56841391-0.00000000 | total 0.27421686\n",
      "Epoch 5, loss: 0.27421686-(best 0.17145333)\n",
      "\n",
      "Detailed Loss: recon 0.24467421-0.24467421 | disen 0.58796263-0.00000000 | temporal 0.56879252-0.00000000 | total 0.24467421\n",
      "Epoch 6, loss: 0.24467421-(best 0.17145333)\n",
      "\n",
      "Detailed Loss: recon 0.20159090-0.20159090 | disen 0.58584213-0.00000000 | temporal 0.56934410-0.00000000 | total 0.20159090\n",
      "Epoch 7, loss: 0.20159090-(best 0.17145333)\n",
      "\n",
      "Detailed Loss: recon 0.19194667-0.19194667 | disen 0.58557063-0.00000000 | temporal 0.56995684-0.00000000 | total 0.19194667\n",
      "Epoch 8, loss: 0.19194667-(best 0.17145333)\n",
      "\n",
      "Detailed Loss: recon 0.20312399-0.20312399 | disen 0.58566976-0.00000000 | temporal 0.57034421-0.00000000 | total 0.20312399\n",
      "Epoch 9, loss: 0.20312399-(best 0.17145333)\n",
      "\n",
      "Detailed Loss: recon 0.21454459-0.21454459 | disen 0.58537918-0.00000000 | temporal 0.57060170-0.00000000 | total 0.21454459\n",
      "Epoch 10, loss: 0.21454459-(best 0.17145333)\n",
      "\n",
      "Detailed Loss: recon 0.21743426-0.21743426 | disen 0.58587736-0.00000000 | temporal 0.57067543-0.00000000 | total 0.21743426\n",
      "Epoch 11, loss: 0.21743426-(best 0.17145333)\n",
      "\n",
      "Detailed Loss: recon 0.21125671-0.21125671 | disen 0.58547175-0.00000000 | temporal 0.57062393-0.00000000 | total 0.21125671\n",
      "Epoch 12, loss: 0.21125671-(best 0.17145333)\n",
      "\n",
      "Detailed Loss: recon 0.20380649-0.20380649 | disen 0.58496094-0.00000000 | temporal 0.57053626-0.00000000 | total 0.20380649\n",
      "Epoch 13, loss: 0.20380649-(best 0.17145333)\n",
      "\n",
      "Detailed Loss: recon 0.19171922-0.19171922 | disen 0.58560967-0.00000000 | temporal 0.57037330-0.00000000 | total 0.19171922\n",
      "Epoch 14, loss: 0.19171922-(best 0.17145333)\n",
      "\n",
      "Detailed Loss: recon 0.18941131-0.18941131 | disen 0.58647543-0.00000000 | temporal 0.57014585-0.00000000 | total 0.18941131\n",
      "Epoch 15, loss: 0.18941131-(best 0.17145333)\n",
      "\n",
      "Detailed Loss: recon 0.18960285-0.18960285 | disen 0.58676487-0.00000000 | temporal 0.56996697-0.00000000 | total 0.18960285\n",
      "Epoch 16, loss: 0.18960285-(best 0.17145333)\n",
      "\n",
      "Detailed Loss: recon 0.19615547-0.19615547 | disen 0.58649302-0.00000000 | temporal 0.56982857-0.00000000 | total 0.19615547\n",
      "Epoch 17, loss: 0.19615547-(best 0.17145333)\n",
      "\n",
      "Detailed Loss: recon 0.19578105-0.19578105 | disen 0.58559871-0.00000000 | temporal 0.56973886-0.00000000 | total 0.19578105\n",
      "Epoch 18, loss: 0.19578105-(best 0.17145333)\n",
      "\n",
      "Detailed Loss: recon 0.19386709-0.19386709 | disen 0.58612800-0.00000000 | temporal 0.56970966-0.00000000 | total 0.19386709\n",
      "Epoch 19, loss: 0.19386709-(best 0.17145333)\n",
      "\n",
      "Detailed Loss: recon 0.18884641-0.18884641 | disen 0.58660686-0.00000000 | temporal 0.56975377-0.00000000 | total 0.18884641\n",
      "Epoch 20, loss: 0.18884641-(best 0.17145333)\n",
      "\n",
      "Detailed Loss: recon 0.18499874-0.18499874 | disen 0.58593822-0.00000000 | temporal 0.56984258-0.00000000 | total 0.18499874\n",
      "Epoch 21, loss: 0.18499874-(best 0.17145333)\n",
      "\n",
      "Detailed Loss: recon 0.17975818-0.17975818 | disen 0.58537889-0.00000000 | temporal 0.56994319-0.00000000 | total 0.17975818\n",
      "Epoch 22, loss: 0.17975818-(best 0.17145333)\n",
      "\n",
      "Detailed Loss: recon 0.18072239-0.18072239 | disen 0.58569247-0.00000000 | temporal 0.57004642-0.00000000 | total 0.18072239\n",
      "Epoch 23, loss: 0.18072239-(best 0.17145333)\n",
      "\n",
      "Detailed Loss: recon 0.18563937-0.18563937 | disen 0.58564067-0.00000000 | temporal 0.57009888-0.00000000 | total 0.18563937\n",
      "Epoch 24, loss: 0.18563937-(best 0.17145333)\n",
      "\n",
      "Detailed Loss: recon 0.18858117-0.18858117 | disen 0.58585423-0.00000000 | temporal 0.57009733-0.00000000 | total 0.18858117\n",
      "Epoch 25, loss: 0.18858117-(best 0.17145333)\n",
      "\n",
      "Detailed Loss: recon 0.18619081-0.18619081 | disen 0.58624136-0.00000000 | temporal 0.57014120-0.00000000 | total 0.18619081\n",
      "Epoch 26, loss: 0.18619081-(best 0.17145333)\n",
      "\n",
      "Detailed Loss: recon 0.18211202-0.18211202 | disen 0.58565211-0.00000000 | temporal 0.57008803-0.00000000 | total 0.18211202\n",
      "Epoch 27, loss: 0.18211202-(best 0.17145333)\n",
      "\n",
      "Detailed Loss: recon 0.17960334-0.17960334 | disen 0.58585274-0.00000000 | temporal 0.57003516-0.00000000 | total 0.17960334\n",
      "Epoch 28, loss: 0.17960334-(best 0.17145333)\n",
      "\n",
      "Detailed Loss: recon 0.18193331-0.18193331 | disen 0.58624786-0.00000000 | temporal 0.56994665-0.00000000 | total 0.18193331\n",
      "Epoch 29, loss: 0.18193331-(best 0.17145333)\n",
      "\n",
      "Detailed Loss: recon 0.17942992-0.17942992 | disen 0.58529812-0.00000000 | temporal 0.56987309-0.00000000 | total 0.17942992\n",
      "Epoch 30, loss: 0.17942992-(best 0.17145333)\n",
      "\n",
      "Detailed Loss: recon 0.18131557-0.18131557 | disen 0.58586740-0.00000000 | temporal 0.56976825-0.00000000 | total 0.18131557\n",
      "Epoch 31, loss: 0.18131557-(best 0.17145333)\n",
      "\n",
      "Detailed Loss: recon 0.17828117-0.17828117 | disen 0.58521008-0.00000000 | temporal 0.56975931-0.00000000 | total 0.17828117\n",
      "Epoch 32, loss: 0.17828117-(best 0.17145333)\n",
      "\n",
      "Detailed Loss: recon 0.17714936-0.17714936 | disen 0.58533305-0.00000000 | temporal 0.56972516-0.00000000 | total 0.17714936\n",
      "Epoch 33, loss: 0.17714936-(best 0.17145333)\n",
      "\n",
      "Detailed Loss: recon 0.17780831-0.17780831 | disen 0.58486617-0.00000000 | temporal 0.56973433-0.00000000 | total 0.17780831\n",
      "Epoch 34, loss: 0.17780831-(best 0.17145333)\n",
      "\n",
      "Detailed Loss: recon 0.17776452-0.17776452 | disen 0.58526707-0.00000000 | temporal 0.56978112-0.00000000 | total 0.17776452\n",
      "Epoch 35, loss: 0.17776452-(best 0.17145333)\n",
      "\n",
      "Detailed Loss: recon 0.17583302-0.17583302 | disen 0.58495057-0.00000000 | temporal 0.56979460-0.00000000 | total 0.17583302\n",
      "Epoch 36, loss: 0.17583302-(best 0.17145333)\n",
      "\n",
      "Detailed Loss: recon 0.17436104-0.17436104 | disen 0.58504009-0.00000000 | temporal 0.56985462-0.00000000 | total 0.17436104\n",
      "Epoch 37, loss: 0.17436104-(best 0.17145333)\n",
      "\n",
      "Detailed Loss: recon 0.17602623-0.17602623 | disen 0.58487964-0.00000000 | temporal 0.56992936-0.00000000 | total 0.17602623\n",
      "Epoch 38, loss: 0.17602623-(best 0.17145333)\n",
      "\n",
      "Detailed Loss: recon 0.17338622-0.17338622 | disen 0.58476150-0.00000000 | temporal 0.56991023-0.00000000 | total 0.17338622\n",
      "Epoch 39, loss: 0.17338622-(best 0.17145333)\n",
      "\n",
      "Detailed Loss: recon 0.17511788-0.17511788 | disen 0.58493769-0.00000000 | temporal 0.56993288-0.00000000 | total 0.17511788\n",
      "Epoch 40, loss: 0.17511788-(best 0.17145333)\n",
      "\n",
      "Detailed Loss: recon 0.17531638-0.17531638 | disen 0.58512878-0.00000000 | temporal 0.56992388-0.00000000 | total 0.17531638\n",
      "Epoch 41, loss: 0.17531638-(best 0.17145333)\n",
      "\n",
      "Detailed Loss: recon 0.17288738-0.17288738 | disen 0.58553350-0.00000000 | temporal 0.56989181-0.00000000 | total 0.17288738\n",
      "Epoch 42, loss: 0.17288738-(best 0.17145333)\n",
      "\n",
      "Detailed Loss: recon 0.17348979-0.17348979 | disen 0.58473706-0.00000000 | temporal 0.56988555-0.00000000 | total 0.17348979\n",
      "Epoch 43, loss: 0.17348979-(best 0.17145333)\n",
      "\n",
      "Detailed Loss: recon 0.17114393-0.17114393 | disen 0.58445692-0.00000000 | temporal 0.56990010-0.00000000 | total 0.17114393\n",
      "Epoch 44, loss: 0.17114393-(best 0.17114393)\n",
      "\n",
      "Detailed Loss: recon 0.17057490-0.17057490 | disen 0.58363461-0.00000000 | temporal 0.56988639-0.00000000 | total 0.17057490\n",
      "Epoch 45, loss: 0.17057490-(best 0.17057490)\n",
      "\n",
      "Detailed Loss: recon 0.17178302-0.17178302 | disen 0.58427417-0.00000000 | temporal 0.56988692-0.00000000 | total 0.17178302\n",
      "Epoch 46, loss: 0.17178302-(best 0.17057490)\n",
      "\n",
      "Detailed Loss: recon 0.17196672-0.17196672 | disen 0.58442831-0.00000000 | temporal 0.56991464-0.00000000 | total 0.17196672\n",
      "Epoch 47, loss: 0.17196672-(best 0.17057490)\n",
      "\n",
      "Detailed Loss: recon 0.17175412-0.17175412 | disen 0.58398926-0.00000000 | temporal 0.56991088-0.00000000 | total 0.17175412\n",
      "Epoch 48, loss: 0.17175412-(best 0.17057490)\n",
      "\n",
      "Detailed Loss: recon 0.16900125-0.16900125 | disen 0.58397323-0.00000000 | temporal 0.56988436-0.00000000 | total 0.16900125\n",
      "Epoch 49, loss: 0.16900125-(best 0.16900125)\n",
      "\n",
      "Detailed Loss: recon 0.16960037-0.16960037 | disen 0.58418334-0.00000000 | temporal 0.56994480-0.00000000 | total 0.16960037\n",
      "Epoch 50, loss: 0.16960037-(best 0.16900125)\n",
      "\n",
      ">> Reached final epoch. Loading best model...\n",
      "TRAINING XGB PREDICTOR\n",
      ">> Ending training!\n",
      "\n",
      "-----------------------\n",
      "TIME COST: 5.129528284072876 s\n",
      "Best Val: REC 55.44 PRE 48.82 MF1 66.00 AUC 76.73 TP 474 FP 497 TN 1766 FN 381\n",
      "-----------------------\n",
      "Generating additional positive data from adversary...\n",
      "Generating additional negative data by duplicating random nodes...\n",
      "\n",
      "ASSIGNED GRAPH FOR EVALUATION: 12345 {1: 2694, 0: 9651} Nodes, 1143738 Edges\n",
      "\n",
      "-------------------\n",
      "STARTING EVALUATION\n",
      "-------------------\n",
      "\n",
      "Dataset - Overall: REC 66.70 PRE 53.74 MF1 73.21 AUC 84.79 TP 1797 FP 1547 TN 8104 FN 897 | 12345 {1: 2694, 0: 9651}\n",
      "Dataset - Train: REC 82.05 PRE 64.76 MF1 79.97 AUC 90.39 TP 1051 FP 572 TN 2822 FN 230 | 4675 {1: 1281, 0: 3394}\n",
      "Dataset - Val: REC 62.11 PRE 47.97 MF1 66.54 AUC 76.59 TP 531 FP 576 TN 1687 FN 324 | 3118 {0: 2263, 1: 855}\n",
      "Dataset - Test: REC 38.53 PRE 35.02 MF1 63.67 AUC 80.00 TP 215 FP 399 TN 3595 FN 343 | 4552 {0: 3994, 1: 558}\n",
      "PREDICTION STATUS - {'1-True': 2351, '0-False': 3595, '0-True': 6056, '1-False': 343}\n",
      "    >> 343 positive nodes left unpredicted...\n",
      "    >> 269 OG positive nodes left unpredicted...\n",
      "-----------------------\n",
      "PREDICTION RESULT - ROUNDS\n",
      "Dataset - Round 0: REC 65.10 PRE 52.63 MF1 72.41 AUC 83.98 TP 830 FP 747 TN 3857 FN 445 | 5879 {0: 4604, 1: 1275}\n",
      "Dataset - Round 1: REC 42.19 PRE 43.55 MF1 63.62 AUC 75.00 TP 54 FP 70 TN 389 FN 74 | 587 {1: 128, 0: 459}\n",
      "-----------------------\n",
      "PREDICTION RESULT - SEEDS\n",
      "Seeds - Current: REC 42.19 PRE 42.52 MF1 63.18 AUC 75.77 TP 54 FP 73 TN 386 FN 74 | 587 {1: 128, 0: 459}\n",
      "Seeds - Prev: REC 0.00 PRE 0.00 MF1 38.15 AUC 75.77 TP 0 FP 97 TN 362 FN 128 | 587 {1: 128, 0: 459}\n",
      "Selecting data as budgeted ground truth for next round...\n",
      "  > Starting round 2...\n",
      "\n",
      "+++++++++++++++++\n",
      "STARTING ROUND 2!\n",
      "+++++++++++++++++\n",
      "        \n",
      "ASSIGNED GRAPH FOR TRAINING: 12345 {1: 2694, 0: 9651} Nodes, 1143738 Edges\n",
      "\n",
      ">> Splitting to train-validation set...\n",
      "Alotting train-val-test split for round 2\n",
      "Initial pool: 5879, Prediction pool: 7044, Budget pool: 0, Full pool: 8407\n",
      "Full graph size: 12345, Training: 5044, Val: 3363, Test: 12345\n",
      "    >> INITIAL DATA SPLIT: 5044 train rows ({1: 1411, 0: 3633}) | 3363 val rows ({0: 2423, 1: 940}) | 3938 test rows ({0: 3595, 1: 343})\n",
      "    >> AUGMENTED DATA SPLIT: 5044 train rows ({1: 1411, 0: 3633}) | 3363 val rows ({0: 2423, 1: 940}) | 3938 test rows ({0: 3595, 1: 343})\n",
      "    >> Updated cross-entropy weight to 2.5747696669029057...\n",
      "\n",
      "-----------------------\n",
      "STARTING MODEL TRAININGps_l\n",
      "-----------------------\n",
      "\n",
      "TRAINING NODE EMBEDDER\n",
      "Detailed Loss: recon 0.16661461-0.16661461 | disen 0.57980299-0.00000000 | temporal 0.59014976-0.00000000 | total 0.16661461\n",
      "Epoch 1, loss: 0.16661461-(best 0.16661461)\n",
      "\n",
      "Detailed Loss: recon 0.26733246-0.26733246 | disen 0.57893181-0.00000000 | temporal 0.59050846-0.00000000 | total 0.26733246\n",
      "Epoch 2, loss: 0.26733246-(best 0.16661461)\n",
      "\n",
      "Detailed Loss: recon 0.17022489-0.17022489 | disen 0.57798481-0.00000000 | temporal 0.58971369-0.00000000 | total 0.17022489\n",
      "Epoch 3, loss: 0.17022489-(best 0.16661461)\n",
      "\n",
      "Detailed Loss: recon 0.21019398-0.21019398 | disen 0.57755280-0.00000000 | temporal 0.58908021-0.00000000 | total 0.21019398\n",
      "Epoch 4, loss: 0.21019398-(best 0.16661461)\n",
      "\n",
      "Detailed Loss: recon 0.21185601-0.21185601 | disen 0.57754707-0.00000000 | temporal 0.58900243-0.00000000 | total 0.21185601\n",
      "Epoch 5, loss: 0.21185601-(best 0.16661461)\n",
      "\n",
      "Detailed Loss: recon 0.19115977-0.19115977 | disen 0.57597995-0.00000000 | temporal 0.58933908-0.00000000 | total 0.19115977\n",
      "Epoch 6, loss: 0.19115977-(best 0.16661461)\n",
      "\n",
      "Detailed Loss: recon 0.17412090-0.17412090 | disen 0.57502782-0.00000000 | temporal 0.58978301-0.00000000 | total 0.17412090\n",
      "Epoch 7, loss: 0.17412090-(best 0.16661461)\n",
      "\n",
      "Detailed Loss: recon 0.17575637-0.17575637 | disen 0.57404739-0.00000000 | temporal 0.59021032-0.00000000 | total 0.17575637\n",
      "Epoch 8, loss: 0.17575637-(best 0.16661461)\n",
      "\n",
      "Detailed Loss: recon 0.18334197-0.18334197 | disen 0.57406312-0.00000000 | temporal 0.59049225-0.00000000 | total 0.18334197\n",
      "Epoch 9, loss: 0.18334197-(best 0.16661461)\n",
      "\n",
      "Detailed Loss: recon 0.18912679-0.18912679 | disen 0.57418406-0.00000000 | temporal 0.59059083-0.00000000 | total 0.18912679\n",
      "Epoch 10, loss: 0.18912679-(best 0.16661461)\n",
      "\n",
      "Detailed Loss: recon 0.18794619-0.18794619 | disen 0.57307076-0.00000000 | temporal 0.59060228-0.00000000 | total 0.18794619\n",
      "Epoch 11, loss: 0.18794619-(best 0.16661461)\n",
      "\n",
      "Detailed Loss: recon 0.17858104-0.17858104 | disen 0.57271791-0.00000000 | temporal 0.59047723-0.00000000 | total 0.17858104\n",
      "Epoch 12, loss: 0.17858104-(best 0.16661461)\n",
      "\n",
      "Detailed Loss: recon 0.17767410-0.17767410 | disen 0.57305837-0.00000000 | temporal 0.59034640-0.00000000 | total 0.17767410\n",
      "Epoch 13, loss: 0.17767410-(best 0.16661461)\n",
      "\n",
      "Detailed Loss: recon 0.17790610-0.17790610 | disen 0.57272100-0.00000000 | temporal 0.59020549-0.00000000 | total 0.17790610\n",
      "Epoch 14, loss: 0.17790610-(best 0.16661461)\n",
      "\n",
      "Detailed Loss: recon 0.17939395-0.17939395 | disen 0.57301956-0.00000000 | temporal 0.59014159-0.00000000 | total 0.17939395\n",
      "Epoch 15, loss: 0.17939395-(best 0.16661461)\n",
      "\n",
      "Detailed Loss: recon 0.18012741-0.18012741 | disen 0.57274395-0.00000000 | temporal 0.59015805-0.00000000 | total 0.18012741\n",
      "Epoch 16, loss: 0.18012741-(best 0.16661461)\n",
      "\n",
      "Detailed Loss: recon 0.17776185-0.17776185 | disen 0.57233894-0.00000000 | temporal 0.59029776-0.00000000 | total 0.17776185\n",
      "Epoch 17, loss: 0.17776185-(best 0.16661461)\n",
      "\n",
      "Detailed Loss: recon 0.17392635-0.17392635 | disen 0.57276452-0.00000000 | temporal 0.59038305-0.00000000 | total 0.17392635\n",
      "Epoch 18, loss: 0.17392635-(best 0.16661461)\n",
      "\n",
      "Detailed Loss: recon 0.17399445-0.17399445 | disen 0.57273006-0.00000000 | temporal 0.59052998-0.00000000 | total 0.17399445\n",
      "Epoch 19, loss: 0.17399445-(best 0.16661461)\n",
      "\n",
      "Detailed Loss: recon 0.17567424-0.17567424 | disen 0.57323635-0.00000000 | temporal 0.59062642-0.00000000 | total 0.17567424\n",
      "Epoch 20, loss: 0.17567424-(best 0.16661461)\n",
      "\n",
      "Detailed Loss: recon 0.17266594-0.17266594 | disen 0.57327706-0.00000000 | temporal 0.59068233-0.00000000 | total 0.17266594\n",
      "Epoch 21, loss: 0.17266594-(best 0.16661461)\n",
      "\n",
      "Detailed Loss: recon 0.17207687-0.17207687 | disen 0.57365572-0.00000000 | temporal 0.59066391-0.00000000 | total 0.17207687\n",
      "Epoch 22, loss: 0.17207687-(best 0.16661461)\n",
      "\n",
      "Detailed Loss: recon 0.17055631-0.17055631 | disen 0.57329613-0.00000000 | temporal 0.59064537-0.00000000 | total 0.17055631\n",
      "Epoch 23, loss: 0.17055631-(best 0.16661461)\n",
      "\n",
      "Detailed Loss: recon 0.17131320-0.17131320 | disen 0.57388985-0.00000000 | temporal 0.59057719-0.00000000 | total 0.17131320\n",
      "Epoch 24, loss: 0.17131320-(best 0.16661461)\n",
      "\n",
      "Detailed Loss: recon 0.16882865-0.16882865 | disen 0.57395244-0.00000000 | temporal 0.59052157-0.00000000 | total 0.16882865\n",
      "Epoch 25, loss: 0.16882865-(best 0.16661461)\n",
      "\n",
      "Detailed Loss: recon 0.16914734-0.16914734 | disen 0.57379615-0.00000000 | temporal 0.59040195-0.00000000 | total 0.16914734\n",
      "Epoch 26, loss: 0.16914734-(best 0.16661461)\n",
      "\n",
      "Detailed Loss: recon 0.16694091-0.16694091 | disen 0.57377779-0.00000000 | temporal 0.59038514-0.00000000 | total 0.16694091\n",
      "Epoch 27, loss: 0.16694091-(best 0.16661461)\n",
      "\n",
      "Detailed Loss: recon 0.16711929-0.16711929 | disen 0.57251143-0.00000000 | temporal 0.59027922-0.00000000 | total 0.16711929\n",
      "Epoch 28, loss: 0.16711929-(best 0.16661461)\n",
      "\n",
      "Detailed Loss: recon 0.16652001-0.16652001 | disen 0.57224643-0.00000000 | temporal 0.59025198-0.00000000 | total 0.16652001\n",
      "Epoch 29, loss: 0.16652001-(best 0.16652001)\n",
      "\n",
      "Detailed Loss: recon 0.16640221-0.16640221 | disen 0.57231152-0.00000000 | temporal 0.59021717-0.00000000 | total 0.16640221\n",
      "Epoch 30, loss: 0.16640221-(best 0.16640221)\n",
      "\n",
      "Detailed Loss: recon 0.16683607-0.16683607 | disen 0.57273942-0.00000000 | temporal 0.59018546-0.00000000 | total 0.16683607\n",
      "Epoch 31, loss: 0.16683607-(best 0.16640221)\n",
      "\n",
      "Detailed Loss: recon 0.16608818-0.16608818 | disen 0.57132506-0.00000000 | temporal 0.59017920-0.00000000 | total 0.16608818\n",
      "Epoch 32, loss: 0.16608818-(best 0.16608818)\n",
      "\n",
      "Detailed Loss: recon 0.16614830-0.16614830 | disen 0.57232922-0.00000000 | temporal 0.59017825-0.00000000 | total 0.16614830\n",
      "Epoch 33, loss: 0.16614830-(best 0.16608818)\n",
      "\n",
      "Detailed Loss: recon 0.16728424-0.16728424 | disen 0.57178074-0.00000000 | temporal 0.59018445-0.00000000 | total 0.16728424\n",
      "Epoch 34, loss: 0.16728424-(best 0.16608818)\n",
      "\n",
      "Detailed Loss: recon 0.16489828-0.16489828 | disen 0.57156634-0.00000000 | temporal 0.59026247-0.00000000 | total 0.16489828\n",
      "Epoch 35, loss: 0.16489828-(best 0.16489828)\n",
      "\n",
      "Detailed Loss: recon 0.16490918-0.16490918 | disen 0.57200909-0.00000000 | temporal 0.59030128-0.00000000 | total 0.16490918\n",
      "Epoch 36, loss: 0.16490918-(best 0.16489828)\n",
      "\n",
      "Detailed Loss: recon 0.16463050-0.16463050 | disen 0.57195055-0.00000000 | temporal 0.59039944-0.00000000 | total 0.16463050\n",
      "Epoch 37, loss: 0.16463050-(best 0.16463050)\n",
      "\n",
      "Detailed Loss: recon 0.16448578-0.16448578 | disen 0.57253039-0.00000000 | temporal 0.59046918-0.00000000 | total 0.16448578\n",
      "Epoch 38, loss: 0.16448578-(best 0.16448578)\n",
      "\n",
      "Detailed Loss: recon 0.16258800-0.16258800 | disen 0.57193810-0.00000000 | temporal 0.59052247-0.00000000 | total 0.16258800\n",
      "Epoch 39, loss: 0.16258800-(best 0.16258800)\n",
      "\n",
      "Detailed Loss: recon 0.16310635-0.16310635 | disen 0.57224500-0.00000000 | temporal 0.59059262-0.00000000 | total 0.16310635\n",
      "Epoch 40, loss: 0.16310635-(best 0.16258800)\n",
      "\n",
      "Detailed Loss: recon 0.16368309-0.16368309 | disen 0.57242405-0.00000000 | temporal 0.59056246-0.00000000 | total 0.16368309\n",
      "Epoch 41, loss: 0.16368309-(best 0.16258800)\n",
      "\n",
      "Detailed Loss: recon 0.16223358-0.16223358 | disen 0.57238543-0.00000000 | temporal 0.59060049-0.00000000 | total 0.16223358\n",
      "Epoch 42, loss: 0.16223358-(best 0.16223358)\n",
      "\n",
      "Detailed Loss: recon 0.16238706-0.16238706 | disen 0.57179153-0.00000000 | temporal 0.59053957-0.00000000 | total 0.16238706\n",
      "Epoch 43, loss: 0.16238706-(best 0.16223358)\n",
      "\n",
      "Detailed Loss: recon 0.16165321-0.16165321 | disen 0.57191610-0.00000000 | temporal 0.59051299-0.00000000 | total 0.16165321\n",
      "Epoch 44, loss: 0.16165321-(best 0.16165321)\n",
      "\n",
      "Detailed Loss: recon 0.16202292-0.16202292 | disen 0.57205272-0.00000000 | temporal 0.59046173-0.00000000 | total 0.16202292\n",
      "Epoch 45, loss: 0.16202292-(best 0.16165321)\n",
      "\n",
      "Detailed Loss: recon 0.16126841-0.16126841 | disen 0.57213867-0.00000000 | temporal 0.59045184-0.00000000 | total 0.16126841\n",
      "Epoch 46, loss: 0.16126841-(best 0.16126841)\n",
      "\n",
      "Detailed Loss: recon 0.16102630-0.16102630 | disen 0.57208204-0.00000000 | temporal 0.59049082-0.00000000 | total 0.16102630\n",
      "Epoch 47, loss: 0.16102630-(best 0.16102630)\n",
      "\n",
      "Detailed Loss: recon 0.16030975-0.16030975 | disen 0.57226598-0.00000000 | temporal 0.59049618-0.00000000 | total 0.16030975\n",
      "Epoch 48, loss: 0.16030975-(best 0.16030975)\n",
      "\n",
      "Detailed Loss: recon 0.15979981-0.15979981 | disen 0.57114100-0.00000000 | temporal 0.59051210-0.00000000 | total 0.15979981\n",
      "Epoch 49, loss: 0.15979981-(best 0.15979981)\n",
      "\n",
      "Detailed Loss: recon 0.16073415-0.16073415 | disen 0.57210517-0.00000000 | temporal 0.59052366-0.00000000 | total 0.16073415\n",
      "Epoch 50, loss: 0.16073415-(best 0.15979981)\n",
      "\n",
      ">> Reached final epoch. Loading best model...\n",
      "TRAINING XGB PREDICTOR\n",
      ">> Ending training!\n",
      "\n",
      "-----------------------\n",
      "TIME COST: 6.734372138977051 s\n",
      "Best Val: REC 65.00 PRE 47.96 MF1 66.60 AUC 74.87 TP 611 FP 663 TN 1760 FN 329\n",
      "-----------------------\n",
      "Generating additional positive data from adversary...\n",
      "Generating additional negative data by duplicating random nodes...\n",
      "\n",
      "ASSIGNED GRAPH FOR EVALUATION: 12932 {1: 2822, 0: 10110} Nodes, 1240514 Edges\n",
      "\n",
      "-------------------\n",
      "STARTING EVALUATION\n",
      "-------------------\n",
      "\n",
      "Dataset - Overall: REC 59.14 PRE 49.79 MF1 69.83 AUC 79.68 TP 1669 FP 1683 TN 8427 FN 1153 | 12932 {1: 2822, 0: 10110}\n",
      "Dataset - Train: REC 72.22 PRE 58.77 MF1 74.43 AUC 83.70 TP 1019 FP 715 TN 2918 FN 392 | 5044 {1: 1411, 0: 3633}\n",
      "Dataset - Val: REC 57.98 PRE 47.52 MF1 65.37 AUC 73.55 TP 545 FP 602 TN 1821 FN 395 | 3363 {0: 2423, 1: 940}\n",
      "Dataset - Test: REC 22.29 PRE 22.29 MF1 56.63 AUC 70.95 TP 105 FP 366 TN 3688 FN 366 | 4525 {0: 4054, 1: 471}\n",
      "PREDICTION STATUS - {'1-True': 2456, '0-False': 3688, '0-True': 6422, '1-False': 366}\n",
      "    >> 366 positive nodes left unpredicted...\n",
      "    >> 211 OG positive nodes left unpredicted...\n",
      "-----------------------\n",
      "PREDICTION RESULT - ROUNDS\n",
      "Dataset - Round 0: REC 59.69 PRE 49.74 MF1 69.96 AUC 80.46 TP 761 FP 769 TN 3835 FN 514 | 5879 {0: 4604, 1: 1275}\n",
      "Dataset - Round 1: REC 42.97 PRE 43.65 MF1 63.83 AUC 72.30 TP 55 FP 71 TN 388 FN 73 | 587 {1: 128, 0: 459}\n",
      "Dataset - Round 2: REC 25.78 PRE 30.00 MF1 54.68 AUC 61.52 TP 33 FP 77 TN 382 FN 95 | 587 {1: 128, 0: 459}\n",
      "-----------------------\n",
      "PREDICTION RESULT - SEEDS\n",
      "Seeds - Current: REC 21.09 PRE 27.00 MF1 52.65 AUC 60.81 TP 27 FP 73 TN 386 FN 101 | 587 {1: 128, 0: 459}\n",
      "Seeds - Prev: REC 0.00 PRE 0.00 MF1 39.73 AUC 60.81 TP 0 FP 72 TN 387 FN 128 | 587 {1: 128, 0: 459}\n",
      "Selecting data as budgeted ground truth for next round...\n",
      "  > Starting round 3...\n",
      "\n",
      "+++++++++++++++++\n",
      "STARTING ROUND 3!\n",
      "+++++++++++++++++\n",
      "        \n",
      "ASSIGNED GRAPH FOR TRAINING: 12932 {1: 2822, 0: 10110} Nodes, 1240514 Edges\n",
      "\n",
      ">> Splitting to train-validation set...\n",
      "Alotting train-val-test split for round 3\n",
      "Initial pool: 5879, Prediction pool: 10396, Budget pool: 0, Full pool: 8878\n",
      "Full graph size: 12932, Training: 5326, Val: 3552, Test: 12932\n",
      "    >> INITIAL DATA SPLIT: 5326 train rows ({0: 3853, 1: 1473}) | 3552 val rows ({1: 983, 0: 2569}) | 4054 test rows ({0: 3688, 1: 366})\n",
      "    >> AUGMENTED DATA SPLIT: 5326 train rows ({0: 3853, 1: 1473}) | 3552 val rows ({1: 983, 0: 2569}) | 4054 test rows ({0: 3688, 1: 366})\n",
      "    >> Updated cross-entropy weight to 2.6157501697216565...\n",
      "\n",
      "-----------------------\n",
      "STARTING MODEL TRAININGps_l\n",
      "-----------------------\n",
      "\n",
      "TRAINING NODE EMBEDDER\n",
      "Detailed Loss: recon 0.16201304-0.16201304 | disen 0.57188207-0.00000000 | temporal 0.61039078-0.00000000 | total 0.16201304\n",
      "Epoch 1, loss: 0.16201304-(best 0.16201304)\n",
      "\n",
      "Detailed Loss: recon 0.19925115-0.19925115 | disen 0.57329702-0.00000000 | temporal 0.61027491-0.00000000 | total 0.19925115\n",
      "Epoch 2, loss: 0.19925115-(best 0.16201304)\n",
      "\n",
      "Detailed Loss: recon 0.18531725-0.18531725 | disen 0.57322538-0.00000000 | temporal 0.61112964-0.00000000 | total 0.18531725\n",
      "Epoch 3, loss: 0.18531725-(best 0.16201304)\n",
      "\n",
      "Detailed Loss: recon 0.18790261-0.18790261 | disen 0.57358789-0.00000000 | temporal 0.61119384-0.00000000 | total 0.18790261\n",
      "Epoch 4, loss: 0.18790261-(best 0.16201304)\n",
      "\n",
      "Detailed Loss: recon 0.17364353-0.17364353 | disen 0.57437110-0.00000000 | temporal 0.61091322-0.00000000 | total 0.17364353\n",
      "Epoch 5, loss: 0.17364353-(best 0.16201304)\n",
      "\n",
      "Detailed Loss: recon 0.17764962-0.17764962 | disen 0.57456762-0.00000000 | temporal 0.61057484-0.00000000 | total 0.17764962\n",
      "Epoch 6, loss: 0.17764962-(best 0.16201304)\n",
      "\n",
      "Detailed Loss: recon 0.18117951-0.18117951 | disen 0.57493150-0.00000000 | temporal 0.61043155-0.00000000 | total 0.18117951\n",
      "Epoch 7, loss: 0.18117951-(best 0.16201304)\n",
      "\n",
      "Detailed Loss: recon 0.17337418-0.17337418 | disen 0.57425952-0.00000000 | temporal 0.61053365-0.00000000 | total 0.17337418\n",
      "Epoch 8, loss: 0.17337418-(best 0.16201304)\n",
      "\n",
      "Detailed Loss: recon 0.16900246-0.16900246 | disen 0.57444203-0.00000000 | temporal 0.61070508-0.00000000 | total 0.16900246\n",
      "Epoch 9, loss: 0.16900246-(best 0.16201304)\n",
      "\n",
      "Detailed Loss: recon 0.17458946-0.17458946 | disen 0.57415581-0.00000000 | temporal 0.61087745-0.00000000 | total 0.17458946\n",
      "Epoch 10, loss: 0.17458946-(best 0.16201304)\n",
      "\n",
      "Detailed Loss: recon 0.17177403-0.17177403 | disen 0.57332385-0.00000000 | temporal 0.61092061-0.00000000 | total 0.17177403\n",
      "Epoch 11, loss: 0.17177403-(best 0.16201304)\n",
      "\n",
      "Detailed Loss: recon 0.17062971-0.17062971 | disen 0.57268548-0.00000000 | temporal 0.61087155-0.00000000 | total 0.17062971\n",
      "Epoch 12, loss: 0.17062971-(best 0.16201304)\n",
      "\n",
      "Detailed Loss: recon 0.16816650-0.16816650 | disen 0.57333517-0.00000000 | temporal 0.61076623-0.00000000 | total 0.16816650\n",
      "Epoch 13, loss: 0.16816650-(best 0.16201304)\n",
      "\n",
      "Detailed Loss: recon 0.17090353-0.17090353 | disen 0.57199001-0.00000000 | temporal 0.61072773-0.00000000 | total 0.17090353\n",
      "Epoch 14, loss: 0.17090353-(best 0.16201304)\n",
      "\n",
      "Detailed Loss: recon 0.16782910-0.16782910 | disen 0.57236755-0.00000000 | temporal 0.61081004-0.00000000 | total 0.16782910\n",
      "Epoch 15, loss: 0.16782910-(best 0.16201304)\n",
      "\n",
      "Detailed Loss: recon 0.16557907-0.16557907 | disen 0.57294464-0.00000000 | temporal 0.61091423-0.00000000 | total 0.16557907\n",
      "Epoch 16, loss: 0.16557907-(best 0.16201304)\n",
      "\n",
      "Detailed Loss: recon 0.16653100-0.16653100 | disen 0.57282460-0.00000000 | temporal 0.61105913-0.00000000 | total 0.16653100\n",
      "Epoch 17, loss: 0.16653100-(best 0.16201304)\n",
      "\n",
      "Detailed Loss: recon 0.16754964-0.16754964 | disen 0.57239348-0.00000000 | temporal 0.61103809-0.00000000 | total 0.16754964\n",
      "Epoch 18, loss: 0.16754964-(best 0.16201304)\n",
      "\n",
      "Detailed Loss: recon 0.16664980-0.16664980 | disen 0.57291842-0.00000000 | temporal 0.61091006-0.00000000 | total 0.16664980\n",
      "Epoch 19, loss: 0.16664980-(best 0.16201304)\n",
      "\n",
      "Detailed Loss: recon 0.16371372-0.16371372 | disen 0.57186639-0.00000000 | temporal 0.61077762-0.00000000 | total 0.16371372\n",
      "Epoch 20, loss: 0.16371372-(best 0.16201304)\n",
      "\n",
      "Detailed Loss: recon 0.16471198-0.16471198 | disen 0.57228714-0.00000000 | temporal 0.61072773-0.00000000 | total 0.16471198\n",
      "Epoch 21, loss: 0.16471198-(best 0.16201304)\n",
      "\n",
      "Detailed Loss: recon 0.16440436-0.16440436 | disen 0.57212126-0.00000000 | temporal 0.61081713-0.00000000 | total 0.16440436\n",
      "Epoch 22, loss: 0.16440436-(best 0.16201304)\n",
      "\n",
      "Detailed Loss: recon 0.16298780-0.16298780 | disen 0.57277673-0.00000000 | temporal 0.61091304-0.00000000 | total 0.16298780\n",
      "Epoch 23, loss: 0.16298780-(best 0.16201304)\n",
      "\n",
      "Detailed Loss: recon 0.16342717-0.16342717 | disen 0.57283974-0.00000000 | temporal 0.61099428-0.00000000 | total 0.16342717\n",
      "Epoch 24, loss: 0.16342717-(best 0.16201304)\n",
      "\n",
      "Detailed Loss: recon 0.16247998-0.16247998 | disen 0.57262462-0.00000000 | temporal 0.61098766-0.00000000 | total 0.16247998\n",
      "Epoch 25, loss: 0.16247998-(best 0.16201304)\n",
      "\n",
      "Detailed Loss: recon 0.16289680-0.16289680 | disen 0.57223529-0.00000000 | temporal 0.61087376-0.00000000 | total 0.16289680\n",
      "Epoch 26, loss: 0.16289680-(best 0.16201304)\n",
      "\n",
      "Detailed Loss: recon 0.16258432-0.16258432 | disen 0.57336974-0.00000000 | temporal 0.61085731-0.00000000 | total 0.16258432\n",
      "Epoch 27, loss: 0.16258432-(best 0.16201304)\n",
      "\n",
      "Detailed Loss: recon 0.16107029-0.16107029 | disen 0.57269877-0.00000000 | temporal 0.61093807-0.00000000 | total 0.16107029\n",
      "Epoch 28, loss: 0.16107029-(best 0.16107029)\n",
      "\n",
      "Detailed Loss: recon 0.16267049-0.16267049 | disen 0.57209265-0.00000000 | temporal 0.61099243-0.00000000 | total 0.16267049\n",
      "Epoch 29, loss: 0.16267049-(best 0.16107029)\n",
      "\n",
      "Detailed Loss: recon 0.16178432-0.16178432 | disen 0.57338381-0.00000000 | temporal 0.61104584-0.00000000 | total 0.16178432\n",
      "Epoch 30, loss: 0.16178432-(best 0.16107029)\n",
      "\n",
      "Detailed Loss: recon 0.16006783-0.16006783 | disen 0.57271630-0.00000000 | temporal 0.61094451-0.00000000 | total 0.16006783\n",
      "Epoch 31, loss: 0.16006783-(best 0.16006783)\n",
      "\n",
      "Detailed Loss: recon 0.16090086-0.16090086 | disen 0.57239014-0.00000000 | temporal 0.61086470-0.00000000 | total 0.16090086\n",
      "Epoch 32, loss: 0.16090086-(best 0.16006783)\n",
      "\n",
      "Detailed Loss: recon 0.16167256-0.16167256 | disen 0.57231998-0.00000000 | temporal 0.61086458-0.00000000 | total 0.16167256\n",
      "Epoch 33, loss: 0.16167256-(best 0.16006783)\n",
      "\n",
      "Detailed Loss: recon 0.16006795-0.16006795 | disen 0.57219350-0.00000000 | temporal 0.61091745-0.00000000 | total 0.16006795\n",
      "Epoch 34, loss: 0.16006795-(best 0.16006783)\n",
      "\n",
      "Detailed Loss: recon 0.15982027-0.15982027 | disen 0.57274616-0.00000000 | temporal 0.61100775-0.00000000 | total 0.15982027\n",
      "Epoch 35, loss: 0.15982027-(best 0.15982027)\n",
      "\n",
      "Detailed Loss: recon 0.16118792-0.16118792 | disen 0.57259703-0.00000000 | temporal 0.61101562-0.00000000 | total 0.16118792\n",
      "Epoch 36, loss: 0.16118792-(best 0.15982027)\n",
      "\n",
      "Detailed Loss: recon 0.15931411-0.15931411 | disen 0.57288826-0.00000000 | temporal 0.61093974-0.00000000 | total 0.15931411\n",
      "Epoch 37, loss: 0.15931411-(best 0.15931411)\n",
      "\n",
      "Detailed Loss: recon 0.16055274-0.16055274 | disen 0.57276893-0.00000000 | temporal 0.61089867-0.00000000 | total 0.16055274\n",
      "Epoch 38, loss: 0.16055274-(best 0.15931411)\n",
      "\n",
      "Detailed Loss: recon 0.15885933-0.15885933 | disen 0.57257080-0.00000000 | temporal 0.61093199-0.00000000 | total 0.15885933\n",
      "Epoch 39, loss: 0.15885933-(best 0.15885933)\n",
      "\n",
      "Detailed Loss: recon 0.15913396-0.15913396 | disen 0.57184565-0.00000000 | temporal 0.61097097-0.00000000 | total 0.15913396\n",
      "Epoch 40, loss: 0.15913396-(best 0.15885933)\n",
      "\n",
      "Detailed Loss: recon 0.15896896-0.15896896 | disen 0.57125956-0.00000000 | temporal 0.61096293-0.00000000 | total 0.15896896\n",
      "Epoch 41, loss: 0.15896896-(best 0.15885933)\n",
      "\n",
      "Detailed Loss: recon 0.15752961-0.15752961 | disen 0.57102847-0.00000000 | temporal 0.61096978-0.00000000 | total 0.15752961\n",
      "Epoch 42, loss: 0.15752961-(best 0.15752961)\n",
      "\n",
      "Detailed Loss: recon 0.15836957-0.15836957 | disen 0.57013226-0.00000000 | temporal 0.61100578-0.00000000 | total 0.15836957\n",
      "Epoch 43, loss: 0.15836957-(best 0.15752961)\n",
      "\n",
      "Detailed Loss: recon 0.15754335-0.15754335 | disen 0.57098019-0.00000000 | temporal 0.61105514-0.00000000 | total 0.15754335\n",
      "Epoch 44, loss: 0.15754335-(best 0.15752961)\n",
      "\n",
      "Detailed Loss: recon 0.15686747-0.15686747 | disen 0.57100439-0.00000000 | temporal 0.61112165-0.00000000 | total 0.15686747\n",
      "Epoch 45, loss: 0.15686747-(best 0.15686747)\n",
      "\n",
      "Detailed Loss: recon 0.15822038-0.15822038 | disen 0.57101339-0.00000000 | temporal 0.61113459-0.00000000 | total 0.15822038\n",
      "Epoch 46, loss: 0.15822038-(best 0.15686747)\n",
      "\n",
      "Detailed Loss: recon 0.15788497-0.15788497 | disen 0.57111549-0.00000000 | temporal 0.61116904-0.00000000 | total 0.15788497\n",
      "Epoch 47, loss: 0.15788497-(best 0.15686747)\n",
      "\n",
      "Detailed Loss: recon 0.15838313-0.15838313 | disen 0.57068467-0.00000000 | temporal 0.61108118-0.00000000 | total 0.15838313\n",
      "Epoch 48, loss: 0.15838313-(best 0.15686747)\n",
      "\n",
      "Detailed Loss: recon 0.15928918-0.15928918 | disen 0.57082844-0.00000000 | temporal 0.61103994-0.00000000 | total 0.15928918\n",
      "Epoch 49, loss: 0.15928918-(best 0.15686747)\n",
      "\n",
      "Detailed Loss: recon 0.15679976-0.15679976 | disen 0.56993264-0.00000000 | temporal 0.61097270-0.00000000 | total 0.15679976\n",
      "Epoch 50, loss: 0.15679976-(best 0.15679976)\n",
      "\n",
      ">> Reached final epoch. Loading best model...\n",
      "TRAINING XGB PREDICTOR\n",
      ">> Ending training!\n",
      "\n",
      "-----------------------\n",
      "TIME COST: 6.528570175170898 s\n",
      "Best Val: REC 71.01 PRE 44.01 MF1 64.24 AUC 73.43 TP 698 FP 888 TN 1681 FN 285\n",
      "-----------------------\n",
      "Generating additional positive data from adversary...\n",
      "Generating additional negative data by duplicating random nodes...\n",
      "\n",
      "ASSIGNED GRAPH FOR EVALUATION: 13519 {1: 2950, 0: 10569} Nodes, 1354690 Edges\n",
      "\n",
      "-------------------\n",
      "STARTING EVALUATION\n",
      "-------------------\n",
      "\n",
      "Dataset - Overall: REC 57.08 PRE 49.96 MF1 69.51 AUC 80.95 TP 1684 FP 1687 TN 8882 FN 1266 | 13519 {1: 2950, 0: 10569}\n",
      "Dataset - Train: REC 68.23 PRE 58.91 MF1 73.79 AUC 83.63 TP 1005 FP 701 TN 3152 FN 468 | 5326 {0: 3853, 1: 1473}\n",
      "Dataset - Val: REC 54.22 PRE 47.21 MF1 64.76 AUC 74.72 TP 533 FP 596 TN 1973 FN 450 | 3552 {1: 983, 0: 2569}\n",
      "Dataset - Test: REC 29.55 PRE 27.24 MF1 59.70 AUC 76.58 TP 146 FP 390 TN 3757 FN 348 | 4641 {0: 4147, 1: 494}\n",
      "PREDICTION STATUS - {'1-True': 2602, '0-False': 3757, '0-True': 6812, '1-False': 348}\n",
      "    >> 348 positive nodes left unpredicted...\n",
      "    >> 149 OG positive nodes left unpredicted...\n",
      "-----------------------\n",
      "PREDICTION RESULT - ROUNDS\n",
      "Dataset - Round 0: REC 57.96 PRE 50.27 MF1 69.89 AUC 82.40 TP 739 FP 731 TN 3873 FN 536 | 5879 {0: 4604, 1: 1275}\n",
      "Dataset - Round 1: REC 50.00 PRE 50.00 MF1 68.03 AUC 79.29 TP 64 FP 64 TN 395 FN 64 | 587 {1: 128, 0: 459}\n",
      "Dataset - Round 2: REC 30.47 PRE 30.95 MF1 55.79 AUC 67.06 TP 39 FP 87 TN 372 FN 89 | 587 {1: 128, 0: 459}\n",
      "Dataset - Round 3: REC 30.47 PRE 28.06 MF1 54.19 AUC 63.85 TP 39 FP 100 TN 359 FN 89 | 587 {1: 128, 0: 459}\n",
      "-----------------------\n",
      "PREDICTION RESULT - SEEDS\n",
      "Seeds - Current: REC 28.91 PRE 29.60 MF1 54.91 AUC 64.41 TP 37 FP 88 TN 371 FN 91 | 587 {1: 128, 0: 459}\n",
      "Seeds - Prev: REC 0.00 PRE 0.00 MF1 38.85 AUC 64.41 TP 0 FP 86 TN 373 FN 128 | 587 {1: 128, 0: 459}\n",
      "Selecting data as budgeted ground truth for next round...\n",
      "  > Starting round 4...\n",
      "\n",
      "+++++++++++++++++\n",
      "STARTING ROUND 4!\n",
      "+++++++++++++++++\n",
      "        \n",
      "ASSIGNED GRAPH FOR TRAINING: 13519 {1: 2950, 0: 10569} Nodes, 1354690 Edges\n",
      "\n",
      ">> Splitting to train-validation set...\n",
      "Alotting train-val-test split for round 4\n",
      "Initial pool: 5879, Prediction pool: 13767, Budget pool: 0, Full pool: 9414\n",
      "Full graph size: 13519, Training: 5648, Val: 3766, Test: 13519\n",
      "    >> INITIAL DATA SPLIT: 5648 train rows ({0: 4087, 1: 1561}) | 3766 val rows ({1: 1041, 0: 2725}) | 4105 test rows ({0: 3757, 1: 348})\n",
      "    >> AUGMENTED DATA SPLIT: 5648 train rows ({0: 4087, 1: 1561}) | 3766 val rows ({1: 1041, 0: 2725}) | 4105 test rows ({0: 3757, 1: 348})\n",
      "    >> Updated cross-entropy weight to 2.618193465727098...\n",
      "\n",
      "-----------------------\n",
      "STARTING MODEL TRAININGps_l\n",
      "-----------------------\n",
      "\n",
      "TRAINING NODE EMBEDDER\n",
      "Detailed Loss: recon 0.16662991-0.16662991 | disen 0.55540192-0.00000000 | temporal 0.63160914-0.00000000 | total 0.16662991\n",
      "Epoch 1, loss: 0.16662991-(best 0.16662991)\n",
      "\n",
      "Detailed Loss: recon 0.19704723-0.19704723 | disen 0.55778867-0.00000000 | temporal 0.63235134-0.00000000 | total 0.19704723\n",
      "Epoch 2, loss: 0.19704723-(best 0.16662991)\n",
      "\n",
      "Detailed Loss: recon 0.19801591-0.19801591 | disen 0.56027740-0.00000000 | temporal 0.63153577-0.00000000 | total 0.19801591\n",
      "Epoch 3, loss: 0.19801591-(best 0.16662991)\n",
      "\n",
      "Detailed Loss: recon 0.19443655-0.19443655 | disen 0.55792999-0.00000000 | temporal 0.63143510-0.00000000 | total 0.19443655\n",
      "Epoch 4, loss: 0.19443655-(best 0.16662991)\n",
      "\n",
      "Detailed Loss: recon 0.17212784-0.17212784 | disen 0.55656338-0.00000000 | temporal 0.63159341-0.00000000 | total 0.17212784\n",
      "Epoch 5, loss: 0.17212784-(best 0.16662991)\n",
      "\n",
      "Detailed Loss: recon 0.17789654-0.17789654 | disen 0.55456740-0.00000000 | temporal 0.63189107-0.00000000 | total 0.17789654\n",
      "Epoch 6, loss: 0.17789654-(best 0.16662991)\n",
      "\n",
      "Detailed Loss: recon 0.18059371-0.18059371 | disen 0.55369967-0.00000000 | temporal 0.63185918-0.00000000 | total 0.18059371\n",
      "Epoch 7, loss: 0.18059371-(best 0.16662991)\n",
      "\n",
      "Detailed Loss: recon 0.17308132-0.17308132 | disen 0.55363715-0.00000000 | temporal 0.63164216-0.00000000 | total 0.17308132\n",
      "Epoch 8, loss: 0.17308132-(best 0.16662991)\n",
      "\n",
      "Detailed Loss: recon 0.17737806-0.17737806 | disen 0.55491996-0.00000000 | temporal 0.63143772-0.00000000 | total 0.17737806\n",
      "Epoch 9, loss: 0.17737806-(best 0.16662991)\n",
      "\n",
      "Detailed Loss: recon 0.17700033-0.17700033 | disen 0.55525231-0.00000000 | temporal 0.63147265-0.00000000 | total 0.17700033\n",
      "Epoch 10, loss: 0.17700033-(best 0.16662991)\n",
      "\n",
      "Detailed Loss: recon 0.17181477-0.17181477 | disen 0.55507851-0.00000000 | temporal 0.63164783-0.00000000 | total 0.17181477\n",
      "Epoch 11, loss: 0.17181477-(best 0.16662991)\n",
      "\n",
      "Detailed Loss: recon 0.17143247-0.17143247 | disen 0.55531466-0.00000000 | temporal 0.63187099-0.00000000 | total 0.17143247\n",
      "Epoch 12, loss: 0.17143247-(best 0.16662991)\n",
      "\n",
      "Detailed Loss: recon 0.17354119-0.17354119 | disen 0.55543542-0.00000000 | temporal 0.63196951-0.00000000 | total 0.17354119\n",
      "Epoch 13, loss: 0.17354119-(best 0.16662991)\n",
      "\n",
      "Detailed Loss: recon 0.17194805-0.17194805 | disen 0.55559212-0.00000000 | temporal 0.63193929-0.00000000 | total 0.17194805\n",
      "Epoch 14, loss: 0.17194805-(best 0.16662991)\n",
      "\n",
      "Detailed Loss: recon 0.16954997-0.16954997 | disen 0.55591261-0.00000000 | temporal 0.63182670-0.00000000 | total 0.16954997\n",
      "Epoch 15, loss: 0.16954997-(best 0.16662991)\n",
      "\n",
      "Detailed Loss: recon 0.16792379-0.16792379 | disen 0.55579424-0.00000000 | temporal 0.63165784-0.00000000 | total 0.16792379\n",
      "Epoch 16, loss: 0.16792379-(best 0.16662991)\n",
      "\n",
      "Detailed Loss: recon 0.16836223-0.16836223 | disen 0.55534804-0.00000000 | temporal 0.63158596-0.00000000 | total 0.16836223\n",
      "Epoch 17, loss: 0.16836223-(best 0.16662991)\n",
      "\n",
      "Detailed Loss: recon 0.16777328-0.16777328 | disen 0.55555248-0.00000000 | temporal 0.63155603-0.00000000 | total 0.16777328\n",
      "Epoch 18, loss: 0.16777328-(best 0.16662991)\n",
      "\n",
      "Detailed Loss: recon 0.16637100-0.16637100 | disen 0.55384088-0.00000000 | temporal 0.63159406-0.00000000 | total 0.16637100\n",
      "Epoch 19, loss: 0.16637100-(best 0.16637100)\n",
      "\n",
      "Detailed Loss: recon 0.16665773-0.16665773 | disen 0.55428076-0.00000000 | temporal 0.63161469-0.00000000 | total 0.16665773\n",
      "Epoch 20, loss: 0.16665773-(best 0.16637100)\n",
      "\n",
      "Detailed Loss: recon 0.16690284-0.16690284 | disen 0.55442262-0.00000000 | temporal 0.63163280-0.00000000 | total 0.16690284\n",
      "Epoch 21, loss: 0.16690284-(best 0.16637100)\n",
      "\n",
      "Detailed Loss: recon 0.16774723-0.16774723 | disen 0.55394125-0.00000000 | temporal 0.63163906-0.00000000 | total 0.16774723\n",
      "Epoch 22, loss: 0.16774723-(best 0.16637100)\n",
      "\n",
      "Detailed Loss: recon 0.16490230-0.16490230 | disen 0.55421913-0.00000000 | temporal 0.63161016-0.00000000 | total 0.16490230\n",
      "Epoch 23, loss: 0.16490230-(best 0.16490230)\n",
      "\n",
      "Detailed Loss: recon 0.16562790-0.16562790 | disen 0.55441940-0.00000000 | temporal 0.63164425-0.00000000 | total 0.16562790\n",
      "Epoch 24, loss: 0.16562790-(best 0.16490230)\n",
      "\n",
      "Detailed Loss: recon 0.16444817-0.16444817 | disen 0.55440474-0.00000000 | temporal 0.63165098-0.00000000 | total 0.16444817\n",
      "Epoch 25, loss: 0.16444817-(best 0.16444817)\n",
      "\n",
      "Detailed Loss: recon 0.16606081-0.16606081 | disen 0.55444288-0.00000000 | temporal 0.63168854-0.00000000 | total 0.16606081\n",
      "Epoch 26, loss: 0.16606081-(best 0.16444817)\n",
      "\n",
      "Detailed Loss: recon 0.16392647-0.16392647 | disen 0.55327129-0.00000000 | temporal 0.63173974-0.00000000 | total 0.16392647\n",
      "Epoch 27, loss: 0.16392647-(best 0.16392647)\n",
      "\n",
      "Detailed Loss: recon 0.16554977-0.16554977 | disen 0.55304897-0.00000000 | temporal 0.63184059-0.00000000 | total 0.16554977\n",
      "Epoch 28, loss: 0.16554977-(best 0.16392647)\n",
      "\n",
      "Detailed Loss: recon 0.16448021-0.16448021 | disen 0.55288160-0.00000000 | temporal 0.63187206-0.00000000 | total 0.16448021\n",
      "Epoch 29, loss: 0.16448021-(best 0.16392647)\n",
      "\n",
      "Detailed Loss: recon 0.16453028-0.16453028 | disen 0.55322295-0.00000000 | temporal 0.63183874-0.00000000 | total 0.16453028\n",
      "Epoch 30, loss: 0.16453028-(best 0.16392647)\n",
      "\n",
      "Detailed Loss: recon 0.16309917-0.16309917 | disen 0.55357122-0.00000000 | temporal 0.63179511-0.00000000 | total 0.16309917\n",
      "Epoch 31, loss: 0.16309917-(best 0.16309917)\n",
      "\n",
      "Detailed Loss: recon 0.16643730-0.16643730 | disen 0.55245996-0.00000000 | temporal 0.63186407-0.00000000 | total 0.16643730\n",
      "Epoch 32, loss: 0.16643730-(best 0.16309917)\n",
      "\n",
      "Detailed Loss: recon 0.16269299-0.16269299 | disen 0.55236232-0.00000000 | temporal 0.63188517-0.00000000 | total 0.16269299\n",
      "Epoch 33, loss: 0.16269299-(best 0.16269299)\n",
      "\n",
      "Detailed Loss: recon 0.16217683-0.16217683 | disen 0.55244851-0.00000000 | temporal 0.63188785-0.00000000 | total 0.16217683\n",
      "Epoch 34, loss: 0.16217683-(best 0.16217683)\n",
      "\n",
      "Detailed Loss: recon 0.16298914-0.16298914 | disen 0.55270112-0.00000000 | temporal 0.63183284-0.00000000 | total 0.16298914\n",
      "Epoch 35, loss: 0.16298914-(best 0.16217683)\n",
      "\n",
      "Detailed Loss: recon 0.16279927-0.16279927 | disen 0.55270445-0.00000000 | temporal 0.63183308-0.00000000 | total 0.16279927\n",
      "Epoch 36, loss: 0.16279927-(best 0.16217683)\n",
      "\n",
      "Detailed Loss: recon 0.16216701-0.16216701 | disen 0.55224800-0.00000000 | temporal 0.63184696-0.00000000 | total 0.16216701\n",
      "Epoch 37, loss: 0.16216701-(best 0.16216701)\n",
      "\n",
      "Detailed Loss: recon 0.16220391-0.16220391 | disen 0.55214155-0.00000000 | temporal 0.63184696-0.00000000 | total 0.16220391\n",
      "Epoch 38, loss: 0.16220391-(best 0.16216701)\n",
      "\n",
      "Detailed Loss: recon 0.16117537-0.16117537 | disen 0.55276620-0.00000000 | temporal 0.63185620-0.00000000 | total 0.16117537\n",
      "Epoch 39, loss: 0.16117537-(best 0.16117537)\n",
      "\n",
      "Detailed Loss: recon 0.16058922-0.16058922 | disen 0.55186170-0.00000000 | temporal 0.63190651-0.00000000 | total 0.16058922\n",
      "Epoch 40, loss: 0.16058922-(best 0.16058922)\n",
      "\n",
      "Detailed Loss: recon 0.16217178-0.16217178 | disen 0.55182314-0.00000000 | temporal 0.63184988-0.00000000 | total 0.16217178\n",
      "Epoch 41, loss: 0.16217178-(best 0.16058922)\n",
      "\n",
      "Detailed Loss: recon 0.16102670-0.16102670 | disen 0.55226505-0.00000000 | temporal 0.63185811-0.00000000 | total 0.16102670\n",
      "Epoch 42, loss: 0.16102670-(best 0.16058922)\n",
      "\n",
      "Detailed Loss: recon 0.16061457-0.16061457 | disen 0.55223727-0.00000000 | temporal 0.63190913-0.00000000 | total 0.16061457\n",
      "Epoch 43, loss: 0.16061457-(best 0.16058922)\n",
      "\n",
      "Detailed Loss: recon 0.16055340-0.16055340 | disen 0.55173373-0.00000000 | temporal 0.63187975-0.00000000 | total 0.16055340\n",
      "Epoch 44, loss: 0.16055340-(best 0.16055340)\n",
      "\n",
      "Detailed Loss: recon 0.16213569-0.16213569 | disen 0.55112267-0.00000000 | temporal 0.63190299-0.00000000 | total 0.16213569\n",
      "Epoch 45, loss: 0.16213569-(best 0.16055340)\n",
      "\n",
      "Detailed Loss: recon 0.16039336-0.16039336 | disen 0.55123001-0.00000000 | temporal 0.63184172-0.00000000 | total 0.16039336\n",
      "Epoch 46, loss: 0.16039336-(best 0.16039336)\n",
      "\n",
      "Detailed Loss: recon 0.15941656-0.15941656 | disen 0.55188859-0.00000000 | temporal 0.63179380-0.00000000 | total 0.15941656\n",
      "Epoch 47, loss: 0.15941656-(best 0.15941656)\n",
      "\n",
      "Detailed Loss: recon 0.16001132-0.16001132 | disen 0.55157584-0.00000000 | temporal 0.63178247-0.00000000 | total 0.16001132\n",
      "Epoch 48, loss: 0.16001132-(best 0.15941656)\n",
      "\n",
      "Detailed Loss: recon 0.15975142-0.15975142 | disen 0.55146056-0.00000000 | temporal 0.63171303-0.00000000 | total 0.15975142\n",
      "Epoch 49, loss: 0.15975142-(best 0.15941656)\n",
      "\n",
      "Detailed Loss: recon 0.15985131-0.15985131 | disen 0.55162728-0.00000000 | temporal 0.63168067-0.00000000 | total 0.15985131\n",
      "Epoch 50, loss: 0.15985131-(best 0.15941656)\n",
      "\n",
      ">> Reached final epoch. Loading best model...\n",
      "TRAINING XGB PREDICTOR\n",
      ">> Ending training!\n",
      "\n",
      "-----------------------\n",
      "TIME COST: 7.034813404083252 s\n",
      "Best Val: REC 55.14 PRE 47.71 MF1 65.22 AUC 73.42 TP 574 FP 629 TN 2096 FN 467\n",
      "-----------------------\n",
      "Generating additional positive data from adversary...\n",
      "Generating additional negative data by duplicating random nodes...\n",
      "\n",
      "ASSIGNED GRAPH FOR EVALUATION: 14106 {1: 3078, 0: 11028} Nodes, 1461794 Edges\n",
      "\n",
      "-------------------\n",
      "STARTING EVALUATION\n",
      "-------------------\n",
      "\n",
      "Dataset - Overall: REC 59.75 PRE 43.51 MF1 66.50 AUC 76.67 TP 1839 FP 2388 TN 8640 FN 1239 | 14106 {1: 3078, 0: 11028}\n",
      "Dataset - Train: REC 68.23 PRE 48.88 MF1 67.82 AUC 77.34 TP 1065 FP 1114 TN 2973 FN 496 | 5648 {0: 4087, 1: 1561}\n",
      "Dataset - Val: REC 60.13 PRE 43.14 MF1 62.82 AUC 70.82 TP 626 FP 825 TN 1900 FN 415 | 3766 {1: 1041, 0: 2725}\n",
      "Dataset - Test: REC 31.09 PRE 24.79 MF1 59.12 AUC 70.96 TP 148 FP 449 TN 3767 FN 328 | 4692 {0: 4216, 1: 476}\n",
      "PREDICTION STATUS - {'1-True': 2750, '0-False': 3767, '0-True': 7261, '1-False': 328}\n",
      "    >> 328 positive nodes left unpredicted...\n",
      "    >> 100 OG positive nodes left unpredicted...\n",
      "-----------------------\n",
      "PREDICTION RESULT - ROUNDS\n",
      "Dataset - Round 0: REC 64.16 PRE 43.60 MF1 67.16 AUC 78.42 TP 818 FP 1058 TN 3546 FN 457 | 5879 {0: 4604, 1: 1275}\n",
      "Dataset - Round 1: REC 54.69 PRE 45.75 MF1 67.02 AUC 75.55 TP 70 FP 83 TN 376 FN 58 | 587 {1: 128, 0: 459}\n",
      "Dataset - Round 2: REC 46.88 PRE 38.71 MF1 62.05 AUC 68.59 TP 60 FP 95 TN 364 FN 68 | 587 {1: 128, 0: 459}\n",
      "Dataset - Round 3: REC 41.41 PRE 32.12 MF1 57.48 AUC 63.34 TP 53 FP 112 TN 347 FN 75 | 587 {1: 128, 0: 459}\n",
      "Dataset - Round 4: REC 27.34 PRE 31.25 MF1 55.48 AUC 62.25 TP 35 FP 77 TN 382 FN 93 | 587 {1: 128, 0: 459}\n",
      "-----------------------\n",
      "PREDICTION RESULT - SEEDS\n",
      "Seeds - Current: REC 32.03 PRE 33.06 MF1 57.05 AUC 64.01 TP 41 FP 83 TN 376 FN 87 | 587 {1: 128, 0: 459}\n",
      "Seeds - Prev: REC 0.00 PRE 0.00 MF1 40.47 AUC 64.01 TP 0 FP 60 TN 399 FN 128 | 587 {1: 128, 0: 459}\n",
      "Selecting data as budgeted ground truth for next round...\n",
      "The file ../checkpoint/working_model_file_1756868543.5012364_epoch.py does not exist\n",
      "The file ../checkpoint/working_model_file_1756868543.5012364_round-M.json does not exist\n",
      "The file ../checkpoint/working_model_file_1756868543.5012364_round-T.json does not exist\n",
      "The file ../checkpoint/working_model_file_1756868543.5012364_round.pt does not exist\n",
      "Experiment ended, experienced 0 failures\n",
      "Elapsed experiment time 39.77074814s\n",
      "================\n",
      "++++++++++++++++\n",
      "TRIAL NUMBER 0\n",
      "++++++++++++++++\n",
      "================\n",
      "  > Rereading graph data...\n",
      "  > Initializing multiround object...\n",
      "11758 1049758\n",
      "\n",
      "=========================\n",
      "STARTING ADVER EXPERIMENT\n",
      "=========================\n",
      "MAIN CONFIG: [('device', 'cuda:0'), ('exp_type', 'ADVER'), ('full_oracle', False), ('node_expiration_time', 1), ('round_budget_neg', 0), ('round_budget_pos', 0), ('round_new_neg', 459), ('round_new_pos', 128), ('round_num', 5), ('save_df', True), ('save_embedding', False), ('save_temp_sne', False), ('setting_type', 'INDUCTIVE'), ('task_type', 'NODE'), ('verbose', 4)]\n",
      "MODEL CONFIG: [('act_name', 'LeakyReLU'), ('addon_internal_agg', 'OR'), ('addon_name', 'NONE'), ('addon_perc', 0.05), ('addon_round_window', 7), ('alpha', 0), ('att_heads', 2), ('beta', 1), ('boost_agg_backbone', <class 'models.benchmarks_supervised.booster.GIN_noparam'>), ('boost_metric', <function average_precision_score at 0x000001FC8CFC5620>), ('boost_predictor', <class 'xgboost.sklearn.XGBClassifier'>), ('device', 'cuda:0'), ('drop_rate', 0), ('dropout_rate', 0.1), ('early_stopping', 75), ('embed_type', 'temporal'), ('etypes', ['none']), ('gamma', 1), ('h_feats', 64), ('k', 1), ('lambda1', 1), ('lambda2', 1), ('lambda_', 1), ('loss_sample', True), ('loss_sample_ratio', 0.1), ('loss_type', 'ndist'), ('mlp_feats', 64), ('mlp_layers', 2), ('model_name', 'XGB-SP'), ('n_estimator', 500), ('norm_name', 'layer'), ('num_epoch', 100), ('num_layers', 2), ('num_round_epoch', 50), ('round_window', 7), ('temporal_agg', 'mean_final'), ('tloss_type', 'normal'), ('training_type', 'round'), ('verbose', 4)]\n",
      "STRAT CONFIG: [('augment_name', 'REAGE'), ('augment_neg_ratio', 0.09772070079945569), ('augment_pos_ratio', 0.02727929920054431), ('augment_prob', 0.5), ('augment_round_split', 8), ('device', 'cuda:0'), ('pseudo_name', 'NONE'), ('pseudo_new_pos_only', False), ('pseudo_skip_initial', False), ('verbose', 4)]\n",
      "TRAIN CONFIG: [('ce_weight', 2.618193465727098), ('device', 'cuda:0'), ('dset_name', 'tolokers_bid'), ('early_stopping', 75), ('learning_rate', 0.01), ('loss', <function cross_entropy at 0x000001FC85EA3740>), ('loss_gamma', 5), ('num_epoch', 100), ('num_round_epoch', 50), ('optimizer', <class 'torch.optim.adam.Adam'>), ('random_state', 7777777), ('ratio_initial', 0.5), ('ratio_test', 0.2), ('ratio_train', 0.6), ('ratio_val', 0.2), ('round_reset_model', False), ('round_train_list', []), ('stuck_stopping', 50), ('stuck_threshold', 0), ('train_max_retry', 10), ('verbose', 4)]\n",
      "ADVER CONFIG: [('adver_choose_name', 'GREEDY'), ('adver_conn_coef', 0.1), ('adver_feat_coef', 1), ('adver_mod_name', 'REPLAY'), ('device', 'cuda:0'), ('verbose', 4)]\n",
      "\n",
      "INITIAL GRAPH: 11758 {1: 2566, 0: 9192} Nodes, 1049758 Edges\n",
      "=========================\n",
      "        \n",
      ">> Initializing graph...\n",
      "MAIN GRAPH: 11758 {1: 2566, 0: 9192} Nodes, 1049758 Edges\n",
      "\n",
      ">> Initializing experiment type-specific modules:\n",
      "  >> Initializing adversarial modules...\n",
      ">> Initializing task-specific modules:\n",
      ">> Initializing classifier model...\n",
      ">> Initializing other strategies...\n",
      "\n",
      "FINISH INITIALIZATION\n",
      "=====================\n",
      "        \n",
      "  > Starting round 0...\n",
      "\n",
      "+++++++++++++++++\n",
      "STARTING ROUND 0!\n",
      "+++++++++++++++++\n",
      "        \n",
      "ASSIGNED GRAPH FOR TRAINING: 5879 {0: 4608, 1: 1271} Nodes, 263687 Edges\n",
      "\n",
      ">> Splitting to train-validation set...\n",
      "Alotting train-val-test split for round 0\n",
      "Initial pool: 0, Prediction pool: 0, Budget pool: 0, Full pool: 5879\n",
      "Full graph size: 5879, Training: 3527, Val: 2352, Test: 5879\n",
      "    >> INITIAL DATA SPLIT: 3527 train rows ({0: 2764, 1: 763}) | 2352 val rows ({0: 1844, 1: 508}) | 0 test rows ({})\n",
      "    >> AUGMENTED DATA SPLIT: 3527 train rows ({0: 2764, 1: 763}) | 2352 val rows ({0: 1844, 1: 508}) | 0 test rows ({})\n",
      "    >> Updated cross-entropy weight to 3.622542595019659...\n",
      "\n",
      "-----------------------\n",
      "STARTING MODEL TRAININGps_l\n",
      "-----------------------\n",
      "\n",
      "TRAINING NODE EMBEDDER\n",
      "Detailed Loss: recon 0.70655322-0.35327661 | disen 0.70285541-0.00000000 | temporal 0.64892519-0.32446259 | total 0.67773920\n",
      "Epoch 1, loss: 0.67773920-(best 0.67773920)\n",
      "\n",
      "Detailed Loss: recon 0.50134301-0.25067151 | disen 0.71789950-0.00000000 | temporal 0.65974599-0.32987300 | total 0.58054447\n",
      "Epoch 2, loss: 0.58054447-(best 0.58054447)\n",
      "\n",
      "Detailed Loss: recon 0.41231078-0.20615539 | disen 0.72231251-0.00000000 | temporal 0.66241699-0.33120850 | total 0.53736389\n",
      "Epoch 3, loss: 0.53736389-(best 0.53736389)\n",
      "\n",
      "Detailed Loss: recon 0.39391789-0.19695894 | disen 0.72755718-0.00000000 | temporal 0.66379321-0.33189660 | total 0.52885556\n",
      "Epoch 4, loss: 0.52885556-(best 0.52885556)\n",
      "\n",
      "Detailed Loss: recon 0.36355394-0.18177697 | disen 0.72843468-0.00000000 | temporal 0.66459894-0.33229947 | total 0.51407647\n",
      "Epoch 5, loss: 0.51407647-(best 0.51407647)\n",
      "\n",
      "Detailed Loss: recon 0.35333294-0.17666647 | disen 0.72928226-0.00000000 | temporal 0.66460341-0.33230171 | total 0.50896817\n",
      "Epoch 6, loss: 0.50896817-(best 0.50896817)\n",
      "\n",
      "Detailed Loss: recon 0.33916384-0.16958192 | disen 0.73247951-0.00000000 | temporal 0.66419041-0.33209521 | total 0.50167716\n",
      "Epoch 7, loss: 0.50167716-(best 0.50167716)\n",
      "\n",
      "Detailed Loss: recon 0.34159708-0.17079854 | disen 0.73176777-0.00000000 | temporal 0.66357088-0.33178544 | total 0.50258398\n",
      "Epoch 8, loss: 0.50258398-(best 0.50167716)\n",
      "\n",
      "Detailed Loss: recon 0.32719094-0.16359547 | disen 0.73227906-0.00000000 | temporal 0.66319102-0.33159551 | total 0.49519098\n",
      "Epoch 9, loss: 0.49519098-(best 0.49519098)\n",
      "\n",
      "Detailed Loss: recon 0.32401419-0.16200709 | disen 0.73206514-0.00000000 | temporal 0.66290760-0.33145380 | total 0.49346089\n",
      "Epoch 10, loss: 0.49346089-(best 0.49346089)\n",
      "\n",
      "Detailed Loss: recon 0.32629186-0.16314593 | disen 0.73149383-0.00000000 | temporal 0.66262239-0.33131120 | total 0.49445713\n",
      "Epoch 11, loss: 0.49445713-(best 0.49346089)\n",
      "\n",
      "Detailed Loss: recon 0.31602126-0.15801063 | disen 0.73288488-0.00000000 | temporal 0.66240877-0.33120438 | total 0.48921502\n",
      "Epoch 12, loss: 0.48921502-(best 0.48921502)\n",
      "\n",
      "Detailed Loss: recon 0.31347880-0.15673940 | disen 0.73005557-0.00000000 | temporal 0.66261196-0.33130598 | total 0.48804539\n",
      "Epoch 13, loss: 0.48804539-(best 0.48804539)\n",
      "\n",
      "Detailed Loss: recon 0.30681828-0.15340914 | disen 0.73047674-0.00000000 | temporal 0.66298115-0.33149058 | total 0.48489970\n",
      "Epoch 14, loss: 0.48489970-(best 0.48489970)\n",
      "\n",
      "Detailed Loss: recon 0.29353911-0.14676955 | disen 0.72945106-0.00000000 | temporal 0.66332328-0.33166164 | total 0.47843120\n",
      "Epoch 15, loss: 0.47843120-(best 0.47843120)\n",
      "\n",
      "Detailed Loss: recon 0.28940141-0.14470071 | disen 0.73075455-0.00000000 | temporal 0.66402054-0.33201027 | total 0.47671098\n",
      "Epoch 16, loss: 0.47671098-(best 0.47671098)\n",
      "\n",
      "Detailed Loss: recon 0.29129374-0.14564687 | disen 0.72969937-0.00000000 | temporal 0.66427988-0.33213994 | total 0.47778681\n",
      "Epoch 17, loss: 0.47778681-(best 0.47671098)\n",
      "\n",
      "Detailed Loss: recon 0.27963227-0.13981614 | disen 0.73040730-0.00000000 | temporal 0.66450876-0.33225438 | total 0.47207052\n",
      "Epoch 18, loss: 0.47207052-(best 0.47207052)\n",
      "\n",
      "Detailed Loss: recon 0.27319211-0.13659605 | disen 0.73196149-0.00000000 | temporal 0.66457826-0.33228913 | total 0.46888518\n",
      "Epoch 19, loss: 0.46888518-(best 0.46888518)\n",
      "\n",
      "Detailed Loss: recon 0.27245146-0.13622573 | disen 0.73148596-0.00000000 | temporal 0.66457969-0.33228984 | total 0.46851557\n",
      "Epoch 20, loss: 0.46851557-(best 0.46851557)\n",
      "\n",
      "Detailed Loss: recon 0.26554108-0.13277054 | disen 0.72673625-0.00000000 | temporal 0.66458327-0.33229163 | total 0.46506217\n",
      "Epoch 21, loss: 0.46506217-(best 0.46506217)\n",
      "\n",
      "Detailed Loss: recon 0.27751648-0.13875824 | disen 0.72762156-0.00000000 | temporal 0.66431099-0.33215550 | total 0.47091374\n",
      "Epoch 22, loss: 0.47091374-(best 0.46506217)\n",
      "\n",
      "Detailed Loss: recon 0.26426244-0.13213122 | disen 0.72841859-0.00000000 | temporal 0.66444111-0.33222055 | total 0.46435177\n",
      "Epoch 23, loss: 0.46435177-(best 0.46435177)\n",
      "\n",
      "Detailed Loss: recon 0.26172557-0.13086279 | disen 0.72817743-0.00000000 | temporal 0.66475528-0.33237764 | total 0.46324044\n",
      "Epoch 24, loss: 0.46324044-(best 0.46324044)\n",
      "\n",
      "Detailed Loss: recon 0.25334203-0.12667102 | disen 0.72784221-0.00000000 | temporal 0.66495359-0.33247679 | total 0.45914781\n",
      "Epoch 25, loss: 0.45914781-(best 0.45914781)\n",
      "\n",
      "Detailed Loss: recon 0.24986532-0.12493266 | disen 0.72812903-0.00000000 | temporal 0.66510206-0.33255103 | total 0.45748371\n",
      "Epoch 26, loss: 0.45748371-(best 0.45748371)\n",
      "\n",
      "Detailed Loss: recon 0.25469148-0.12734574 | disen 0.72780502-0.00000000 | temporal 0.66494888-0.33247444 | total 0.45982018\n",
      "Epoch 27, loss: 0.45982018-(best 0.45748371)\n",
      "\n",
      "Detailed Loss: recon 0.24720857-0.12360428 | disen 0.72668469-0.00000000 | temporal 0.66481382-0.33240691 | total 0.45601118\n",
      "Epoch 28, loss: 0.45601118-(best 0.45601118)\n",
      "\n",
      "Detailed Loss: recon 0.24662396-0.12331198 | disen 0.72764003-0.00000000 | temporal 0.66450608-0.33225304 | total 0.45556504\n",
      "Epoch 29, loss: 0.45556504-(best 0.45556504)\n",
      "\n",
      "Detailed Loss: recon 0.24278413-0.12139206 | disen 0.72876507-0.00000000 | temporal 0.66477275-0.33238637 | total 0.45377845\n",
      "Epoch 30, loss: 0.45377845-(best 0.45377845)\n",
      "\n",
      "Detailed Loss: recon 0.23929690-0.11964845 | disen 0.72839355-0.00000000 | temporal 0.66540861-0.33270431 | total 0.45235276\n",
      "Epoch 31, loss: 0.45235276-(best 0.45235276)\n",
      "\n",
      "Detailed Loss: recon 0.23633146-0.11816573 | disen 0.72905058-0.00000000 | temporal 0.66550505-0.33275253 | total 0.45091826\n",
      "Epoch 32, loss: 0.45091826-(best 0.45091826)\n",
      "\n",
      "Detailed Loss: recon 0.23492584-0.11746292 | disen 0.72931802-0.00000000 | temporal 0.66554713-0.33277357 | total 0.45023650\n",
      "Epoch 33, loss: 0.45023650-(best 0.45023650)\n",
      "\n",
      "Detailed Loss: recon 0.23420881-0.11710440 | disen 0.72716212-0.00000000 | temporal 0.66527194-0.33263597 | total 0.44974038\n",
      "Epoch 34, loss: 0.44974038-(best 0.44974038)\n",
      "\n",
      "Detailed Loss: recon 0.23393890-0.11696945 | disen 0.73003626-0.00000000 | temporal 0.66539174-0.33269587 | total 0.44966531\n",
      "Epoch 35, loss: 0.44966531-(best 0.44966531)\n",
      "\n",
      "Detailed Loss: recon 0.23211972-0.11605986 | disen 0.72872591-0.00000000 | temporal 0.66546965-0.33273482 | total 0.44879469\n",
      "Epoch 36, loss: 0.44879469-(best 0.44879469)\n",
      "\n",
      "Detailed Loss: recon 0.22666311-0.11333156 | disen 0.72990924-0.00000000 | temporal 0.66577095-0.33288547 | total 0.44621703\n",
      "Epoch 37, loss: 0.44621703-(best 0.44621703)\n",
      "\n",
      "Detailed Loss: recon 0.23560986-0.11780493 | disen 0.72951114-0.00000000 | temporal 0.66594803-0.33297402 | total 0.45077896\n",
      "Epoch 38, loss: 0.45077896-(best 0.44621703)\n",
      "\n",
      "Detailed Loss: recon 0.22076246-0.11038123 | disen 0.72921371-0.00000000 | temporal 0.66550559-0.33275279 | total 0.44313401\n",
      "Epoch 39, loss: 0.44313401-(best 0.44313401)\n",
      "\n",
      "Detailed Loss: recon 0.22259206-0.11129603 | disen 0.72787380-0.00000000 | temporal 0.66523719-0.33261859 | total 0.44391462\n",
      "Epoch 40, loss: 0.44391462-(best 0.44313401)\n",
      "\n",
      "Detailed Loss: recon 0.22516309-0.11258154 | disen 0.73042220-0.00000000 | temporal 0.66539901-0.33269951 | total 0.44528106\n",
      "Epoch 41, loss: 0.44528106-(best 0.44313401)\n",
      "\n",
      "Detailed Loss: recon 0.21899760-0.10949880 | disen 0.73133385-0.00000000 | temporal 0.66556054-0.33278027 | total 0.44227907\n",
      "Epoch 42, loss: 0.44227907-(best 0.44227907)\n",
      "\n",
      "Detailed Loss: recon 0.22256140-0.11128070 | disen 0.73166698-0.00000000 | temporal 0.66555882-0.33277941 | total 0.44406012\n",
      "Epoch 43, loss: 0.44406012-(best 0.44227907)\n",
      "\n",
      "Detailed Loss: recon 0.21526426-0.10763213 | disen 0.73148733-0.00000000 | temporal 0.66557890-0.33278945 | total 0.44042158\n",
      "Epoch 44, loss: 0.44042158-(best 0.44042158)\n",
      "\n",
      "Detailed Loss: recon 0.21439569-0.10719784 | disen 0.73178816-0.00000000 | temporal 0.66544324-0.33272162 | total 0.43991947\n",
      "Epoch 45, loss: 0.43991947-(best 0.43991947)\n",
      "\n",
      "Detailed Loss: recon 0.22118363-0.11059181 | disen 0.73268521-0.00000000 | temporal 0.66515529-0.33257765 | total 0.44316947\n",
      "Epoch 46, loss: 0.44316947-(best 0.43991947)\n",
      "\n",
      "Detailed Loss: recon 0.21163486-0.10581743 | disen 0.73264223-0.00000000 | temporal 0.66528100-0.33264050 | total 0.43845794\n",
      "Epoch 47, loss: 0.43845794-(best 0.43845794)\n",
      "\n",
      "Detailed Loss: recon 0.21337810-0.10668905 | disen 0.73118758-0.00000000 | temporal 0.66511697-0.33255848 | total 0.43924755\n",
      "Epoch 48, loss: 0.43924755-(best 0.43845794)\n",
      "\n",
      "Detailed Loss: recon 0.21180335-0.10590167 | disen 0.73254168-0.00000000 | temporal 0.66480350-0.33240175 | total 0.43830341\n",
      "Epoch 49, loss: 0.43830341-(best 0.43830341)\n",
      "\n",
      "Detailed Loss: recon 0.21745947-0.10872974 | disen 0.73224413-0.00000000 | temporal 0.66440094-0.33220047 | total 0.44093019\n",
      "Epoch 50, loss: 0.44093019-(best 0.43830341)\n",
      "\n",
      "Detailed Loss: recon 0.21391135-0.10695568 | disen 0.73084342-0.00000000 | temporal 0.66489470-0.33244735 | total 0.43940303\n",
      "Epoch 51, loss: 0.43940303-(best 0.43830341)\n",
      "\n",
      "Detailed Loss: recon 0.21139827-0.10569914 | disen 0.73437792-0.00000000 | temporal 0.66502392-0.33251196 | total 0.43821108\n",
      "Epoch 52, loss: 0.43821108-(best 0.43821108)\n",
      "\n",
      "Detailed Loss: recon 0.21328247-0.10664123 | disen 0.73476613-0.00000000 | temporal 0.66435421-0.33217710 | total 0.43881834\n",
      "Epoch 53, loss: 0.43881834-(best 0.43821108)\n",
      "\n",
      "Detailed Loss: recon 0.21032339-0.10516170 | disen 0.73137695-0.00000000 | temporal 0.66443592-0.33221796 | total 0.43737966\n",
      "Epoch 54, loss: 0.43737966-(best 0.43737966)\n",
      "\n",
      "Detailed Loss: recon 0.21159992-0.10579996 | disen 0.73342431-0.00000000 | temporal 0.66464919-0.33232459 | total 0.43812454\n",
      "Epoch 55, loss: 0.43812454-(best 0.43737966)\n",
      "\n",
      "Detailed Loss: recon 0.20992091-0.10496046 | disen 0.73245049-0.00000000 | temporal 0.66446364-0.33223182 | total 0.43719226\n",
      "Epoch 56, loss: 0.43719226-(best 0.43719226)\n",
      "\n",
      "Detailed Loss: recon 0.21020572-0.10510286 | disen 0.73497903-0.00000000 | temporal 0.66436392-0.33218196 | total 0.43728483\n",
      "Epoch 57, loss: 0.43728483-(best 0.43719226)\n",
      "\n",
      "Detailed Loss: recon 0.20724306-0.10362153 | disen 0.73229969-0.00000000 | temporal 0.66441482-0.33220741 | total 0.43582892\n",
      "Epoch 58, loss: 0.43582892-(best 0.43582892)\n",
      "\n",
      "Detailed Loss: recon 0.20593278-0.10296639 | disen 0.73424494-0.00000000 | temporal 0.66461831-0.33230916 | total 0.43527555\n",
      "Epoch 59, loss: 0.43527555-(best 0.43527555)\n",
      "\n",
      "Detailed Loss: recon 0.20741795-0.10370898 | disen 0.73297501-0.00000000 | temporal 0.66428012-0.33214006 | total 0.43584904\n",
      "Epoch 60, loss: 0.43584904-(best 0.43527555)\n",
      "\n",
      "Detailed Loss: recon 0.20692883-0.10346442 | disen 0.73368597-0.00000000 | temporal 0.66411543-0.33205771 | total 0.43552214\n",
      "Epoch 61, loss: 0.43552214-(best 0.43527555)\n",
      "\n",
      "Detailed Loss: recon 0.20931661-0.10465831 | disen 0.73037553-0.00000000 | temporal 0.66435617-0.33217809 | total 0.43683639\n",
      "Epoch 62, loss: 0.43683639-(best 0.43527555)\n",
      "\n",
      "Detailed Loss: recon 0.20360056-0.10180028 | disen 0.73469126-0.00000000 | temporal 0.66430908-0.33215454 | total 0.43395483\n",
      "Epoch 63, loss: 0.43395483-(best 0.43395483)\n",
      "\n",
      "Detailed Loss: recon 0.20984130-0.10492065 | disen 0.73281950-0.00000000 | temporal 0.66373122-0.33186561 | total 0.43678626\n",
      "Epoch 64, loss: 0.43678626-(best 0.43395483)\n",
      "\n",
      "Detailed Loss: recon 0.20583086-0.10291543 | disen 0.73405898-0.00000000 | temporal 0.66406840-0.33203420 | total 0.43494964\n",
      "Epoch 65, loss: 0.43494964-(best 0.43395483)\n",
      "\n",
      "Detailed Loss: recon 0.20774202-0.10387101 | disen 0.73867649-0.00000000 | temporal 0.66419959-0.33209980 | total 0.43597081\n",
      "Epoch 66, loss: 0.43597081-(best 0.43395483)\n",
      "\n",
      "Detailed Loss: recon 0.20370552-0.10185276 | disen 0.73389328-0.00000000 | temporal 0.66376138-0.33188069 | total 0.43373346\n",
      "Epoch 67, loss: 0.43373346-(best 0.43373346)\n",
      "\n",
      "Detailed Loss: recon 0.20500860-0.10250430 | disen 0.73441839-0.00000000 | temporal 0.66388381-0.33194190 | total 0.43444622\n",
      "Epoch 68, loss: 0.43444622-(best 0.43373346)\n",
      "\n",
      "Detailed Loss: recon 0.20405243-0.10202622 | disen 0.73521280-0.00000000 | temporal 0.66393280-0.33196640 | total 0.43399262\n",
      "Epoch 69, loss: 0.43399262-(best 0.43373346)\n",
      "\n",
      "Detailed Loss: recon 0.20131704-0.10065852 | disen 0.73736620-0.00000000 | temporal 0.66401476-0.33200738 | total 0.43266588\n",
      "Epoch 70, loss: 0.43266588-(best 0.43266588)\n",
      "\n",
      "Detailed Loss: recon 0.20766471-0.10383236 | disen 0.73611295-0.00000000 | temporal 0.66361505-0.33180752 | total 0.43563989\n",
      "Epoch 71, loss: 0.43563989-(best 0.43266588)\n",
      "\n",
      "Detailed Loss: recon 0.20394030-0.10197015 | disen 0.73552531-0.00000000 | temporal 0.66379184-0.33189592 | total 0.43386608\n",
      "Epoch 72, loss: 0.43386608-(best 0.43266588)\n",
      "\n",
      "Detailed Loss: recon 0.20231353-0.10115676 | disen 0.73507279-0.00000000 | temporal 0.66376138-0.33188069 | total 0.43303746\n",
      "Epoch 73, loss: 0.43303746-(best 0.43266588)\n",
      "\n",
      "Detailed Loss: recon 0.20687805-0.10343903 | disen 0.73444057-0.00000000 | temporal 0.66337061-0.33168530 | total 0.43512434\n",
      "Epoch 74, loss: 0.43512434-(best 0.43266588)\n",
      "\n",
      "Detailed Loss: recon 0.19962339-0.09981170 | disen 0.73376083-0.00000000 | temporal 0.66368610-0.33184305 | total 0.43165475\n",
      "Epoch 75, loss: 0.43165475-(best 0.43165475)\n",
      "\n",
      "Detailed Loss: recon 0.20214902-0.10107451 | disen 0.73369062-0.00000000 | temporal 0.66358858-0.33179429 | total 0.43286881\n",
      "Epoch 76, loss: 0.43286881-(best 0.43165475)\n",
      "\n",
      "Detailed Loss: recon 0.20207047-0.10103524 | disen 0.73565346-0.00000000 | temporal 0.66351306-0.33175653 | total 0.43279177\n",
      "Epoch 77, loss: 0.43279177-(best 0.43165475)\n",
      "\n",
      "Detailed Loss: recon 0.20328073-0.10164037 | disen 0.73516643-0.00000000 | temporal 0.66353226-0.33176613 | total 0.43340650\n",
      "Epoch 78, loss: 0.43340650-(best 0.43165475)\n",
      "\n",
      "Detailed Loss: recon 0.20310609-0.10155305 | disen 0.73419344-0.00000000 | temporal 0.66366154-0.33183077 | total 0.43338382\n",
      "Epoch 79, loss: 0.43338382-(best 0.43165475)\n",
      "\n",
      "Detailed Loss: recon 0.20048311-0.10024156 | disen 0.73404521-0.00000000 | temporal 0.66336513-0.33168256 | total 0.43192410\n",
      "Epoch 80, loss: 0.43192410-(best 0.43165475)\n",
      "\n",
      "Detailed Loss: recon 0.20028387-0.10014194 | disen 0.73263723-0.00000000 | temporal 0.66331798-0.33165899 | total 0.43180093\n",
      "Epoch 81, loss: 0.43180093-(best 0.43165475)\n",
      "\n",
      "Detailed Loss: recon 0.19901079-0.09950539 | disen 0.73280841-0.00000000 | temporal 0.66341400-0.33170700 | total 0.43121240\n",
      "Epoch 82, loss: 0.43121240-(best 0.43121240)\n",
      "\n",
      "Detailed Loss: recon 0.20163070-0.10081535 | disen 0.73044217-0.00000000 | temporal 0.66329330-0.33164665 | total 0.43246201\n",
      "Epoch 83, loss: 0.43246201-(best 0.43121240)\n",
      "\n",
      "Detailed Loss: recon 0.19904833-0.09952416 | disen 0.73309469-0.00000000 | temporal 0.66298485-0.33149242 | total 0.43101659\n",
      "Epoch 84, loss: 0.43101659-(best 0.43101659)\n",
      "\n",
      "Detailed Loss: recon 0.19863580-0.09931790 | disen 0.73532224-0.00000000 | temporal 0.66316986-0.33158493 | total 0.43090284\n",
      "Epoch 85, loss: 0.43090284-(best 0.43090284)\n",
      "\n",
      "Detailed Loss: recon 0.20074175-0.10037088 | disen 0.73051655-0.00000000 | temporal 0.66328907-0.33164454 | total 0.43201542\n",
      "Epoch 86, loss: 0.43201542-(best 0.43090284)\n",
      "\n",
      "Detailed Loss: recon 0.20119131-0.10059565 | disen 0.73271036-0.00000000 | temporal 0.66316259-0.33158129 | total 0.43217695\n",
      "Epoch 87, loss: 0.43217695-(best 0.43090284)\n",
      "\n",
      "Detailed Loss: recon 0.20185821-0.10092910 | disen 0.73432463-0.00000000 | temporal 0.66290689-0.33145344 | total 0.43238255\n",
      "Epoch 88, loss: 0.43238255-(best 0.43090284)\n",
      "\n",
      "Detailed Loss: recon 0.19820736-0.09910368 | disen 0.73212135-0.00000000 | temporal 0.66317439-0.33158720 | total 0.43069088\n",
      "Epoch 89, loss: 0.43069088-(best 0.43069088)\n",
      "\n",
      "Detailed Loss: recon 0.19837634-0.09918817 | disen 0.73534691-0.00000000 | temporal 0.66342592-0.33171296 | total 0.43090114\n",
      "Epoch 90, loss: 0.43090114-(best 0.43069088)\n",
      "\n",
      "Detailed Loss: recon 0.19783390-0.09891695 | disen 0.73313594-0.00000000 | temporal 0.66317111-0.33158556 | total 0.43050250\n",
      "Epoch 91, loss: 0.43050250-(best 0.43050250)\n",
      "\n",
      "Detailed Loss: recon 0.19619784-0.09809892 | disen 0.73313725-0.00000000 | temporal 0.66322798-0.33161399 | total 0.42971289\n",
      "Epoch 92, loss: 0.42971289-(best 0.42971289)\n",
      "\n",
      "Detailed Loss: recon 0.19625494-0.09812747 | disen 0.73210752-0.00000000 | temporal 0.66283977-0.33141989 | total 0.42954737\n",
      "Epoch 93, loss: 0.42954737-(best 0.42954737)\n",
      "\n",
      "Detailed Loss: recon 0.19944605-0.09972303 | disen 0.73106521-0.00000000 | temporal 0.66296685-0.33148342 | total 0.43120646\n",
      "Epoch 94, loss: 0.43120646-(best 0.42954737)\n",
      "\n",
      "Detailed Loss: recon 0.19655232-0.09827616 | disen 0.73296177-0.00000000 | temporal 0.66299647-0.33149824 | total 0.42977440\n",
      "Epoch 95, loss: 0.42977440-(best 0.42954737)\n",
      "\n",
      "Detailed Loss: recon 0.19937235-0.09968618 | disen 0.73187524-0.00000000 | temporal 0.66276169-0.33138084 | total 0.43106702\n",
      "Epoch 96, loss: 0.43106702-(best 0.42954737)\n",
      "\n",
      "Detailed Loss: recon 0.19631720-0.09815860 | disen 0.73323172-0.00000000 | temporal 0.66297638-0.33148819 | total 0.42964679\n",
      "Epoch 97, loss: 0.42964679-(best 0.42954737)\n",
      "\n",
      "Detailed Loss: recon 0.19709653-0.09854826 | disen 0.73186731-0.00000000 | temporal 0.66320884-0.33160442 | total 0.43015268\n",
      "Epoch 98, loss: 0.43015268-(best 0.42954737)\n",
      "\n",
      "Detailed Loss: recon 0.19685774-0.09842887 | disen 0.73270810-0.00000000 | temporal 0.66287524-0.33143762 | total 0.42986649\n",
      "Epoch 99, loss: 0.42986649-(best 0.42954737)\n",
      "\n",
      "Detailed Loss: recon 0.19633673-0.09816837 | disen 0.73215187-0.00000000 | temporal 0.66258705-0.33129352 | total 0.42946190\n",
      "CHECKPOINTING ERROR File ../checkpoint/working_model_file_1756868583.2905984_hybrid_epoch.pt cannot be opened.\n",
      "Epoch 100, loss: 0.42946190-(best 0.42946190)\n",
      "\n",
      ">> Reached final epoch. Loading best model...\n",
      "TRAINING XGB PREDICTOR\n",
      ">> Ending training!\n",
      "\n",
      "-----------------------\n",
      "TIME COST: 6.151936769485474 s\n",
      "Best Val: REC 69.49 PRE 41.43 MF1 66.18 AUC 79.56 TP 353 FP 499 TN 1345 FN 155\n",
      "-----------------------\n",
      "\n",
      "ASSIGNED GRAPH FOR EVALUATION: 11758 {1: 2566, 0: 9192} Nodes, 1049758 Edges\n",
      "\n",
      "-------------------\n",
      "STARTING EVALUATION\n",
      "-------------------\n",
      "\n",
      "Dataset - Overall: REC 63.60 PRE 46.35 MF1 68.71 AUC 81.88 TP 1632 FP 1889 TN 7303 FN 934 | 11758 {1: 2566, 0: 9192}\n",
      "Dataset - Train: REC 72.35 PRE 51.73 MF1 73.22 AUC 86.41 TP 552 FP 515 TN 2249 FN 211 | 3527 {0: 2764, 1: 763}\n",
      "Dataset - Val: REC 60.43 PRE 42.00 MF1 65.76 AUC 79.00 TP 307 FP 424 TN 1420 FN 201 | 2352 {0: 1844, 1: 508}\n",
      "Dataset - Test: REC 59.69 PRE 44.86 MF1 67.19 AUC 80.19 TP 773 FP 950 TN 3634 FN 522 | 5879 {1: 1295, 0: 4584}\n",
      "PREDICTION STATUS - {'1-True': 2044, '0-False': 3634, '0-True': 5558, '1-False': 522}\n",
      "    >> 522 positive nodes left unpredicted...\n",
      "    >> 522 OG positive nodes left unpredicted...\n",
      "Selecting data as budgeted ground truth for next round...\n",
      "  > Starting round 1...\n",
      "\n",
      "+++++++++++++++++\n",
      "STARTING ROUND 1!\n",
      "+++++++++++++++++\n",
      "        \n",
      "ASSIGNED GRAPH FOR TRAINING: 11758 {1: 2566, 0: 9192} Nodes, 1049758 Edges\n",
      "\n",
      ">> Splitting to train-validation set...\n",
      "Alotting train-val-test split for round 1\n",
      "Initial pool: 5879, Prediction pool: 3521, Budget pool: 0, Full pool: 7602\n",
      "Full graph size: 11758, Training: 4561, Val: 3041, Test: 11758\n",
      "    >> INITIAL DATA SPLIT: 4561 train rows ({1: 1226, 0: 3335}) | 3041 val rows ({1: 818, 0: 2223}) | 4156 test rows ({0: 3634, 1: 522})\n",
      "    >> AUGMENTED DATA SPLIT: 4561 train rows ({1: 1226, 0: 3335}) | 3041 val rows ({1: 818, 0: 2223}) | 4156 test rows ({0: 3634, 1: 522})\n",
      "    >> Updated cross-entropy weight to 2.7202283849918434...\n",
      "\n",
      "-----------------------\n",
      "STARTING MODEL TRAININGps_l\n",
      "-----------------------\n",
      "\n",
      "TRAINING NODE EMBEDDER\n",
      "Detailed Loss: recon 0.18688051-0.09344026 | disen 0.59872115-0.00000000 | temporal 0.56132138-0.28066069 | total 0.37410095\n",
      "Epoch 1, loss: 0.37410095-(best 0.37410095)\n",
      "\n",
      "Detailed Loss: recon 0.39161915-0.19580957 | disen 0.59141111-0.00000000 | temporal 0.55845529-0.27922764 | total 0.47503722\n",
      "Epoch 2, loss: 0.47503722-(best 0.37410095)\n",
      "\n",
      "Detailed Loss: recon 0.20214318-0.10107159 | disen 0.59887600-0.00000000 | temporal 0.56121975-0.28060988 | total 0.38168147\n",
      "Epoch 3, loss: 0.38168147-(best 0.37410095)\n",
      "\n",
      "Detailed Loss: recon 0.30730966-0.15365483 | disen 0.60333836-0.00000000 | temporal 0.56272781-0.28136390 | total 0.43501872\n",
      "Epoch 4, loss: 0.43501872-(best 0.37410095)\n",
      "\n",
      "Detailed Loss: recon 0.25655866-0.12827933 | disen 0.60143805-0.00000000 | temporal 0.56270200-0.28135100 | total 0.40963033\n",
      "Epoch 5, loss: 0.40963033-(best 0.37410095)\n",
      "\n",
      "Detailed Loss: recon 0.20820639-0.10410319 | disen 0.59948444-0.00000000 | temporal 0.56226468-0.28113234 | total 0.38523555\n",
      "Epoch 6, loss: 0.38523555-(best 0.37410095)\n",
      "\n",
      "Detailed Loss: recon 0.21459293-0.10729647 | disen 0.59792775-0.00000000 | temporal 0.56183010-0.28091505 | total 0.38821152\n",
      "Epoch 7, loss: 0.38821152-(best 0.37410095)\n",
      "\n",
      "Detailed Loss: recon 0.22555076-0.11277538 | disen 0.59776217-0.00000000 | temporal 0.56139469-0.28069735 | total 0.39347273\n",
      "Epoch 8, loss: 0.39347273-(best 0.37410095)\n",
      "\n",
      "Detailed Loss: recon 0.23298274-0.11649137 | disen 0.59812391-0.00000000 | temporal 0.56116444-0.28058222 | total 0.39707360\n",
      "Epoch 9, loss: 0.39707360-(best 0.37410095)\n",
      "\n",
      "Detailed Loss: recon 0.23261313-0.11630657 | disen 0.59652066-0.00000000 | temporal 0.56086528-0.28043264 | total 0.39673921\n",
      "Epoch 10, loss: 0.39673921-(best 0.37410095)\n",
      "\n",
      "Detailed Loss: recon 0.23383181-0.11691590 | disen 0.59780496-0.00000000 | temporal 0.56092370-0.28046185 | total 0.39737776\n",
      "Epoch 11, loss: 0.39737776-(best 0.37410095)\n",
      "\n",
      "Detailed Loss: recon 0.22599328-0.11299664 | disen 0.59906971-0.00000000 | temporal 0.56110513-0.28055257 | total 0.39354920\n",
      "Epoch 12, loss: 0.39354920-(best 0.37410095)\n",
      "\n",
      "Detailed Loss: recon 0.22140364-0.11070182 | disen 0.59811163-0.00000000 | temporal 0.56121892-0.28060946 | total 0.39131129\n",
      "Epoch 13, loss: 0.39131129-(best 0.37410095)\n",
      "\n",
      "Detailed Loss: recon 0.21736416-0.10868208 | disen 0.59868395-0.00000000 | temporal 0.56147534-0.28073767 | total 0.38941973\n",
      "Epoch 14, loss: 0.38941973-(best 0.37410095)\n",
      "\n",
      "Detailed Loss: recon 0.21763448-0.10881724 | disen 0.59925568-0.00000000 | temporal 0.56164557-0.28082278 | total 0.38964003\n",
      "Epoch 15, loss: 0.38964003-(best 0.37410095)\n",
      "\n",
      "Detailed Loss: recon 0.22051120-0.11025560 | disen 0.59962451-0.00000000 | temporal 0.56175470-0.28087735 | total 0.39113295\n",
      "Epoch 16, loss: 0.39113295-(best 0.37410095)\n",
      "\n",
      "Detailed Loss: recon 0.21777141-0.10888571 | disen 0.59887898-0.00000000 | temporal 0.56171477-0.28085738 | total 0.38974309\n",
      "Epoch 17, loss: 0.38974309-(best 0.37410095)\n",
      "\n",
      "Detailed Loss: recon 0.21171236-0.10585618 | disen 0.59918654-0.00000000 | temporal 0.56168813-0.28084406 | total 0.38670024\n",
      "Epoch 18, loss: 0.38670024-(best 0.37410095)\n",
      "\n",
      "Detailed Loss: recon 0.20982468-0.10491234 | disen 0.59975421-0.00000000 | temporal 0.56151187-0.28075594 | total 0.38566828\n",
      "Epoch 19, loss: 0.38566828-(best 0.37410095)\n",
      "\n",
      "Detailed Loss: recon 0.20988885-0.10494442 | disen 0.60007596-0.00000000 | temporal 0.56129330-0.28064665 | total 0.38559109\n",
      "Epoch 20, loss: 0.38559109-(best 0.37410095)\n",
      "\n",
      "Detailed Loss: recon 0.20957819-0.10478909 | disen 0.59817123-0.00000000 | temporal 0.56100005-0.28050002 | total 0.38528913\n",
      "Epoch 21, loss: 0.38528913-(best 0.37410095)\n",
      "\n",
      "Detailed Loss: recon 0.20740685-0.10370342 | disen 0.59892642-0.00000000 | temporal 0.56081492-0.28040746 | total 0.38411087\n",
      "Epoch 22, loss: 0.38411087-(best 0.37410095)\n",
      "\n",
      "Detailed Loss: recon 0.20745397-0.10372698 | disen 0.59859824-0.00000000 | temporal 0.56069881-0.28034940 | total 0.38407639\n",
      "Epoch 23, loss: 0.38407639-(best 0.37410095)\n",
      "\n",
      "Detailed Loss: recon 0.20557055-0.10278527 | disen 0.59898096-0.00000000 | temporal 0.56073469-0.28036734 | total 0.38315260\n",
      "Epoch 24, loss: 0.38315260-(best 0.37410095)\n",
      "\n",
      "Detailed Loss: recon 0.20245002-0.10122501 | disen 0.59887213-0.00000000 | temporal 0.56075799-0.28037900 | total 0.38160402\n",
      "Epoch 25, loss: 0.38160402-(best 0.37410095)\n",
      "\n",
      "Detailed Loss: recon 0.20200782-0.10100391 | disen 0.60064018-0.00000000 | temporal 0.56086248-0.28043124 | total 0.38143516\n",
      "Epoch 26, loss: 0.38143516-(best 0.37410095)\n",
      "\n",
      "Detailed Loss: recon 0.20160119-0.10080060 | disen 0.60058492-0.00000000 | temporal 0.56096053-0.28048027 | total 0.38128087\n",
      "Epoch 27, loss: 0.38128087-(best 0.37410095)\n",
      "\n",
      "Detailed Loss: recon 0.20059013-0.10029507 | disen 0.60039270-0.00000000 | temporal 0.56101328-0.28050664 | total 0.38080171\n",
      "Epoch 28, loss: 0.38080171-(best 0.37410095)\n",
      "\n",
      "Detailed Loss: recon 0.19791912-0.09895956 | disen 0.59995389-0.00000000 | temporal 0.56099564-0.28049782 | total 0.37945738\n",
      "Epoch 29, loss: 0.37945738-(best 0.37410095)\n",
      "\n",
      "Detailed Loss: recon 0.19816284-0.09908142 | disen 0.60080552-0.00000000 | temporal 0.56104094-0.28052047 | total 0.37960190\n",
      "Epoch 30, loss: 0.37960190-(best 0.37410095)\n",
      "\n",
      "Detailed Loss: recon 0.19837406-0.09918703 | disen 0.60003644-0.00000000 | temporal 0.56097198-0.28048599 | total 0.37967300\n",
      "Epoch 31, loss: 0.37967300-(best 0.37410095)\n",
      "\n",
      "Detailed Loss: recon 0.19374888-0.09687444 | disen 0.60016322-0.00000000 | temporal 0.56090516-0.28045258 | total 0.37732702\n",
      "Epoch 32, loss: 0.37732702-(best 0.37410095)\n",
      "\n",
      "Detailed Loss: recon 0.19417638-0.09708819 | disen 0.59993863-0.00000000 | temporal 0.56084722-0.28042361 | total 0.37751180\n",
      "Epoch 33, loss: 0.37751180-(best 0.37410095)\n",
      "\n",
      "Detailed Loss: recon 0.19623625-0.09811813 | disen 0.59978342-0.00000000 | temporal 0.56087857-0.28043929 | total 0.37855741\n",
      "Epoch 34, loss: 0.37855741-(best 0.37410095)\n",
      "\n",
      "Detailed Loss: recon 0.19187868-0.09593934 | disen 0.60083032-0.00000000 | temporal 0.56084567-0.28042284 | total 0.37636217\n",
      "Epoch 35, loss: 0.37636217-(best 0.37410095)\n",
      "\n",
      "Detailed Loss: recon 0.19212392-0.09606196 | disen 0.60031807-0.00000000 | temporal 0.56092054-0.28046027 | total 0.37652224\n",
      "Epoch 36, loss: 0.37652224-(best 0.37410095)\n",
      "\n",
      "Detailed Loss: recon 0.19285387-0.09642693 | disen 0.60023510-0.00000000 | temporal 0.56089276-0.28044638 | total 0.37687331\n",
      "Epoch 37, loss: 0.37687331-(best 0.37410095)\n",
      "\n",
      "Detailed Loss: recon 0.19277647-0.09638824 | disen 0.60052651-0.00000000 | temporal 0.56087250-0.28043625 | total 0.37682450\n",
      "Epoch 38, loss: 0.37682450-(best 0.37410095)\n",
      "\n",
      "Detailed Loss: recon 0.19048735-0.09524368 | disen 0.59978807-0.00000000 | temporal 0.56080019-0.28040010 | total 0.37564379\n",
      "Epoch 39, loss: 0.37564379-(best 0.37410095)\n",
      "\n",
      "Detailed Loss: recon 0.19183871-0.09591936 | disen 0.59974003-0.00000000 | temporal 0.56072640-0.28036320 | total 0.37628257\n",
      "Epoch 40, loss: 0.37628257-(best 0.37410095)\n",
      "\n",
      "Detailed Loss: recon 0.18755344-0.09377672 | disen 0.59982526-0.00000000 | temporal 0.56071943-0.28035972 | total 0.37413645\n",
      "Epoch 41, loss: 0.37413645-(best 0.37410095)\n",
      "\n",
      "Detailed Loss: recon 0.19029820-0.09514910 | disen 0.59932590-0.00000000 | temporal 0.56057608-0.28028804 | total 0.37543714\n",
      "Epoch 42, loss: 0.37543714-(best 0.37410095)\n",
      "\n",
      "Detailed Loss: recon 0.18806128-0.09403064 | disen 0.60002106-0.00000000 | temporal 0.56057948-0.28028974 | total 0.37432039\n",
      "Epoch 43, loss: 0.37432039-(best 0.37410095)\n",
      "\n",
      "Detailed Loss: recon 0.18636820-0.09318410 | disen 0.59954500-0.00000000 | temporal 0.56054407-0.28027204 | total 0.37345612\n",
      "Epoch 44, loss: 0.37345612-(best 0.37345612)\n",
      "\n",
      "Detailed Loss: recon 0.18849280-0.09424640 | disen 0.59956443-0.00000000 | temporal 0.56052190-0.28026095 | total 0.37450737\n",
      "Epoch 45, loss: 0.37450737-(best 0.37345612)\n",
      "\n",
      "Detailed Loss: recon 0.18707132-0.09353566 | disen 0.60010600-0.00000000 | temporal 0.56050032-0.28025016 | total 0.37378582\n",
      "Epoch 46, loss: 0.37378582-(best 0.37345612)\n",
      "\n",
      "Detailed Loss: recon 0.18741465-0.09370732 | disen 0.60064465-0.00000000 | temporal 0.56051403-0.28025702 | total 0.37396434\n",
      "Epoch 47, loss: 0.37396434-(best 0.37345612)\n",
      "\n",
      "Detailed Loss: recon 0.18522905-0.09261452 | disen 0.60007447-0.00000000 | temporal 0.56042826-0.28021413 | total 0.37282866\n",
      "Epoch 48, loss: 0.37282866-(best 0.37282866)\n",
      "\n",
      "Detailed Loss: recon 0.18655401-0.09327701 | disen 0.59946501-0.00000000 | temporal 0.56033814-0.28016907 | total 0.37344608\n",
      "Epoch 49, loss: 0.37344608-(best 0.37282866)\n",
      "\n",
      "Detailed Loss: recon 0.18548456-0.09274228 | disen 0.59946680-0.00000000 | temporal 0.56021780-0.28010890 | total 0.37285119\n",
      "Epoch 50, loss: 0.37285119-(best 0.37282866)\n",
      "\n",
      ">> Reached final epoch. Loading best model...\n",
      "TRAINING XGB PREDICTOR\n",
      ">> Ending training!\n",
      "\n",
      "-----------------------\n",
      "TIME COST: 4.900082349777222 s\n",
      "Best Val: REC 57.70 PRE 51.64 MF1 68.19 AUC 79.18 TP 472 FP 442 TN 1781 FN 346\n",
      "-----------------------\n",
      "Generating additional positive data from adversary...\n",
      "Generating additional negative data by duplicating random nodes...\n",
      "\n",
      "ASSIGNED GRAPH FOR EVALUATION: 12345 {1: 2694, 0: 9651} Nodes, 1156722 Edges\n",
      "\n",
      "-------------------\n",
      "STARTING EVALUATION\n",
      "-------------------\n",
      "\n",
      "Dataset - Overall: REC 62.69 PRE 54.55 MF1 72.79 AUC 84.27 TP 1689 FP 1407 TN 8244 FN 1005 | 12345 {1: 2694, 0: 9651}\n",
      "Dataset - Train: REC 79.12 PRE 64.58 MF1 79.40 AUC 90.02 TP 970 FP 532 TN 2803 FN 256 | 4561 {1: 1226, 0: 3335}\n",
      "Dataset - Val: REC 61.86 PRE 51.90 MF1 69.12 AUC 79.39 TP 506 FP 469 TN 1754 FN 312 | 3041 {1: 818, 0: 2223}\n",
      "Dataset - Test: REC 32.77 PRE 34.41 MF1 61.66 AUC 77.15 TP 213 FP 406 TN 3687 FN 437 | 4743 {0: 4093, 1: 650}\n",
      "PREDICTION STATUS - {'1-True': 2257, '0-False': 3687, '0-True': 5964, '1-False': 437}\n",
      "    >> 437 positive nodes left unpredicted...\n",
      "    >> 362 OG positive nodes left unpredicted...\n",
      "-----------------------\n",
      "PREDICTION RESULT - ROUNDS\n",
      "Dataset - Round 0: REC 58.22 PRE 52.65 MF1 70.90 AUC 81.99 TP 754 FP 678 TN 3906 FN 541 | 5879 {1: 1295, 0: 4584}\n",
      "Dataset - Round 1: REC 41.41 PRE 43.44 MF1 63.41 AUC 73.93 TP 53 FP 69 TN 390 FN 75 | 587 {1: 128, 0: 459}\n",
      "-----------------------\n",
      "PREDICTION RESULT - SEEDS\n",
      "Seeds - Current: REC 35.94 PRE 44.23 MF1 62.40 AUC 73.55 TP 46 FP 58 TN 401 FN 82 | 587 {1: 128, 0: 459}\n",
      "Seeds - Prev: REC 0.00 PRE 0.00 MF1 38.15 AUC 73.55 TP 0 FP 97 TN 362 FN 128 | 587 {1: 128, 0: 459}\n",
      "Selecting data as budgeted ground truth for next round...\n",
      "  > Starting round 2...\n",
      "\n",
      "+++++++++++++++++\n",
      "STARTING ROUND 2!\n",
      "+++++++++++++++++\n",
      "        \n",
      "ASSIGNED GRAPH FOR TRAINING: 12345 {1: 2694, 0: 9651} Nodes, 1156722 Edges\n",
      "\n",
      ">> Splitting to train-validation set...\n",
      "Alotting train-val-test split for round 2\n",
      "Initial pool: 5879, Prediction pool: 6617, Budget pool: 0, Full pool: 8221\n",
      "Full graph size: 12345, Training: 4932, Val: 3289, Test: 12345\n",
      "    >> INITIAL DATA SPLIT: 4932 train rows ({1: 1354, 0: 3578}) | 3289 val rows ({0: 2386, 1: 903}) | 4124 test rows ({0: 3687, 1: 437})\n",
      "    >> AUGMENTED DATA SPLIT: 4932 train rows ({1: 1354, 0: 3578}) | 3289 val rows ({0: 2386, 1: 903}) | 4124 test rows ({0: 3687, 1: 437})\n",
      "    >> Updated cross-entropy weight to 2.642540620384047...\n",
      "\n",
      "-----------------------\n",
      "STARTING MODEL TRAININGps_l\n",
      "-----------------------\n",
      "\n",
      "TRAINING NODE EMBEDDER\n",
      "Detailed Loss: recon 0.17937922-0.08968961 | disen 0.59526640-0.00000000 | temporal 0.58049583-0.29024792 | total 0.37993753\n",
      "Epoch 1, loss: 0.37993753-(best 0.37993753)\n",
      "\n",
      "Detailed Loss: recon 0.26620913-0.13310456 | disen 0.58565116-0.00000000 | temporal 0.57886684-0.28943342 | total 0.42253798\n",
      "Epoch 2, loss: 0.42253798-(best 0.37993753)\n",
      "\n",
      "Detailed Loss: recon 0.18714248-0.09357124 | disen 0.59291530-0.00000000 | temporal 0.58054042-0.29027021 | total 0.38384145\n",
      "Epoch 3, loss: 0.38384145-(best 0.37993753)\n",
      "\n",
      "Detailed Loss: recon 0.22793394-0.11396697 | disen 0.59538329-0.00000000 | temporal 0.58142692-0.29071346 | total 0.40468043\n",
      "Epoch 4, loss: 0.40468043-(best 0.37993753)\n",
      "\n",
      "Detailed Loss: recon 0.20622426-0.10311213 | disen 0.59519434-0.00000000 | temporal 0.58129847-0.29064924 | total 0.39376137\n",
      "Epoch 5, loss: 0.39376137-(best 0.37993753)\n",
      "\n",
      "Detailed Loss: recon 0.18633178-0.09316589 | disen 0.59375882-0.00000000 | temporal 0.58091199-0.29045600 | total 0.38362187\n",
      "Epoch 6, loss: 0.38362187-(best 0.37993753)\n",
      "\n",
      "Detailed Loss: recon 0.19054489-0.09527244 | disen 0.59168023-0.00000000 | temporal 0.58052456-0.29026228 | total 0.38553473\n",
      "Epoch 7, loss: 0.38553473-(best 0.37993753)\n",
      "\n",
      "Detailed Loss: recon 0.19610600-0.09805300 | disen 0.59080482-0.00000000 | temporal 0.58039451-0.29019725 | total 0.38825026\n",
      "Epoch 8, loss: 0.38825026-(best 0.37993753)\n",
      "\n",
      "Detailed Loss: recon 0.19698781-0.09849390 | disen 0.59067571-0.00000000 | temporal 0.58036417-0.29018208 | total 0.38867599\n",
      "Epoch 9, loss: 0.38867599-(best 0.37993753)\n",
      "\n",
      "Detailed Loss: recon 0.19108634-0.09554317 | disen 0.59152561-0.00000000 | temporal 0.58060753-0.29030377 | total 0.38584694\n",
      "Epoch 10, loss: 0.38584694-(best 0.37993753)\n",
      "\n",
      "Detailed Loss: recon 0.18802857-0.09401429 | disen 0.59200275-0.00000000 | temporal 0.58090514-0.29045257 | total 0.38446686\n",
      "Epoch 11, loss: 0.38446686-(best 0.37993753)\n",
      "\n",
      "Detailed Loss: recon 0.19046062-0.09523031 | disen 0.59220362-0.00000000 | temporal 0.58113247-0.29056624 | total 0.38579655\n",
      "Epoch 12, loss: 0.38579655-(best 0.37993753)\n",
      "\n",
      "Detailed Loss: recon 0.19280373-0.09640186 | disen 0.59113210-0.00000000 | temporal 0.58122492-0.29061246 | total 0.38701433\n",
      "Epoch 13, loss: 0.38701433-(best 0.37993753)\n",
      "\n",
      "Detailed Loss: recon 0.19024536-0.09512268 | disen 0.59222770-0.00000000 | temporal 0.58120632-0.29060316 | total 0.38572586\n",
      "Epoch 14, loss: 0.38572586-(best 0.37993753)\n",
      "\n",
      "Detailed Loss: recon 0.18840270-0.09420135 | disen 0.59148633-0.00000000 | temporal 0.58105314-0.29052657 | total 0.38472793\n",
      "Epoch 15, loss: 0.38472793-(best 0.37993753)\n",
      "\n",
      "Detailed Loss: recon 0.18685171-0.09342586 | disen 0.59109235-0.00000000 | temporal 0.58089948-0.29044974 | total 0.38387561\n",
      "Epoch 16, loss: 0.38387561-(best 0.37993753)\n",
      "\n",
      "Detailed Loss: recon 0.18634707-0.09317353 | disen 0.59019828-0.00000000 | temporal 0.58067858-0.29033929 | total 0.38351282\n",
      "Epoch 17, loss: 0.38351282-(best 0.37993753)\n",
      "\n",
      "Detailed Loss: recon 0.18818912-0.09409456 | disen 0.59006566-0.00000000 | temporal 0.58062124-0.29031062 | total 0.38440520\n",
      "Epoch 18, loss: 0.38440520-(best 0.37993753)\n",
      "\n",
      "Detailed Loss: recon 0.18660343-0.09330171 | disen 0.58938122-0.00000000 | temporal 0.58048970-0.29024485 | total 0.38354656\n",
      "Epoch 19, loss: 0.38354656-(best 0.37993753)\n",
      "\n",
      "Detailed Loss: recon 0.18503425-0.09251712 | disen 0.58989036-0.00000000 | temporal 0.58059424-0.29029712 | total 0.38281423\n",
      "Epoch 20, loss: 0.38281423-(best 0.37993753)\n",
      "\n",
      "Detailed Loss: recon 0.18311936-0.09155968 | disen 0.59065366-0.00000000 | temporal 0.58070421-0.29035211 | total 0.38191178\n",
      "Epoch 21, loss: 0.38191178-(best 0.37993753)\n",
      "\n",
      "Detailed Loss: recon 0.18119106-0.09059553 | disen 0.59107155-0.00000000 | temporal 0.58080691-0.29040346 | total 0.38099897\n",
      "Epoch 22, loss: 0.38099897-(best 0.37993753)\n",
      "\n",
      "Detailed Loss: recon 0.18209502-0.09104751 | disen 0.59185594-0.00000000 | temporal 0.58085567-0.29042783 | total 0.38147533\n",
      "Epoch 23, loss: 0.38147533-(best 0.37993753)\n",
      "\n",
      "Detailed Loss: recon 0.18186021-0.09093010 | disen 0.59203720-0.00000000 | temporal 0.58086199-0.29043099 | total 0.38136110\n",
      "Epoch 24, loss: 0.38136110-(best 0.37993753)\n",
      "\n",
      "Detailed Loss: recon 0.18209419-0.09104709 | disen 0.59190881-0.00000000 | temporal 0.58075351-0.29037675 | total 0.38142383\n",
      "Epoch 25, loss: 0.38142383-(best 0.37993753)\n",
      "\n",
      "Detailed Loss: recon 0.18010388-0.09005194 | disen 0.59107590-0.00000000 | temporal 0.58063686-0.29031843 | total 0.38037038\n",
      "Epoch 26, loss: 0.38037038-(best 0.37993753)\n",
      "\n",
      "Detailed Loss: recon 0.18088761-0.09044380 | disen 0.59140813-0.00000000 | temporal 0.58050680-0.29025340 | total 0.38069719\n",
      "Epoch 27, loss: 0.38069719-(best 0.37993753)\n",
      "\n",
      "Detailed Loss: recon 0.18038508-0.09019254 | disen 0.59185624-0.00000000 | temporal 0.58046156-0.29023078 | total 0.38042331\n",
      "Epoch 28, loss: 0.38042331-(best 0.37993753)\n",
      "\n",
      "Detailed Loss: recon 0.18066272-0.09033136 | disen 0.59154177-0.00000000 | temporal 0.58046591-0.29023296 | total 0.38056433\n",
      "Epoch 29, loss: 0.38056433-(best 0.37993753)\n",
      "\n",
      "Detailed Loss: recon 0.17917216-0.08958608 | disen 0.59207022-0.00000000 | temporal 0.58048266-0.29024133 | total 0.37982741\n",
      "Epoch 30, loss: 0.37982741-(best 0.37982741)\n",
      "\n",
      "Detailed Loss: recon 0.17870241-0.08935121 | disen 0.59222758-0.00000000 | temporal 0.58060557-0.29030278 | total 0.37965399\n",
      "Epoch 31, loss: 0.37965399-(best 0.37965399)\n",
      "\n",
      "Detailed Loss: recon 0.17910859-0.08955429 | disen 0.59201562-0.00000000 | temporal 0.58063728-0.29031864 | total 0.37987292\n",
      "Epoch 32, loss: 0.37987292-(best 0.37965399)\n",
      "\n",
      "Detailed Loss: recon 0.17996369-0.08998185 | disen 0.59217882-0.00000000 | temporal 0.58064687-0.29032344 | total 0.38030529\n",
      "Epoch 33, loss: 0.38030529-(best 0.37965399)\n",
      "\n",
      "Detailed Loss: recon 0.17847571-0.08923785 | disen 0.59250802-0.00000000 | temporal 0.58053911-0.29026955 | total 0.37950742\n",
      "Epoch 34, loss: 0.37950742-(best 0.37950742)\n",
      "\n",
      "Detailed Loss: recon 0.17694432-0.08847216 | disen 0.59191549-0.00000000 | temporal 0.58038956-0.29019478 | total 0.37866694\n",
      "Epoch 35, loss: 0.37866694-(best 0.37866694)\n",
      "\n",
      "Detailed Loss: recon 0.17767267-0.08883633 | disen 0.59122157-0.00000000 | temporal 0.58033150-0.29016575 | total 0.37900209\n",
      "Epoch 36, loss: 0.37900209-(best 0.37866694)\n",
      "\n",
      "Detailed Loss: recon 0.17640293-0.08820146 | disen 0.59224057-0.00000000 | temporal 0.58030736-0.29015368 | total 0.37835515\n",
      "Epoch 37, loss: 0.37835515-(best 0.37835515)\n",
      "\n",
      "Detailed Loss: recon 0.17761002-0.08880501 | disen 0.59112173-0.00000000 | temporal 0.58023190-0.29011595 | total 0.37892097\n",
      "Epoch 38, loss: 0.37892097-(best 0.37835515)\n",
      "\n",
      "Detailed Loss: recon 0.17677325-0.08838663 | disen 0.59172589-0.00000000 | temporal 0.58024555-0.29012278 | total 0.37850940\n",
      "Epoch 39, loss: 0.37850940-(best 0.37835515)\n",
      "\n",
      "Detailed Loss: recon 0.17695677-0.08847839 | disen 0.59168804-0.00000000 | temporal 0.58026981-0.29013491 | total 0.37861329\n",
      "Epoch 40, loss: 0.37861329-(best 0.37835515)\n",
      "\n",
      "Detailed Loss: recon 0.17569289-0.08784644 | disen 0.59223509-0.00000000 | temporal 0.58027577-0.29013789 | total 0.37798434\n",
      "Epoch 41, loss: 0.37798434-(best 0.37798434)\n",
      "\n",
      "Detailed Loss: recon 0.17741428-0.08870714 | disen 0.59178680-0.00000000 | temporal 0.58017612-0.29008806 | total 0.37879521\n",
      "Epoch 42, loss: 0.37879521-(best 0.37798434)\n",
      "\n",
      "Detailed Loss: recon 0.17592542-0.08796271 | disen 0.59109020-0.00000000 | temporal 0.58011514-0.29005757 | total 0.37802029\n",
      "Epoch 43, loss: 0.37802029-(best 0.37798434)\n",
      "\n",
      "Detailed Loss: recon 0.17642555-0.08821277 | disen 0.59158176-0.00000000 | temporal 0.58011883-0.29005942 | total 0.37827218\n",
      "Epoch 44, loss: 0.37827218-(best 0.37798434)\n",
      "\n",
      "Detailed Loss: recon 0.17735361-0.08867680 | disen 0.59148514-0.00000000 | temporal 0.58008176-0.29004088 | total 0.37871769\n",
      "Epoch 45, loss: 0.37871769-(best 0.37798434)\n",
      "\n",
      "Detailed Loss: recon 0.17625338-0.08812669 | disen 0.59138900-0.00000000 | temporal 0.58010715-0.29005358 | total 0.37818027\n",
      "Epoch 46, loss: 0.37818027-(best 0.37798434)\n",
      "\n",
      "Detailed Loss: recon 0.17658207-0.08829103 | disen 0.59094220-0.00000000 | temporal 0.58008498-0.29004249 | total 0.37833351\n",
      "Epoch 47, loss: 0.37833351-(best 0.37798434)\n",
      "\n",
      "Detailed Loss: recon 0.17532298-0.08766149 | disen 0.59107035-0.00000000 | temporal 0.58008116-0.29004058 | total 0.37770206\n",
      "Epoch 48, loss: 0.37770206-(best 0.37770206)\n",
      "\n",
      "Detailed Loss: recon 0.17575720-0.08787860 | disen 0.59121180-0.00000000 | temporal 0.58006722-0.29003361 | total 0.37791222\n",
      "Epoch 49, loss: 0.37791222-(best 0.37770206)\n",
      "\n",
      "Detailed Loss: recon 0.17554912-0.08777456 | disen 0.59075361-0.00000000 | temporal 0.57997942-0.28998971 | total 0.37776428\n",
      "Epoch 50, loss: 0.37776428-(best 0.37770206)\n",
      "\n",
      ">> Reached final epoch. Loading best model...\n",
      "TRAINING XGB PREDICTOR\n",
      ">> Ending training!\n",
      "\n",
      "-----------------------\n",
      "TIME COST: 5.3363282680511475 s\n",
      "Best Val: REC 48.50 PRE 56.66 MF1 67.97 AUC 77.60 TP 438 FP 335 TN 2051 FN 465\n",
      "-----------------------\n",
      "Generating additional positive data from adversary...\n",
      "Generating additional negative data by duplicating random nodes...\n",
      "\n",
      "ASSIGNED GRAPH FOR EVALUATION: 12932 {1: 2822, 0: 10110} Nodes, 1263436 Edges\n",
      "\n",
      "-------------------\n",
      "STARTING EVALUATION\n",
      "-------------------\n",
      "\n",
      "Dataset - Overall: REC 56.98 PRE 53.25 MF1 70.97 AUC 81.96 TP 1608 FP 1412 TN 8698 FN 1214 | 12932 {1: 2822, 0: 10110}\n",
      "Dataset - Train: REC 72.75 PRE 63.34 MF1 77.11 AUC 86.99 TP 985 FP 570 TN 3008 FN 369 | 4932 {1: 1354, 0: 3578}\n",
      "Dataset - Val: REC 54.15 PRE 50.73 MF1 66.76 AUC 76.37 TP 489 FP 475 TN 1911 FN 414 | 3289 {0: 2386, 1: 903}\n",
      "Dataset - Test: REC 23.72 PRE 26.75 MF1 57.80 AUC 74.37 TP 134 FP 367 TN 3779 FN 431 | 4711 {0: 4146, 1: 565}\n",
      "PREDICTION STATUS - {'1-True': 2391, '0-False': 3779, '0-True': 6331, '1-False': 431}\n",
      "    >> 431 positive nodes left unpredicted...\n",
      "    >> 273 OG positive nodes left unpredicted...\n",
      "-----------------------\n",
      "PREDICTION RESULT - ROUNDS\n",
      "Dataset - Round 0: REC 57.53 PRE 52.65 MF1 70.75 AUC 81.54 TP 745 FP 670 TN 3914 FN 550 | 5879 {1: 1295, 0: 4584}\n",
      "Dataset - Round 1: REC 35.94 PRE 39.66 MF1 60.68 AUC 71.17 TP 46 FP 70 TN 389 FN 82 | 587 {1: 128, 0: 459}\n",
      "Dataset - Round 2: REC 23.44 PRE 34.48 MF1 55.87 AUC 68.01 TP 30 FP 57 TN 402 FN 98 | 587 {1: 128, 0: 459}\n",
      "-----------------------\n",
      "PREDICTION RESULT - SEEDS\n",
      "Seeds - Current: REC 29.69 PRE 36.54 MF1 58.10 AUC 67.40 TP 38 FP 66 TN 393 FN 90 | 587 {1: 128, 0: 459}\n",
      "Seeds - Prev: REC 0.00 PRE 0.00 MF1 39.61 AUC 67.40 TP 0 FP 74 TN 385 FN 128 | 587 {1: 128, 0: 459}\n",
      "Selecting data as budgeted ground truth for next round...\n",
      "  > Starting round 3...\n",
      "\n",
      "+++++++++++++++++\n",
      "STARTING ROUND 3!\n",
      "+++++++++++++++++\n",
      "        \n",
      "ASSIGNED GRAPH FOR TRAINING: 12932 {1: 2822, 0: 10110} Nodes, 1263436 Edges\n",
      "\n",
      ">> Splitting to train-validation set...\n",
      "Alotting train-val-test split for round 3\n",
      "Initial pool: 5879, Prediction pool: 9637, Budget pool: 0, Full pool: 8722\n",
      "Full graph size: 12932, Training: 5233, Val: 3489, Test: 12932\n",
      "    >> INITIAL DATA SPLIT: 5233 train rows ({0: 3798, 1: 1435}) | 3489 val rows ({1: 956, 0: 2533}) | 4210 test rows ({0: 3779, 1: 431})\n",
      "    >> AUGMENTED DATA SPLIT: 5233 train rows ({0: 3798, 1: 1435}) | 3489 val rows ({1: 956, 0: 2533}) | 4210 test rows ({0: 3779, 1: 431})\n",
      "    >> Updated cross-entropy weight to 2.6466898954703835...\n",
      "\n",
      "-----------------------\n",
      "STARTING MODEL TRAININGps_l\n",
      "-----------------------\n",
      "\n",
      "TRAINING NODE EMBEDDER\n",
      "Detailed Loss: recon 0.18068677-0.09034339 | disen 0.58545160-0.00000000 | temporal 0.60176325-0.30088162 | total 0.39122501\n",
      "Epoch 1, loss: 0.39122501-(best 0.39122501)\n",
      "\n",
      "Detailed Loss: recon 0.23089704-0.11544852 | disen 0.58312178-0.00000000 | temporal 0.60036063-0.30018032 | total 0.41562885\n",
      "Epoch 2, loss: 0.41562885-(best 0.39122501)\n",
      "\n",
      "Detailed Loss: recon 0.19369704-0.09684852 | disen 0.58893430-0.00000000 | temporal 0.60180062-0.30090031 | total 0.39774883\n",
      "Epoch 3, loss: 0.39774883-(best 0.39122501)\n",
      "\n",
      "Detailed Loss: recon 0.21202007-0.10601003 | disen 0.59056062-0.00000000 | temporal 0.60237420-0.30118710 | total 0.40719712\n",
      "Epoch 4, loss: 0.40719712-(best 0.39122501)\n",
      "\n",
      "Detailed Loss: recon 0.19078580-0.09539290 | disen 0.58955681-0.00000000 | temporal 0.60218489-0.30109245 | total 0.39648533\n",
      "Epoch 5, loss: 0.39648533-(best 0.39122501)\n",
      "\n",
      "Detailed Loss: recon 0.18876213-0.09438106 | disen 0.58683115-0.00000000 | temporal 0.60170668-0.30085334 | total 0.39523441\n",
      "Epoch 6, loss: 0.39523441-(best 0.39122501)\n",
      "\n",
      "Detailed Loss: recon 0.19889967-0.09944984 | disen 0.58748710-0.00000000 | temporal 0.60151726-0.30075863 | total 0.40020847\n",
      "Epoch 7, loss: 0.40020847-(best 0.39122501)\n",
      "\n",
      "Detailed Loss: recon 0.19537354-0.09768677 | disen 0.58816648-0.00000000 | temporal 0.60156089-0.30078045 | total 0.39846721\n",
      "Epoch 8, loss: 0.39846721-(best 0.39122501)\n",
      "\n",
      "Detailed Loss: recon 0.18672900-0.09336450 | disen 0.58706528-0.00000000 | temporal 0.60174978-0.30087489 | total 0.39423940\n",
      "Epoch 9, loss: 0.39423940-(best 0.39122501)\n",
      "\n",
      "Detailed Loss: recon 0.18471861-0.09235930 | disen 0.58883512-0.00000000 | temporal 0.60208476-0.30104238 | total 0.39340168\n",
      "Epoch 10, loss: 0.39340168-(best 0.39122501)\n",
      "\n",
      "Detailed Loss: recon 0.18786642-0.09393321 | disen 0.58800697-0.00000000 | temporal 0.60222971-0.30111486 | total 0.39504808\n",
      "Epoch 11, loss: 0.39504808-(best 0.39122501)\n",
      "\n",
      "Detailed Loss: recon 0.19171928-0.09585964 | disen 0.58917326-0.00000000 | temporal 0.60226345-0.30113173 | total 0.39699137\n",
      "Epoch 12, loss: 0.39699137-(best 0.39122501)\n",
      "\n",
      "Detailed Loss: recon 0.18726020-0.09363010 | disen 0.58886832-0.00000000 | temporal 0.60219133-0.30109566 | total 0.39472577\n",
      "Epoch 13, loss: 0.39472577-(best 0.39122501)\n",
      "\n",
      "Detailed Loss: recon 0.18342826-0.09171413 | disen 0.58872026-0.00000000 | temporal 0.60203201-0.30101600 | total 0.39273012\n",
      "Epoch 14, loss: 0.39273012-(best 0.39122501)\n",
      "\n",
      "Detailed Loss: recon 0.18356131-0.09178066 | disen 0.58781350-0.00000000 | temporal 0.60173833-0.30086917 | total 0.39264983\n",
      "Epoch 15, loss: 0.39264983-(best 0.39122501)\n",
      "\n",
      "Detailed Loss: recon 0.18405193-0.09202597 | disen 0.58739376-0.00000000 | temporal 0.60162854-0.30081427 | total 0.39284024\n",
      "Epoch 16, loss: 0.39284024-(best 0.39122501)\n",
      "\n",
      "Detailed Loss: recon 0.18342578-0.09171289 | disen 0.58750987-0.00000000 | temporal 0.60163432-0.30081716 | total 0.39253005\n",
      "Epoch 17, loss: 0.39253005-(best 0.39122501)\n",
      "\n",
      "Detailed Loss: recon 0.18372649-0.09186324 | disen 0.58679330-0.00000000 | temporal 0.60159880-0.30079940 | total 0.39266264\n",
      "Epoch 18, loss: 0.39266264-(best 0.39122501)\n",
      "\n",
      "Detailed Loss: recon 0.18211320-0.09105660 | disen 0.58663315-0.00000000 | temporal 0.60170048-0.30085024 | total 0.39190686\n",
      "Epoch 19, loss: 0.39190686-(best 0.39122501)\n",
      "\n",
      "Detailed Loss: recon 0.18213430-0.09106715 | disen 0.58577979-0.00000000 | temporal 0.60177732-0.30088866 | total 0.39195579\n",
      "Epoch 20, loss: 0.39195579-(best 0.39122501)\n",
      "\n",
      "Detailed Loss: recon 0.18192874-0.09096437 | disen 0.58609354-0.00000000 | temporal 0.60174567-0.30087283 | total 0.39183721\n",
      "Epoch 21, loss: 0.39183721-(best 0.39122501)\n",
      "\n",
      "Detailed Loss: recon 0.18131952-0.09065976 | disen 0.58577114-0.00000000 | temporal 0.60172606-0.30086303 | total 0.39152279\n",
      "Epoch 22, loss: 0.39152279-(best 0.39122501)\n",
      "\n",
      "Detailed Loss: recon 0.18081906-0.09040953 | disen 0.58637881-0.00000000 | temporal 0.60161889-0.30080944 | total 0.39121896\n",
      "Epoch 23, loss: 0.39121896-(best 0.39121896)\n",
      "\n",
      "Detailed Loss: recon 0.17991266-0.08995633 | disen 0.58638614-0.00000000 | temporal 0.60153198-0.30076599 | total 0.39072233\n",
      "Epoch 24, loss: 0.39072233-(best 0.39072233)\n",
      "\n",
      "Detailed Loss: recon 0.18110661-0.09055331 | disen 0.58638084-0.00000000 | temporal 0.60147941-0.30073971 | total 0.39129302\n",
      "Epoch 25, loss: 0.39129302-(best 0.39072233)\n",
      "\n",
      "Detailed Loss: recon 0.18015079-0.09007540 | disen 0.58687449-0.00000000 | temporal 0.60150248-0.30075124 | total 0.39082664\n",
      "Epoch 26, loss: 0.39082664-(best 0.39072233)\n",
      "\n",
      "Detailed Loss: recon 0.17873739-0.08936869 | disen 0.58607316-0.00000000 | temporal 0.60143858-0.30071929 | total 0.39008799\n",
      "Epoch 27, loss: 0.39008799-(best 0.39008799)\n",
      "\n",
      "Detailed Loss: recon 0.18021941-0.09010971 | disen 0.58674645-0.00000000 | temporal 0.60148060-0.30074030 | total 0.39085001\n",
      "Epoch 28, loss: 0.39085001-(best 0.39008799)\n",
      "\n",
      "Detailed Loss: recon 0.17874381-0.08937190 | disen 0.58705926-0.00000000 | temporal 0.60147381-0.30073690 | total 0.39010882\n",
      "Epoch 29, loss: 0.39010882-(best 0.39008799)\n",
      "\n",
      "Detailed Loss: recon 0.17964479-0.08982240 | disen 0.58671957-0.00000000 | temporal 0.60139984-0.30069992 | total 0.39052230\n",
      "Epoch 30, loss: 0.39052230-(best 0.39008799)\n",
      "\n",
      "Detailed Loss: recon 0.17887351-0.08943675 | disen 0.58717591-0.00000000 | temporal 0.60130376-0.30065188 | total 0.39008862\n",
      "Epoch 31, loss: 0.39008862-(best 0.39008799)\n",
      "\n",
      "Detailed Loss: recon 0.17976531-0.08988266 | disen 0.58612823-0.00000000 | temporal 0.60118240-0.30059120 | total 0.39047384\n",
      "Epoch 32, loss: 0.39047384-(best 0.39008799)\n",
      "\n",
      "Detailed Loss: recon 0.18002568-0.09001284 | disen 0.58555746-0.00000000 | temporal 0.60118181-0.30059090 | total 0.39060375\n",
      "Epoch 33, loss: 0.39060375-(best 0.39008799)\n",
      "\n",
      "Detailed Loss: recon 0.17861407-0.08930703 | disen 0.58620703-0.00000000 | temporal 0.60118377-0.30059189 | total 0.38989893\n",
      "Epoch 34, loss: 0.38989893-(best 0.38989893)\n",
      "\n",
      "Detailed Loss: recon 0.17932060-0.08966030 | disen 0.58804637-0.00000000 | temporal 0.60132891-0.30066445 | total 0.39032477\n",
      "Epoch 35, loss: 0.39032477-(best 0.38989893)\n",
      "\n",
      "Detailed Loss: recon 0.17824081-0.08912040 | disen 0.58653855-0.00000000 | temporal 0.60129857-0.30064929 | total 0.38976967\n",
      "Epoch 36, loss: 0.38976967-(best 0.38976967)\n",
      "\n",
      "Detailed Loss: recon 0.17801189-0.08900595 | disen 0.58780491-0.00000000 | temporal 0.60140628-0.30070314 | total 0.38970909\n",
      "Epoch 37, loss: 0.38970909-(best 0.38970909)\n",
      "\n",
      "Detailed Loss: recon 0.17728153-0.08864076 | disen 0.58758128-0.00000000 | temporal 0.60140693-0.30070347 | total 0.38934422\n",
      "Epoch 38, loss: 0.38934422-(best 0.38934422)\n",
      "\n",
      "Detailed Loss: recon 0.17762256-0.08881128 | disen 0.58732140-0.00000000 | temporal 0.60135108-0.30067554 | total 0.38948682\n",
      "Epoch 39, loss: 0.38948682-(best 0.38934422)\n",
      "\n",
      "Detailed Loss: recon 0.17758028-0.08879014 | disen 0.58569300-0.00000000 | temporal 0.60131317-0.30065659 | total 0.38944674\n",
      "Epoch 40, loss: 0.38944674-(best 0.38934422)\n",
      "\n",
      "Detailed Loss: recon 0.17677742-0.08838871 | disen 0.58673298-0.00000000 | temporal 0.60128874-0.30064437 | total 0.38903308\n",
      "Epoch 41, loss: 0.38903308-(best 0.38903308)\n",
      "\n",
      "Detailed Loss: recon 0.17696773-0.08848386 | disen 0.58604723-0.00000000 | temporal 0.60120481-0.30060241 | total 0.38908628\n",
      "Epoch 42, loss: 0.38908628-(best 0.38903308)\n",
      "\n",
      "Detailed Loss: recon 0.17780110-0.08890055 | disen 0.58574462-0.00000000 | temporal 0.60112911-0.30056456 | total 0.38946509\n",
      "Epoch 43, loss: 0.38946509-(best 0.38903308)\n",
      "\n",
      "Detailed Loss: recon 0.17707869-0.08853935 | disen 0.58449602-0.00000000 | temporal 0.60105222-0.30052611 | total 0.38906544\n",
      "Epoch 44, loss: 0.38906544-(best 0.38903308)\n",
      "\n",
      "Detailed Loss: recon 0.17736128-0.08868064 | disen 0.58500516-0.00000000 | temporal 0.60115421-0.30057710 | total 0.38925773\n",
      "Epoch 45, loss: 0.38925773-(best 0.38903308)\n",
      "\n",
      "Detailed Loss: recon 0.17598812-0.08799406 | disen 0.58521008-0.00000000 | temporal 0.60118878-0.30059439 | total 0.38858846\n",
      "Epoch 46, loss: 0.38858846-(best 0.38858846)\n",
      "\n",
      "Detailed Loss: recon 0.17645459-0.08822729 | disen 0.58491600-0.00000000 | temporal 0.60112244-0.30056122 | total 0.38878852\n",
      "Epoch 47, loss: 0.38878852-(best 0.38858846)\n",
      "\n",
      "Detailed Loss: recon 0.17567334-0.08783667 | disen 0.58545446-0.00000000 | temporal 0.60113555-0.30056778 | total 0.38840443\n",
      "Epoch 48, loss: 0.38840443-(best 0.38840443)\n",
      "\n",
      "Detailed Loss: recon 0.17564349-0.08782174 | disen 0.58588791-0.00000000 | temporal 0.60110617-0.30055308 | total 0.38837484\n",
      "Epoch 49, loss: 0.38837484-(best 0.38837484)\n",
      "\n",
      "Detailed Loss: recon 0.17605287-0.08802643 | disen 0.58433664-0.00000000 | temporal 0.60099608-0.30049804 | total 0.38852447\n",
      "Epoch 50, loss: 0.38852447-(best 0.38837484)\n",
      "\n",
      ">> Reached final epoch. Loading best model...\n",
      "TRAINING XGB PREDICTOR\n",
      ">> Ending training!\n",
      "\n",
      "-----------------------\n",
      "TIME COST: 7.153270244598389 s\n",
      "Best Val: REC 55.65 PRE 42.70 MF1 62.25 AUC 70.43 TP 532 FP 714 TN 1819 FN 424\n",
      "-----------------------\n",
      "Generating additional positive data from adversary...\n",
      "Generating additional negative data by duplicating random nodes...\n",
      "\n",
      "ASSIGNED GRAPH FOR EVALUATION: 13519 {1: 2950, 0: 10569} Nodes, 1380192 Edges\n",
      "\n",
      "-------------------\n",
      "STARTING EVALUATION\n",
      "-------------------\n",
      "\n",
      "Dataset - Overall: REC 51.63 PRE 41.02 MF1 63.98 AUC 72.03 TP 1523 FP 2190 TN 8379 FN 1427 | 13519 {1: 2950, 0: 10569}\n",
      "Dataset - Train: REC 59.93 PRE 48.21 MF1 66.37 AUC 72.87 TP 860 FP 924 TN 2874 FN 575 | 5233 {0: 3798, 1: 1435}\n",
      "Dataset - Val: REC 52.72 PRE 41.72 MF1 61.28 AUC 67.49 TP 504 FP 704 TN 1829 FN 452 | 3489 {1: 956, 0: 2533}\n",
      "Dataset - Test: REC 28.44 PRE 22.05 MF1 56.64 AUC 69.00 TP 159 FP 562 TN 3676 FN 400 | 4797 {0: 4238, 1: 559}\n",
      "PREDICTION STATUS - {'1-True': 2550, '0-False': 3676, '0-True': 6893, '1-False': 400}\n",
      "    >> 400 positive nodes left unpredicted...\n",
      "    >> 196 OG positive nodes left unpredicted...\n",
      "-----------------------\n",
      "PREDICTION RESULT - ROUNDS\n",
      "Dataset - Round 0: REC 52.43 PRE 42.60 MF1 64.87 AUC 72.62 TP 679 FP 915 TN 3669 FN 616 | 5879 {1: 1295, 0: 4584}\n",
      "Dataset - Round 1: REC 35.16 PRE 30.41 MF1 55.95 AUC 67.51 TP 45 FP 103 TN 356 FN 83 | 587 {1: 128, 0: 459}\n",
      "Dataset - Round 2: REC 38.28 PRE 35.51 MF1 59.17 AUC 66.08 TP 49 FP 89 TN 370 FN 79 | 587 {1: 128, 0: 459}\n",
      "Dataset - Round 3: REC 31.25 PRE 27.21 MF1 53.70 AUC 61.33 TP 40 FP 107 TN 352 FN 88 | 587 {1: 128, 0: 459}\n",
      "-----------------------\n",
      "PREDICTION RESULT - SEEDS\n",
      "Seeds - Current: REC 32.03 PRE 28.47 MF1 54.54 AUC 62.36 TP 41 FP 103 TN 356 FN 87 | 587 {1: 128, 0: 459}\n",
      "Seeds - Prev: REC 0.00 PRE 0.00 MF1 40.41 AUC 62.36 TP 0 FP 61 TN 398 FN 128 | 587 {1: 128, 0: 459}\n",
      "Selecting data as budgeted ground truth for next round...\n",
      "  > Starting round 4...\n",
      "\n",
      "+++++++++++++++++\n",
      "STARTING ROUND 4!\n",
      "+++++++++++++++++\n",
      "        \n",
      "ASSIGNED GRAPH FOR TRAINING: 13519 {1: 2950, 0: 10569} Nodes, 1380192 Edges\n",
      "\n",
      ">> Splitting to train-validation set...\n",
      "Alotting train-val-test split for round 4\n",
      "Initial pool: 5879, Prediction pool: 13350, Budget pool: 0, Full pool: 9443\n",
      "Full graph size: 13519, Training: 5665, Val: 3778, Test: 13519\n",
      "    >> INITIAL DATA SPLIT: 5665 train rows ({0: 4135, 1: 1530}) | 3778 val rows ({1: 1020, 0: 2758}) | 4076 test rows ({0: 3676, 1: 400})\n",
      "    >> AUGMENTED DATA SPLIT: 5665 train rows ({0: 4135, 1: 1530}) | 3778 val rows ({1: 1020, 0: 2758}) | 4076 test rows ({0: 3676, 1: 400})\n",
      "    >> Updated cross-entropy weight to 2.7026143790849675...\n",
      "\n",
      "-----------------------\n",
      "STARTING MODEL TRAININGps_l\n",
      "-----------------------\n",
      "\n",
      "TRAINING NODE EMBEDDER\n",
      "Detailed Loss: recon 0.17373547-0.08686773 | disen 0.58425891-0.00000000 | temporal 0.62332267-0.31166133 | total 0.39852905\n",
      "Epoch 1, loss: 0.39852905-(best 0.39852905)\n",
      "\n",
      "Detailed Loss: recon 0.21204618-0.10602309 | disen 0.58684689-0.00000000 | temporal 0.62385708-0.31192854 | total 0.41795164\n",
      "Epoch 2, loss: 0.41795164-(best 0.39852905)\n",
      "\n",
      "Detailed Loss: recon 0.19909039-0.09954520 | disen 0.58447087-0.00000000 | temporal 0.62284011-0.31142005 | total 0.41096526\n",
      "Epoch 3, loss: 0.41096526-(best 0.39852905)\n",
      "\n",
      "Detailed Loss: recon 0.19421141-0.09710570 | disen 0.58580333-0.00000000 | temporal 0.62302881-0.31151441 | total 0.40862012\n",
      "Epoch 4, loss: 0.40862012-(best 0.39852905)\n",
      "\n",
      "Detailed Loss: recon 0.18109970-0.09054985 | disen 0.58519769-0.00000000 | temporal 0.62346900-0.31173450 | total 0.40228435\n",
      "Epoch 5, loss: 0.40228435-(best 0.39852905)\n",
      "\n",
      "Detailed Loss: recon 0.18214849-0.09107424 | disen 0.58595902-0.00000000 | temporal 0.62391323-0.31195661 | total 0.40303087\n",
      "Epoch 6, loss: 0.40303087-(best 0.39852905)\n",
      "\n",
      "Detailed Loss: recon 0.18819068-0.09409534 | disen 0.58610380-0.00000000 | temporal 0.62405902-0.31202951 | total 0.40612486\n",
      "Epoch 7, loss: 0.40612486-(best 0.39852905)\n",
      "\n",
      "Detailed Loss: recon 0.18337703-0.09168851 | disen 0.58454049-0.00000000 | temporal 0.62388408-0.31194204 | total 0.40363055\n",
      "Epoch 8, loss: 0.40363055-(best 0.39852905)\n",
      "\n",
      "Detailed Loss: recon 0.18162720-0.09081360 | disen 0.58394444-0.00000000 | temporal 0.62362748-0.31181374 | total 0.40262735\n",
      "Epoch 9, loss: 0.40262735-(best 0.39852905)\n",
      "\n",
      "Detailed Loss: recon 0.18236403-0.09118202 | disen 0.58422297-0.00000000 | temporal 0.62331986-0.31165993 | total 0.40284196\n",
      "Epoch 10, loss: 0.40284196-(best 0.39852905)\n",
      "\n",
      "Detailed Loss: recon 0.18405619-0.09202810 | disen 0.58328539-0.00000000 | temporal 0.62316322-0.31158161 | total 0.40360969\n",
      "Epoch 11, loss: 0.40360969-(best 0.39852905)\n",
      "\n",
      "Detailed Loss: recon 0.18367194-0.09183597 | disen 0.58258903-0.00000000 | temporal 0.62313336-0.31156668 | total 0.40340266\n",
      "Epoch 12, loss: 0.40340266-(best 0.39852905)\n",
      "\n",
      "Detailed Loss: recon 0.17834221-0.08917110 | disen 0.58351994-0.00000000 | temporal 0.62329006-0.31164503 | total 0.40081614\n",
      "Epoch 13, loss: 0.40081614-(best 0.39852905)\n",
      "\n",
      "Detailed Loss: recon 0.17913124-0.08956562 | disen 0.58367777-0.00000000 | temporal 0.62340128-0.31170064 | total 0.40126628\n",
      "Epoch 14, loss: 0.40126628-(best 0.39852905)\n",
      "\n",
      "Detailed Loss: recon 0.17939501-0.08969750 | disen 0.58410180-0.00000000 | temporal 0.62347955-0.31173977 | total 0.40143728\n",
      "Epoch 15, loss: 0.40143728-(best 0.39852905)\n",
      "\n",
      "Detailed Loss: recon 0.17898463-0.08949231 | disen 0.58402508-0.00000000 | temporal 0.62347311-0.31173655 | total 0.40122887\n",
      "Epoch 16, loss: 0.40122887-(best 0.39852905)\n",
      "\n",
      "Detailed Loss: recon 0.17905664-0.08952832 | disen 0.58369434-0.00000000 | temporal 0.62343186-0.31171593 | total 0.40124425\n",
      "Epoch 17, loss: 0.40124425-(best 0.39852905)\n",
      "\n",
      "Detailed Loss: recon 0.17594700-0.08797350 | disen 0.58390546-0.00000000 | temporal 0.62325013-0.31162506 | total 0.39959857\n",
      "Epoch 18, loss: 0.39959857-(best 0.39852905)\n",
      "\n",
      "Detailed Loss: recon 0.17642014-0.08821007 | disen 0.58340406-0.00000000 | temporal 0.62306410-0.31153205 | total 0.39974213\n",
      "Epoch 19, loss: 0.39974213-(best 0.39852905)\n",
      "\n",
      "Detailed Loss: recon 0.17626402-0.08813201 | disen 0.58247125-0.00000000 | temporal 0.62300938-0.31150469 | total 0.39963669\n",
      "Epoch 20, loss: 0.39963669-(best 0.39852905)\n",
      "\n",
      "Detailed Loss: recon 0.17716597-0.08858299 | disen 0.58206391-0.00000000 | temporal 0.62297392-0.31148696 | total 0.40006995\n",
      "Epoch 21, loss: 0.40006995-(best 0.39852905)\n",
      "\n",
      "Detailed Loss: recon 0.17598817-0.08799408 | disen 0.58201009-0.00000000 | temporal 0.62298954-0.31149477 | total 0.39948887\n",
      "Epoch 22, loss: 0.39948887-(best 0.39852905)\n",
      "\n",
      "Detailed Loss: recon 0.17459396-0.08729698 | disen 0.58174706-0.00000000 | temporal 0.62300360-0.31150180 | total 0.39879876\n",
      "Epoch 23, loss: 0.39879876-(best 0.39852905)\n",
      "\n",
      "Detailed Loss: recon 0.17462039-0.08731019 | disen 0.58179593-0.00000000 | temporal 0.62297624-0.31148812 | total 0.39879832\n",
      "Epoch 24, loss: 0.39879832-(best 0.39852905)\n",
      "\n",
      "Detailed Loss: recon 0.17511176-0.08755588 | disen 0.58121032-0.00000000 | temporal 0.62298614-0.31149307 | total 0.39904895\n",
      "Epoch 25, loss: 0.39904895-(best 0.39852905)\n",
      "\n",
      "Detailed Loss: recon 0.17392638-0.08696319 | disen 0.58104193-0.00000000 | temporal 0.62285280-0.31142640 | total 0.39838958\n",
      "Epoch 26, loss: 0.39838958-(best 0.39838958)\n",
      "\n",
      "Detailed Loss: recon 0.17298900-0.08649450 | disen 0.58201838-0.00000000 | temporal 0.62275916-0.31137958 | total 0.39787409\n",
      "Epoch 27, loss: 0.39787409-(best 0.39787409)\n",
      "\n",
      "Detailed Loss: recon 0.17417239-0.08708619 | disen 0.58204961-0.00000000 | temporal 0.62273550-0.31136775 | total 0.39845395\n",
      "Epoch 28, loss: 0.39845395-(best 0.39787409)\n",
      "\n",
      "Detailed Loss: recon 0.17406540-0.08703270 | disen 0.58263171-0.00000000 | temporal 0.62277198-0.31138599 | total 0.39841869\n",
      "Epoch 29, loss: 0.39841869-(best 0.39787409)\n",
      "\n",
      "Detailed Loss: recon 0.17312011-0.08656006 | disen 0.58172870-0.00000000 | temporal 0.62277287-0.31138644 | total 0.39794648\n",
      "Epoch 30, loss: 0.39794648-(best 0.39787409)\n",
      "\n",
      "Detailed Loss: recon 0.17179552-0.08589776 | disen 0.58250189-0.00000000 | temporal 0.62286061-0.31143031 | total 0.39732808\n",
      "Epoch 31, loss: 0.39732808-(best 0.39732808)\n",
      "\n",
      "Detailed Loss: recon 0.17224126-0.08612063 | disen 0.58255970-0.00000000 | temporal 0.62278646-0.31139323 | total 0.39751387\n",
      "Epoch 32, loss: 0.39751387-(best 0.39732808)\n",
      "\n",
      "Detailed Loss: recon 0.17191653-0.08595826 | disen 0.58306909-0.00000000 | temporal 0.62279028-0.31139514 | total 0.39735341\n",
      "Epoch 33, loss: 0.39735341-(best 0.39732808)\n",
      "\n",
      "Detailed Loss: recon 0.17199790-0.08599895 | disen 0.58244061-0.00000000 | temporal 0.62272328-0.31136164 | total 0.39736059\n",
      "Epoch 34, loss: 0.39736059-(best 0.39732808)\n",
      "\n",
      "Detailed Loss: recon 0.17144567-0.08572283 | disen 0.58254337-0.00000000 | temporal 0.62267673-0.31133837 | total 0.39706120\n",
      "Epoch 35, loss: 0.39706120-(best 0.39706120)\n",
      "\n",
      "Detailed Loss: recon 0.17164445-0.08582222 | disen 0.58353043-0.00000000 | temporal 0.62267888-0.31133944 | total 0.39716166\n",
      "Epoch 36, loss: 0.39716166-(best 0.39706120)\n",
      "\n",
      "Detailed Loss: recon 0.17203568-0.08601784 | disen 0.58299804-0.00000000 | temporal 0.62266833-0.31133416 | total 0.39735201\n",
      "Epoch 37, loss: 0.39735201-(best 0.39706120)\n",
      "\n",
      "Detailed Loss: recon 0.17149524-0.08574762 | disen 0.58249366-0.00000000 | temporal 0.62269056-0.31134528 | total 0.39709291\n",
      "Epoch 38, loss: 0.39709291-(best 0.39706120)\n",
      "\n",
      "Detailed Loss: recon 0.17042863-0.08521432 | disen 0.58275747-0.00000000 | temporal 0.62256014-0.31128007 | total 0.39649439\n",
      "Epoch 39, loss: 0.39649439-(best 0.39649439)\n",
      "\n",
      "Detailed Loss: recon 0.17090133-0.08545066 | disen 0.58326614-0.00000000 | temporal 0.62248462-0.31124231 | total 0.39669299\n",
      "Epoch 40, loss: 0.39669299-(best 0.39649439)\n",
      "\n",
      "Detailed Loss: recon 0.17104326-0.08552163 | disen 0.58234143-0.00000000 | temporal 0.62230408-0.31115204 | total 0.39667368\n",
      "Epoch 41, loss: 0.39667368-(best 0.39649439)\n",
      "\n",
      "Detailed Loss: recon 0.17136672-0.08568336 | disen 0.58227134-0.00000000 | temporal 0.62222135-0.31111068 | total 0.39679402\n",
      "Epoch 42, loss: 0.39679402-(best 0.39649439)\n",
      "\n",
      "Detailed Loss: recon 0.17050815-0.08525407 | disen 0.58250403-0.00000000 | temporal 0.62217790-0.31108895 | total 0.39634302\n",
      "Epoch 43, loss: 0.39634302-(best 0.39634302)\n",
      "\n",
      "Detailed Loss: recon 0.16993940-0.08496970 | disen 0.58209980-0.00000000 | temporal 0.62214732-0.31107366 | total 0.39604336\n",
      "Epoch 44, loss: 0.39604336-(best 0.39604336)\n",
      "\n",
      "Detailed Loss: recon 0.16999224-0.08499612 | disen 0.58184266-0.00000000 | temporal 0.62216610-0.31108305 | total 0.39607918\n",
      "Epoch 45, loss: 0.39607918-(best 0.39604336)\n",
      "\n",
      "Detailed Loss: recon 0.17034724-0.08517362 | disen 0.58261818-0.00000000 | temporal 0.62203848-0.31101924 | total 0.39619285\n",
      "Epoch 46, loss: 0.39619285-(best 0.39604336)\n",
      "\n",
      "Detailed Loss: recon 0.17052464-0.08526232 | disen 0.58197248-0.00000000 | temporal 0.62193203-0.31096601 | total 0.39622834\n",
      "Epoch 47, loss: 0.39622834-(best 0.39604336)\n",
      "\n",
      "Detailed Loss: recon 0.16962409-0.08481205 | disen 0.58145267-0.00000000 | temporal 0.62196678-0.31098339 | total 0.39579543\n",
      "Epoch 48, loss: 0.39579543-(best 0.39579543)\n",
      "\n",
      "Detailed Loss: recon 0.17022662-0.08511331 | disen 0.58146691-0.00000000 | temporal 0.62188113-0.31094056 | total 0.39605388\n",
      "Epoch 49, loss: 0.39605388-(best 0.39579543)\n",
      "\n",
      "Detailed Loss: recon 0.16939873-0.08469936 | disen 0.58214557-0.00000000 | temporal 0.62183356-0.31091678 | total 0.39561614\n",
      "Epoch 50, loss: 0.39561614-(best 0.39561614)\n",
      "\n",
      ">> Reached final epoch. Loading best model...\n",
      "TRAINING XGB PREDICTOR\n",
      ">> Ending training!\n",
      "\n",
      "-----------------------\n",
      "TIME COST: 7.938586235046387 s\n",
      "Best Val: REC 51.76 PRE 43.17 MF1 62.37 AUC 69.77 TP 528 FP 695 TN 2063 FN 492\n",
      "-----------------------\n",
      "Generating additional positive data from adversary...\n",
      "Generating additional negative data by duplicating random nodes...\n",
      "\n",
      "ASSIGNED GRAPH FOR EVALUATION: 14106 {1: 3078, 0: 11028} Nodes, 1484608 Edges\n",
      "\n",
      "-------------------\n",
      "STARTING EVALUATION\n",
      "-------------------\n",
      "\n",
      "Dataset - Overall: REC 49.19 PRE 42.87 MF1 64.61 AUC 70.26 TP 1514 FP 2018 TN 9010 FN 1564 | 14106 {1: 3078, 0: 11028}\n",
      "Dataset - Train: REC 55.95 PRE 48.61 MF1 66.19 AUC 71.60 TP 856 FP 905 TN 3230 FN 674 | 5665 {0: 4135, 1: 1530}\n",
      "Dataset - Val: REC 50.78 PRE 43.24 MF1 62.28 AUC 65.84 TP 518 FP 680 TN 2078 FN 502 | 3778 {1: 1020, 0: 2758}\n",
      "Dataset - Test: REC 26.52 PRE 24.43 MF1 57.72 AUC 65.66 TP 140 FP 433 TN 3702 FN 388 | 4663 {0: 4135, 1: 528}\n",
      "PREDICTION STATUS - {'1-True': 2690, '0-False': 3702, '0-True': 7326, '1-False': 388}\n",
      "    >> 388 positive nodes left unpredicted...\n",
      "    >> 138 OG positive nodes left unpredicted...\n",
      "-----------------------\n",
      "PREDICTION RESULT - ROUNDS\n",
      "Dataset - Round 0: REC 50.73 PRE 42.72 MF1 64.68 AUC 70.56 TP 657 FP 881 TN 3703 FN 638 | 5879 {1: 1295, 0: 4584}\n",
      "Dataset - Round 1: REC 48.44 PRE 43.66 MF1 64.89 AUC 74.07 TP 62 FP 80 TN 379 FN 66 | 587 {1: 128, 0: 459}\n",
      "Dataset - Round 2: REC 45.31 PRE 45.67 MF1 65.18 AUC 68.73 TP 58 FP 69 TN 390 FN 70 | 587 {1: 128, 0: 459}\n",
      "Dataset - Round 3: REC 29.69 PRE 33.04 MF1 56.67 AUC 61.00 TP 38 FP 77 TN 382 FN 90 | 587 {1: 128, 0: 459}\n",
      "Dataset - Round 4: REC 21.88 PRE 22.95 MF1 50.70 AUC 56.92 TP 28 FP 94 TN 365 FN 100 | 587 {1: 128, 0: 459}\n",
      "-----------------------\n",
      "PREDICTION RESULT - SEEDS\n",
      "Seeds - Current: REC 25.00 PRE 26.67 MF1 52.97 AUC 57.45 TP 32 FP 88 TN 371 FN 96 | 587 {1: 128, 0: 459}\n",
      "Seeds - Prev: REC 0.00 PRE 0.00 MF1 37.82 AUC 57.45 TP 0 FP 102 TN 357 FN 128 | 587 {1: 128, 0: 459}\n",
      "Selecting data as budgeted ground truth for next round...\n",
      "The file ../checkpoint/working_model_file_1756868583.2905984_epoch.py does not exist\n",
      "The file ../checkpoint/working_model_file_1756868583.2905984_round-M.json does not exist\n",
      "The file ../checkpoint/working_model_file_1756868583.2905984_round-T.json does not exist\n",
      "The file ../checkpoint/working_model_file_1756868583.2905984_round.pt does not exist\n",
      "Experiment ended, experienced 0 failures\n",
      "Elapsed experiment time 36.62540650s\n",
      "================\n",
      "++++++++++++++++\n",
      "TRIAL NUMBER 0\n",
      "++++++++++++++++\n",
      "================\n",
      "  > Rereading graph data...\n",
      "  > Initializing multiround object...\n",
      "11758 1049758\n",
      "\n",
      "=========================\n",
      "STARTING ADVER EXPERIMENT\n",
      "=========================\n",
      "MAIN CONFIG: [('device', 'cuda:0'), ('exp_type', 'ADVER'), ('full_oracle', False), ('node_expiration_time', 1), ('round_budget_neg', 0), ('round_budget_pos', 0), ('round_new_neg', 459), ('round_new_pos', 128), ('round_num', 5), ('save_df', True), ('save_embedding', False), ('save_temp_sne', False), ('setting_type', 'INDUCTIVE'), ('task_type', 'NODE'), ('verbose', 4)]\n",
      "MODEL CONFIG: [('act_name', 'LeakyReLU'), ('addon_internal_agg', 'OR'), ('addon_name', 'NONE'), ('addon_perc', 0.05), ('addon_round_window', 7), ('alpha', 1), ('att_heads', 2), ('beta', 0), ('boost_agg_backbone', <class 'models.benchmarks_supervised.booster.GIN_noparam'>), ('boost_metric', <function average_precision_score at 0x000001FC8CFC5620>), ('boost_predictor', <class 'xgboost.sklearn.XGBClassifier'>), ('device', 'cuda:0'), ('drop_rate', 0), ('dropout_rate', 0.1), ('early_stopping', 75), ('embed_type', 'temporal'), ('etypes', ['none']), ('gamma', 1), ('h_feats', 64), ('k', 1), ('lambda1', 1), ('lambda2', 1), ('lambda_', 1), ('loss_sample', True), ('loss_sample_ratio', 0.1), ('loss_type', 'ndist'), ('mlp_feats', 64), ('mlp_layers', 2), ('model_name', 'XGB-SP'), ('n_estimator', 500), ('norm_name', 'layer'), ('num_epoch', 100), ('num_layers', 2), ('num_round_epoch', 50), ('round_window', 7), ('temporal_agg', 'mean_final'), ('tloss_type', 'normal'), ('training_type', 'round'), ('verbose', 4)]\n",
      "STRAT CONFIG: [('augment_name', 'REAGE'), ('augment_neg_ratio', 0.09772070079945569), ('augment_pos_ratio', 0.02727929920054431), ('augment_prob', 0.5), ('augment_round_split', 8), ('device', 'cuda:0'), ('pseudo_name', 'NONE'), ('pseudo_new_pos_only', False), ('pseudo_skip_initial', False), ('verbose', 4)]\n",
      "TRAIN CONFIG: [('ce_weight', 2.7026143790849675), ('device', 'cuda:0'), ('dset_name', 'tolokers_bid'), ('early_stopping', 75), ('learning_rate', 0.01), ('loss', <function cross_entropy at 0x000001FC85EA3740>), ('loss_gamma', 5), ('num_epoch', 100), ('num_round_epoch', 50), ('optimizer', <class 'torch.optim.adam.Adam'>), ('random_state', 7777777), ('ratio_initial', 0.5), ('ratio_test', 0.2), ('ratio_train', 0.6), ('ratio_val', 0.2), ('round_reset_model', False), ('round_train_list', []), ('stuck_stopping', 50), ('stuck_threshold', 0), ('train_max_retry', 10), ('verbose', 4)]\n",
      "ADVER CONFIG: [('adver_choose_name', 'GREEDY'), ('adver_conn_coef', 0.1), ('adver_feat_coef', 1), ('adver_mod_name', 'REPLAY'), ('device', 'cuda:0'), ('verbose', 4)]\n",
      "\n",
      "INITIAL GRAPH: 11758 {1: 2566, 0: 9192} Nodes, 1049758 Edges\n",
      "=========================\n",
      "        \n",
      ">> Initializing graph...\n",
      "MAIN GRAPH: 11758 {1: 2566, 0: 9192} Nodes, 1049758 Edges\n",
      "\n",
      ">> Initializing experiment type-specific modules:\n",
      "  >> Initializing adversarial modules...\n",
      ">> Initializing task-specific modules:\n",
      ">> Initializing classifier model...\n",
      ">> Initializing other strategies...\n",
      "\n",
      "FINISH INITIALIZATION\n",
      "=====================\n",
      "        \n",
      "  > Starting round 0...\n",
      "\n",
      "+++++++++++++++++\n",
      "STARTING ROUND 0!\n",
      "+++++++++++++++++\n",
      "        \n",
      "ASSIGNED GRAPH FOR TRAINING: 5879 {1: 1275, 0: 4604} Nodes, 269581 Edges\n",
      "\n",
      ">> Splitting to train-validation set...\n",
      "Alotting train-val-test split for round 0\n",
      "Initial pool: 0, Prediction pool: 0, Budget pool: 0, Full pool: 5879\n",
      "Full graph size: 5879, Training: 3527, Val: 2352, Test: 5879\n",
      "    >> INITIAL DATA SPLIT: 3527 train rows ({0: 2762, 1: 765}) | 2352 val rows ({1: 510, 0: 1842}) | 0 test rows ({})\n",
      "    >> AUGMENTED DATA SPLIT: 3527 train rows ({0: 2762, 1: 765}) | 2352 val rows ({1: 510, 0: 1842}) | 0 test rows ({})\n",
      "    >> Updated cross-entropy weight to 3.6104575163398693...\n",
      "\n",
      "-----------------------\n",
      "STARTING MODEL TRAININGps_l\n",
      "-----------------------\n",
      "\n",
      "TRAINING NODE EMBEDDER\n",
      "Detailed Loss: recon 0.56260037-0.28130019 | disen 0.71203840-0.35601920 | temporal 0.65124881-0.00000000 | total 0.63731939\n",
      "Epoch 1, loss: 0.63731939-(best 0.63731939)\n",
      "\n",
      "Detailed Loss: recon 0.41489288-0.20744644 | disen 0.72312987-0.36156493 | temporal 0.66229838-0.00000000 | total 0.56901139\n",
      "Epoch 2, loss: 0.56901139-(best 0.56901139)\n",
      "\n",
      "Detailed Loss: recon 0.38196570-0.19098285 | disen 0.72278035-0.36139017 | temporal 0.66509056-0.00000000 | total 0.55237305\n",
      "Epoch 3, loss: 0.55237305-(best 0.55237305)\n",
      "\n",
      "Detailed Loss: recon 0.35870013-0.17935006 | disen 0.72121561-0.36060780 | temporal 0.66617286-0.00000000 | total 0.53995788\n",
      "Epoch 4, loss: 0.53995788-(best 0.53995788)\n",
      "\n",
      "Detailed Loss: recon 0.34534007-0.17267004 | disen 0.71328723-0.35664362 | temporal 0.66640162-0.00000000 | total 0.52931368\n",
      "Epoch 5, loss: 0.52931368-(best 0.52931368)\n",
      "\n",
      "Detailed Loss: recon 0.32313734-0.16156867 | disen 0.71670592-0.35835296 | temporal 0.66640860-0.00000000 | total 0.51992166\n",
      "Epoch 6, loss: 0.51992166-(best 0.51992166)\n",
      "\n",
      "Detailed Loss: recon 0.30407879-0.15203939 | disen 0.70531482-0.35265741 | temporal 0.66577220-0.00000000 | total 0.50469679\n",
      "Epoch 7, loss: 0.50469679-(best 0.50469679)\n",
      "\n",
      "Detailed Loss: recon 0.30296394-0.15148197 | disen 0.70387191-0.35193595 | temporal 0.66547728-0.00000000 | total 0.50341791\n",
      "Epoch 8, loss: 0.50341791-(best 0.50341791)\n",
      "\n",
      "Detailed Loss: recon 0.29470375-0.14735188 | disen 0.70490044-0.35245022 | temporal 0.66513211-0.00000000 | total 0.49980211\n",
      "Epoch 9, loss: 0.49980211-(best 0.49980211)\n",
      "\n",
      "Detailed Loss: recon 0.28643984-0.14321992 | disen 0.68970656-0.34485328 | temporal 0.66567212-0.00000000 | total 0.48807320\n",
      "Epoch 10, loss: 0.48807320-(best 0.48807320)\n",
      "\n",
      "Detailed Loss: recon 0.27880234-0.13940117 | disen 0.68613857-0.34306929 | temporal 0.66655326-0.00000000 | total 0.48247045\n",
      "Epoch 11, loss: 0.48247045-(best 0.48247045)\n",
      "\n",
      "Detailed Loss: recon 0.27484041-0.13742021 | disen 0.68313622-0.34156811 | temporal 0.66760904-0.00000000 | total 0.47898832\n",
      "Epoch 12, loss: 0.47898832-(best 0.47898832)\n",
      "\n",
      "Detailed Loss: recon 0.26985887-0.13492943 | disen 0.68079686-0.34039843 | temporal 0.66832858-0.00000000 | total 0.47532785\n",
      "Epoch 13, loss: 0.47532785-(best 0.47532785)\n",
      "\n",
      "Detailed Loss: recon 0.26307529-0.13153765 | disen 0.66381943-0.33190972 | temporal 0.66878462-0.00000000 | total 0.46344736\n",
      "Epoch 14, loss: 0.46344736-(best 0.46344736)\n",
      "\n",
      "Detailed Loss: recon 0.26139903-0.13069952 | disen 0.66164547-0.33082274 | temporal 0.66879427-0.00000000 | total 0.46152225\n",
      "Epoch 15, loss: 0.46152225-(best 0.46152225)\n",
      "\n",
      "Detailed Loss: recon 0.26588136-0.13294068 | disen 0.65332139-0.32666069 | temporal 0.66890424-0.00000000 | total 0.45960137\n",
      "Epoch 16, loss: 0.45960137-(best 0.45960137)\n",
      "\n",
      "Detailed Loss: recon 0.25804693-0.12902346 | disen 0.65091723-0.32545862 | temporal 0.66922104-0.00000000 | total 0.45448208\n",
      "Epoch 17, loss: 0.45448208-(best 0.45448208)\n",
      "\n",
      "Detailed Loss: recon 0.25783908-0.12891954 | disen 0.63046318-0.31523159 | temporal 0.66964984-0.00000000 | total 0.44415113\n",
      "Epoch 18, loss: 0.44415113-(best 0.44415113)\n",
      "\n",
      "Detailed Loss: recon 0.25129673-0.12564836 | disen 0.63238007-0.31619003 | temporal 0.67011189-0.00000000 | total 0.44183838\n",
      "Epoch 19, loss: 0.44183838-(best 0.44183838)\n",
      "\n",
      "Detailed Loss: recon 0.25183374-0.12591687 | disen 0.63832635-0.31916317 | temporal 0.67071247-0.00000000 | total 0.44508004\n",
      "Epoch 20, loss: 0.44508004-(best 0.44183838)\n",
      "\n",
      "Detailed Loss: recon 0.25647411-0.12823705 | disen 0.61607414-0.30803707 | temporal 0.67086399-0.00000000 | total 0.43627411\n",
      "Epoch 21, loss: 0.43627411-(best 0.43627411)\n",
      "\n",
      "Detailed Loss: recon 0.24946116-0.12473058 | disen 0.62155175-0.31077588 | temporal 0.67093635-0.00000000 | total 0.43550646\n",
      "Epoch 22, loss: 0.43550646-(best 0.43550646)\n",
      "\n",
      "Detailed Loss: recon 0.24833135-0.12416568 | disen 0.60799587-0.30399793 | temporal 0.67124033-0.00000000 | total 0.42816362\n",
      "Epoch 23, loss: 0.42816362-(best 0.42816362)\n",
      "\n",
      "Detailed Loss: recon 0.25449517-0.12724759 | disen 0.59460199-0.29730099 | temporal 0.67102808-0.00000000 | total 0.42454857\n",
      "Epoch 24, loss: 0.42454857-(best 0.42454857)\n",
      "\n",
      "Detailed Loss: recon 0.24901211-0.12450606 | disen 0.60446435-0.30223218 | temporal 0.67105448-0.00000000 | total 0.42673823\n",
      "Epoch 25, loss: 0.42673823-(best 0.42454857)\n",
      "\n",
      "Detailed Loss: recon 0.24741130-0.12370565 | disen 0.58062553-0.29031277 | temporal 0.67097449-0.00000000 | total 0.41401842\n",
      "Epoch 26, loss: 0.41401842-(best 0.41401842)\n",
      "\n",
      "Detailed Loss: recon 0.24674731-0.12337366 | disen 0.58910525-0.29455262 | temporal 0.67083156-0.00000000 | total 0.41792628\n",
      "Epoch 27, loss: 0.41792628-(best 0.41401842)\n",
      "\n",
      "Detailed Loss: recon 0.24661689-0.12330844 | disen 0.58576107-0.29288054 | temporal 0.67065090-0.00000000 | total 0.41618899\n",
      "Epoch 28, loss: 0.41618899-(best 0.41401842)\n",
      "\n",
      "Detailed Loss: recon 0.24705395-0.12352698 | disen 0.58218414-0.29109207 | temporal 0.67079651-0.00000000 | total 0.41461903\n",
      "Epoch 29, loss: 0.41461903-(best 0.41401842)\n",
      "\n",
      "Detailed Loss: recon 0.24272740-0.12136370 | disen 0.58435488-0.29217744 | temporal 0.67122227-0.00000000 | total 0.41354114\n",
      "Epoch 30, loss: 0.41354114-(best 0.41354114)\n",
      "\n",
      "Detailed Loss: recon 0.23960647-0.11980323 | disen 0.57769024-0.28884512 | temporal 0.67079294-0.00000000 | total 0.40864837\n",
      "Epoch 31, loss: 0.40864837-(best 0.40864837)\n",
      "\n",
      "Detailed Loss: recon 0.24301033-0.12150516 | disen 0.56791639-0.28395820 | temporal 0.67051339-0.00000000 | total 0.40546337\n",
      "Epoch 32, loss: 0.40546337-(best 0.40546337)\n",
      "\n",
      "Detailed Loss: recon 0.24408202-0.12204101 | disen 0.55594295-0.27797148 | temporal 0.67018574-0.00000000 | total 0.40001249\n",
      "Epoch 33, loss: 0.40001249-(best 0.40001249)\n",
      "\n",
      "Detailed Loss: recon 0.24199900-0.12099950 | disen 0.54734200-0.27367100 | temporal 0.66989851-0.00000000 | total 0.39467049\n",
      "Epoch 34, loss: 0.39467049-(best 0.39467049)\n",
      "\n",
      "Detailed Loss: recon 0.24202132-0.12101066 | disen 0.55622965-0.27811483 | temporal 0.67045480-0.00000000 | total 0.39912549\n",
      "Epoch 35, loss: 0.39912549-(best 0.39467049)\n",
      "\n",
      "Detailed Loss: recon 0.24046026-0.12023013 | disen 0.53874153-0.26937076 | temporal 0.67037672-0.00000000 | total 0.38960090\n",
      "Epoch 36, loss: 0.38960090-(best 0.38960090)\n",
      "\n",
      "Detailed Loss: recon 0.24453783-0.12226892 | disen 0.54404026-0.27202013 | temporal 0.67070502-0.00000000 | total 0.39428905\n",
      "Epoch 37, loss: 0.39428905-(best 0.38960090)\n",
      "\n",
      "Detailed Loss: recon 0.24158567-0.12079284 | disen 0.54402435-0.27201217 | temporal 0.67062420-0.00000000 | total 0.39280501\n",
      "Epoch 38, loss: 0.39280501-(best 0.38960090)\n",
      "\n",
      "Detailed Loss: recon 0.23830220-0.11915110 | disen 0.53286660-0.26643330 | temporal 0.67042607-0.00000000 | total 0.38558441\n",
      "Epoch 39, loss: 0.38558441-(best 0.38558441)\n",
      "\n",
      "Detailed Loss: recon 0.23849343-0.11924671 | disen 0.52858198-0.26429099 | temporal 0.67053282-0.00000000 | total 0.38353771\n",
      "Epoch 40, loss: 0.38353771-(best 0.38353771)\n",
      "\n",
      "Detailed Loss: recon 0.23890889-0.11945444 | disen 0.52548766-0.26274383 | temporal 0.67066813-0.00000000 | total 0.38219827\n",
      "Epoch 41, loss: 0.38219827-(best 0.38219827)\n",
      "\n",
      "Detailed Loss: recon 0.23794961-0.11897480 | disen 0.51735997-0.25867999 | temporal 0.67053932-0.00000000 | total 0.37765479\n",
      "Epoch 42, loss: 0.37765479-(best 0.37765479)\n",
      "\n",
      "Detailed Loss: recon 0.24290654-0.12145327 | disen 0.52109253-0.26054627 | temporal 0.67028034-0.00000000 | total 0.38199955\n",
      "Epoch 43, loss: 0.38199955-(best 0.37765479)\n",
      "\n",
      "Detailed Loss: recon 0.24146304-0.12073152 | disen 0.51166117-0.25583059 | temporal 0.66992229-0.00000000 | total 0.37656212\n",
      "Epoch 44, loss: 0.37656212-(best 0.37656212)\n",
      "\n",
      "Detailed Loss: recon 0.24383388-0.12191694 | disen 0.50922370-0.25461185 | temporal 0.67017281-0.00000000 | total 0.37652880\n",
      "Epoch 45, loss: 0.37652880-(best 0.37652880)\n",
      "\n",
      "Detailed Loss: recon 0.24433950-0.12216975 | disen 0.50028765-0.25014383 | temporal 0.66979355-0.00000000 | total 0.37231356\n",
      "Epoch 46, loss: 0.37231356-(best 0.37231356)\n",
      "\n",
      "Detailed Loss: recon 0.24344641-0.12172320 | disen 0.50264174-0.25132087 | temporal 0.67004234-0.00000000 | total 0.37304407\n",
      "Epoch 47, loss: 0.37304407-(best 0.37231356)\n",
      "\n",
      "Detailed Loss: recon 0.24665670-0.12332835 | disen 0.50554597-0.25277299 | temporal 0.66991013-0.00000000 | total 0.37610134\n",
      "Epoch 48, loss: 0.37610134-(best 0.37231356)\n",
      "\n",
      "Detailed Loss: recon 0.24976879-0.12488440 | disen 0.48262185-0.24131092 | temporal 0.66953552-0.00000000 | total 0.36619532\n",
      "Epoch 49, loss: 0.36619532-(best 0.36619532)\n",
      "\n",
      "Detailed Loss: recon 0.24606638-0.12303319 | disen 0.48390019-0.24195009 | temporal 0.66945392-0.00000000 | total 0.36498329\n",
      "Epoch 50, loss: 0.36498329-(best 0.36498329)\n",
      "\n",
      "Detailed Loss: recon 0.24743366-0.12371683 | disen 0.47971976-0.23985988 | temporal 0.66948551-0.00000000 | total 0.36357671\n",
      "Epoch 51, loss: 0.36357671-(best 0.36357671)\n",
      "\n",
      "Detailed Loss: recon 0.24384460-0.12192230 | disen 0.48379099-0.24189550 | temporal 0.66976804-0.00000000 | total 0.36381781\n",
      "Epoch 52, loss: 0.36381781-(best 0.36357671)\n",
      "\n",
      "Detailed Loss: recon 0.24255696-0.12127848 | disen 0.48956019-0.24478009 | temporal 0.66966349-0.00000000 | total 0.36605859\n",
      "Epoch 53, loss: 0.36605859-(best 0.36357671)\n",
      "\n",
      "Detailed Loss: recon 0.24856427-0.12428214 | disen 0.47197354-0.23598677 | temporal 0.66853416-0.00000000 | total 0.36026889\n",
      "Epoch 54, loss: 0.36026889-(best 0.36026889)\n",
      "\n",
      "Detailed Loss: recon 0.24380319-0.12190159 | disen 0.48259181-0.24129590 | temporal 0.66891187-0.00000000 | total 0.36319751\n",
      "Epoch 55, loss: 0.36319751-(best 0.36026889)\n",
      "\n",
      "Detailed Loss: recon 0.24721365-0.12360682 | disen 0.47735941-0.23867971 | temporal 0.66899616-0.00000000 | total 0.36228654\n",
      "Epoch 56, loss: 0.36228654-(best 0.36026889)\n",
      "\n",
      "Detailed Loss: recon 0.24494997-0.12247498 | disen 0.46821988-0.23410994 | temporal 0.66900319-0.00000000 | total 0.35658491\n",
      "Epoch 57, loss: 0.35658491-(best 0.35658491)\n",
      "\n",
      "Detailed Loss: recon 0.24571690-0.12285845 | disen 0.46668267-0.23334134 | temporal 0.66849440-0.00000000 | total 0.35619980\n",
      "Epoch 58, loss: 0.35619980-(best 0.35619980)\n",
      "\n",
      "Detailed Loss: recon 0.24993333-0.12496667 | disen 0.46767747-0.23383874 | temporal 0.66836798-0.00000000 | total 0.35880542\n",
      "Epoch 59, loss: 0.35880542-(best 0.35619980)\n",
      "\n",
      "Detailed Loss: recon 0.24996011-0.12498005 | disen 0.46471959-0.23235980 | temporal 0.66852647-0.00000000 | total 0.35733986\n",
      "Epoch 60, loss: 0.35733986-(best 0.35619980)\n",
      "\n",
      "Detailed Loss: recon 0.24314873-0.12157436 | disen 0.46009213-0.23004606 | temporal 0.66874290-0.00000000 | total 0.35162044\n",
      "Epoch 61, loss: 0.35162044-(best 0.35162044)\n",
      "\n",
      "Detailed Loss: recon 0.24050145-0.12025072 | disen 0.46711075-0.23355538 | temporal 0.66895723-0.00000000 | total 0.35380611\n",
      "Epoch 62, loss: 0.35380611-(best 0.35162044)\n",
      "\n",
      "Detailed Loss: recon 0.24056183-0.12028091 | disen 0.46620506-0.23310253 | temporal 0.66908681-0.00000000 | total 0.35338345\n",
      "Epoch 63, loss: 0.35338345-(best 0.35162044)\n",
      "\n",
      "Detailed Loss: recon 0.24560726-0.12280363 | disen 0.46207184-0.23103592 | temporal 0.66877973-0.00000000 | total 0.35383955\n",
      "Epoch 64, loss: 0.35383955-(best 0.35162044)\n",
      "\n",
      "Detailed Loss: recon 0.24699533-0.12349766 | disen 0.45465660-0.22732830 | temporal 0.66835344-0.00000000 | total 0.35082597\n",
      "Epoch 65, loss: 0.35082597-(best 0.35082597)\n",
      "\n",
      "Detailed Loss: recon 0.24801102-0.12400551 | disen 0.46411616-0.23205808 | temporal 0.66876513-0.00000000 | total 0.35606360\n",
      "Epoch 66, loss: 0.35606360-(best 0.35082597)\n",
      "\n",
      "Detailed Loss: recon 0.24360153-0.12180077 | disen 0.45866609-0.22933304 | temporal 0.66890991-0.00000000 | total 0.35113382\n",
      "Epoch 67, loss: 0.35113382-(best 0.35082597)\n",
      "\n",
      "Detailed Loss: recon 0.23965232-0.11982616 | disen 0.45538455-0.22769228 | temporal 0.66920543-0.00000000 | total 0.34751844\n",
      "Epoch 68, loss: 0.34751844-(best 0.34751844)\n",
      "\n",
      "Detailed Loss: recon 0.24581878-0.12290939 | disen 0.45435286-0.22717643 | temporal 0.66837806-0.00000000 | total 0.35008582\n",
      "Epoch 69, loss: 0.35008582-(best 0.34751844)\n",
      "\n",
      "Detailed Loss: recon 0.24979955-0.12489977 | disen 0.44936985-0.22468492 | temporal 0.66762620-0.00000000 | total 0.34958470\n",
      "Epoch 70, loss: 0.34958470-(best 0.34751844)\n",
      "\n",
      "Detailed Loss: recon 0.24452077-0.12226038 | disen 0.45703667-0.22851834 | temporal 0.66859901-0.00000000 | total 0.35077873\n",
      "Epoch 71, loss: 0.35077873-(best 0.34751844)\n",
      "\n",
      "Detailed Loss: recon 0.24022231-0.12011115 | disen 0.44825947-0.22412974 | temporal 0.66869032-0.00000000 | total 0.34424090\n",
      "Epoch 72, loss: 0.34424090-(best 0.34424090)\n",
      "\n",
      "Detailed Loss: recon 0.25019872-0.12509936 | disen 0.44489557-0.22244778 | temporal 0.66817635-0.00000000 | total 0.34754714\n",
      "Epoch 73, loss: 0.34754714-(best 0.34424090)\n",
      "\n",
      "Detailed Loss: recon 0.24491684-0.12245842 | disen 0.44141710-0.22070855 | temporal 0.66772330-0.00000000 | total 0.34316698\n",
      "Epoch 74, loss: 0.34316698-(best 0.34316698)\n",
      "\n",
      "Detailed Loss: recon 0.24982648-0.12491324 | disen 0.44227135-0.22113568 | temporal 0.66759288-0.00000000 | total 0.34604892\n",
      "Epoch 75, loss: 0.34604892-(best 0.34316698)\n",
      "\n",
      "Detailed Loss: recon 0.24282017-0.12141009 | disen 0.44358492-0.22179246 | temporal 0.66848278-0.00000000 | total 0.34320253\n",
      "Epoch 76, loss: 0.34320253-(best 0.34316698)\n",
      "\n",
      "Detailed Loss: recon 0.24412128-0.12206064 | disen 0.43593234-0.21796617 | temporal 0.66766894-0.00000000 | total 0.34002680\n",
      "Epoch 77, loss: 0.34002680-(best 0.34002680)\n",
      "\n",
      "Detailed Loss: recon 0.25300804-0.12650402 | disen 0.42801553-0.21400777 | temporal 0.66652805-0.00000000 | total 0.34051180\n",
      "Epoch 78, loss: 0.34051180-(best 0.34002680)\n",
      "\n",
      "Detailed Loss: recon 0.24490103-0.12245052 | disen 0.43274391-0.21637195 | temporal 0.66730261-0.00000000 | total 0.33882248\n",
      "Epoch 79, loss: 0.33882248-(best 0.33882248)\n",
      "\n",
      "Detailed Loss: recon 0.24949943-0.12474971 | disen 0.43431675-0.21715838 | temporal 0.66698730-0.00000000 | total 0.34190810\n",
      "Epoch 80, loss: 0.34190810-(best 0.33882248)\n",
      "\n",
      "Detailed Loss: recon 0.24487476-0.12243738 | disen 0.43579501-0.21789750 | temporal 0.66769922-0.00000000 | total 0.34033489\n",
      "Epoch 81, loss: 0.34033489-(best 0.33882248)\n",
      "\n",
      "Detailed Loss: recon 0.24240884-0.12120442 | disen 0.44027102-0.22013551 | temporal 0.66823363-0.00000000 | total 0.34133995\n",
      "Epoch 82, loss: 0.34133995-(best 0.33882248)\n",
      "\n",
      "Detailed Loss: recon 0.25121558-0.12560779 | disen 0.42432678-0.21216339 | temporal 0.66683513-0.00000000 | total 0.33777118\n",
      "Epoch 83, loss: 0.33777118-(best 0.33777118)\n",
      "\n",
      "Detailed Loss: recon 0.25356945-0.12678473 | disen 0.43008840-0.21504420 | temporal 0.66651869-0.00000000 | total 0.34182894\n",
      "Epoch 84, loss: 0.34182894-(best 0.33777118)\n",
      "\n",
      "Detailed Loss: recon 0.24327689-0.12163845 | disen 0.43915474-0.21957737 | temporal 0.66777444-0.00000000 | total 0.34121582\n",
      "Epoch 85, loss: 0.34121582-(best 0.33777118)\n",
      "\n",
      "Detailed Loss: recon 0.24162582-0.12081291 | disen 0.43501598-0.21750799 | temporal 0.66782773-0.00000000 | total 0.33832091\n",
      "Epoch 86, loss: 0.33832091-(best 0.33777118)\n",
      "\n",
      "Detailed Loss: recon 0.24450573-0.12225287 | disen 0.42186224-0.21093112 | temporal 0.66675961-0.00000000 | total 0.33318400\n",
      "Epoch 87, loss: 0.33318400-(best 0.33318400)\n",
      "\n",
      "Detailed Loss: recon 0.25191641-0.12595820 | disen 0.42463422-0.21231711 | temporal 0.66581786-0.00000000 | total 0.33827531\n",
      "Epoch 88, loss: 0.33827531-(best 0.33318400)\n",
      "\n",
      "Detailed Loss: recon 0.24169874-0.12084937 | disen 0.43118852-0.21559426 | temporal 0.66689032-0.00000000 | total 0.33644363\n",
      "Epoch 89, loss: 0.33644363-(best 0.33318400)\n",
      "\n",
      "Detailed Loss: recon 0.23701766-0.11850883 | disen 0.43123811-0.21561906 | temporal 0.66765416-0.00000000 | total 0.33412790\n",
      "Epoch 90, loss: 0.33412790-(best 0.33318400)\n",
      "\n",
      "Detailed Loss: recon 0.24818817-0.12409408 | disen 0.42098308-0.21049154 | temporal 0.66615885-0.00000000 | total 0.33458561\n",
      "Epoch 91, loss: 0.33458561-(best 0.33318400)\n",
      "\n",
      "Detailed Loss: recon 0.24661198-0.12330599 | disen 0.42236966-0.21118483 | temporal 0.66701931-0.00000000 | total 0.33449084\n",
      "Epoch 92, loss: 0.33449084-(best 0.33318400)\n",
      "\n",
      "Detailed Loss: recon 0.24230161-0.12115081 | disen 0.42805028-0.21402514 | temporal 0.66767788-0.00000000 | total 0.33517593\n",
      "Epoch 93, loss: 0.33517593-(best 0.33318400)\n",
      "\n",
      "Detailed Loss: recon 0.23920892-0.11960446 | disen 0.42680293-0.21340147 | temporal 0.66739273-0.00000000 | total 0.33300593\n",
      "Epoch 94, loss: 0.33300593-(best 0.33300593)\n",
      "\n",
      "Detailed Loss: recon 0.24341083-0.12170541 | disen 0.41935039-0.20967519 | temporal 0.66698831-0.00000000 | total 0.33138061\n",
      "Epoch 95, loss: 0.33138061-(best 0.33138061)\n",
      "\n",
      "Detailed Loss: recon 0.24548011-0.12274005 | disen 0.41318434-0.20659217 | temporal 0.66614848-0.00000000 | total 0.32933223\n",
      "Epoch 96, loss: 0.32933223-(best 0.32933223)\n",
      "\n",
      "Detailed Loss: recon 0.24206565-0.12103283 | disen 0.41688520-0.20844260 | temporal 0.66719288-0.00000000 | total 0.32947543\n",
      "Epoch 97, loss: 0.32947543-(best 0.32933223)\n",
      "\n",
      "Detailed Loss: recon 0.24070764-0.12035382 | disen 0.41828436-0.20914218 | temporal 0.66619951-0.00000000 | total 0.32949600\n",
      "Epoch 98, loss: 0.32949600-(best 0.32933223)\n",
      "\n",
      "Detailed Loss: recon 0.24915883-0.12457941 | disen 0.40631795-0.20315897 | temporal 0.66532671-0.00000000 | total 0.32773840\n",
      "Epoch 99, loss: 0.32773840-(best 0.32773840)\n",
      "\n",
      "Detailed Loss: recon 0.23689315-0.11844657 | disen 0.41977561-0.20988780 | temporal 0.66714865-0.00000000 | total 0.32833439\n",
      "Epoch 100, loss: 0.32833439-(best 0.32773840)\n",
      "\n",
      ">> Reached final epoch. Loading best model...\n",
      "TRAINING XGB PREDICTOR\n",
      ">> Ending training!\n",
      "\n",
      "-----------------------\n",
      "TIME COST: 6.2863078117370605 s\n",
      "Best Val: REC 57.45 PRE 44.60 MF1 66.89 AUC 79.10 TP 293 FP 364 TN 1478 FN 217\n",
      "-----------------------\n",
      "\n",
      "ASSIGNED GRAPH FOR EVALUATION: 11758 {1: 2566, 0: 9192} Nodes, 1049758 Edges\n",
      "\n",
      "-------------------\n",
      "STARTING EVALUATION\n",
      "-------------------\n",
      "\n",
      "Dataset - Overall: REC 52.69 PRE 47.66 MF1 67.57 AUC 80.34 TP 1352 FP 1485 TN 7707 FN 1214 | 11758 {1: 2566, 0: 9192}\n",
      "Dataset - Train: REC 60.92 PRE 53.32 MF1 71.90 AUC 84.42 TP 466 FP 408 TN 2354 FN 299 | 3527 {0: 2762, 1: 765}\n",
      "Dataset - Val: REC 50.59 PRE 43.95 MF1 65.46 AUC 78.73 TP 258 FP 329 TN 1513 FN 252 | 2352 {1: 510, 0: 1842}\n",
      "Dataset - Test: REC 48.64 PRE 45.64 MF1 65.79 AUC 78.56 TP 628 FP 748 TN 3840 FN 663 | 5879 {0: 4588, 1: 1291}\n",
      "PREDICTION STATUS - {'1-True': 1903, '0-True': 5352, '0-False': 3840, '1-False': 663}\n",
      "    >> 663 positive nodes left unpredicted...\n",
      "    >> 663 OG positive nodes left unpredicted...\n",
      "Selecting data as budgeted ground truth for next round...\n",
      "  > Starting round 1...\n",
      "\n",
      "+++++++++++++++++\n",
      "STARTING ROUND 1!\n",
      "+++++++++++++++++\n",
      "        \n",
      "ASSIGNED GRAPH FOR TRAINING: 11758 {1: 2566, 0: 9192} Nodes, 1049758 Edges\n",
      "\n",
      ">> Splitting to train-validation set...\n",
      "Alotting train-val-test split for round 1\n",
      "Initial pool: 5879, Prediction pool: 2837, Budget pool: 0, Full pool: 7255\n",
      "Full graph size: 11758, Training: 4353, Val: 2902, Test: 11758\n",
      "    >> INITIAL DATA SPLIT: 4353 train rows ({1: 1142, 0: 3211}) | 2902 val rows ({0: 2141, 1: 761}) | 4503 test rows ({0: 3840, 1: 663})\n",
      "    >> AUGMENTED DATA SPLIT: 4353 train rows ({1: 1142, 0: 3211}) | 2902 val rows ({0: 2141, 1: 761}) | 4503 test rows ({0: 3840, 1: 663})\n",
      "    >> Updated cross-entropy weight to 2.8117338003502628...\n",
      "\n",
      "-----------------------\n",
      "STARTING MODEL TRAININGps_l\n",
      "-----------------------\n",
      "\n",
      "TRAINING NODE EMBEDDER\n",
      "Detailed Loss: recon 0.22997050-0.11498525 | disen 0.38339877-0.19169939 | temporal 0.56298554-0.00000000 | total 0.30668464\n",
      "Epoch 1, loss: 0.30668464-(best 0.30668464)\n",
      "\n",
      "Detailed Loss: recon 0.42011875-0.21005937 | disen 0.42601854-0.21300927 | temporal 0.56514192-0.00000000 | total 0.42306864\n",
      "Epoch 2, loss: 0.42306864-(best 0.30668464)\n",
      "\n",
      "Detailed Loss: recon 0.35932153-0.17966077 | disen 0.48021108-0.24010554 | temporal 0.56965786-0.00000000 | total 0.41976631\n",
      "Epoch 3, loss: 0.41976631-(best 0.30668464)\n",
      "\n",
      "Detailed Loss: recon 0.34913117-0.17456558 | disen 0.48318088-0.24159044 | temporal 0.57004791-0.00000000 | total 0.41615602\n",
      "Epoch 4, loss: 0.41615602-(best 0.30668464)\n",
      "\n",
      "Detailed Loss: recon 0.29370713-0.14685357 | disen 0.47981620-0.23990810 | temporal 0.56978941-0.00000000 | total 0.38676167\n",
      "Epoch 5, loss: 0.38676167-(best 0.30668464)\n",
      "\n",
      "Detailed Loss: recon 0.26267296-0.13133648 | disen 0.47191590-0.23595795 | temporal 0.56916064-0.00000000 | total 0.36729443\n",
      "Epoch 6, loss: 0.36729443-(best 0.30668464)\n",
      "\n",
      "Detailed Loss: recon 0.26556858-0.13278429 | disen 0.45677131-0.22838566 | temporal 0.56850332-0.00000000 | total 0.36116993\n",
      "Epoch 7, loss: 0.36116993-(best 0.30668464)\n",
      "\n",
      "Detailed Loss: recon 0.29342264-0.14671132 | disen 0.44776171-0.22388086 | temporal 0.56781256-0.00000000 | total 0.37059218\n",
      "Epoch 8, loss: 0.37059218-(best 0.30668464)\n",
      "\n",
      "Detailed Loss: recon 0.29022390-0.14511195 | disen 0.44422781-0.22211391 | temporal 0.56788093-0.00000000 | total 0.36722586\n",
      "Epoch 9, loss: 0.36722586-(best 0.30668464)\n",
      "\n",
      "Detailed Loss: recon 0.27284667-0.13642333 | disen 0.44617814-0.22308907 | temporal 0.56811696-0.00000000 | total 0.35951239\n",
      "Epoch 10, loss: 0.35951239-(best 0.30668464)\n",
      "\n",
      "Detailed Loss: recon 0.25533268-0.12766634 | disen 0.44532335-0.22266167 | temporal 0.56854975-0.00000000 | total 0.35032803\n",
      "Epoch 11, loss: 0.35032803-(best 0.30668464)\n",
      "\n",
      "Detailed Loss: recon 0.24803263-0.12401631 | disen 0.44446701-0.22223350 | temporal 0.56885087-0.00000000 | total 0.34624982\n",
      "Epoch 12, loss: 0.34624982-(best 0.30668464)\n",
      "\n",
      "Detailed Loss: recon 0.24972346-0.12486173 | disen 0.44651669-0.22325835 | temporal 0.56903547-0.00000000 | total 0.34812009\n",
      "Epoch 13, loss: 0.34812009-(best 0.30668464)\n",
      "\n",
      "Detailed Loss: recon 0.24782793-0.12391397 | disen 0.44196248-0.22098124 | temporal 0.56918740-0.00000000 | total 0.34489521\n",
      "Epoch 14, loss: 0.34489521-(best 0.30668464)\n",
      "\n",
      "Detailed Loss: recon 0.24399911-0.12199955 | disen 0.43986291-0.21993145 | temporal 0.56921226-0.00000000 | total 0.34193102\n",
      "Epoch 15, loss: 0.34193102-(best 0.30668464)\n",
      "\n",
      "Detailed Loss: recon 0.24364635-0.12182318 | disen 0.43419534-0.21709767 | temporal 0.56920916-0.00000000 | total 0.33892083\n",
      "Epoch 16, loss: 0.33892083-(best 0.30668464)\n",
      "\n",
      "Detailed Loss: recon 0.23693024-0.11846512 | disen 0.43126327-0.21563163 | temporal 0.56909978-0.00000000 | total 0.33409676\n",
      "Epoch 17, loss: 0.33409676-(best 0.30668464)\n",
      "\n",
      "Detailed Loss: recon 0.23909740-0.11954870 | disen 0.42727160-0.21363580 | temporal 0.56897724-0.00000000 | total 0.33318451\n",
      "Epoch 18, loss: 0.33318451-(best 0.30668464)\n",
      "\n",
      "Detailed Loss: recon 0.23686248-0.11843124 | disen 0.42507887-0.21253943 | temporal 0.56880879-0.00000000 | total 0.33097067\n",
      "Epoch 19, loss: 0.33097067-(best 0.30668464)\n",
      "\n",
      "Detailed Loss: recon 0.23705611-0.11852805 | disen 0.42115748-0.21057874 | temporal 0.56875819-0.00000000 | total 0.32910681\n",
      "Epoch 20, loss: 0.32910681-(best 0.30668464)\n",
      "\n",
      "Detailed Loss: recon 0.23582336-0.11791168 | disen 0.41884446-0.20942223 | temporal 0.56861079-0.00000000 | total 0.32733393\n",
      "Epoch 21, loss: 0.32733393-(best 0.30668464)\n",
      "\n",
      "Detailed Loss: recon 0.23637903-0.11818951 | disen 0.41409075-0.20704538 | temporal 0.56853002-0.00000000 | total 0.32523489\n",
      "Epoch 22, loss: 0.32523489-(best 0.30668464)\n",
      "\n",
      "Detailed Loss: recon 0.23033068-0.11516534 | disen 0.41218501-0.20609251 | temporal 0.56851411-0.00000000 | total 0.32125783\n",
      "Epoch 23, loss: 0.32125783-(best 0.30668464)\n",
      "\n",
      "Detailed Loss: recon 0.22700241-0.11350121 | disen 0.40966195-0.20483097 | temporal 0.56854242-0.00000000 | total 0.31833220\n",
      "Epoch 24, loss: 0.31833220-(best 0.30668464)\n",
      "\n",
      "Detailed Loss: recon 0.22672731-0.11336365 | disen 0.40669519-0.20334759 | temporal 0.56856728-0.00000000 | total 0.31671125\n",
      "Epoch 25, loss: 0.31671125-(best 0.30668464)\n",
      "\n",
      "Detailed Loss: recon 0.22549605-0.11274803 | disen 0.40488911-0.20244455 | temporal 0.56858969-0.00000000 | total 0.31519258\n",
      "Epoch 26, loss: 0.31519258-(best 0.30668464)\n",
      "\n",
      "Detailed Loss: recon 0.22288129-0.11144064 | disen 0.40341467-0.20170733 | temporal 0.56860912-0.00000000 | total 0.31314796\n",
      "Epoch 27, loss: 0.31314796-(best 0.30668464)\n",
      "\n",
      "Detailed Loss: recon 0.22079633-0.11039817 | disen 0.40042639-0.20021319 | temporal 0.56855780-0.00000000 | total 0.31061137\n",
      "Epoch 28, loss: 0.31061137-(best 0.30668464)\n",
      "\n",
      "Detailed Loss: recon 0.21775821-0.10887910 | disen 0.39836186-0.19918093 | temporal 0.56848234-0.00000000 | total 0.30806005\n",
      "Epoch 29, loss: 0.30806005-(best 0.30668464)\n",
      "\n",
      "Detailed Loss: recon 0.21697968-0.10848984 | disen 0.39508873-0.19754437 | temporal 0.56837523-0.00000000 | total 0.30603421\n",
      "Epoch 30, loss: 0.30603421-(best 0.30603421)\n",
      "\n",
      "Detailed Loss: recon 0.21769488-0.10884744 | disen 0.39432359-0.19716179 | temporal 0.56829345-0.00000000 | total 0.30600923\n",
      "Epoch 31, loss: 0.30600923-(best 0.30600923)\n",
      "\n",
      "Detailed Loss: recon 0.21874744-0.10937372 | disen 0.39038676-0.19519338 | temporal 0.56823593-0.00000000 | total 0.30456710\n",
      "Epoch 32, loss: 0.30456710-(best 0.30456710)\n",
      "\n",
      "Detailed Loss: recon 0.21803771-0.10901885 | disen 0.38879281-0.19439641 | temporal 0.56823653-0.00000000 | total 0.30341527\n",
      "Epoch 33, loss: 0.30341527-(best 0.30341527)\n",
      "\n",
      "Detailed Loss: recon 0.21379241-0.10689621 | disen 0.38514578-0.19257289 | temporal 0.56822884-0.00000000 | total 0.29946911\n",
      "Epoch 34, loss: 0.29946911-(best 0.29946911)\n",
      "\n",
      "Detailed Loss: recon 0.21209191-0.10604595 | disen 0.38479882-0.19239941 | temporal 0.56832814-0.00000000 | total 0.29844537\n",
      "Epoch 35, loss: 0.29844537-(best 0.29844537)\n",
      "\n",
      "Detailed Loss: recon 0.21219707-0.10609853 | disen 0.38347745-0.19173872 | temporal 0.56836265-0.00000000 | total 0.29783726\n",
      "Epoch 36, loss: 0.29783726-(best 0.29783726)\n",
      "\n",
      "Detailed Loss: recon 0.21197149-0.10598575 | disen 0.38128591-0.19064295 | temporal 0.56839573-0.00000000 | total 0.29662871\n",
      "Epoch 37, loss: 0.29662871-(best 0.29662871)\n",
      "\n",
      "Detailed Loss: recon 0.20991382-0.10495691 | disen 0.38131398-0.19065699 | temporal 0.56844962-0.00000000 | total 0.29561388\n",
      "Epoch 38, loss: 0.29561388-(best 0.29561388)\n",
      "\n",
      "Detailed Loss: recon 0.20852229-0.10426114 | disen 0.37983435-0.18991718 | temporal 0.56847525-0.00000000 | total 0.29417831\n",
      "Epoch 39, loss: 0.29417831-(best 0.29417831)\n",
      "\n",
      "Detailed Loss: recon 0.20695665-0.10347833 | disen 0.37877542-0.18938771 | temporal 0.56847399-0.00000000 | total 0.29286605\n",
      "Epoch 40, loss: 0.29286605-(best 0.29286605)\n",
      "\n",
      "Detailed Loss: recon 0.20565510-0.10282755 | disen 0.37722754-0.18861377 | temporal 0.56846666-0.00000000 | total 0.29144132\n",
      "Epoch 41, loss: 0.29144132-(best 0.29144132)\n",
      "\n",
      "Detailed Loss: recon 0.20579997-0.10289998 | disen 0.37675488-0.18837744 | temporal 0.56844479-0.00000000 | total 0.29127741\n",
      "Epoch 42, loss: 0.29127741-(best 0.29127741)\n",
      "\n",
      "Detailed Loss: recon 0.20346180-0.10173090 | disen 0.37505329-0.18752664 | temporal 0.56845552-0.00000000 | total 0.28925753\n",
      "Epoch 43, loss: 0.28925753-(best 0.28925753)\n",
      "\n",
      "Detailed Loss: recon 0.20160161-0.10080080 | disen 0.37457806-0.18728903 | temporal 0.56848389-0.00000000 | total 0.28808984\n",
      "Epoch 44, loss: 0.28808984-(best 0.28808984)\n",
      "\n",
      "Detailed Loss: recon 0.20079416-0.10039708 | disen 0.37447518-0.18723759 | temporal 0.56853330-0.00000000 | total 0.28763467\n",
      "Epoch 45, loss: 0.28763467-(best 0.28763467)\n",
      "\n",
      "Detailed Loss: recon 0.20134419-0.10067210 | disen 0.37167323-0.18583661 | temporal 0.56856346-0.00000000 | total 0.28650871\n",
      "Epoch 46, loss: 0.28650871-(best 0.28650871)\n",
      "\n",
      "Detailed Loss: recon 0.20202476-0.10101238 | disen 0.37103325-0.18551663 | temporal 0.56860507-0.00000000 | total 0.28652900\n",
      "Epoch 47, loss: 0.28652900-(best 0.28650871)\n",
      "\n",
      "Detailed Loss: recon 0.19965993-0.09982996 | disen 0.36989355-0.18494678 | temporal 0.56864274-0.00000000 | total 0.28477675\n",
      "Epoch 48, loss: 0.28477675-(best 0.28477675)\n",
      "\n",
      "Detailed Loss: recon 0.19942620-0.09971310 | disen 0.36800939-0.18400469 | temporal 0.56864554-0.00000000 | total 0.28371781\n",
      "Epoch 49, loss: 0.28371781-(best 0.28371781)\n",
      "\n",
      "Detailed Loss: recon 0.20059574-0.10029787 | disen 0.36600846-0.18300423 | temporal 0.56859398-0.00000000 | total 0.28330210\n",
      "Epoch 50, loss: 0.28330210-(best 0.28330210)\n",
      "\n",
      ">> Reached final epoch. Loading best model...\n",
      "TRAINING XGB PREDICTOR\n",
      ">> Ending training!\n",
      "\n",
      "-----------------------\n",
      "TIME COST: 4.880615711212158 s\n",
      "Best Val: REC 62.94 PRE 48.43 MF1 67.60 AUC 79.00 TP 479 FP 510 TN 1631 FN 282\n",
      "-----------------------\n",
      "Generating additional positive data from adversary...\n",
      "Generating additional negative data by duplicating random nodes...\n",
      "\n",
      "ASSIGNED GRAPH FOR EVALUATION: 12345 {1: 2694, 0: 9651} Nodes, 1157906 Edges\n",
      "\n",
      "-------------------\n",
      "STARTING EVALUATION\n",
      "-------------------\n",
      "\n",
      "Dataset - Overall: REC 56.57 PRE 53.31 MF1 70.90 AUC 82.96 TP 1524 FP 1335 TN 8316 FN 1170 | 12345 {1: 2694, 0: 9651}\n",
      "Dataset - Train: REC 71.80 PRE 67.49 MF1 79.14 AUC 89.94 TP 820 FP 395 TN 2816 FN 322 | 4353 {1: 1142, 0: 3211}\n",
      "Dataset - Val: REC 53.88 PRE 49.52 MF1 66.68 AUC 77.58 TP 410 FP 418 TN 1723 FN 351 | 2902 {0: 2141, 1: 761}\n",
      "Dataset - Test: REC 37.17 PRE 36.03 MF1 62.35 AUC 77.42 TP 294 FP 522 TN 3777 FN 497 | 5090 {0: 4299, 1: 791}\n",
      "PREDICTION STATUS - {'1-True': 2197, '0-True': 5874, '0-False': 3777, '1-False': 497}\n",
      "    >> 497 positive nodes left unpredicted...\n",
      "    >> 418 OG positive nodes left unpredicted...\n",
      "-----------------------\n",
      "PREDICTION RESULT - ROUNDS\n",
      "Dataset - Round 0: REC 55.54 PRE 49.79 MF1 69.07 AUC 81.14 TP 717 FP 723 TN 3865 FN 574 | 5879 {0: 4588, 1: 1291}\n",
      "Dataset - Round 1: REC 38.28 PRE 41.18 MF1 61.80 AUC 72.24 TP 49 FP 70 TN 389 FN 79 | 587 {1: 128, 0: 459}\n",
      "-----------------------\n",
      "PREDICTION RESULT - SEEDS\n",
      "Seeds - Current: REC 36.72 PRE 38.21 MF1 60.22 AUC 71.27 TP 47 FP 76 TN 383 FN 81 | 587 {1: 128, 0: 459}\n",
      "Seeds - Prev: REC 0.00 PRE 0.00 MF1 38.66 AUC 71.27 TP 0 FP 89 TN 370 FN 128 | 587 {1: 128, 0: 459}\n",
      "Selecting data as budgeted ground truth for next round...\n",
      "  > Starting round 2...\n",
      "\n",
      "+++++++++++++++++\n",
      "STARTING ROUND 2!\n",
      "+++++++++++++++++\n",
      "        \n",
      "ASSIGNED GRAPH FOR TRAINING: 12345 {1: 2694, 0: 9651} Nodes, 1157906 Edges\n",
      "\n",
      ">> Splitting to train-validation set...\n",
      "Alotting train-val-test split for round 2\n",
      "Initial pool: 5879, Prediction pool: 5696, Budget pool: 0, Full pool: 8071\n",
      "Full graph size: 12345, Training: 4842, Val: 3229, Test: 12345\n",
      "    >> INITIAL DATA SPLIT: 4842 train rows ({1: 1318, 0: 3524}) | 3229 val rows ({0: 2350, 1: 879}) | 4274 test rows ({0: 3777, 1: 497})\n",
      "    >> AUGMENTED DATA SPLIT: 4842 train rows ({1: 1318, 0: 3524}) | 3229 val rows ({0: 2350, 1: 879}) | 4274 test rows ({0: 3777, 1: 497})\n",
      "    >> Updated cross-entropy weight to 2.6737481031866466...\n",
      "\n",
      "-----------------------\n",
      "STARTING MODEL TRAININGps_l\n",
      "-----------------------\n",
      "\n",
      "TRAINING NODE EMBEDDER\n",
      "Detailed Loss: recon 0.20047536-0.10023768 | disen 0.36379081-0.18189541 | temporal 0.59090203-0.00000000 | total 0.28213310\n",
      "Epoch 1, loss: 0.28213310-(best 0.28213310)\n",
      "\n",
      "Detailed Loss: recon 0.22043692-0.11021846 | disen 0.36751550-0.18375775 | temporal 0.59067696-0.00000000 | total 0.29397622\n",
      "Epoch 2, loss: 0.29397622-(best 0.28213310)\n",
      "\n",
      "Detailed Loss: recon 0.25639948-0.12819974 | disen 0.36516118-0.18258059 | temporal 0.59126413-0.00000000 | total 0.31078035\n",
      "Epoch 3, loss: 0.31078035-(best 0.28213310)\n",
      "\n",
      "Detailed Loss: recon 0.21296041-0.10648020 | disen 0.36841202-0.18420601 | temporal 0.59122795-0.00000000 | total 0.29068622\n",
      "Epoch 4, loss: 0.29068622-(best 0.28213310)\n",
      "\n",
      "Detailed Loss: recon 0.22481093-0.11240546 | disen 0.37058848-0.18529424 | temporal 0.59089822-0.00000000 | total 0.29769969\n",
      "Epoch 5, loss: 0.29769969-(best 0.28213310)\n",
      "\n",
      "Detailed Loss: recon 0.22953939-0.11476970 | disen 0.36825424-0.18412712 | temporal 0.59080613-0.00000000 | total 0.29889682\n",
      "Epoch 6, loss: 0.29889682-(best 0.28213310)\n",
      "\n",
      "Detailed Loss: recon 0.21001151-0.10500576 | disen 0.36582333-0.18291166 | temporal 0.59093720-0.00000000 | total 0.28791744\n",
      "Epoch 7, loss: 0.28791744-(best 0.28213310)\n",
      "\n",
      "Detailed Loss: recon 0.20538110-0.10269055 | disen 0.36373252-0.18186626 | temporal 0.59116197-0.00000000 | total 0.28455681\n",
      "Epoch 8, loss: 0.28455681-(best 0.28213310)\n",
      "\n",
      "Detailed Loss: recon 0.21169876-0.10584938 | disen 0.36516875-0.18258438 | temporal 0.59129637-0.00000000 | total 0.28843376\n",
      "Epoch 9, loss: 0.28843376-(best 0.28213310)\n",
      "\n",
      "Detailed Loss: recon 0.20432568-0.10216284 | disen 0.36528635-0.18264318 | temporal 0.59124780-0.00000000 | total 0.28480601\n",
      "Epoch 10, loss: 0.28480601-(best 0.28213310)\n",
      "\n",
      "Detailed Loss: recon 0.20340823-0.10170411 | disen 0.36469889-0.18234944 | temporal 0.59117371-0.00000000 | total 0.28405356\n",
      "Epoch 11, loss: 0.28405356-(best 0.28213310)\n",
      "\n",
      "Detailed Loss: recon 0.20276861-0.10138430 | disen 0.36532938-0.18266469 | temporal 0.59105831-0.00000000 | total 0.28404900\n",
      "Epoch 12, loss: 0.28404900-(best 0.28213310)\n",
      "\n",
      "Detailed Loss: recon 0.20326531-0.10163265 | disen 0.36304092-0.18152046 | temporal 0.59099704-0.00000000 | total 0.28315312\n",
      "Epoch 13, loss: 0.28315312-(best 0.28213310)\n",
      "\n",
      "Detailed Loss: recon 0.20163667-0.10081834 | disen 0.36185741-0.18092871 | temporal 0.59101087-0.00000000 | total 0.28174704\n",
      "Epoch 14, loss: 0.28174704-(best 0.28174704)\n",
      "\n",
      "Detailed Loss: recon 0.19826373-0.09913187 | disen 0.36249930-0.18124965 | temporal 0.59108615-0.00000000 | total 0.28038150\n",
      "Epoch 15, loss: 0.28038150-(best 0.28038150)\n",
      "\n",
      "Detailed Loss: recon 0.19925885-0.09962942 | disen 0.36213040-0.18106520 | temporal 0.59110558-0.00000000 | total 0.28069463\n",
      "Epoch 16, loss: 0.28069463-(best 0.28038150)\n",
      "\n",
      "Detailed Loss: recon 0.19913988-0.09956994 | disen 0.36087275-0.18043637 | temporal 0.59112495-0.00000000 | total 0.28000632\n",
      "Epoch 17, loss: 0.28000632-(best 0.28000632)\n",
      "\n",
      "Detailed Loss: recon 0.19833712-0.09916856 | disen 0.35700905-0.17850453 | temporal 0.59107065-0.00000000 | total 0.27767310\n",
      "Epoch 18, loss: 0.27767310-(best 0.27767310)\n",
      "\n",
      "Detailed Loss: recon 0.19643745-0.09821872 | disen 0.35638082-0.17819041 | temporal 0.59104931-0.00000000 | total 0.27640915\n",
      "Epoch 19, loss: 0.27640915-(best 0.27640915)\n",
      "\n",
      "Detailed Loss: recon 0.19523501-0.09761751 | disen 0.35523868-0.17761934 | temporal 0.59107089-0.00000000 | total 0.27523685\n",
      "Epoch 20, loss: 0.27523685-(best 0.27523685)\n",
      "\n",
      "Detailed Loss: recon 0.19589053-0.09794527 | disen 0.35443926-0.17721963 | temporal 0.59109718-0.00000000 | total 0.27516490\n",
      "Epoch 21, loss: 0.27516490-(best 0.27516490)\n",
      "\n",
      "Detailed Loss: recon 0.19452131-0.09726065 | disen 0.35355908-0.17677954 | temporal 0.59115726-0.00000000 | total 0.27404019\n",
      "Epoch 22, loss: 0.27404019-(best 0.27404019)\n",
      "\n",
      "Detailed Loss: recon 0.19526514-0.09763257 | disen 0.35168606-0.17584303 | temporal 0.59115738-0.00000000 | total 0.27347559\n",
      "Epoch 23, loss: 0.27347559-(best 0.27347559)\n",
      "\n",
      "Detailed Loss: recon 0.19491367-0.09745684 | disen 0.34972870-0.17486435 | temporal 0.59116828-0.00000000 | total 0.27232119\n",
      "Epoch 24, loss: 0.27232119-(best 0.27232119)\n",
      "\n",
      "Detailed Loss: recon 0.19589636-0.09794818 | disen 0.35000390-0.17500195 | temporal 0.59117460-0.00000000 | total 0.27295011\n",
      "Epoch 25, loss: 0.27295011-(best 0.27232119)\n",
      "\n",
      "Detailed Loss: recon 0.19493043-0.09746522 | disen 0.34759104-0.17379552 | temporal 0.59111094-0.00000000 | total 0.27126074\n",
      "Epoch 26, loss: 0.27126074-(best 0.27126074)\n",
      "\n",
      "Detailed Loss: recon 0.19635370-0.09817685 | disen 0.34702510-0.17351255 | temporal 0.59109342-0.00000000 | total 0.27168941\n",
      "Epoch 27, loss: 0.27168941-(best 0.27126074)\n",
      "\n",
      "Detailed Loss: recon 0.19538346-0.09769173 | disen 0.34603077-0.17301539 | temporal 0.59109789-0.00000000 | total 0.27070713\n",
      "Epoch 28, loss: 0.27070713-(best 0.27070713)\n",
      "\n",
      "Detailed Loss: recon 0.19414125-0.09707063 | disen 0.34493256-0.17246628 | temporal 0.59112555-0.00000000 | total 0.26953691\n",
      "Epoch 29, loss: 0.26953691-(best 0.26953691)\n",
      "\n",
      "Detailed Loss: recon 0.19504949-0.09752475 | disen 0.34305209-0.17152604 | temporal 0.59107202-0.00000000 | total 0.26905078\n",
      "Epoch 30, loss: 0.26905078-(best 0.26905078)\n",
      "\n",
      "Detailed Loss: recon 0.19367057-0.09683529 | disen 0.34342700-0.17171350 | temporal 0.59106791-0.00000000 | total 0.26854879\n",
      "Epoch 31, loss: 0.26854879-(best 0.26854879)\n",
      "\n",
      "Detailed Loss: recon 0.19307750-0.09653875 | disen 0.34283185-0.17141593 | temporal 0.59104717-0.00000000 | total 0.26795468\n",
      "Epoch 32, loss: 0.26795468-(best 0.26795468)\n",
      "\n",
      "Detailed Loss: recon 0.19471258-0.09735629 | disen 0.33947659-0.16973829 | temporal 0.59100169-0.00000000 | total 0.26709458\n",
      "Epoch 33, loss: 0.26709458-(best 0.26709458)\n",
      "\n",
      "Detailed Loss: recon 0.19484264-0.09742132 | disen 0.34044391-0.17022195 | temporal 0.59101230-0.00000000 | total 0.26764327\n",
      "Epoch 34, loss: 0.26764327-(best 0.26709458)\n",
      "\n",
      "Detailed Loss: recon 0.19434468-0.09717234 | disen 0.34017825-0.17008913 | temporal 0.59099120-0.00000000 | total 0.26726148\n",
      "Epoch 35, loss: 0.26726148-(best 0.26709458)\n",
      "\n",
      "Detailed Loss: recon 0.19346790-0.09673395 | disen 0.33882493-0.16941246 | temporal 0.59098303-0.00000000 | total 0.26614642\n",
      "Epoch 36, loss: 0.26614642-(best 0.26614642)\n",
      "\n",
      "Detailed Loss: recon 0.19251201-0.09625600 | disen 0.33801138-0.16900569 | temporal 0.59096664-0.00000000 | total 0.26526171\n",
      "Epoch 37, loss: 0.26526171-(best 0.26526171)\n",
      "\n",
      "Detailed Loss: recon 0.19326574-0.09663287 | disen 0.33764094-0.16882047 | temporal 0.59097493-0.00000000 | total 0.26545334\n",
      "Epoch 38, loss: 0.26545334-(best 0.26526171)\n",
      "\n",
      "Detailed Loss: recon 0.19427571-0.09713785 | disen 0.33692938-0.16846469 | temporal 0.59099686-0.00000000 | total 0.26560253\n",
      "Epoch 39, loss: 0.26560253-(best 0.26526171)\n",
      "\n",
      "Detailed Loss: recon 0.19317049-0.09658524 | disen 0.33551884-0.16775942 | temporal 0.59099525-0.00000000 | total 0.26434466\n",
      "Epoch 40, loss: 0.26434466-(best 0.26434466)\n",
      "\n",
      "Detailed Loss: recon 0.19287258-0.09643629 | disen 0.33483243-0.16741621 | temporal 0.59095931-0.00000000 | total 0.26385251\n",
      "Epoch 41, loss: 0.26385251-(best 0.26385251)\n",
      "\n",
      "Detailed Loss: recon 0.19406334-0.09703167 | disen 0.33351588-0.16675794 | temporal 0.59097546-0.00000000 | total 0.26378959\n",
      "Epoch 42, loss: 0.26378959-(best 0.26378959)\n",
      "\n",
      "Detailed Loss: recon 0.19217041-0.09608521 | disen 0.33254337-0.16627169 | temporal 0.59098589-0.00000000 | total 0.26235688\n",
      "Epoch 43, loss: 0.26235688-(best 0.26235688)\n",
      "\n",
      "Detailed Loss: recon 0.19229899-0.09614950 | disen 0.33186942-0.16593471 | temporal 0.59103668-0.00000000 | total 0.26208422\n",
      "Epoch 44, loss: 0.26208422-(best 0.26208422)\n",
      "\n",
      "Detailed Loss: recon 0.19287571-0.09643786 | disen 0.33059448-0.16529724 | temporal 0.59102893-0.00000000 | total 0.26173508\n",
      "Epoch 45, loss: 0.26173508-(best 0.26173508)\n",
      "\n",
      "Detailed Loss: recon 0.19311522-0.09655761 | disen 0.32984424-0.16492212 | temporal 0.59100217-0.00000000 | total 0.26147974\n",
      "Epoch 46, loss: 0.26147974-(best 0.26147974)\n",
      "\n",
      "Detailed Loss: recon 0.19112127-0.09556063 | disen 0.33064133-0.16532066 | temporal 0.59098911-0.00000000 | total 0.26088130\n",
      "Epoch 47, loss: 0.26088130-(best 0.26088130)\n",
      "\n",
      "Detailed Loss: recon 0.19241612-0.09620806 | disen 0.32818520-0.16409260 | temporal 0.59097421-0.00000000 | total 0.26030067\n",
      "Epoch 48, loss: 0.26030067-(best 0.26030067)\n",
      "\n",
      "Detailed Loss: recon 0.19257605-0.09628803 | disen 0.32769471-0.16384736 | temporal 0.59099114-0.00000000 | total 0.26013538\n",
      "Epoch 49, loss: 0.26013538-(best 0.26013538)\n",
      "\n",
      "Detailed Loss: recon 0.19365183-0.09682591 | disen 0.32645428-0.16322714 | temporal 0.59094101-0.00000000 | total 0.26005304\n",
      "Epoch 50, loss: 0.26005304-(best 0.26005304)\n",
      "\n",
      ">> Reached final epoch. Loading best model...\n",
      "TRAINING XGB PREDICTOR\n",
      ">> Ending training!\n",
      "\n",
      "-----------------------\n",
      "TIME COST: 6.001131296157837 s\n",
      "Best Val: REC 54.15 PRE 48.42 MF1 65.66 AUC 73.57 TP 476 FP 507 TN 1843 FN 403\n",
      "-----------------------\n",
      "Generating additional positive data from adversary...\n",
      "Generating additional negative data by duplicating random nodes...\n",
      "\n",
      "ASSIGNED GRAPH FOR EVALUATION: 12932 {1: 2822, 0: 10110} Nodes, 1258448 Edges\n",
      "\n",
      "-------------------\n",
      "STARTING EVALUATION\n",
      "-------------------\n",
      "\n",
      "Dataset - Overall: REC 48.94 PRE 48.39 MF1 67.11 AUC 75.89 TP 1381 FP 1473 TN 8637 FN 1441 | 12932 {1: 2822, 0: 10110}\n",
      "Dataset - Train: REC 61.15 PRE 59.97 MF1 72.80 AUC 81.27 TP 806 FP 538 TN 2986 FN 512 | 4842 {1: 1318, 0: 3524}\n",
      "Dataset - Val: REC 48.12 PRE 44.71 MF1 62.61 AUC 69.77 TP 423 FP 523 TN 1827 FN 456 | 3229 {0: 2350, 1: 879}\n",
      "Dataset - Test: REC 24.32 PRE 26.95 MF1 57.60 AUC 69.13 TP 152 FP 412 TN 3824 FN 473 | 4861 {0: 4236, 1: 625}\n",
      "PREDICTION STATUS - {'1-True': 2349, '0-True': 6286, '0-False': 3824, '1-False': 473}\n",
      "    >> 473 positive nodes left unpredicted...\n",
      "    >> 314 OG positive nodes left unpredicted...\n",
      "-----------------------\n",
      "PREDICTION RESULT - ROUNDS\n",
      "Dataset - Round 0: REC 51.28 PRE 49.77 MF1 68.16 AUC 76.78 TP 662 FP 668 TN 3920 FN 629 | 5879 {0: 4588, 1: 1291}\n",
      "Dataset - Round 1: REC 35.16 PRE 38.46 MF1 60.03 AUC 69.08 TP 45 FP 72 TN 387 FN 83 | 587 {1: 128, 0: 459}\n",
      "Dataset - Round 2: REC 22.66 PRE 29.59 MF1 53.97 AUC 63.50 TP 29 FP 69 TN 390 FN 99 | 587 {1: 128, 0: 459}\n",
      "-----------------------\n",
      "PREDICTION RESULT - SEEDS\n",
      "Seeds - Current: REC 20.31 PRE 28.89 MF1 53.24 AUC 62.16 TP 26 FP 64 TN 395 FN 102 | 587 {1: 128, 0: 459}\n",
      "Seeds - Prev: REC 0.00 PRE 0.00 MF1 40.83 AUC 62.16 TP 0 FP 54 TN 405 FN 128 | 587 {1: 128, 0: 459}\n",
      "Selecting data as budgeted ground truth for next round...\n",
      "  > Starting round 3...\n",
      "\n",
      "+++++++++++++++++\n",
      "STARTING ROUND 3!\n",
      "+++++++++++++++++\n",
      "        \n",
      "ASSIGNED GRAPH FOR TRAINING: 12932 {1: 2822, 0: 10110} Nodes, 1258448 Edges\n",
      "\n",
      ">> Splitting to train-validation set...\n",
      "Alotting train-val-test split for round 3\n",
      "Initial pool: 5879, Prediction pool: 8550, Budget pool: 0, Full pool: 8635\n",
      "Full graph size: 12932, Training: 5181, Val: 3454, Test: 12932\n",
      "    >> INITIAL DATA SPLIT: 5181 train rows ({0: 3772, 1: 1409}) | 3454 val rows ({1: 940, 0: 2514}) | 4297 test rows ({0: 3824, 1: 473})\n",
      "    >> AUGMENTED DATA SPLIT: 5181 train rows ({0: 3772, 1: 1409}) | 3454 val rows ({1: 940, 0: 2514}) | 4297 test rows ({0: 3824, 1: 473})\n",
      "    >> Updated cross-entropy weight to 2.6770759403832507...\n",
      "\n",
      "-----------------------\n",
      "STARTING MODEL TRAININGps_l\n",
      "-----------------------\n",
      "\n",
      "TRAINING NODE EMBEDDER\n",
      "Detailed Loss: recon 0.19181895-0.09590948 | disen 0.31900018-0.15950009 | temporal 0.61067289-0.00000000 | total 0.25540957\n",
      "Epoch 1, loss: 0.25540957-(best 0.25540957)\n",
      "\n",
      "Detailed Loss: recon 0.28054979-0.14027490 | disen 0.33054632-0.16527316 | temporal 0.61019951-0.00000000 | total 0.30554807\n",
      "Epoch 2, loss: 0.30554807-(best 0.25540957)\n",
      "\n",
      "Detailed Loss: recon 0.23187569-0.11593784 | disen 0.33283818-0.16641909 | temporal 0.61109346-0.00000000 | total 0.28235692\n",
      "Epoch 3, loss: 0.28235692-(best 0.25540957)\n",
      "\n",
      "Detailed Loss: recon 0.25415671-0.12707835 | disen 0.33042043-0.16521022 | temporal 0.61144203-0.00000000 | total 0.29228857\n",
      "Epoch 4, loss: 0.29228857-(best 0.25540957)\n",
      "\n",
      "Detailed Loss: recon 0.22973254-0.11486627 | disen 0.33024788-0.16512394 | temporal 0.61132771-0.00000000 | total 0.27999020\n",
      "Epoch 5, loss: 0.27999020-(best 0.25540957)\n",
      "\n",
      "Detailed Loss: recon 0.21070741-0.10535371 | disen 0.33056533-0.16528267 | temporal 0.61106503-0.00000000 | total 0.27063638\n",
      "Epoch 6, loss: 0.27063638-(best 0.25540957)\n",
      "\n",
      "Detailed Loss: recon 0.20447700-0.10223850 | disen 0.32903975-0.16451988 | temporal 0.61065912-0.00000000 | total 0.26675838\n",
      "Epoch 7, loss: 0.26675838-(best 0.25540957)\n",
      "\n",
      "Detailed Loss: recon 0.20628336-0.10314168 | disen 0.32882482-0.16441241 | temporal 0.61040288-0.00000000 | total 0.26755410\n",
      "Epoch 8, loss: 0.26755410-(best 0.25540957)\n",
      "\n",
      "Detailed Loss: recon 0.20748734-0.10374367 | disen 0.32909650-0.16454825 | temporal 0.61035556-0.00000000 | total 0.26829192\n",
      "Epoch 9, loss: 0.26829192-(best 0.25540957)\n",
      "\n",
      "Detailed Loss: recon 0.20536143-0.10268071 | disen 0.32991970-0.16495985 | temporal 0.61034852-0.00000000 | total 0.26764056\n",
      "Epoch 10, loss: 0.26764056-(best 0.25540957)\n",
      "\n",
      "Detailed Loss: recon 0.20221607-0.10110804 | disen 0.33044177-0.16522089 | temporal 0.61044949-0.00000000 | total 0.26632893\n",
      "Epoch 11, loss: 0.26632893-(best 0.25540957)\n",
      "\n",
      "Detailed Loss: recon 0.19967733-0.09983867 | disen 0.33090091-0.16545045 | temporal 0.61058903-0.00000000 | total 0.26528913\n",
      "Epoch 12, loss: 0.26528913-(best 0.25540957)\n",
      "\n",
      "Detailed Loss: recon 0.19710562-0.09855281 | disen 0.33038670-0.16519335 | temporal 0.61066270-0.00000000 | total 0.26374614\n",
      "Epoch 13, loss: 0.26374614-(best 0.25540957)\n",
      "\n",
      "Detailed Loss: recon 0.19629282-0.09814641 | disen 0.32942492-0.16471246 | temporal 0.61068338-0.00000000 | total 0.26285887\n",
      "Epoch 14, loss: 0.26285887-(best 0.25540957)\n",
      "\n",
      "Detailed Loss: recon 0.19796751-0.09898376 | disen 0.32799196-0.16399598 | temporal 0.61063224-0.00000000 | total 0.26297975\n",
      "Epoch 15, loss: 0.26297975-(best 0.25540957)\n",
      "\n",
      "Detailed Loss: recon 0.19742195-0.09871098 | disen 0.32653493-0.16326746 | temporal 0.61062044-0.00000000 | total 0.26197845\n",
      "Epoch 16, loss: 0.26197845-(best 0.25540957)\n",
      "\n",
      "Detailed Loss: recon 0.19593906-0.09796953 | disen 0.32535130-0.16267565 | temporal 0.61067182-0.00000000 | total 0.26064518\n",
      "Epoch 17, loss: 0.26064518-(best 0.25540957)\n",
      "\n",
      "Detailed Loss: recon 0.19358677-0.09679338 | disen 0.32532716-0.16266358 | temporal 0.61076927-0.00000000 | total 0.25945696\n",
      "Epoch 18, loss: 0.25945696-(best 0.25540957)\n",
      "\n",
      "Detailed Loss: recon 0.19251131-0.09625565 | disen 0.32383156-0.16191578 | temporal 0.61082840-0.00000000 | total 0.25817144\n",
      "Epoch 19, loss: 0.25817144-(best 0.25540957)\n",
      "\n",
      "Detailed Loss: recon 0.19198120-0.09599060 | disen 0.32334399-0.16167200 | temporal 0.61086500-0.00000000 | total 0.25766259\n",
      "Epoch 20, loss: 0.25766259-(best 0.25540957)\n",
      "\n",
      "Detailed Loss: recon 0.19337675-0.09668837 | disen 0.32251000-0.16125500 | temporal 0.61089194-0.00000000 | total 0.25794339\n",
      "Epoch 21, loss: 0.25794339-(best 0.25540957)\n",
      "\n",
      "Detailed Loss: recon 0.19408536-0.09704268 | disen 0.32033336-0.16016668 | temporal 0.61080635-0.00000000 | total 0.25720936\n",
      "Epoch 22, loss: 0.25720936-(best 0.25540957)\n",
      "\n",
      "Detailed Loss: recon 0.19181499-0.09590749 | disen 0.31830966-0.15915483 | temporal 0.61075813-0.00000000 | total 0.25506234\n",
      "Epoch 23, loss: 0.25506234-(best 0.25506234)\n",
      "\n",
      "Detailed Loss: recon 0.19068456-0.09534228 | disen 0.31772929-0.15886465 | temporal 0.61073911-0.00000000 | total 0.25420693\n",
      "Epoch 24, loss: 0.25420693-(best 0.25420693)\n",
      "\n",
      "Detailed Loss: recon 0.19016980-0.09508490 | disen 0.31635940-0.15817970 | temporal 0.61070877-0.00000000 | total 0.25326461\n",
      "Epoch 25, loss: 0.25326461-(best 0.25326461)\n",
      "\n",
      "Detailed Loss: recon 0.19091301-0.09545650 | disen 0.31488943-0.15744472 | temporal 0.61064279-0.00000000 | total 0.25290123\n",
      "Epoch 26, loss: 0.25290123-(best 0.25290123)\n",
      "\n",
      "Detailed Loss: recon 0.18938331-0.09469166 | disen 0.31393880-0.15696940 | temporal 0.61062187-0.00000000 | total 0.25166106\n",
      "Epoch 27, loss: 0.25166106-(best 0.25166106)\n",
      "\n",
      "Detailed Loss: recon 0.19003934-0.09501967 | disen 0.31470305-0.15735152 | temporal 0.61067313-0.00000000 | total 0.25237119\n",
      "Epoch 28, loss: 0.25237119-(best 0.25166106)\n",
      "\n",
      "Detailed Loss: recon 0.18857101-0.09428550 | disen 0.31265980-0.15632990 | temporal 0.61063355-0.00000000 | total 0.25061542\n",
      "Epoch 29, loss: 0.25061542-(best 0.25061542)\n",
      "\n",
      "Detailed Loss: recon 0.18933767-0.09466884 | disen 0.31285203-0.15642601 | temporal 0.61069387-0.00000000 | total 0.25109485\n",
      "Epoch 30, loss: 0.25109485-(best 0.25061542)\n",
      "\n",
      "Detailed Loss: recon 0.18763641-0.09381820 | disen 0.31312406-0.15656203 | temporal 0.61067200-0.00000000 | total 0.25038022\n",
      "Epoch 31, loss: 0.25038022-(best 0.25038022)\n",
      "\n",
      "Detailed Loss: recon 0.18830661-0.09415331 | disen 0.31157863-0.15578932 | temporal 0.61061943-0.00000000 | total 0.24994263\n",
      "Epoch 32, loss: 0.24994263-(best 0.24994263)\n",
      "\n",
      "Detailed Loss: recon 0.18893456-0.09446728 | disen 0.31134528-0.15567264 | temporal 0.61058939-0.00000000 | total 0.25013992\n",
      "Epoch 33, loss: 0.25013992-(best 0.24994263)\n",
      "\n",
      "Detailed Loss: recon 0.18923722-0.09461861 | disen 0.31061417-0.15530708 | temporal 0.61055690-0.00000000 | total 0.24992570\n",
      "Epoch 34, loss: 0.24992570-(best 0.24992570)\n",
      "\n",
      "Detailed Loss: recon 0.18866432-0.09433216 | disen 0.30975318-0.15487659 | temporal 0.61045796-0.00000000 | total 0.24920875\n",
      "Epoch 35, loss: 0.24920875-(best 0.24920875)\n",
      "\n",
      "Detailed Loss: recon 0.18931469-0.09465735 | disen 0.30959529-0.15479764 | temporal 0.61050105-0.00000000 | total 0.24945499\n",
      "Epoch 36, loss: 0.24945499-(best 0.24920875)\n",
      "\n",
      "Detailed Loss: recon 0.18841080-0.09420540 | disen 0.30827355-0.15413678 | temporal 0.61041582-0.00000000 | total 0.24834219\n",
      "Epoch 37, loss: 0.24834219-(best 0.24834219)\n",
      "\n",
      "Detailed Loss: recon 0.18819776-0.09409888 | disen 0.30829203-0.15414602 | temporal 0.61042994-0.00000000 | total 0.24824490\n",
      "Epoch 38, loss: 0.24824490-(best 0.24824490)\n",
      "\n",
      "Detailed Loss: recon 0.18775856-0.09387928 | disen 0.30692238-0.15346119 | temporal 0.61041218-0.00000000 | total 0.24734047\n",
      "Epoch 39, loss: 0.24734047-(best 0.24734047)\n",
      "\n",
      "Detailed Loss: recon 0.18671298-0.09335649 | disen 0.30674785-0.15337393 | temporal 0.61040640-0.00000000 | total 0.24673042\n",
      "Epoch 40, loss: 0.24673042-(best 0.24673042)\n",
      "\n",
      "Detailed Loss: recon 0.18694526-0.09347263 | disen 0.30718213-0.15359107 | temporal 0.61044222-0.00000000 | total 0.24706370\n",
      "Epoch 41, loss: 0.24706370-(best 0.24673042)\n",
      "\n",
      "Detailed Loss: recon 0.18634197-0.09317099 | disen 0.30580181-0.15290090 | temporal 0.61045140-0.00000000 | total 0.24607189\n",
      "Epoch 42, loss: 0.24607189-(best 0.24607189)\n",
      "\n",
      "Detailed Loss: recon 0.18609504-0.09304752 | disen 0.30475944-0.15237972 | temporal 0.61043572-0.00000000 | total 0.24542725\n",
      "Epoch 43, loss: 0.24542725-(best 0.24542725)\n",
      "\n",
      "Detailed Loss: recon 0.18598142-0.09299071 | disen 0.30584210-0.15292105 | temporal 0.61044478-0.00000000 | total 0.24591176\n",
      "Epoch 44, loss: 0.24591176-(best 0.24542725)\n",
      "\n",
      "Detailed Loss: recon 0.18607533-0.09303766 | disen 0.30485606-0.15242803 | temporal 0.61036718-0.00000000 | total 0.24546570\n",
      "Epoch 45, loss: 0.24546570-(best 0.24542725)\n",
      "\n",
      "Detailed Loss: recon 0.18667911-0.09333955 | disen 0.30459213-0.15229607 | temporal 0.61044508-0.00000000 | total 0.24563563\n",
      "Epoch 46, loss: 0.24563563-(best 0.24542725)\n",
      "\n",
      "Detailed Loss: recon 0.18601298-0.09300649 | disen 0.30408418-0.15204209 | temporal 0.61048788-0.00000000 | total 0.24504858\n",
      "Epoch 47, loss: 0.24504858-(best 0.24504858)\n",
      "\n",
      "Detailed Loss: recon 0.18626200-0.09313100 | disen 0.30364370-0.15182185 | temporal 0.61044216-0.00000000 | total 0.24495286\n",
      "Epoch 48, loss: 0.24495286-(best 0.24495286)\n",
      "\n",
      "Detailed Loss: recon 0.18557313-0.09278657 | disen 0.30367738-0.15183869 | temporal 0.61043698-0.00000000 | total 0.24462526\n",
      "Epoch 49, loss: 0.24462526-(best 0.24462526)\n",
      "\n",
      "Detailed Loss: recon 0.18487969-0.09243985 | disen 0.30263180-0.15131590 | temporal 0.61042732-0.00000000 | total 0.24375574\n",
      "Epoch 50, loss: 0.24375574-(best 0.24375574)\n",
      "\n",
      ">> Reached final epoch. Loading best model...\n",
      "TRAINING XGB PREDICTOR\n",
      ">> Ending training!\n",
      "\n",
      "-----------------------\n",
      "TIME COST: 6.840722560882568 s\n",
      "Best Val: REC 56.70 PRE 46.63 MF1 65.05 AUC 74.03 TP 533 FP 610 TN 1904 FN 407\n",
      "-----------------------\n",
      "Generating additional positive data from adversary...\n",
      "Generating additional negative data by duplicating random nodes...\n",
      "\n",
      "ASSIGNED GRAPH FOR EVALUATION: 13519 {1: 2950, 0: 10569} Nodes, 1382464 Edges\n",
      "\n",
      "-------------------\n",
      "STARTING EVALUATION\n",
      "-------------------\n",
      "\n",
      "Dataset - Overall: REC 58.03 PRE 44.85 MF1 67.05 AUC 77.32 TP 1712 FP 2105 TN 8464 FN 1238 | 13519 {1: 2950, 0: 10569}\n",
      "Dataset - Train: REC 68.56 PRE 53.40 MF1 71.02 AUC 80.59 TP 966 FP 843 TN 2929 FN 443 | 5181 {0: 3772, 1: 1409}\n",
      "Dataset - Val: REC 58.62 PRE 43.28 MF1 63.07 AUC 70.92 TP 551 FP 722 TN 1792 FN 389 | 3454 {1: 940, 0: 2514}\n",
      "Dataset - Test: REC 32.45 PRE 26.53 MF1 58.99 AUC 72.55 TP 195 FP 540 TN 3743 FN 406 | 4884 {0: 4283, 1: 601}\n",
      "PREDICTION STATUS - {'1-True': 2544, '0-True': 6826, '0-False': 3743, '1-False': 406}\n",
      "    >> 406 positive nodes left unpredicted...\n",
      "    >> 207 OG positive nodes left unpredicted...\n",
      "-----------------------\n",
      "PREDICTION RESULT - ROUNDS\n",
      "Dataset - Round 0: REC 62.66 PRE 44.92 MF1 67.66 AUC 77.86 TP 809 FP 992 TN 3596 FN 482 | 5879 {0: 4588, 1: 1291}\n",
      "Dataset - Round 1: REC 50.78 PRE 43.05 MF1 64.97 AUC 71.32 TP 65 FP 86 TN 373 FN 63 | 587 {1: 128, 0: 459}\n",
      "Dataset - Round 2: REC 39.06 PRE 32.26 MF1 57.40 AUC 69.20 TP 50 FP 105 TN 354 FN 78 | 587 {1: 128, 0: 459}\n",
      "Dataset - Round 3: REC 25.78 PRE 26.83 MF1 53.13 AUC 64.18 TP 33 FP 90 TN 369 FN 95 | 587 {1: 128, 0: 459}\n",
      "-----------------------\n",
      "PREDICTION RESULT - SEEDS\n",
      "Seeds - Current: REC 34.38 PRE 31.88 MF1 56.74 AUC 63.81 TP 44 FP 94 TN 365 FN 84 | 587 {1: 128, 0: 459}\n",
      "Seeds - Prev: REC 0.00 PRE 0.00 MF1 39.92 AUC 63.81 TP 0 FP 69 TN 390 FN 128 | 587 {1: 128, 0: 459}\n",
      "Selecting data as budgeted ground truth for next round...\n",
      "  > Starting round 4...\n",
      "\n",
      "+++++++++++++++++\n",
      "STARTING ROUND 4!\n",
      "+++++++++++++++++\n",
      "        \n",
      "ASSIGNED GRAPH FOR TRAINING: 13519 {1: 2950, 0: 10569} Nodes, 1382464 Edges\n",
      "\n",
      ">> Splitting to train-validation set...\n",
      "Alotting train-val-test split for round 4\n",
      "Initial pool: 5879, Prediction pool: 12367, Budget pool: 0, Full pool: 9370\n",
      "Full graph size: 13519, Training: 5622, Val: 3748, Test: 13519\n",
      "    >> INITIAL DATA SPLIT: 5622 train rows ({0: 4096, 1: 1526}) | 3748 val rows ({1: 1018, 0: 2730}) | 4149 test rows ({0: 3743, 1: 406})\n",
      "    >> AUGMENTED DATA SPLIT: 5622 train rows ({0: 4096, 1: 1526}) | 3748 val rows ({1: 1018, 0: 2730}) | 4149 test rows ({0: 3743, 1: 406})\n",
      "    >> Updated cross-entropy weight to 2.6841415465268676...\n",
      "\n",
      "-----------------------\n",
      "STARTING MODEL TRAININGps_l\n",
      "-----------------------\n",
      "\n",
      "TRAINING NODE EMBEDDER\n",
      "Detailed Loss: recon 0.19376162-0.09688081 | disen 0.32739317-0.16369659 | temporal 0.63374549-0.00000000 | total 0.26057738\n",
      "Epoch 1, loss: 0.26057738-(best 0.26057738)\n",
      "\n",
      "Detailed Loss: recon 0.24752989-0.12376495 | disen 0.33110148-0.16555074 | temporal 0.63441885-0.00000000 | total 0.28931570\n",
      "Epoch 2, loss: 0.28931570-(best 0.26057738)\n",
      "\n",
      "Detailed Loss: recon 0.21614301-0.10807151 | disen 0.33274764-0.16637382 | temporal 0.63395947-0.00000000 | total 0.27444533\n",
      "Epoch 3, loss: 0.27444533-(best 0.26057738)\n",
      "\n",
      "Detailed Loss: recon 0.21820363-0.10910182 | disen 0.32846135-0.16423067 | temporal 0.63367814-0.00000000 | total 0.27333248\n",
      "Epoch 4, loss: 0.27333248-(best 0.26057738)\n",
      "\n",
      "Detailed Loss: recon 0.20464545-0.10232273 | disen 0.32719213-0.16359606 | temporal 0.63387835-0.00000000 | total 0.26591879\n",
      "Epoch 5, loss: 0.26591879-(best 0.26057738)\n",
      "\n",
      "Detailed Loss: recon 0.20187461-0.10093731 | disen 0.33094740-0.16547370 | temporal 0.63390768-0.00000000 | total 0.26641101\n",
      "Epoch 6, loss: 0.26641101-(best 0.26057738)\n",
      "\n",
      "Detailed Loss: recon 0.19846308-0.09923154 | disen 0.33311278-0.16655639 | temporal 0.63386261-0.00000000 | total 0.26578793\n",
      "Epoch 7, loss: 0.26578793-(best 0.26057738)\n",
      "\n",
      "Detailed Loss: recon 0.19863169-0.09931584 | disen 0.33285719-0.16642860 | temporal 0.63374293-0.00000000 | total 0.26574445\n",
      "Epoch 8, loss: 0.26574445-(best 0.26057738)\n",
      "\n",
      "Detailed Loss: recon 0.19947177-0.09973589 | disen 0.33297551-0.16648775 | temporal 0.63375759-0.00000000 | total 0.26622364\n",
      "Epoch 9, loss: 0.26622364-(best 0.26057738)\n",
      "\n",
      "Detailed Loss: recon 0.19490707-0.09745353 | disen 0.33247435-0.16623718 | temporal 0.63392770-0.00000000 | total 0.26369071\n",
      "Epoch 10, loss: 0.26369071-(best 0.26057738)\n",
      "\n",
      "Detailed Loss: recon 0.19716029-0.09858014 | disen 0.32982069-0.16491035 | temporal 0.63403940-0.00000000 | total 0.26349050\n",
      "Epoch 11, loss: 0.26349050-(best 0.26057738)\n",
      "\n",
      "Detailed Loss: recon 0.19863918-0.09931959 | disen 0.32882196-0.16441098 | temporal 0.63412547-0.00000000 | total 0.26373059\n",
      "Epoch 12, loss: 0.26373059-(best 0.26057738)\n",
      "\n",
      "Detailed Loss: recon 0.19589904-0.09794952 | disen 0.32761431-0.16380715 | temporal 0.63413852-0.00000000 | total 0.26175666\n",
      "Epoch 13, loss: 0.26175666-(best 0.26057738)\n",
      "\n",
      "Detailed Loss: recon 0.19738689-0.09869345 | disen 0.32734823-0.16367412 | temporal 0.63413411-0.00000000 | total 0.26236755\n",
      "Epoch 14, loss: 0.26236755-(best 0.26057738)\n",
      "\n",
      "Detailed Loss: recon 0.19698152-0.09849076 | disen 0.32523352-0.16261676 | temporal 0.63412589-0.00000000 | total 0.26110750\n",
      "Epoch 15, loss: 0.26110750-(best 0.26057738)\n",
      "\n",
      "Detailed Loss: recon 0.19594029-0.09797014 | disen 0.32333183-0.16166592 | temporal 0.63418806-0.00000000 | total 0.25963604\n",
      "Epoch 16, loss: 0.25963604-(best 0.25963604)\n",
      "\n",
      "Detailed Loss: recon 0.19566445-0.09783223 | disen 0.32281464-0.16140732 | temporal 0.63418871-0.00000000 | total 0.25923955\n",
      "Epoch 17, loss: 0.25923955-(best 0.25923955)\n",
      "\n",
      "Detailed Loss: recon 0.19502799-0.09751400 | disen 0.32316595-0.16158298 | temporal 0.63418108-0.00000000 | total 0.25909698\n",
      "Epoch 18, loss: 0.25909698-(best 0.25909698)\n",
      "\n",
      "Detailed Loss: recon 0.19512670-0.09756335 | disen 0.32260948-0.16130474 | temporal 0.63407153-0.00000000 | total 0.25886810\n",
      "Epoch 19, loss: 0.25886810-(best 0.25886810)\n",
      "\n",
      "Detailed Loss: recon 0.19384174-0.09692087 | disen 0.32178283-0.16089141 | temporal 0.63401568-0.00000000 | total 0.25781229\n",
      "Epoch 20, loss: 0.25781229-(best 0.25781229)\n",
      "\n",
      "Detailed Loss: recon 0.19362359-0.09681179 | disen 0.31952620-0.15976310 | temporal 0.63396376-0.00000000 | total 0.25657490\n",
      "Epoch 21, loss: 0.25657490-(best 0.25657490)\n",
      "\n",
      "Detailed Loss: recon 0.19320127-0.09660064 | disen 0.31896031-0.15948015 | temporal 0.63393456-0.00000000 | total 0.25608081\n",
      "Epoch 22, loss: 0.25608081-(best 0.25608081)\n",
      "\n",
      "Detailed Loss: recon 0.19292369-0.09646185 | disen 0.31956768-0.15978384 | temporal 0.63390732-0.00000000 | total 0.25624567\n",
      "Epoch 23, loss: 0.25624567-(best 0.25608081)\n",
      "\n",
      "Detailed Loss: recon 0.19218592-0.09609296 | disen 0.31711763-0.15855882 | temporal 0.63385481-0.00000000 | total 0.25465178\n",
      "Epoch 24, loss: 0.25465178-(best 0.25465178)\n",
      "\n",
      "Detailed Loss: recon 0.19246995-0.09623498 | disen 0.31745267-0.15872633 | temporal 0.63386929-0.00000000 | total 0.25496131\n",
      "Epoch 25, loss: 0.25496131-(best 0.25465178)\n",
      "\n",
      "Detailed Loss: recon 0.19227813-0.09613907 | disen 0.31706005-0.15853003 | temporal 0.63385069-0.00000000 | total 0.25466910\n",
      "Epoch 26, loss: 0.25466910-(best 0.25465178)\n",
      "\n",
      "Detailed Loss: recon 0.19087183-0.09543592 | disen 0.31725013-0.15862507 | temporal 0.63380545-0.00000000 | total 0.25406098\n",
      "Epoch 27, loss: 0.25406098-(best 0.25406098)\n",
      "\n",
      "Detailed Loss: recon 0.19180036-0.09590018 | disen 0.31630033-0.15815017 | temporal 0.63372326-0.00000000 | total 0.25405034\n",
      "Epoch 28, loss: 0.25405034-(best 0.25405034)\n",
      "\n",
      "Detailed Loss: recon 0.19144943-0.09572472 | disen 0.31548297-0.15774149 | temporal 0.63370663-0.00000000 | total 0.25346619\n",
      "Epoch 29, loss: 0.25346619-(best 0.25346619)\n",
      "\n",
      "Detailed Loss: recon 0.19122939-0.09561469 | disen 0.31349844-0.15674922 | temporal 0.63370150-0.00000000 | total 0.25236392\n",
      "Epoch 30, loss: 0.25236392-(best 0.25236392)\n",
      "\n",
      "Detailed Loss: recon 0.19129071-0.09564535 | disen 0.31298047-0.15649024 | temporal 0.63366139-0.00000000 | total 0.25213557\n",
      "Epoch 31, loss: 0.25213557-(best 0.25213557)\n",
      "\n",
      "Detailed Loss: recon 0.18986619-0.09493309 | disen 0.31421930-0.15710965 | temporal 0.63365173-0.00000000 | total 0.25204274\n",
      "Epoch 32, loss: 0.25204274-(best 0.25204274)\n",
      "\n",
      "Detailed Loss: recon 0.18886927-0.09443463 | disen 0.31233937-0.15616968 | temporal 0.63364589-0.00000000 | total 0.25060433\n",
      "Epoch 33, loss: 0.25060433-(best 0.25060433)\n",
      "\n",
      "Detailed Loss: recon 0.18951970-0.09475985 | disen 0.31249362-0.15624681 | temporal 0.63367772-0.00000000 | total 0.25100666\n",
      "Epoch 34, loss: 0.25100666-(best 0.25060433)\n",
      "\n",
      "Detailed Loss: recon 0.18967998-0.09483999 | disen 0.31209296-0.15604648 | temporal 0.63365340-0.00000000 | total 0.25088647\n",
      "Epoch 35, loss: 0.25088647-(best 0.25060433)\n",
      "\n",
      "Detailed Loss: recon 0.19080248-0.09540124 | disen 0.31198162-0.15599081 | temporal 0.63370961-0.00000000 | total 0.25139207\n",
      "Epoch 36, loss: 0.25139207-(best 0.25060433)\n",
      "\n",
      "Detailed Loss: recon 0.18942764-0.09471382 | disen 0.31050611-0.15525305 | temporal 0.63368922-0.00000000 | total 0.24996687\n",
      "Epoch 37, loss: 0.24996687-(best 0.24996687)\n",
      "\n",
      "Detailed Loss: recon 0.18877640-0.09438820 | disen 0.31027067-0.15513533 | temporal 0.63370687-0.00000000 | total 0.24952354\n",
      "Epoch 38, loss: 0.24952354-(best 0.24952354)\n",
      "\n",
      "Detailed Loss: recon 0.18922892-0.09461446 | disen 0.30977869-0.15488935 | temporal 0.63371843-0.00000000 | total 0.24950381\n",
      "Epoch 39, loss: 0.24950381-(best 0.24950381)\n",
      "\n",
      "Detailed Loss: recon 0.18958750-0.09479375 | disen 0.30857497-0.15428749 | temporal 0.63375783-0.00000000 | total 0.24908124\n",
      "Epoch 40, loss: 0.24908124-(best 0.24908124)\n",
      "\n",
      "Detailed Loss: recon 0.18958938-0.09479469 | disen 0.30941248-0.15470624 | temporal 0.63375276-0.00000000 | total 0.24950093\n",
      "Epoch 41, loss: 0.24950093-(best 0.24908124)\n",
      "\n",
      "Detailed Loss: recon 0.18976939-0.09488469 | disen 0.30774051-0.15387025 | temporal 0.63376260-0.00000000 | total 0.24875495\n",
      "Epoch 42, loss: 0.24875495-(best 0.24875495)\n",
      "\n",
      "Detailed Loss: recon 0.19014713-0.09507357 | disen 0.30797482-0.15398741 | temporal 0.63372070-0.00000000 | total 0.24906097\n",
      "Epoch 43, loss: 0.24906097-(best 0.24875495)\n",
      "\n",
      "Detailed Loss: recon 0.18959156-0.09479578 | disen 0.30749619-0.15374810 | temporal 0.63369054-0.00000000 | total 0.24854387\n",
      "Epoch 44, loss: 0.24854387-(best 0.24854387)\n",
      "\n",
      "Detailed Loss: recon 0.19086954-0.09543477 | disen 0.30710483-0.15355241 | temporal 0.63365829-0.00000000 | total 0.24898718\n",
      "Epoch 45, loss: 0.24898718-(best 0.24854387)\n",
      "\n",
      "Detailed Loss: recon 0.18879724-0.09439862 | disen 0.30678338-0.15339169 | temporal 0.63362926-0.00000000 | total 0.24779031\n",
      "Epoch 46, loss: 0.24779031-(best 0.24779031)\n",
      "\n",
      "Detailed Loss: recon 0.18840528-0.09420264 | disen 0.30646020-0.15323010 | temporal 0.63360184-0.00000000 | total 0.24743274\n",
      "Epoch 47, loss: 0.24743274-(best 0.24743274)\n",
      "\n",
      "Detailed Loss: recon 0.18927801-0.09463900 | disen 0.30557609-0.15278804 | temporal 0.63359743-0.00000000 | total 0.24742705\n",
      "Epoch 48, loss: 0.24742705-(best 0.24742705)\n",
      "\n",
      "Detailed Loss: recon 0.18827057-0.09413528 | disen 0.30604249-0.15302125 | temporal 0.63360125-0.00000000 | total 0.24715653\n",
      "Epoch 49, loss: 0.24715653-(best 0.24715653)\n",
      "\n",
      "Detailed Loss: recon 0.18849331-0.09424666 | disen 0.30487967-0.15243983 | temporal 0.63357490-0.00000000 | total 0.24668649\n",
      "Epoch 50, loss: 0.24668649-(best 0.24668649)\n",
      "\n",
      ">> Reached final epoch. Loading best model...\n",
      "TRAINING XGB PREDICTOR\n",
      ">> Ending training!\n",
      "\n",
      "-----------------------\n",
      "TIME COST: 7.770445823669434 s\n",
      "Best Val: REC 61.79 PRE 45.25 MF1 64.82 AUC 72.46 TP 629 FP 761 TN 1969 FN 389\n",
      "-----------------------\n",
      "Generating additional positive data from adversary...\n",
      "Generating additional negative data by duplicating random nodes...\n",
      "\n",
      "ASSIGNED GRAPH FOR EVALUATION: 14106 {1: 3078, 0: 11028} Nodes, 1524112 Edges\n",
      "\n",
      "-------------------\n",
      "STARTING EVALUATION\n",
      "-------------------\n",
      "\n",
      "Dataset - Overall: REC 50.29 PRE 45.79 MF1 66.24 AUC 74.96 TP 1548 FP 1833 TN 9195 FN 1530 | 14106 {1: 3078, 0: 11028}\n",
      "Dataset - Train: REC 60.09 PRE 51.49 MF1 68.45 AUC 76.15 TP 917 FP 864 TN 3232 FN 609 | 5622 {0: 4096, 1: 1526}\n",
      "Dataset - Val: REC 49.90 PRE 45.24 MF1 63.22 AUC 70.12 TP 508 FP 615 TN 2115 FN 510 | 3748 {1: 1018, 0: 2730}\n",
      "Dataset - Test: REC 23.03 PRE 25.79 MF1 57.65 AUC 69.61 TP 123 FP 354 TN 3848 FN 411 | 4736 {0: 4202, 1: 534}\n",
      "PREDICTION STATUS - {'1-True': 2667, '0-True': 7180, '0-False': 3848, '1-False': 411}\n",
      "    >> 411 positive nodes left unpredicted...\n",
      "    >> 159 OG positive nodes left unpredicted...\n",
      "-----------------------\n",
      "PREDICTION RESULT - ROUNDS\n",
      "Dataset - Round 0: REC 55.15 PRE 47.69 MF1 68.00 AUC 77.83 TP 712 FP 781 TN 3807 FN 579 | 5879 {0: 4588, 1: 1291}\n",
      "Dataset - Round 1: REC 47.66 PRE 48.03 MF1 66.69 AUC 70.50 TP 61 FP 66 TN 393 FN 67 | 587 {1: 128, 0: 459}\n",
      "Dataset - Round 2: REC 37.50 PRE 41.74 MF1 61.86 AUC 70.27 TP 48 FP 67 TN 392 FN 80 | 587 {1: 128, 0: 459}\n",
      "Dataset - Round 3: REC 24.22 PRE 34.83 MF1 56.19 AUC 64.33 TP 31 FP 58 TN 401 FN 97 | 587 {1: 128, 0: 459}\n",
      "Dataset - Round 4: REC 23.44 PRE 32.61 MF1 55.25 AUC 61.96 TP 30 FP 62 TN 397 FN 98 | 587 {1: 128, 0: 459}\n",
      "-----------------------\n",
      "PREDICTION RESULT - SEEDS\n",
      "Seeds - Current: REC 21.88 PRE 28.28 MF1 53.31 AUC 61.22 TP 28 FP 71 TN 388 FN 100 | 587 {1: 128, 0: 459}\n",
      "Seeds - Prev: REC 0.00 PRE 0.00 MF1 39.42 AUC 61.22 TP 0 FP 77 TN 382 FN 128 | 587 {1: 128, 0: 459}\n",
      "Selecting data as budgeted ground truth for next round...\n",
      "The file ../checkpoint/working_model_file_1756868619.9230094_epoch.py does not exist\n",
      "The file ../checkpoint/working_model_file_1756868619.9230094_round-M.json does not exist\n",
      "The file ../checkpoint/working_model_file_1756868619.9230094_round-T.json does not exist\n",
      "The file ../checkpoint/working_model_file_1756868619.9230094_round.pt does not exist\n",
      "Experiment ended, experienced 0 failures\n",
      "Elapsed experiment time 37.23277187s\n",
      "================\n",
      "++++++++++++++++\n",
      "TRIAL NUMBER 0\n",
      "++++++++++++++++\n",
      "================\n",
      "  > Rereading graph data...\n",
      "  > Initializing multiround object...\n",
      "11758 1049758\n",
      "\n",
      "=========================\n",
      "STARTING ADVER EXPERIMENT\n",
      "=========================\n",
      "MAIN CONFIG: [('device', 'cuda:0'), ('exp_type', 'ADVER'), ('full_oracle', False), ('node_expiration_time', 1), ('round_budget_neg', 0), ('round_budget_pos', 0), ('round_new_neg', 459), ('round_new_pos', 128), ('round_num', 5), ('save_df', True), ('save_embedding', False), ('save_temp_sne', False), ('setting_type', 'INDUCTIVE'), ('task_type', 'NODE'), ('verbose', 4)]\n",
      "MODEL CONFIG: [('act_name', 'LeakyReLU'), ('addon_internal_agg', 'OR'), ('addon_name', 'NONE'), ('addon_perc', 0.05), ('addon_round_window', 7), ('alpha', 1), ('att_heads', 2), ('beta', 1), ('boost_agg_backbone', <class 'models.benchmarks_supervised.booster.GIN_noparam'>), ('boost_metric', <function average_precision_score at 0x000001FC8CFC5620>), ('boost_predictor', <class 'xgboost.sklearn.XGBClassifier'>), ('device', 'cuda:0'), ('drop_rate', 0), ('dropout_rate', 0.1), ('early_stopping', 75), ('embed_type', 'temporal'), ('etypes', ['none']), ('gamma', 1), ('h_feats', 64), ('k', 1), ('lambda1', 1), ('lambda2', 1), ('lambda_', 1), ('loss_sample', True), ('loss_sample_ratio', 0.1), ('loss_type', 'ndist'), ('mlp_feats', 64), ('mlp_layers', 2), ('model_name', 'XGB-SP'), ('n_estimator', 500), ('norm_name', 'layer'), ('num_epoch', 100), ('num_layers', 2), ('num_round_epoch', 50), ('round_window', 7), ('temporal_agg', 'mean_final'), ('tloss_type', 'normal'), ('training_type', 'round'), ('verbose', 4)]\n",
      "STRAT CONFIG: [('augment_name', 'REAGE'), ('augment_neg_ratio', 0.09772070079945569), ('augment_pos_ratio', 0.02727929920054431), ('augment_prob', 0.5), ('augment_round_split', 8), ('device', 'cuda:0'), ('pseudo_name', 'NONE'), ('pseudo_new_pos_only', False), ('pseudo_skip_initial', False), ('verbose', 4)]\n",
      "TRAIN CONFIG: [('ce_weight', 2.6841415465268676), ('device', 'cuda:0'), ('dset_name', 'tolokers_bid'), ('early_stopping', 75), ('learning_rate', 0.01), ('loss', <function cross_entropy at 0x000001FC85EA3740>), ('loss_gamma', 5), ('num_epoch', 100), ('num_round_epoch', 50), ('optimizer', <class 'torch.optim.adam.Adam'>), ('random_state', 7777777), ('ratio_initial', 0.5), ('ratio_test', 0.2), ('ratio_train', 0.6), ('ratio_val', 0.2), ('round_reset_model', False), ('round_train_list', []), ('stuck_stopping', 50), ('stuck_threshold', 0), ('train_max_retry', 10), ('verbose', 4)]\n",
      "ADVER CONFIG: [('adver_choose_name', 'GREEDY'), ('adver_conn_coef', 0.1), ('adver_feat_coef', 1), ('adver_mod_name', 'REPLAY'), ('device', 'cuda:0'), ('verbose', 4)]\n",
      "\n",
      "INITIAL GRAPH: 11758 {1: 2566, 0: 9192} Nodes, 1049758 Edges\n",
      "=========================\n",
      "        \n",
      ">> Initializing graph...\n",
      "MAIN GRAPH: 11758 {1: 2566, 0: 9192} Nodes, 1049758 Edges\n",
      "\n",
      ">> Initializing experiment type-specific modules:\n",
      "  >> Initializing adversarial modules...\n",
      ">> Initializing task-specific modules:\n",
      ">> Initializing classifier model...\n",
      ">> Initializing other strategies...\n",
      "\n",
      "FINISH INITIALIZATION\n",
      "=====================\n",
      "        \n",
      "  > Starting round 0...\n",
      "\n",
      "+++++++++++++++++\n",
      "STARTING ROUND 0!\n",
      "+++++++++++++++++\n",
      "        \n",
      "ASSIGNED GRAPH FOR TRAINING: 5879 {1: 1321, 0: 4558} Nodes, 255201 Edges\n",
      "\n",
      ">> Splitting to train-validation set...\n",
      "Alotting train-val-test split for round 0\n",
      "Initial pool: 0, Prediction pool: 0, Budget pool: 0, Full pool: 5879\n",
      "Full graph size: 5879, Training: 3527, Val: 2352, Test: 5879\n",
      "    >> INITIAL DATA SPLIT: 3527 train rows ({0: 2734, 1: 793}) | 2352 val rows ({1: 528, 0: 1824}) | 0 test rows ({})\n",
      "    >> AUGMENTED DATA SPLIT: 3527 train rows ({0: 2734, 1: 793}) | 2352 val rows ({1: 528, 0: 1824}) | 0 test rows ({})\n",
      "    >> Updated cross-entropy weight to 3.4476670870113493...\n",
      "\n",
      "-----------------------\n",
      "STARTING MODEL TRAININGps_l\n",
      "-----------------------\n",
      "\n",
      "TRAINING NODE EMBEDDER\n",
      "Detailed Loss: recon 0.74248624-0.24749541 | disen 0.69445437-0.23148479 | temporal 0.64057928-0.21352643 | total 0.69250667\n",
      "Epoch 1, loss: 0.69250667-(best 0.69250667)\n",
      "\n",
      "Detailed Loss: recon 0.45093387-0.15031129 | disen 0.71846819-0.23948940 | temporal 0.65462273-0.21820758 | total 0.60800827\n",
      "Epoch 2, loss: 0.60800827-(best 0.60800827)\n",
      "\n",
      "Detailed Loss: recon 0.38507462-0.12835821 | disen 0.72210246-0.24070082 | temporal 0.66110730-0.22036910 | total 0.58942813\n",
      "Epoch 3, loss: 0.58942813-(best 0.58942813)\n",
      "\n",
      "Detailed Loss: recon 0.36385569-0.12128523 | disen 0.72151756-0.24050585 | temporal 0.66370350-0.22123450 | total 0.58302557\n",
      "Epoch 4, loss: 0.58302557-(best 0.58302557)\n",
      "\n",
      "Detailed Loss: recon 0.36324281-0.12108094 | disen 0.71732235-0.23910745 | temporal 0.66488236-0.22162745 | total 0.58181584\n",
      "Epoch 5, loss: 0.58181584-(best 0.58181584)\n",
      "\n",
      "Detailed Loss: recon 0.35408279-0.11802760 | disen 0.71219087-0.23739696 | temporal 0.66528147-0.22176049 | total 0.57718503\n",
      "Epoch 6, loss: 0.57718503-(best 0.57718503)\n",
      "\n",
      "Detailed Loss: recon 0.33139926-0.11046642 | disen 0.71449316-0.23816439 | temporal 0.66478294-0.22159431 | total 0.57022512\n",
      "Epoch 7, loss: 0.57022512-(best 0.57022512)\n",
      "\n",
      "Detailed Loss: recon 0.32097107-0.10699036 | disen 0.71179825-0.23726608 | temporal 0.66381091-0.22127030 | total 0.56552678\n",
      "Epoch 8, loss: 0.56552678-(best 0.56552678)\n",
      "\n",
      "Detailed Loss: recon 0.31109703-0.10369901 | disen 0.70948082-0.23649361 | temporal 0.66356117-0.22118706 | total 0.56137967\n",
      "Epoch 9, loss: 0.56137967-(best 0.56137967)\n",
      "\n",
      "Detailed Loss: recon 0.30929130-0.10309710 | disen 0.70472753-0.23490918 | temporal 0.66312015-0.22104005 | total 0.55904633\n",
      "Epoch 10, loss: 0.55904633-(best 0.55904633)\n",
      "\n",
      "Detailed Loss: recon 0.30556512-0.10185504 | disen 0.69523835-0.23174612 | temporal 0.66336030-0.22112010 | total 0.55472130\n",
      "Epoch 11, loss: 0.55472130-(best 0.55472130)\n",
      "\n",
      "Detailed Loss: recon 0.29984480-0.09994827 | disen 0.69348872-0.23116291 | temporal 0.66375810-0.22125270 | total 0.55236387\n",
      "Epoch 12, loss: 0.55236387-(best 0.55236387)\n",
      "\n",
      "Detailed Loss: recon 0.29312620-0.09770873 | disen 0.69006777-0.23002259 | temporal 0.66471803-0.22157268 | total 0.54930401\n",
      "Epoch 13, loss: 0.54930401-(best 0.54930401)\n",
      "\n",
      "Detailed Loss: recon 0.28333020-0.09444340 | disen 0.68378282-0.22792761 | temporal 0.66575587-0.22191862 | total 0.54428965\n",
      "Epoch 14, loss: 0.54428965-(best 0.54428965)\n",
      "\n",
      "Detailed Loss: recon 0.28123492-0.09374497 | disen 0.68242681-0.22747560 | temporal 0.66617227-0.22205742 | total 0.54327798\n",
      "Epoch 15, loss: 0.54327798-(best 0.54327798)\n",
      "\n",
      "Detailed Loss: recon 0.27633232-0.09211077 | disen 0.67329371-0.22443124 | temporal 0.66643703-0.22214568 | total 0.53868771\n",
      "Epoch 16, loss: 0.53868771-(best 0.53868771)\n",
      "\n",
      "Detailed Loss: recon 0.26895818-0.08965273 | disen 0.67269498-0.22423166 | temporal 0.66658479-0.22219493 | total 0.53607929\n",
      "Epoch 17, loss: 0.53607929-(best 0.53607929)\n",
      "\n",
      "Detailed Loss: recon 0.27163070-0.09054357 | disen 0.66240364-0.22080121 | temporal 0.66687727-0.22229242 | total 0.53363723\n",
      "Epoch 18, loss: 0.53363723-(best 0.53363723)\n",
      "\n",
      "Detailed Loss: recon 0.27125853-0.09041951 | disen 0.65960497-0.21986832 | temporal 0.66687965-0.22229322 | total 0.53258109\n",
      "Epoch 19, loss: 0.53258109-(best 0.53258109)\n",
      "\n",
      "Detailed Loss: recon 0.26963133-0.08987711 | disen 0.64818072-0.21606024 | temporal 0.66711617-0.22237206 | total 0.52830940\n",
      "Epoch 20, loss: 0.52830940-(best 0.52830940)\n",
      "\n",
      "Detailed Loss: recon 0.26397982-0.08799327 | disen 0.64881599-0.21627200 | temporal 0.66705614-0.22235205 | total 0.52661729\n",
      "Epoch 21, loss: 0.52661729-(best 0.52661729)\n",
      "\n",
      "Detailed Loss: recon 0.26237473-0.08745824 | disen 0.64274049-0.21424683 | temporal 0.66770369-0.22256790 | total 0.52427298\n",
      "Epoch 22, loss: 0.52427298-(best 0.52427298)\n",
      "\n",
      "Detailed Loss: recon 0.26014966-0.08671655 | disen 0.64054585-0.21351528 | temporal 0.66790420-0.22263473 | total 0.52286661\n",
      "Epoch 23, loss: 0.52286661-(best 0.52286661)\n",
      "\n",
      "Detailed Loss: recon 0.26120645-0.08706882 | disen 0.64047658-0.21349219 | temporal 0.66805565-0.22268522 | total 0.52324623\n",
      "Epoch 24, loss: 0.52324623-(best 0.52286661)\n",
      "\n",
      "Detailed Loss: recon 0.25507110-0.08502370 | disen 0.63356042-0.21118681 | temporal 0.66826218-0.22275406 | total 0.51896459\n",
      "Epoch 25, loss: 0.51896459-(best 0.51896459)\n",
      "\n",
      "Detailed Loss: recon 0.25916418-0.08638806 | disen 0.62149507-0.20716502 | temporal 0.66787505-0.22262502 | total 0.51617813\n",
      "Epoch 26, loss: 0.51617813-(best 0.51617813)\n",
      "\n",
      "Detailed Loss: recon 0.25872144-0.08624048 | disen 0.62353331-0.20784444 | temporal 0.66810220-0.22270073 | total 0.51678562\n",
      "Epoch 27, loss: 0.51678562-(best 0.51617813)\n",
      "\n",
      "Detailed Loss: recon 0.25731111-0.08577037 | disen 0.60793102-0.20264367 | temporal 0.66803646-0.22267882 | total 0.51109290\n",
      "Epoch 28, loss: 0.51109290-(best 0.51109290)\n",
      "\n",
      "Detailed Loss: recon 0.25299752-0.08433251 | disen 0.61745131-0.20581710 | temporal 0.66818398-0.22272799 | total 0.51287764\n",
      "Epoch 29, loss: 0.51287764-(best 0.51109290)\n",
      "\n",
      "Detailed Loss: recon 0.24969077-0.08323026 | disen 0.61086327-0.20362109 | temporal 0.66838735-0.22279578 | total 0.50964713\n",
      "Epoch 30, loss: 0.50964713-(best 0.50964713)\n",
      "\n",
      "Detailed Loss: recon 0.25066614-0.08355538 | disen 0.61508548-0.20502849 | temporal 0.66849893-0.22283298 | total 0.51141685\n",
      "Epoch 31, loss: 0.51141685-(best 0.50964713)\n",
      "\n",
      "Detailed Loss: recon 0.24942034-0.08314011 | disen 0.60793328-0.20264443 | temporal 0.66854966-0.22284989 | total 0.50863445\n",
      "Epoch 32, loss: 0.50863445-(best 0.50863445)\n",
      "\n",
      "Detailed Loss: recon 0.24752365-0.08250788 | disen 0.60801661-0.20267220 | temporal 0.66851318-0.22283773 | total 0.50801784\n",
      "Epoch 33, loss: 0.50801784-(best 0.50801784)\n",
      "\n",
      "Detailed Loss: recon 0.24666391-0.08222130 | disen 0.60007119-0.20002373 | temporal 0.66847301-0.22282434 | total 0.50506938\n",
      "Epoch 34, loss: 0.50506938-(best 0.50506938)\n",
      "\n",
      "Detailed Loss: recon 0.25608182-0.08536061 | disen 0.60332274-0.20110758 | temporal 0.66825873-0.22275291 | total 0.50922114\n",
      "Epoch 35, loss: 0.50922114-(best 0.50506938)\n",
      "\n",
      "Detailed Loss: recon 0.24378994-0.08126331 | disen 0.58492303-0.19497434 | temporal 0.66882336-0.22294112 | total 0.49917880\n",
      "Epoch 36, loss: 0.49917880-(best 0.49917880)\n",
      "\n",
      "Detailed Loss: recon 0.24640398-0.08213466 | disen 0.58457065-0.19485688 | temporal 0.66890013-0.22296671 | total 0.49995828\n",
      "Epoch 37, loss: 0.49995828-(best 0.49917880)\n",
      "\n",
      "Detailed Loss: recon 0.24826831-0.08275610 | disen 0.59232557-0.19744186 | temporal 0.66915351-0.22305117 | total 0.50324917\n",
      "Epoch 38, loss: 0.50324917-(best 0.49917880)\n",
      "\n",
      "Detailed Loss: recon 0.24345499-0.08115166 | disen 0.58114612-0.19371537 | temporal 0.66925615-0.22308538 | total 0.49795246\n",
      "Epoch 39, loss: 0.49795246-(best 0.49795246)\n",
      "\n",
      "Detailed Loss: recon 0.24741837-0.08247279 | disen 0.57682729-0.19227576 | temporal 0.66936755-0.22312252 | total 0.49787110\n",
      "Epoch 40, loss: 0.49787110-(best 0.49787110)\n",
      "\n",
      "Detailed Loss: recon 0.24853811-0.08284604 | disen 0.57532597-0.19177532 | temporal 0.66892135-0.22297378 | total 0.49759516\n",
      "Epoch 41, loss: 0.49759516-(best 0.49759516)\n",
      "\n",
      "Detailed Loss: recon 0.24844265-0.08281422 | disen 0.56490785-0.18830262 | temporal 0.66899633-0.22299878 | total 0.49411565\n",
      "Epoch 42, loss: 0.49411565-(best 0.49411565)\n",
      "\n",
      "Detailed Loss: recon 0.24118096-0.08039365 | disen 0.57390779-0.19130260 | temporal 0.66932219-0.22310740 | total 0.49480367\n",
      "Epoch 43, loss: 0.49480367-(best 0.49411565)\n",
      "\n",
      "Detailed Loss: recon 0.24279152-0.08093051 | disen 0.57604736-0.19201579 | temporal 0.66944867-0.22314956 | total 0.49609587\n",
      "Epoch 44, loss: 0.49609587-(best 0.49411565)\n",
      "\n",
      "Detailed Loss: recon 0.24252588-0.08084196 | disen 0.56471252-0.18823751 | temporal 0.66955072-0.22318357 | total 0.49226305\n",
      "Epoch 45, loss: 0.49226305-(best 0.49226305)\n",
      "\n",
      "Detailed Loss: recon 0.24529426-0.08176475 | disen 0.57672477-0.19224159 | temporal 0.66956472-0.22318824 | total 0.49719462\n",
      "Epoch 46, loss: 0.49719462-(best 0.49226305)\n",
      "\n",
      "Detailed Loss: recon 0.24280754-0.08093585 | disen 0.55827481-0.18609160 | temporal 0.66939753-0.22313251 | total 0.49015996\n",
      "Epoch 47, loss: 0.49015996-(best 0.49015996)\n",
      "\n",
      "Detailed Loss: recon 0.24253844-0.08084615 | disen 0.55741799-0.18580600 | temporal 0.66924369-0.22308123 | total 0.48973340\n",
      "Epoch 48, loss: 0.48973340-(best 0.48973340)\n",
      "\n",
      "Detailed Loss: recon 0.24485771-0.08161924 | disen 0.55360138-0.18453379 | temporal 0.66933692-0.22311231 | total 0.48926535\n",
      "Epoch 49, loss: 0.48926535-(best 0.48926535)\n",
      "\n",
      "Detailed Loss: recon 0.24446532-0.08148844 | disen 0.55245018-0.18415006 | temporal 0.66935146-0.22311715 | total 0.48875570\n",
      "Epoch 50, loss: 0.48875570-(best 0.48875570)\n",
      "\n",
      "Detailed Loss: recon 0.24289656-0.08096552 | disen 0.55122375-0.18374125 | temporal 0.66947025-0.22315675 | total 0.48786354\n",
      "Epoch 51, loss: 0.48786354-(best 0.48786354)\n",
      "\n",
      "Detailed Loss: recon 0.24116716-0.08038905 | disen 0.54723787-0.18241262 | temporal 0.66936719-0.22312240 | total 0.48592407\n",
      "Epoch 52, loss: 0.48592407-(best 0.48592407)\n",
      "\n",
      "Detailed Loss: recon 0.24467424-0.08155808 | disen 0.54067314-0.18022438 | temporal 0.66936070-0.22312023 | total 0.48490271\n",
      "Epoch 53, loss: 0.48490271-(best 0.48490271)\n",
      "\n",
      "Detailed Loss: recon 0.24303848-0.08101283 | disen 0.54732215-0.18244072 | temporal 0.66927624-0.22309208 | total 0.48654562\n",
      "Epoch 54, loss: 0.48654562-(best 0.48490271)\n",
      "\n",
      "Detailed Loss: recon 0.24348986-0.08116329 | disen 0.53463477-0.17821159 | temporal 0.66931719-0.22310573 | total 0.48248062\n",
      "Epoch 55, loss: 0.48248062-(best 0.48248062)\n",
      "\n",
      "Detailed Loss: recon 0.24099439-0.08033146 | disen 0.54373133-0.18124378 | temporal 0.66927242-0.22309081 | total 0.48466605\n",
      "Epoch 56, loss: 0.48466605-(best 0.48248062)\n",
      "\n",
      "Detailed Loss: recon 0.24450216-0.08150072 | disen 0.54990327-0.18330109 | temporal 0.66916531-0.22305510 | total 0.48785692\n",
      "Epoch 57, loss: 0.48785692-(best 0.48248062)\n",
      "\n",
      "Detailed Loss: recon 0.24398196-0.08132732 | disen 0.53122389-0.17707463 | temporal 0.66888744-0.22296248 | total 0.48136443\n",
      "Epoch 58, loss: 0.48136443-(best 0.48136443)\n",
      "\n",
      "Detailed Loss: recon 0.24261276-0.08087092 | disen 0.52806282-0.17602094 | temporal 0.66858113-0.22286038 | total 0.47975224\n",
      "Epoch 59, loss: 0.47975224-(best 0.47975224)\n",
      "\n",
      "Detailed Loss: recon 0.24175559-0.08058520 | disen 0.53219104-0.17739701 | temporal 0.66924047-0.22308016 | total 0.48106235\n",
      "Epoch 60, loss: 0.48106235-(best 0.47975224)\n",
      "\n",
      "Detailed Loss: recon 0.24507760-0.08169253 | disen 0.52203941-0.17401314 | temporal 0.66916102-0.22305367 | total 0.47875935\n",
      "Epoch 61, loss: 0.47875935-(best 0.47875935)\n",
      "\n",
      "Detailed Loss: recon 0.24020663-0.08006888 | disen 0.52277708-0.17425903 | temporal 0.66911107-0.22303702 | total 0.47736496\n",
      "Epoch 62, loss: 0.47736496-(best 0.47736496)\n",
      "\n",
      "Detailed Loss: recon 0.24776667-0.08258889 | disen 0.51404911-0.17134970 | temporal 0.66834337-0.22278112 | total 0.47671974\n",
      "Epoch 63, loss: 0.47671974-(best 0.47671974)\n",
      "\n",
      "Detailed Loss: recon 0.24624038-0.08208013 | disen 0.50733697-0.16911232 | temporal 0.66841215-0.22280405 | total 0.47399652\n",
      "Epoch 64, loss: 0.47399652-(best 0.47399652)\n",
      "\n",
      "Detailed Loss: recon 0.24427342-0.08142447 | disen 0.51369011-0.17123004 | temporal 0.66883618-0.22294539 | total 0.47559991\n",
      "Epoch 65, loss: 0.47559991-(best 0.47399652)\n",
      "\n",
      "Detailed Loss: recon 0.24283569-0.08094523 | disen 0.50513434-0.16837811 | temporal 0.66855085-0.22285028 | total 0.47217363\n",
      "Epoch 66, loss: 0.47217363-(best 0.47217363)\n",
      "\n",
      "Detailed Loss: recon 0.24439788-0.08146596 | disen 0.51216125-0.17072042 | temporal 0.66848755-0.22282918 | total 0.47501558\n",
      "Epoch 67, loss: 0.47501558-(best 0.47217363)\n",
      "\n",
      "Detailed Loss: recon 0.24490628-0.08163543 | disen 0.49877417-0.16625806 | temporal 0.66787422-0.22262474 | total 0.47051823\n",
      "Epoch 68, loss: 0.47051823-(best 0.47051823)\n",
      "\n",
      "Detailed Loss: recon 0.24685638-0.08228546 | disen 0.49396735-0.16465578 | temporal 0.66806453-0.22268818 | total 0.46962944\n",
      "Epoch 69, loss: 0.46962944-(best 0.46962944)\n",
      "\n",
      "Detailed Loss: recon 0.24406135-0.08135378 | disen 0.49523544-0.16507848 | temporal 0.66810048-0.22270016 | total 0.46913242\n",
      "Epoch 70, loss: 0.46913242-(best 0.46913242)\n",
      "\n",
      "Detailed Loss: recon 0.25417110-0.08472370 | disen 0.48868632-0.16289544 | temporal 0.66737264-0.22245755 | total 0.47007671\n",
      "Epoch 71, loss: 0.47007671-(best 0.46913242)\n",
      "\n",
      "Detailed Loss: recon 0.25064984-0.08354995 | disen 0.48039621-0.16013207 | temporal 0.66753292-0.22251097 | total 0.46619302\n",
      "Epoch 72, loss: 0.46619302-(best 0.46619302)\n",
      "\n",
      "Detailed Loss: recon 0.24995384-0.08331795 | disen 0.47551674-0.15850558 | temporal 0.66707802-0.22235934 | total 0.46418288\n",
      "Epoch 73, loss: 0.46418288-(best 0.46418288)\n",
      "\n",
      "Detailed Loss: recon 0.25675353-0.08558451 | disen 0.47943538-0.15981179 | temporal 0.66619009-0.22206336 | total 0.46745968\n",
      "Epoch 74, loss: 0.46745968-(best 0.46418288)\n",
      "\n",
      "Detailed Loss: recon 0.25197965-0.08399322 | disen 0.47124904-0.15708301 | temporal 0.66638994-0.22212998 | total 0.46320623\n",
      "Epoch 75, loss: 0.46320623-(best 0.46320623)\n",
      "\n",
      "Detailed Loss: recon 0.24903959-0.08301320 | disen 0.47508126-0.15836042 | temporal 0.66736770-0.22245590 | total 0.46382952\n",
      "Epoch 76, loss: 0.46382952-(best 0.46320623)\n",
      "\n",
      "Detailed Loss: recon 0.25503740-0.08501247 | disen 0.46709424-0.15569808 | temporal 0.66714835-0.22238278 | total 0.46309334\n",
      "Epoch 77, loss: 0.46309334-(best 0.46309334)\n",
      "\n",
      "Detailed Loss: recon 0.24971217-0.08323739 | disen 0.46164352-0.15388117 | temporal 0.66643721-0.22214574 | total 0.45926431\n",
      "Epoch 78, loss: 0.45926431-(best 0.45926431)\n",
      "\n",
      "Detailed Loss: recon 0.25105923-0.08368641 | disen 0.46047485-0.15349162 | temporal 0.66592646-0.22197549 | total 0.45915353\n",
      "Epoch 79, loss: 0.45915353-(best 0.45915353)\n",
      "\n",
      "Detailed Loss: recon 0.25113901-0.08371300 | disen 0.45874500-0.15291500 | temporal 0.66596186-0.22198729 | total 0.45861530\n",
      "Epoch 80, loss: 0.45861530-(best 0.45861530)\n",
      "\n",
      "Detailed Loss: recon 0.25373408-0.08457803 | disen 0.45408195-0.15136065 | temporal 0.66579741-0.22193247 | total 0.45787117\n",
      "Epoch 81, loss: 0.45787117-(best 0.45787117)\n",
      "\n",
      "Detailed Loss: recon 0.24889562-0.08296521 | disen 0.46701801-0.15567267 | temporal 0.66668725-0.22222908 | total 0.46086699\n",
      "Epoch 82, loss: 0.46086699-(best 0.45787117)\n",
      "\n",
      "Detailed Loss: recon 0.24612412-0.08204137 | disen 0.45461231-0.15153744 | temporal 0.66633940-0.22211313 | total 0.45569196\n",
      "Epoch 83, loss: 0.45569196-(best 0.45569196)\n",
      "\n",
      "Detailed Loss: recon 0.24932650-0.08310883 | disen 0.45999557-0.15333186 | temporal 0.66560304-0.22186768 | total 0.45830837\n",
      "Epoch 84, loss: 0.45830837-(best 0.45569196)\n",
      "\n",
      "Detailed Loss: recon 0.25352708-0.08450903 | disen 0.44540137-0.14846712 | temporal 0.66464573-0.22154858 | total 0.45452476\n",
      "Epoch 85, loss: 0.45452476-(best 0.45452476)\n",
      "\n",
      "Detailed Loss: recon 0.25446403-0.08482134 | disen 0.44871855-0.14957285 | temporal 0.66457719-0.22152573 | total 0.45591992\n",
      "Epoch 86, loss: 0.45591992-(best 0.45452476)\n",
      "\n",
      "Detailed Loss: recon 0.25045767-0.08348589 | disen 0.44315040-0.14771680 | temporal 0.66445738-0.22148579 | total 0.45268849\n",
      "Epoch 87, loss: 0.45268849-(best 0.45268849)\n",
      "\n",
      "Detailed Loss: recon 0.25420076-0.08473359 | disen 0.44290996-0.14763665 | temporal 0.66361547-0.22120516 | total 0.45357540\n",
      "Epoch 88, loss: 0.45357540-(best 0.45268849)\n",
      "\n",
      "Detailed Loss: recon 0.25157478-0.08385826 | disen 0.44677436-0.14892479 | temporal 0.66382211-0.22127404 | total 0.45405710\n",
      "Epoch 89, loss: 0.45405710-(best 0.45268849)\n",
      "\n",
      "Detailed Loss: recon 0.25450891-0.08483630 | disen 0.44400287-0.14800096 | temporal 0.66320986-0.22106995 | total 0.45390722\n",
      "Epoch 90, loss: 0.45390722-(best 0.45268849)\n",
      "\n",
      "Detailed Loss: recon 0.25303763-0.08434588 | disen 0.43536949-0.14512316 | temporal 0.66282588-0.22094196 | total 0.45041102\n",
      "Epoch 91, loss: 0.45041102-(best 0.45041102)\n",
      "\n",
      "Detailed Loss: recon 0.25605637-0.08535212 | disen 0.43905419-0.14635140 | temporal 0.66314918-0.22104973 | total 0.45275325\n",
      "Epoch 92, loss: 0.45275325-(best 0.45041102)\n",
      "\n",
      "Detailed Loss: recon 0.25826344-0.08608781 | disen 0.42818362-0.14272787 | temporal 0.66259915-0.22086638 | total 0.44968209\n",
      "Epoch 93, loss: 0.44968209-(best 0.44968209)\n",
      "\n",
      "Detailed Loss: recon 0.26266944-0.08755648 | disen 0.41912228-0.13970743 | temporal 0.66163212-0.22054404 | total 0.44780797\n",
      "Epoch 94, loss: 0.44780797-(best 0.44780797)\n",
      "\n",
      "Detailed Loss: recon 0.25396383-0.08465461 | disen 0.43167645-0.14389215 | temporal 0.66294879-0.22098293 | total 0.44952971\n",
      "Epoch 95, loss: 0.44952971-(best 0.44780797)\n",
      "\n",
      "Detailed Loss: recon 0.25044829-0.08348276 | disen 0.43728191-0.14576064 | temporal 0.66301495-0.22100498 | total 0.45024839\n",
      "Epoch 96, loss: 0.45024839-(best 0.44780797)\n",
      "\n",
      "Detailed Loss: recon 0.26619479-0.08873160 | disen 0.41863847-0.13954616 | temporal 0.66066086-0.22022029 | total 0.44849807\n",
      "Epoch 97, loss: 0.44849807-(best 0.44780797)\n",
      "\n",
      "Detailed Loss: recon 0.25395116-0.08465039 | disen 0.41290027-0.13763342 | temporal 0.66186678-0.22062226 | total 0.44290608\n",
      "Epoch 98, loss: 0.44290608-(best 0.44290608)\n",
      "\n",
      "Detailed Loss: recon 0.25319803-0.08439934 | disen 0.41491789-0.13830596 | temporal 0.66121697-0.22040566 | total 0.44311097\n",
      "Epoch 99, loss: 0.44311097-(best 0.44290608)\n",
      "\n",
      "Detailed Loss: recon 0.26068753-0.08689584 | disen 0.41819799-0.13939933 | temporal 0.66064972-0.22021657 | total 0.44651175\n",
      "Epoch 100, loss: 0.44651175-(best 0.44290608)\n",
      "\n",
      ">> Reached final epoch. Loading best model...\n",
      "TRAINING XGB PREDICTOR\n",
      ">> Ending training!\n",
      "\n",
      "-----------------------\n",
      "TIME COST: 7.380789041519165 s\n",
      "Best Val: REC 56.63 PRE 49.59 MF1 68.98 AUC 80.15 TP 299 FP 304 TN 1520 FN 229\n",
      "-----------------------\n",
      "\n",
      "ASSIGNED GRAPH FOR EVALUATION: 11758 {1: 2566, 0: 9192} Nodes, 1049758 Edges\n",
      "\n",
      "-------------------\n",
      "STARTING EVALUATION\n",
      "-------------------\n",
      "\n",
      "Dataset - Overall: REC 60.13 PRE 47.99 MF1 69.09 AUC 81.83 TP 1543 FP 1672 TN 7520 FN 1023 | 11758 {1: 2566, 0: 9192}\n",
      "Dataset - Train: REC 71.12 PRE 52.08 MF1 72.84 AUC 85.79 TP 564 FP 519 TN 2215 FN 229 | 3527 {0: 2734, 1: 793}\n",
      "Dataset - Val: REC 61.55 PRE 45.45 MF1 67.58 AUC 80.10 TP 325 FP 390 TN 1434 FN 203 | 2352 {1: 528, 0: 1824}\n",
      "Dataset - Test: REC 52.53 PRE 46.15 MF1 67.13 AUC 80.04 TP 654 FP 763 TN 3871 FN 591 | 5879 {1: 1245, 0: 4634}\n",
      "PREDICTION STATUS - {'1-True': 1975, '0-True': 5321, '1-False': 591, '0-False': 3871}\n",
      "    >> 591 positive nodes left unpredicted...\n",
      "    >> 591 OG positive nodes left unpredicted...\n",
      "Selecting data as budgeted ground truth for next round...\n",
      "  > Starting round 1...\n",
      "\n",
      "+++++++++++++++++\n",
      "STARTING ROUND 1!\n",
      "+++++++++++++++++\n",
      "        \n",
      "ASSIGNED GRAPH FOR TRAINING: 11758 {1: 2566, 0: 9192} Nodes, 1049758 Edges\n",
      "\n",
      ">> Splitting to train-validation set...\n",
      "Alotting train-val-test split for round 1\n",
      "Initial pool: 5879, Prediction pool: 3215, Budget pool: 0, Full pool: 7296\n",
      "Full graph size: 11758, Training: 4377, Val: 2919, Test: 11758\n",
      "    >> INITIAL DATA SPLIT: 4377 train rows ({1: 1185, 0: 3192}) | 2919 val rows ({0: 2129, 1: 790}) | 4462 test rows ({1: 591, 0: 3871})\n",
      "    >> AUGMENTED DATA SPLIT: 4377 train rows ({1: 1185, 0: 3192}) | 2919 val rows ({0: 2129, 1: 790}) | 4462 test rows ({1: 591, 0: 3871})\n",
      "    >> Updated cross-entropy weight to 2.6936708860759495...\n",
      "\n",
      "-----------------------\n",
      "STARTING MODEL TRAININGps_l\n",
      "-----------------------\n",
      "\n",
      "TRAINING NODE EMBEDDER\n",
      "Detailed Loss: recon 0.25605035-0.08535012 | disen 0.40088469-0.13362823 | temporal 0.55775893-0.18591964 | total 0.40489799\n",
      "Epoch 1, loss: 0.40489799-(best 0.40489799)\n",
      "\n",
      "Detailed Loss: recon 0.30859846-0.10286615 | disen 0.34284443-0.11428148 | temporal 0.55845296-0.18615099 | total 0.40329862\n",
      "Epoch 2, loss: 0.40329862-(best 0.40329862)\n",
      "\n",
      "Detailed Loss: recon 0.50563335-0.16854445 | disen 0.37084538-0.12361513 | temporal 0.55655152-0.18551717 | total 0.47767675\n",
      "Epoch 3, loss: 0.47767675-(best 0.40329862)\n",
      "\n",
      "Detailed Loss: recon 0.28245649-0.09415216 | disen 0.35442901-0.11814300 | temporal 0.55852282-0.18617427 | total 0.39846945\n",
      "Epoch 4, loss: 0.39846945-(best 0.39846945)\n",
      "\n",
      "Detailed Loss: recon 0.31963333-0.10654444 | disen 0.39262867-0.13087622 | temporal 0.56069267-0.18689756 | total 0.42431825\n",
      "Epoch 5, loss: 0.42431825-(best 0.39846945)\n",
      "\n",
      "Detailed Loss: recon 0.30234808-0.10078269 | disen 0.40182376-0.13394125 | temporal 0.56104678-0.18701559 | total 0.42173955\n",
      "Epoch 6, loss: 0.42173955-(best 0.39846945)\n",
      "\n",
      "Detailed Loss: recon 0.25675666-0.08558555 | disen 0.39404112-0.13134704 | temporal 0.56078398-0.18692799 | total 0.40386060\n",
      "Epoch 7, loss: 0.40386060-(best 0.39846945)\n",
      "\n",
      "Detailed Loss: recon 0.25730461-0.08576820 | disen 0.37857366-0.12619122 | temporal 0.56024045-0.18674682 | total 0.39870626\n",
      "Epoch 8, loss: 0.39870626-(best 0.39846945)\n",
      "\n",
      "Detailed Loss: recon 0.26720366-0.08906789 | disen 0.37207431-0.12402477 | temporal 0.55976951-0.18658984 | total 0.39968249\n",
      "Epoch 9, loss: 0.39968249-(best 0.39846945)\n",
      "\n",
      "Detailed Loss: recon 0.28337499-0.09445833 | disen 0.36975217-0.12325072 | temporal 0.55965745-0.18655248 | total 0.40426156\n",
      "Epoch 10, loss: 0.40426156-(best 0.39846945)\n",
      "\n",
      "Detailed Loss: recon 0.27075687-0.09025229 | disen 0.37045628-0.12348543 | temporal 0.55982298-0.18660766 | total 0.40034539\n",
      "Epoch 11, loss: 0.40034539-(best 0.39846945)\n",
      "\n",
      "Detailed Loss: recon 0.25262675-0.08420892 | disen 0.37323135-0.12441045 | temporal 0.56031632-0.18677211 | total 0.39539146\n",
      "Epoch 12, loss: 0.39539146-(best 0.39539146)\n",
      "\n",
      "Detailed Loss: recon 0.24788904-0.08262968 | disen 0.37223244-0.12407748 | temporal 0.56055343-0.18685114 | total 0.39355832\n",
      "Epoch 13, loss: 0.39355832-(best 0.39355832)\n",
      "\n",
      "Detailed Loss: recon 0.24700558-0.08233519 | disen 0.37209213-0.12403071 | temporal 0.56080055-0.18693352 | total 0.39329943\n",
      "Epoch 14, loss: 0.39329943-(best 0.39329943)\n",
      "\n",
      "Detailed Loss: recon 0.24738109-0.08246036 | disen 0.36955500-0.12318500 | temporal 0.56094939-0.18698313 | total 0.39262849\n",
      "Epoch 15, loss: 0.39262849-(best 0.39262849)\n",
      "\n",
      "Detailed Loss: recon 0.24514055-0.08171352 | disen 0.36733657-0.12244552 | temporal 0.56093448-0.18697816 | total 0.39113721\n",
      "Epoch 16, loss: 0.39113721-(best 0.39113721)\n",
      "\n",
      "Detailed Loss: recon 0.24755716-0.08251905 | disen 0.36616868-0.12205623 | temporal 0.56098866-0.18699622 | total 0.39157152\n",
      "Epoch 17, loss: 0.39157152-(best 0.39113721)\n",
      "\n",
      "Detailed Loss: recon 0.24516092-0.08172031 | disen 0.36137253-0.12045751 | temporal 0.56094700-0.18698233 | total 0.38916016\n",
      "Epoch 18, loss: 0.38916016-(best 0.38916016)\n",
      "\n",
      "Detailed Loss: recon 0.24636662-0.08212221 | disen 0.35777736-0.11925912 | temporal 0.56078660-0.18692887 | total 0.38831019\n",
      "Epoch 19, loss: 0.38831019-(best 0.38831019)\n",
      "\n",
      "Detailed Loss: recon 0.23795037-0.07931679 | disen 0.35598022-0.11866007 | temporal 0.56059951-0.18686650 | total 0.38484338\n",
      "Epoch 20, loss: 0.38484338-(best 0.38484338)\n",
      "\n",
      "Detailed Loss: recon 0.24326602-0.08108867 | disen 0.35433966-0.11811322 | temporal 0.56058210-0.18686070 | total 0.38606262\n",
      "Epoch 21, loss: 0.38606262-(best 0.38484338)\n",
      "\n",
      "Detailed Loss: recon 0.24150074-0.08050025 | disen 0.35466504-0.11822168 | temporal 0.56044012-0.18681337 | total 0.38553530\n",
      "Epoch 22, loss: 0.38553530-(best 0.38484338)\n",
      "\n",
      "Detailed Loss: recon 0.24007015-0.08002338 | disen 0.34990442-0.11663481 | temporal 0.56035084-0.18678361 | total 0.38344181\n",
      "Epoch 23, loss: 0.38344181-(best 0.38344181)\n",
      "\n",
      "Detailed Loss: recon 0.23893842-0.07964614 | disen 0.34813809-0.11604603 | temporal 0.56030083-0.18676694 | total 0.38245913\n",
      "Epoch 24, loss: 0.38245913-(best 0.38245913)\n",
      "\n",
      "Detailed Loss: recon 0.23947874-0.07982625 | disen 0.35023093-0.11674364 | temporal 0.56033260-0.18677753 | total 0.38334742\n",
      "Epoch 25, loss: 0.38334742-(best 0.38245913)\n",
      "\n",
      "Detailed Loss: recon 0.24072951-0.08024317 | disen 0.34794706-0.11598235 | temporal 0.56025404-0.18675135 | total 0.38297689\n",
      "Epoch 26, loss: 0.38297689-(best 0.38245913)\n",
      "\n",
      "Detailed Loss: recon 0.23902071-0.07967357 | disen 0.34270006-0.11423335 | temporal 0.56015700-0.18671900 | total 0.38062593\n",
      "Epoch 27, loss: 0.38062593-(best 0.38062593)\n",
      "\n",
      "Detailed Loss: recon 0.23995212-0.07998404 | disen 0.34105045-0.11368348 | temporal 0.56013852-0.18671284 | total 0.38038039\n",
      "Epoch 28, loss: 0.38038039-(best 0.38038039)\n",
      "\n",
      "Detailed Loss: recon 0.23798290-0.07932763 | disen 0.33822131-0.11274044 | temporal 0.56007743-0.18669248 | total 0.37876055\n",
      "Epoch 29, loss: 0.37876055-(best 0.37876055)\n",
      "\n",
      "Detailed Loss: recon 0.24082026-0.08027342 | disen 0.33805496-0.11268499 | temporal 0.56012505-0.18670835 | total 0.37966678\n",
      "Epoch 30, loss: 0.37966678-(best 0.37876055)\n",
      "\n",
      "Detailed Loss: recon 0.23796342-0.07932114 | disen 0.33671254-0.11223751 | temporal 0.56008661-0.18669554 | total 0.37825420\n",
      "Epoch 31, loss: 0.37825420-(best 0.37825420)\n",
      "\n",
      "Detailed Loss: recon 0.23799591-0.07933197 | disen 0.33534312-0.11178104 | temporal 0.56004900-0.18668300 | total 0.37779602\n",
      "Epoch 32, loss: 0.37779602-(best 0.37779602)\n",
      "\n",
      "Detailed Loss: recon 0.23664388-0.07888129 | disen 0.33644605-0.11214868 | temporal 0.56013745-0.18671248 | total 0.37774247\n",
      "Epoch 33, loss: 0.37774247-(best 0.37774247)\n",
      "\n",
      "Detailed Loss: recon 0.23506521-0.07835507 | disen 0.33387733-0.11129244 | temporal 0.55997622-0.18665874 | total 0.37630627\n",
      "Epoch 34, loss: 0.37630627-(best 0.37630627)\n",
      "\n",
      "Detailed Loss: recon 0.23656985-0.07885662 | disen 0.33148694-0.11049565 | temporal 0.55992991-0.18664330 | total 0.37599558\n",
      "Epoch 35, loss: 0.37599558-(best 0.37599558)\n",
      "\n",
      "Detailed Loss: recon 0.23669049-0.07889683 | disen 0.33069241-0.11023080 | temporal 0.55992740-0.18664247 | total 0.37577009\n",
      "Epoch 36, loss: 0.37577009-(best 0.37577009)\n",
      "\n",
      "Detailed Loss: recon 0.23453054-0.07817685 | disen 0.32961780-0.10987260 | temporal 0.55984491-0.18661497 | total 0.37466443\n",
      "Epoch 37, loss: 0.37466443-(best 0.37466443)\n",
      "\n",
      "Detailed Loss: recon 0.23863629-0.07954543 | disen 0.32710153-0.10903384 | temporal 0.55979282-0.18659761 | total 0.37517691\n",
      "Epoch 38, loss: 0.37517691-(best 0.37466443)\n",
      "\n",
      "Detailed Loss: recon 0.23408204-0.07802735 | disen 0.32533592-0.10844531 | temporal 0.55979633-0.18659878 | total 0.37307143\n",
      "Epoch 39, loss: 0.37307143-(best 0.37307143)\n",
      "\n",
      "Detailed Loss: recon 0.23459910-0.07819970 | disen 0.32431972-0.10810657 | temporal 0.55972695-0.18657565 | total 0.37288192\n",
      "Epoch 40, loss: 0.37288192-(best 0.37288192)\n",
      "\n",
      "Detailed Loss: recon 0.23294327-0.07764776 | disen 0.32318276-0.10772759 | temporal 0.55978698-0.18659566 | total 0.37197101\n",
      "Epoch 41, loss: 0.37197101-(best 0.37197101)\n",
      "\n",
      "Detailed Loss: recon 0.23481634-0.07827211 | disen 0.32191378-0.10730459 | temporal 0.55962837-0.18654279 | total 0.37211949\n",
      "Epoch 42, loss: 0.37211949-(best 0.37197101)\n",
      "\n",
      "Detailed Loss: recon 0.23263054-0.07754351 | disen 0.32259941-0.10753314 | temporal 0.55976075-0.18658692 | total 0.37166357\n",
      "Epoch 43, loss: 0.37166357-(best 0.37166357)\n",
      "\n",
      "Detailed Loss: recon 0.23264734-0.07754911 | disen 0.32277572-0.10759191 | temporal 0.55978942-0.18659647 | total 0.37173751\n",
      "Epoch 44, loss: 0.37173751-(best 0.37166357)\n",
      "\n",
      "Detailed Loss: recon 0.22881210-0.07627070 | disen 0.32059902-0.10686634 | temporal 0.55973208-0.18657736 | total 0.36971441\n",
      "Epoch 45, loss: 0.36971441-(best 0.36971441)\n",
      "\n",
      "Detailed Loss: recon 0.23215948-0.07738649 | disen 0.31875253-0.10625084 | temporal 0.55964625-0.18654875 | total 0.37018609\n",
      "Epoch 46, loss: 0.37018609-(best 0.36971441)\n",
      "\n",
      "Detailed Loss: recon 0.23027277-0.07675759 | disen 0.31684643-0.10561548 | temporal 0.55959302-0.18653101 | total 0.36890408\n",
      "Epoch 47, loss: 0.36890408-(best 0.36890408)\n",
      "\n",
      "Detailed Loss: recon 0.23209706-0.07736569 | disen 0.31445384-0.10481795 | temporal 0.55954611-0.18651537 | total 0.36869901\n",
      "Epoch 48, loss: 0.36869901-(best 0.36869901)\n",
      "\n",
      "Detailed Loss: recon 0.22804312-0.07601437 | disen 0.31814533-0.10604844 | temporal 0.55973977-0.18657992 | total 0.36864275\n",
      "Epoch 49, loss: 0.36864275-(best 0.36864275)\n",
      "\n",
      "Detailed Loss: recon 0.22675115-0.07558372 | disen 0.31725281-0.10575094 | temporal 0.55982697-0.18660899 | total 0.36794364\n",
      "Epoch 50, loss: 0.36794364-(best 0.36794364)\n",
      "\n",
      ">> Reached final epoch. Loading best model...\n",
      "TRAINING XGB PREDICTOR\n",
      ">> Ending training!\n",
      "\n",
      "-----------------------\n",
      "TIME COST: 6.493721961975098 s\n",
      "Best Val: REC 68.86 PRE 49.73 MF1 68.81 AUC 79.84 TP 544 FP 550 TN 1579 FN 246\n",
      "-----------------------\n",
      "Generating additional positive data from adversary...\n",
      "Generating additional negative data by duplicating random nodes...\n",
      "\n",
      "ASSIGNED GRAPH FOR EVALUATION: 12345 {1: 2694, 0: 9651} Nodes, 1154646 Edges\n",
      "\n",
      "-------------------\n",
      "STARTING EVALUATION\n",
      "-------------------\n",
      "\n",
      "Dataset - Overall: REC 58.02 PRE 55.52 MF1 72.16 AUC 83.40 TP 1563 FP 1252 TN 8399 FN 1131 | 12345 {1: 2694, 0: 9651}\n",
      "Dataset - Train: REC 75.53 PRE 65.71 MF1 79.04 AUC 88.98 TP 895 FP 467 TN 2725 FN 290 | 4377 {1: 1185, 0: 3192}\n",
      "Dataset - Val: REC 58.61 PRE 50.82 MF1 67.85 AUC 77.59 TP 463 FP 448 TN 1681 FN 327 | 2919 {0: 2129, 1: 790}\n",
      "Dataset - Test: REC 28.51 PRE 37.82 MF1 61.44 AUC 78.79 TP 205 FP 337 TN 3993 FN 514 | 5049 {1: 719, 0: 4330}\n",
      "PREDICTION STATUS - {'1-True': 2180, '0-True': 5658, '1-False': 514, '0-False': 3993}\n",
      "    >> 514 positive nodes left unpredicted...\n",
      "    >> 418 OG positive nodes left unpredicted...\n",
      "-----------------------\n",
      "PREDICTION RESULT - ROUNDS\n",
      "Dataset - Round 0: REC 51.57 PRE 55.15 MF1 70.63 AUC 82.42 TP 642 FP 522 TN 4112 FN 603 | 5879 {1: 1245, 0: 4634}\n",
      "Dataset - Round 1: REC 25.00 PRE 39.51 MF1 57.80 AUC 74.22 TP 32 FP 49 TN 410 FN 96 | 587 {1: 128, 0: 459}\n",
      "-----------------------\n",
      "PREDICTION RESULT - SEEDS\n",
      "Seeds - Current: REC 29.69 PRE 33.93 MF1 57.05 AUC 69.27 TP 38 FP 74 TN 385 FN 90 | 587 {1: 128, 0: 459}\n",
      "Seeds - Prev: REC 0.00 PRE 0.00 MF1 38.47 AUC 69.27 TP 0 FP 92 TN 367 FN 128 | 587 {1: 128, 0: 459}\n",
      "Selecting data as budgeted ground truth for next round...\n",
      "  > Starting round 2...\n",
      "\n",
      "+++++++++++++++++\n",
      "STARTING ROUND 2!\n",
      "+++++++++++++++++\n",
      "        \n",
      "ASSIGNED GRAPH FOR TRAINING: 12345 {1: 2694, 0: 9651} Nodes, 1154646 Edges\n",
      "\n",
      ">> Splitting to train-validation set...\n",
      "Alotting train-val-test split for round 2\n",
      "Initial pool: 5879, Prediction pool: 6030, Budget pool: 0, Full pool: 7838\n",
      "Full graph size: 12345, Training: 4702, Val: 3136, Test: 12345\n",
      "    >> INITIAL DATA SPLIT: 4702 train rows ({1: 1308, 0: 3394}) | 3136 val rows ({0: 2264, 1: 872}) | 4507 test rows ({1: 514, 0: 3993})\n",
      "    >> AUGMENTED DATA SPLIT: 4702 train rows ({1: 1308, 0: 3394}) | 3136 val rows ({0: 2264, 1: 872}) | 4507 test rows ({1: 514, 0: 3993})\n",
      "    >> Updated cross-entropy weight to 2.59480122324159...\n",
      "\n",
      "-----------------------\n",
      "STARTING MODEL TRAININGps_l\n",
      "-----------------------\n",
      "\n",
      "TRAINING NODE EMBEDDER\n",
      "Detailed Loss: recon 0.21871167-0.07290389 | disen 0.32707316-0.10902439 | temporal 0.58089519-0.19363173 | total 0.37556002\n",
      "Epoch 1, loss: 0.37556002-(best 0.37556002)\n",
      "\n",
      "Detailed Loss: recon 0.36685681-0.12228560 | disen 0.38065898-0.12688633 | temporal 0.58182561-0.19394187 | total 0.44311380\n",
      "Epoch 2, loss: 0.44311380-(best 0.37556002)\n",
      "\n",
      "Detailed Loss: recon 0.28376549-0.09458850 | disen 0.35681510-0.11893837 | temporal 0.58270282-0.19423427 | total 0.40776116\n",
      "Epoch 3, loss: 0.40776116-(best 0.37556002)\n",
      "\n",
      "Detailed Loss: recon 0.29622886-0.09874295 | disen 0.34199303-0.11399768 | temporal 0.58315611-0.19438537 | total 0.40712601\n",
      "Epoch 4, loss: 0.40712601-(best 0.37556002)\n",
      "\n",
      "Detailed Loss: recon 0.28860039-0.09620013 | disen 0.34423751-0.11474584 | temporal 0.58336622-0.19445541 | total 0.40540138\n",
      "Epoch 5, loss: 0.40540138-(best 0.37556002)\n",
      "\n",
      "Detailed Loss: recon 0.24686816-0.08228939 | disen 0.34676325-0.11558775 | temporal 0.58322591-0.19440864 | total 0.39228576\n",
      "Epoch 6, loss: 0.39228576-(best 0.37556002)\n",
      "\n",
      "Detailed Loss: recon 0.25207180-0.08402393 | disen 0.35250163-0.11750054 | temporal 0.58314282-0.19438094 | total 0.39590544\n",
      "Epoch 7, loss: 0.39590544-(best 0.37556002)\n",
      "\n",
      "Detailed Loss: recon 0.25022107-0.08340702 | disen 0.35588497-0.11862832 | temporal 0.58298844-0.19432948 | total 0.39636484\n",
      "Epoch 8, loss: 0.39636484-(best 0.37556002)\n",
      "\n",
      "Detailed Loss: recon 0.24498746-0.08166249 | disen 0.35529995-0.11843332 | temporal 0.58280599-0.19426866 | total 0.39436448\n",
      "Epoch 9, loss: 0.39436448-(best 0.37556002)\n",
      "\n",
      "Detailed Loss: recon 0.23996502-0.07998834 | disen 0.35570222-0.11856741 | temporal 0.58274585-0.19424862 | total 0.39280438\n",
      "Epoch 10, loss: 0.39280438-(best 0.37556002)\n",
      "\n",
      "Detailed Loss: recon 0.23447835-0.07815945 | disen 0.35730934-0.11910311 | temporal 0.58279979-0.19426660 | total 0.39152917\n",
      "Epoch 11, loss: 0.39152917-(best 0.37556002)\n",
      "\n",
      "Detailed Loss: recon 0.22835696-0.07611899 | disen 0.35308033-0.11769344 | temporal 0.58272225-0.19424075 | total 0.38805318\n",
      "Epoch 12, loss: 0.38805318-(best 0.37556002)\n",
      "\n",
      "Detailed Loss: recon 0.22476344-0.07492115 | disen 0.35112673-0.11704224 | temporal 0.58270872-0.19423624 | total 0.38619965\n",
      "Epoch 13, loss: 0.38619965-(best 0.37556002)\n",
      "\n",
      "Detailed Loss: recon 0.22588715-0.07529572 | disen 0.34525526-0.11508509 | temporal 0.58256274-0.19418758 | total 0.38456839\n",
      "Epoch 14, loss: 0.38456839-(best 0.37556002)\n",
      "\n",
      "Detailed Loss: recon 0.22687887-0.07562629 | disen 0.34114128-0.11371376 | temporal 0.58248198-0.19416066 | total 0.38350073\n",
      "Epoch 15, loss: 0.38350073-(best 0.37556002)\n",
      "\n",
      "Detailed Loss: recon 0.22638592-0.07546197 | disen 0.34118491-0.11372830 | temporal 0.58251709-0.19417236 | total 0.38336265\n",
      "Epoch 16, loss: 0.38336265-(best 0.37556002)\n",
      "\n",
      "Detailed Loss: recon 0.22440696-0.07480232 | disen 0.33811694-0.11270565 | temporal 0.58241856-0.19413952 | total 0.38164750\n",
      "Epoch 17, loss: 0.38164750-(best 0.37556002)\n",
      "\n",
      "Detailed Loss: recon 0.22287026-0.07429009 | disen 0.33664274-0.11221425 | temporal 0.58238119-0.19412706 | total 0.38063139\n",
      "Epoch 18, loss: 0.38063139-(best 0.37556002)\n",
      "\n",
      "Detailed Loss: recon 0.21889937-0.07296646 | disen 0.33516002-0.11172001 | temporal 0.58238482-0.19412827 | total 0.37881476\n",
      "Epoch 19, loss: 0.37881476-(best 0.37556002)\n",
      "\n",
      "Detailed Loss: recon 0.21945362-0.07315121 | disen 0.33418292-0.11139431 | temporal 0.58233422-0.19411141 | total 0.37865692\n",
      "Epoch 20, loss: 0.37865692-(best 0.37556002)\n",
      "\n",
      "Detailed Loss: recon 0.22066098-0.07355366 | disen 0.32907110-0.10969037 | temporal 0.58220893-0.19406964 | total 0.37731367\n",
      "Epoch 21, loss: 0.37731367-(best 0.37556002)\n",
      "\n",
      "Detailed Loss: recon 0.22061726-0.07353909 | disen 0.32562989-0.10854330 | temporal 0.58209801-0.19403267 | total 0.37611505\n",
      "Epoch 22, loss: 0.37611505-(best 0.37556002)\n",
      "\n",
      "Detailed Loss: recon 0.21978986-0.07326329 | disen 0.32449400-0.10816467 | temporal 0.58206904-0.19402301 | total 0.37545097\n",
      "Epoch 23, loss: 0.37545097-(best 0.37545097)\n",
      "\n",
      "Detailed Loss: recon 0.21976134-0.07325378 | disen 0.32167840-0.10722613 | temporal 0.58200449-0.19400150 | total 0.37448141\n",
      "Epoch 24, loss: 0.37448141-(best 0.37448141)\n",
      "\n",
      "Detailed Loss: recon 0.21731398-0.07243799 | disen 0.32236320-0.10745440 | temporal 0.58208227-0.19402742 | total 0.37391981\n",
      "Epoch 25, loss: 0.37391981-(best 0.37391981)\n",
      "\n",
      "Detailed Loss: recon 0.22039109-0.07346370 | disen 0.32070190-0.10690063 | temporal 0.58207536-0.19402512 | total 0.37438947\n",
      "Epoch 26, loss: 0.37438947-(best 0.37391981)\n",
      "\n",
      "Detailed Loss: recon 0.21910213-0.07303404 | disen 0.31790614-0.10596871 | temporal 0.58199143-0.19399714 | total 0.37299991\n",
      "Epoch 27, loss: 0.37299991-(best 0.37299991)\n",
      "\n",
      "Detailed Loss: recon 0.21663849-0.07221283 | disen 0.31800145-0.10600048 | temporal 0.58186245-0.19395415 | total 0.37216747\n",
      "Epoch 28, loss: 0.37216747-(best 0.37216747)\n",
      "\n",
      "Detailed Loss: recon 0.21665469-0.07221823 | disen 0.31470561-0.10490187 | temporal 0.58172774-0.19390925 | total 0.37102938\n",
      "Epoch 29, loss: 0.37102938-(best 0.37102938)\n",
      "\n",
      "Detailed Loss: recon 0.21727630-0.07242543 | disen 0.31296945-0.10432315 | temporal 0.58161885-0.19387295 | total 0.37062156\n",
      "Epoch 30, loss: 0.37062156-(best 0.37062156)\n",
      "\n",
      "Detailed Loss: recon 0.21744512-0.07248171 | disen 0.31143725-0.10381242 | temporal 0.58158839-0.19386280 | total 0.37015691\n",
      "Epoch 31, loss: 0.37015691-(best 0.37015691)\n",
      "\n",
      "Detailed Loss: recon 0.21845609-0.07281870 | disen 0.31066394-0.10355465 | temporal 0.58152473-0.19384158 | total 0.37021494\n",
      "Epoch 32, loss: 0.37021494-(best 0.37015691)\n",
      "\n",
      "Detailed Loss: recon 0.21617967-0.07205989 | disen 0.31081563-0.10360521 | temporal 0.58156711-0.19385570 | total 0.36952081\n",
      "Epoch 33, loss: 0.36952081-(best 0.36952081)\n",
      "\n",
      "Detailed Loss: recon 0.21762887-0.07254296 | disen 0.30833650-0.10277883 | temporal 0.58133733-0.19377911 | total 0.36910090\n",
      "Epoch 34, loss: 0.36910090-(best 0.36910090)\n",
      "\n",
      "Detailed Loss: recon 0.21658936-0.07219645 | disen 0.30798143-0.10266048 | temporal 0.58125424-0.19375141 | total 0.36860836\n",
      "Epoch 35, loss: 0.36860836-(best 0.36860836)\n",
      "\n",
      "Detailed Loss: recon 0.21327853-0.07109284 | disen 0.30781752-0.10260584 | temporal 0.58119804-0.19373268 | total 0.36743137\n",
      "Epoch 36, loss: 0.36743137-(best 0.36743137)\n",
      "\n",
      "Detailed Loss: recon 0.21109204-0.07036401 | disen 0.30710256-0.10236752 | temporal 0.58120346-0.19373449 | total 0.36646605\n",
      "Epoch 37, loss: 0.36646605-(best 0.36646605)\n",
      "\n",
      "Detailed Loss: recon 0.21142718-0.07047573 | disen 0.30629939-0.10209980 | temporal 0.58132714-0.19377571 | total 0.36635125\n",
      "Epoch 38, loss: 0.36635125-(best 0.36635125)\n",
      "\n",
      "Detailed Loss: recon 0.21354836-0.07118279 | disen 0.30518788-0.10172929 | temporal 0.58117348-0.19372449 | total 0.36663657\n",
      "Epoch 39, loss: 0.36663657-(best 0.36635125)\n",
      "\n",
      "Detailed Loss: recon 0.21255776-0.07085259 | disen 0.30376792-0.10125597 | temporal 0.58104753-0.19368251 | total 0.36579108\n",
      "Epoch 40, loss: 0.36579108-(best 0.36579108)\n",
      "\n",
      "Detailed Loss: recon 0.21252450-0.07084150 | disen 0.30366433-0.10122144 | temporal 0.58111882-0.19370627 | total 0.36576921\n",
      "Epoch 41, loss: 0.36576921-(best 0.36576921)\n",
      "\n",
      "Detailed Loss: recon 0.21096621-0.07032207 | disen 0.30315572-0.10105191 | temporal 0.58115363-0.19371788 | total 0.36509186\n",
      "Epoch 42, loss: 0.36509186-(best 0.36509186)\n",
      "\n",
      "Detailed Loss: recon 0.21069799-0.07023266 | disen 0.30280793-0.10093598 | temporal 0.58116484-0.19372161 | total 0.36489028\n",
      "Epoch 43, loss: 0.36489028-(best 0.36489028)\n",
      "\n",
      "Detailed Loss: recon 0.21142235-0.07047412 | disen 0.30203253-0.10067751 | temporal 0.58114135-0.19371378 | total 0.36486542\n",
      "Epoch 44, loss: 0.36486542-(best 0.36486542)\n",
      "\n",
      "Detailed Loss: recon 0.21036732-0.07012244 | disen 0.30113769-0.10037923 | temporal 0.58116746-0.19372249 | total 0.36422417\n",
      "Epoch 45, loss: 0.36422417-(best 0.36422417)\n",
      "\n",
      "Detailed Loss: recon 0.21120602-0.07040201 | disen 0.29974920-0.09991640 | temporal 0.58107799-0.19369266 | total 0.36401108\n",
      "Epoch 46, loss: 0.36401108-(best 0.36401108)\n",
      "\n",
      "Detailed Loss: recon 0.20990956-0.06996985 | disen 0.30151224-0.10050408 | temporal 0.58122766-0.19374255 | total 0.36421651\n",
      "Epoch 47, loss: 0.36421651-(best 0.36401108)\n",
      "\n",
      "Detailed Loss: recon 0.20910060-0.06970020 | disen 0.29968411-0.09989470 | temporal 0.58118349-0.19372783 | total 0.36332273\n",
      "Epoch 48, loss: 0.36332273-(best 0.36332273)\n",
      "\n",
      "Detailed Loss: recon 0.20975566-0.06991855 | disen 0.30037135-0.10012378 | temporal 0.58121842-0.19373947 | total 0.36378181\n",
      "Epoch 49, loss: 0.36378181-(best 0.36332273)\n",
      "\n",
      "Detailed Loss: recon 0.20940265-0.06980088 | disen 0.29808593-0.09936198 | temporal 0.58097160-0.19365720 | total 0.36282009\n",
      "Epoch 50, loss: 0.36282009-(best 0.36282009)\n",
      "\n",
      ">> Reached final epoch. Loading best model...\n",
      "TRAINING XGB PREDICTOR\n",
      ">> Ending training!\n",
      "\n",
      "-----------------------\n",
      "TIME COST: 6.512282371520996 s\n",
      "Best Val: REC 67.43 PRE 50.21 MF1 68.53 AUC 78.51 TP 588 FP 583 TN 1681 FN 284\n",
      "-----------------------\n",
      "Generating additional positive data from adversary...\n",
      "Generating additional negative data by duplicating random nodes...\n",
      "\n",
      "ASSIGNED GRAPH FOR EVALUATION: 12932 {1: 2822, 0: 10110} Nodes, 1253212 Edges\n",
      "\n",
      "-------------------\n",
      "STARTING EVALUATION\n",
      "-------------------\n",
      "\n",
      "Dataset - Overall: REC 63.29 PRE 48.11 MF1 69.67 AUC 81.58 TP 1786 FP 1926 TN 8184 FN 1036 | 12932 {1: 2822, 0: 10110}\n",
      "Dataset - Train: REC 78.52 PRE 58.12 MF1 75.33 AUC 86.52 TP 1027 FP 740 TN 2654 FN 281 | 4702 {1: 1308, 0: 3394}\n",
      "Dataset - Val: REC 64.22 PRE 47.58 MF1 66.33 AUC 76.40 TP 560 FP 617 TN 1647 FN 312 | 3136 {0: 2264, 1: 872}\n",
      "Dataset - Test: REC 31.00 PRE 25.91 MF1 58.35 AUC 74.74 TP 199 FP 569 TN 3883 FN 443 | 5094 {1: 642, 0: 4452}\n",
      "PREDICTION STATUS - {'1-True': 2379, '0-True': 6227, '0-False': 3883, '1-False': 443}\n",
      "    >> 443 positive nodes left unpredicted...\n",
      "    >> 290 OG positive nodes left unpredicted...\n",
      "-----------------------\n",
      "PREDICTION RESULT - ROUNDS\n",
      "Dataset - Round 0: REC 61.12 PRE 46.74 MF1 68.89 AUC 81.10 TP 761 FP 867 TN 3767 FN 484 | 5879 {1: 1245, 0: 4634}\n",
      "Dataset - Round 1: REC 40.62 PRE 41.27 MF1 62.32 AUC 73.17 TP 52 FP 74 TN 385 FN 76 | 587 {1: 128, 0: 459}\n",
      "Dataset - Round 2: REC 33.59 PRE 31.16 MF1 56.25 AUC 65.72 TP 43 FP 95 TN 364 FN 85 | 587 {1: 128, 0: 459}\n",
      "-----------------------\n",
      "PREDICTION RESULT - SEEDS\n",
      "Seeds - Current: REC 30.47 PRE 28.89 MF1 54.68 AUC 63.62 TP 39 FP 96 TN 363 FN 89 | 587 {1: 128, 0: 459}\n",
      "Seeds - Prev: REC 0.00 PRE 0.00 MF1 40.77 AUC 63.62 TP 0 FP 55 TN 404 FN 128 | 587 {1: 128, 0: 459}\n",
      "Selecting data as budgeted ground truth for next round...\n",
      "  > Starting round 3...\n",
      "\n",
      "+++++++++++++++++\n",
      "STARTING ROUND 3!\n",
      "+++++++++++++++++\n",
      "        \n",
      "ASSIGNED GRAPH FOR TRAINING: 12932 {1: 2822, 0: 10110} Nodes, 1253212 Edges\n",
      "\n",
      ">> Splitting to train-validation set...\n",
      "Alotting train-val-test split for round 3\n",
      "Initial pool: 5879, Prediction pool: 9742, Budget pool: 0, Full pool: 8606\n",
      "Full graph size: 12932, Training: 5163, Val: 3443, Test: 12932\n",
      "    >> INITIAL DATA SPLIT: 5163 train rows ({0: 3736, 1: 1427}) | 3443 val rows ({1: 952, 0: 2491}) | 4326 test rows ({0: 3883, 1: 443})\n",
      "    >> AUGMENTED DATA SPLIT: 5163 train rows ({0: 3736, 1: 1427}) | 3443 val rows ({1: 952, 0: 2491}) | 4326 test rows ({0: 3883, 1: 443})\n",
      "    >> Updated cross-entropy weight to 2.6180798878766645...\n",
      "\n",
      "-----------------------\n",
      "STARTING MODEL TRAININGps_l\n",
      "-----------------------\n",
      "\n",
      "TRAINING NODE EMBEDDER\n",
      "Detailed Loss: recon 0.22448048-0.07482683 | disen 0.30169863-0.10056621 | temporal 0.60170114-0.20056705 | total 0.37596011\n",
      "Epoch 1, loss: 0.37596011-(best 0.37596011)\n",
      "\n",
      "Detailed Loss: recon 0.29453459-0.09817820 | disen 0.32642448-0.10880816 | temporal 0.60331643-0.20110548 | total 0.40809184\n",
      "Epoch 2, loss: 0.40809184-(best 0.37596011)\n",
      "\n",
      "Detailed Loss: recon 0.24771681-0.08257227 | disen 0.32114738-0.10704913 | temporal 0.60270756-0.20090252 | total 0.39052391\n",
      "Epoch 3, loss: 0.39052391-(best 0.37596011)\n",
      "\n",
      "Detailed Loss: recon 0.26182285-0.08727428 | disen 0.32309717-0.10769906 | temporal 0.60253000-0.20084333 | total 0.39581668\n",
      "Epoch 4, loss: 0.39581668-(best 0.37596011)\n",
      "\n",
      "Detailed Loss: recon 0.24063355-0.08021118 | disen 0.32497662-0.10832554 | temporal 0.60280818-0.20093606 | total 0.38947278\n",
      "Epoch 5, loss: 0.38947278-(best 0.37596011)\n",
      "\n",
      "Detailed Loss: recon 0.22880074-0.07626691 | disen 0.33045858-0.11015286 | temporal 0.60323745-0.20107915 | total 0.38749894\n",
      "Epoch 6, loss: 0.38749894-(best 0.37596011)\n",
      "\n",
      "Detailed Loss: recon 0.23364578-0.07788193 | disen 0.32899517-0.10966506 | temporal 0.60349435-0.20116478 | total 0.38871178\n",
      "Epoch 7, loss: 0.38871178-(best 0.37596011)\n",
      "\n",
      "Detailed Loss: recon 0.23451123-0.07817041 | disen 0.32631505-0.10877168 | temporal 0.60342884-0.20114295 | total 0.38808507\n",
      "Epoch 8, loss: 0.38808507-(best 0.37596011)\n",
      "\n",
      "Detailed Loss: recon 0.23577206-0.07859069 | disen 0.32237482-0.10745827 | temporal 0.60336035-0.20112012 | total 0.38716909\n",
      "Epoch 9, loss: 0.38716909-(best 0.37596011)\n",
      "\n",
      "Detailed Loss: recon 0.23404893-0.07801631 | disen 0.32053208-0.10684403 | temporal 0.60311520-0.20103840 | total 0.38589877\n",
      "Epoch 10, loss: 0.38589877-(best 0.37596011)\n",
      "\n",
      "Detailed Loss: recon 0.23616943-0.07872314 | disen 0.31946725-0.10648908 | temporal 0.60305583-0.20101861 | total 0.38623083\n",
      "Epoch 11, loss: 0.38623083-(best 0.37596011)\n",
      "\n",
      "Detailed Loss: recon 0.23293775-0.07764592 | disen 0.31859714-0.10619905 | temporal 0.60296214-0.20098738 | total 0.38483235\n",
      "Epoch 12, loss: 0.38483235-(best 0.37596011)\n",
      "\n",
      "Detailed Loss: recon 0.23035032-0.07678344 | disen 0.31505001-0.10501667 | temporal 0.60277528-0.20092509 | total 0.38272521\n",
      "Epoch 13, loss: 0.38272521-(best 0.37596011)\n",
      "\n",
      "Detailed Loss: recon 0.22858101-0.07619367 | disen 0.31663489-0.10554496 | temporal 0.60270852-0.20090284 | total 0.38264149\n",
      "Epoch 14, loss: 0.38264149-(best 0.37596011)\n",
      "\n",
      "Detailed Loss: recon 0.22726783-0.07575594 | disen 0.31264251-0.10421417 | temporal 0.60260224-0.20086741 | total 0.38083753\n",
      "Epoch 15, loss: 0.38083753-(best 0.37596011)\n",
      "\n",
      "Detailed Loss: recon 0.22780564-0.07593521 | disen 0.31133956-0.10377985 | temporal 0.60252035-0.20084012 | total 0.38055518\n",
      "Epoch 16, loss: 0.38055518-(best 0.37596011)\n",
      "\n",
      "Detailed Loss: recon 0.22408715-0.07469572 | disen 0.31175196-0.10391732 | temporal 0.60253036-0.20084345 | total 0.37945649\n",
      "Epoch 17, loss: 0.37945649-(best 0.37596011)\n",
      "\n",
      "Detailed Loss: recon 0.22712418-0.07570806 | disen 0.31127030-0.10375677 | temporal 0.60253364-0.20084455 | total 0.38030940\n",
      "Epoch 18, loss: 0.38030940-(best 0.37596011)\n",
      "\n",
      "Detailed Loss: recon 0.22395121-0.07465040 | disen 0.30985713-0.10328571 | temporal 0.60254341-0.20084780 | total 0.37878394\n",
      "Epoch 19, loss: 0.37878394-(best 0.37596011)\n",
      "\n",
      "Detailed Loss: recon 0.22297129-0.07432376 | disen 0.31121939-0.10373980 | temporal 0.60260892-0.20086964 | total 0.37893322\n",
      "Epoch 20, loss: 0.37893322-(best 0.37596011)\n",
      "\n",
      "Detailed Loss: recon 0.22115135-0.07371712 | disen 0.30721867-0.10240622 | temporal 0.60260230-0.20086743 | total 0.37699080\n",
      "Epoch 21, loss: 0.37699080-(best 0.37596011)\n",
      "\n",
      "Detailed Loss: recon 0.22285283-0.07428428 | disen 0.30700022-0.10233341 | temporal 0.60268843-0.20089614 | total 0.37751383\n",
      "Epoch 22, loss: 0.37751383-(best 0.37596011)\n",
      "\n",
      "Detailed Loss: recon 0.22179978-0.07393326 | disen 0.30462027-0.10154009 | temporal 0.60261315-0.20087105 | total 0.37634438\n",
      "Epoch 23, loss: 0.37634438-(best 0.37596011)\n",
      "\n",
      "Detailed Loss: recon 0.22300743-0.07433581 | disen 0.30384129-0.10128043 | temporal 0.60261559-0.20087186 | total 0.37648812\n",
      "Epoch 24, loss: 0.37648812-(best 0.37596011)\n",
      "\n",
      "Detailed Loss: recon 0.21937382-0.07312461 | disen 0.30624902-0.10208301 | temporal 0.60269260-0.20089753 | total 0.37610516\n",
      "Epoch 25, loss: 0.37610516-(best 0.37596011)\n",
      "\n",
      "Detailed Loss: recon 0.21976107-0.07325369 | disen 0.30467993-0.10155998 | temporal 0.60265183-0.20088394 | total 0.37569761\n",
      "Epoch 26, loss: 0.37569761-(best 0.37569761)\n",
      "\n",
      "Detailed Loss: recon 0.22040305-0.07346768 | disen 0.30264693-0.10088231 | temporal 0.60263830-0.20087943 | total 0.37522942\n",
      "Epoch 27, loss: 0.37522942-(best 0.37522942)\n",
      "\n",
      "Detailed Loss: recon 0.22012310-0.07337437 | disen 0.30038625-0.10012875 | temporal 0.60252792-0.20084264 | total 0.37434578\n",
      "Epoch 28, loss: 0.37434578-(best 0.37434578)\n",
      "\n",
      "Detailed Loss: recon 0.22069609-0.07356536 | disen 0.29981458-0.09993819 | temporal 0.60254753-0.20084918 | total 0.37435275\n",
      "Epoch 29, loss: 0.37435275-(best 0.37434578)\n",
      "\n",
      "Detailed Loss: recon 0.21940562-0.07313521 | disen 0.30135518-0.10045173 | temporal 0.60273910-0.20091303 | total 0.37449998\n",
      "Epoch 30, loss: 0.37449998-(best 0.37434578)\n",
      "\n",
      "Detailed Loss: recon 0.21903384-0.07301128 | disen 0.30058271-0.10019424 | temporal 0.60272312-0.20090771 | total 0.37411323\n",
      "Epoch 31, loss: 0.37411323-(best 0.37411323)\n",
      "\n",
      "Detailed Loss: recon 0.21797714-0.07265905 | disen 0.29907137-0.09969046 | temporal 0.60275537-0.20091846 | total 0.37326798\n",
      "Epoch 32, loss: 0.37326798-(best 0.37326798)\n",
      "\n",
      "Detailed Loss: recon 0.21617299-0.07205766 | disen 0.29734725-0.09911575 | temporal 0.60270619-0.20090206 | total 0.37207550\n",
      "Epoch 33, loss: 0.37207550-(best 0.37207550)\n",
      "\n",
      "Detailed Loss: recon 0.21631524-0.07210508 | disen 0.29637432-0.09879144 | temporal 0.60270596-0.20090199 | total 0.37179852\n",
      "Epoch 34, loss: 0.37179852-(best 0.37179852)\n",
      "\n",
      "Detailed Loss: recon 0.21737951-0.07245984 | disen 0.29564929-0.09854976 | temporal 0.60267276-0.20089092 | total 0.37190053\n",
      "Epoch 35, loss: 0.37190053-(best 0.37179852)\n",
      "\n",
      "Detailed Loss: recon 0.21539348-0.07179783 | disen 0.29575330-0.09858443 | temporal 0.60278571-0.20092857 | total 0.37131083\n",
      "Epoch 36, loss: 0.37131083-(best 0.37131083)\n",
      "\n",
      "Detailed Loss: recon 0.21660054-0.07220018 | disen 0.29622191-0.09874064 | temporal 0.60285234-0.20095078 | total 0.37189162\n",
      "Epoch 37, loss: 0.37189162-(best 0.37131083)\n",
      "\n",
      "Detailed Loss: recon 0.21657400-0.07219133 | disen 0.29554880-0.09851627 | temporal 0.60281003-0.20093668 | total 0.37164429\n",
      "Epoch 38, loss: 0.37164429-(best 0.37131083)\n",
      "\n",
      "Detailed Loss: recon 0.21519236-0.07173079 | disen 0.29523373-0.09841124 | temporal 0.60292411-0.20097470 | total 0.37111676\n",
      "Epoch 39, loss: 0.37111676-(best 0.37111676)\n",
      "\n",
      "Detailed Loss: recon 0.21386021-0.07128674 | disen 0.29503256-0.09834419 | temporal 0.60286987-0.20095662 | total 0.37058756\n",
      "Epoch 40, loss: 0.37058756-(best 0.37058756)\n",
      "\n",
      "Detailed Loss: recon 0.21288793-0.07096264 | disen 0.29331589-0.09777196 | temporal 0.60283649-0.20094550 | total 0.36968011\n",
      "Epoch 41, loss: 0.36968011-(best 0.36968011)\n",
      "\n",
      "Detailed Loss: recon 0.21346650-0.07115550 | disen 0.29543698-0.09847899 | temporal 0.60292625-0.20097542 | total 0.37060991\n",
      "Epoch 42, loss: 0.37060991-(best 0.36968011)\n",
      "\n",
      "Detailed Loss: recon 0.21371035-0.07123678 | disen 0.29249525-0.09749842 | temporal 0.60291201-0.20097067 | total 0.36970589\n",
      "Epoch 43, loss: 0.36970589-(best 0.36968011)\n",
      "\n",
      "Detailed Loss: recon 0.21365479-0.07121826 | disen 0.29330295-0.09776765 | temporal 0.60298884-0.20099628 | total 0.36998218\n",
      "Epoch 44, loss: 0.36998218-(best 0.36968011)\n",
      "\n",
      "Detailed Loss: recon 0.21240506-0.07080169 | disen 0.29255736-0.09751912 | temporal 0.60295212-0.20098404 | total 0.36930484\n",
      "Epoch 45, loss: 0.36930484-(best 0.36930484)\n",
      "\n",
      "Detailed Loss: recon 0.21231626-0.07077209 | disen 0.29106075-0.09702025 | temporal 0.60288715-0.20096238 | total 0.36875474\n",
      "Epoch 46, loss: 0.36875474-(best 0.36875474)\n",
      "\n",
      "Detailed Loss: recon 0.21287733-0.07095911 | disen 0.29047853-0.09682618 | temporal 0.60290152-0.20096717 | total 0.36875248\n",
      "Epoch 47, loss: 0.36875248-(best 0.36875248)\n",
      "\n",
      "Detailed Loss: recon 0.21211213-0.07070404 | disen 0.29158956-0.09719652 | temporal 0.60292828-0.20097609 | total 0.36887667\n",
      "Epoch 48, loss: 0.36887667-(best 0.36875248)\n",
      "\n",
      "Detailed Loss: recon 0.21196006-0.07065335 | disen 0.29011708-0.09670569 | temporal 0.60289168-0.20096389 | total 0.36832297\n",
      "Epoch 49, loss: 0.36832297-(best 0.36832297)\n",
      "\n",
      "Detailed Loss: recon 0.21241698-0.07080566 | disen 0.28889090-0.09629697 | temporal 0.60292035-0.20097345 | total 0.36807609\n",
      "Epoch 50, loss: 0.36807609-(best 0.36807609)\n",
      "\n",
      ">> Reached final epoch. Loading best model...\n",
      "TRAINING XGB PREDICTOR\n",
      ">> Ending training!\n",
      "\n",
      "-----------------------\n",
      "TIME COST: 7.389396667480469 s\n",
      "Best Val: REC 58.09 PRE 49.24 MF1 66.58 AUC 76.08 TP 553 FP 570 TN 1921 FN 399\n",
      "-----------------------\n",
      "Generating additional positive data from adversary...\n",
      "Generating additional negative data by duplicating random nodes...\n",
      "\n",
      "ASSIGNED GRAPH FOR EVALUATION: 13519 {1: 2950, 0: 10569} Nodes, 1365380 Edges\n",
      "\n",
      "-------------------\n",
      "STARTING EVALUATION\n",
      "-------------------\n",
      "\n",
      "Dataset - Overall: REC 55.22 PRE 48.30 MF1 68.37 AUC 79.90 TP 1629 FP 1744 TN 8825 FN 1321 | 13519 {1: 2950, 0: 10569}\n",
      "Dataset - Train: REC 68.68 PRE 56.75 MF1 72.75 AUC 83.08 TP 980 FP 747 TN 2989 FN 447 | 5163 {0: 3736, 1: 1427}\n",
      "Dataset - Val: REC 55.99 PRE 46.07 MF1 64.36 AUC 73.78 TP 533 FP 624 TN 1867 FN 419 | 3443 {1: 952, 0: 2491}\n",
      "Dataset - Test: REC 20.32 PRE 23.72 MF1 56.22 AUC 73.80 TP 116 FP 373 TN 3969 FN 455 | 4913 {0: 4342, 1: 571}\n",
      "PREDICTION STATUS - {'1-True': 2495, '0-True': 6600, '0-False': 3969, '1-False': 455}\n",
      "    >> 455 positive nodes left unpredicted...\n",
      "    >> 227 OG positive nodes left unpredicted...\n",
      "-----------------------\n",
      "PREDICTION RESULT - ROUNDS\n",
      "Dataset - Round 0: REC 57.83 PRE 48.00 MF1 68.99 AUC 80.86 TP 720 FP 780 TN 3854 FN 525 | 5879 {1: 1245, 0: 4634}\n",
      "Dataset - Round 1: REC 39.06 PRE 45.05 MF1 63.49 AUC 74.55 TP 50 FP 61 TN 398 FN 78 | 587 {1: 128, 0: 459}\n",
      "Dataset - Round 2: REC 28.91 PRE 40.22 MF1 59.17 AUC 69.32 TP 37 FP 55 TN 404 FN 91 | 587 {1: 128, 0: 459}\n",
      "Dataset - Round 3: REC 16.41 PRE 25.00 MF1 51.07 AUC 64.18 TP 21 FP 63 TN 396 FN 107 | 587 {1: 128, 0: 459}\n",
      "-----------------------\n",
      "PREDICTION RESULT - SEEDS\n",
      "Seeds - Current: REC 18.75 PRE 21.43 MF1 49.72 AUC 60.89 TP 24 FP 88 TN 371 FN 104 | 587 {1: 128, 0: 459}\n",
      "Seeds - Prev: REC 0.00 PRE 0.00 MF1 39.42 AUC 60.89 TP 0 FP 77 TN 382 FN 128 | 587 {1: 128, 0: 459}\n",
      "Selecting data as budgeted ground truth for next round...\n",
      "  > Starting round 4...\n",
      "\n",
      "+++++++++++++++++\n",
      "STARTING ROUND 4!\n",
      "+++++++++++++++++\n",
      "        \n",
      "ASSIGNED GRAPH FOR TRAINING: 13519 {1: 2950, 0: 10569} Nodes, 1365380 Edges\n",
      "\n",
      ">> Splitting to train-validation set...\n",
      "Alotting train-val-test split for round 4\n",
      "Initial pool: 5879, Prediction pool: 13115, Budget pool: 0, Full pool: 9095\n",
      "Full graph size: 13519, Training: 5457, Val: 3638, Test: 13519\n",
      "    >> INITIAL DATA SPLIT: 5457 train rows ({0: 3960, 1: 1497}) | 3638 val rows ({1: 998, 0: 2640}) | 4424 test rows ({0: 3969, 1: 455})\n",
      "    >> AUGMENTED DATA SPLIT: 5457 train rows ({0: 3960, 1: 1497}) | 3638 val rows ({1: 998, 0: 2640}) | 4424 test rows ({0: 3969, 1: 455})\n",
      "    >> Updated cross-entropy weight to 2.6452905811623246...\n",
      "\n",
      "-----------------------\n",
      "STARTING MODEL TRAININGps_l\n",
      "-----------------------\n",
      "\n",
      "TRAINING NODE EMBEDDER\n",
      "Detailed Loss: recon 0.21149105-0.07049702 | disen 0.30103439-0.10034480 | temporal 0.62865222-0.20955074 | total 0.38039255\n",
      "Epoch 1, loss: 0.38039255-(best 0.38039255)\n",
      "\n",
      "Detailed Loss: recon 0.29216611-0.09738870 | disen 0.31764621-0.10588207 | temporal 0.62831402-0.20943801 | total 0.41270879\n",
      "Epoch 2, loss: 0.41270879-(best 0.38039255)\n",
      "\n",
      "Detailed Loss: recon 0.22698949-0.07566316 | disen 0.32014066-0.10671355 | temporal 0.62967712-0.20989237 | total 0.39226907\n",
      "Epoch 3, loss: 0.39226907-(best 0.38039255)\n",
      "\n",
      "Detailed Loss: recon 0.24404755-0.08134918 | disen 0.32347655-0.10782552 | temporal 0.63027120-0.21009040 | total 0.39926511\n",
      "Epoch 4, loss: 0.39926511-(best 0.38039255)\n",
      "\n",
      "Detailed Loss: recon 0.23137395-0.07712465 | disen 0.32994735-0.10998245 | temporal 0.63025415-0.21008472 | total 0.39719182\n",
      "Epoch 5, loss: 0.39719182-(best 0.38039255)\n",
      "\n",
      "Detailed Loss: recon 0.21362522-0.07120841 | disen 0.32369107-0.10789702 | temporal 0.62983900-0.20994633 | total 0.38905177\n",
      "Epoch 6, loss: 0.38905177-(best 0.38039255)\n",
      "\n",
      "Detailed Loss: recon 0.21423957-0.07141319 | disen 0.31991255-0.10663752 | temporal 0.62947357-0.20982452 | total 0.38787526\n",
      "Epoch 7, loss: 0.38787526-(best 0.38039255)\n",
      "\n",
      "Detailed Loss: recon 0.21841335-0.07280445 | disen 0.31513757-0.10504586 | temporal 0.62907988-0.20969329 | total 0.38754362\n",
      "Epoch 8, loss: 0.38754362-(best 0.38039255)\n",
      "\n",
      "Detailed Loss: recon 0.21540324-0.07180108 | disen 0.31376112-0.10458704 | temporal 0.62902939-0.20967646 | total 0.38606459\n",
      "Epoch 9, loss: 0.38606459-(best 0.38039255)\n",
      "\n",
      "Detailed Loss: recon 0.21471255-0.07157085 | disen 0.30889338-0.10296446 | temporal 0.62897062-0.20965687 | total 0.38419217\n",
      "Epoch 10, loss: 0.38419217-(best 0.38039255)\n",
      "\n",
      "Detailed Loss: recon 0.21451084-0.07150361 | disen 0.30696422-0.10232141 | temporal 0.62899375-0.20966458 | total 0.38348961\n",
      "Epoch 11, loss: 0.38348961-(best 0.38039255)\n",
      "\n",
      "Detailed Loss: recon 0.21434246-0.07144749 | disen 0.30831635-0.10277212 | temporal 0.62903076-0.20967692 | total 0.38389653\n",
      "Epoch 12, loss: 0.38389653-(best 0.38039255)\n",
      "\n",
      "Detailed Loss: recon 0.21355270-0.07118423 | disen 0.30563962-0.10187987 | temporal 0.62889189-0.20963063 | total 0.38269475\n",
      "Epoch 13, loss: 0.38269475-(best 0.38039255)\n",
      "\n",
      "Detailed Loss: recon 0.21221069-0.07073690 | disen 0.30722862-0.10240954 | temporal 0.62886673-0.20962224 | total 0.38276869\n",
      "Epoch 14, loss: 0.38276869-(best 0.38039255)\n",
      "\n",
      "Detailed Loss: recon 0.21151516-0.07050505 | disen 0.30431795-0.10143932 | temporal 0.62874269-0.20958090 | total 0.38152528\n",
      "Epoch 15, loss: 0.38152528-(best 0.38039255)\n",
      "\n",
      "Detailed Loss: recon 0.21143268-0.07047756 | disen 0.30316633-0.10105544 | temporal 0.62877673-0.20959224 | total 0.38112527\n",
      "Epoch 16, loss: 0.38112527-(best 0.38039255)\n",
      "\n",
      "Detailed Loss: recon 0.20937972-0.06979324 | disen 0.30318713-0.10106238 | temporal 0.62893075-0.20964358 | total 0.38049918\n",
      "Epoch 17, loss: 0.38049918-(best 0.38039255)\n",
      "\n",
      "Detailed Loss: recon 0.20879409-0.06959803 | disen 0.30076647-0.10025549 | temporal 0.62887979-0.20962660 | total 0.37948012\n",
      "Epoch 18, loss: 0.37948012-(best 0.37948012)\n",
      "\n",
      "Detailed Loss: recon 0.20816410-0.06938803 | disen 0.30033869-0.10011290 | temporal 0.62891698-0.20963899 | total 0.37913993\n",
      "Epoch 19, loss: 0.37913993-(best 0.37913993)\n",
      "\n",
      "Detailed Loss: recon 0.21135406-0.07045135 | disen 0.29784703-0.09928234 | temporal 0.62897146-0.20965715 | total 0.37939087\n",
      "Epoch 20, loss: 0.37939087-(best 0.37913993)\n",
      "\n",
      "Detailed Loss: recon 0.20818232-0.06939411 | disen 0.29707295-0.09902432 | temporal 0.62897998-0.20965999 | total 0.37807843\n",
      "Epoch 21, loss: 0.37807843-(best 0.37807843)\n",
      "\n",
      "Detailed Loss: recon 0.20607066-0.06869022 | disen 0.29912233-0.09970744 | temporal 0.62902397-0.20967466 | total 0.37807232\n",
      "Epoch 22, loss: 0.37807232-(best 0.37807232)\n",
      "\n",
      "Detailed Loss: recon 0.20857976-0.06952659 | disen 0.29778439-0.09926146 | temporal 0.62899232-0.20966411 | total 0.37845215\n",
      "Epoch 23, loss: 0.37845215-(best 0.37807232)\n",
      "\n",
      "Detailed Loss: recon 0.20665082-0.06888361 | disen 0.29821801-0.09940600 | temporal 0.62899619-0.20966540 | total 0.37795502\n",
      "Epoch 24, loss: 0.37795502-(best 0.37795502)\n",
      "\n",
      "Detailed Loss: recon 0.20733254-0.06911085 | disen 0.29756123-0.09918708 | temporal 0.62899995-0.20966665 | total 0.37796456\n",
      "Epoch 25, loss: 0.37796456-(best 0.37795502)\n",
      "\n",
      "Detailed Loss: recon 0.20765010-0.06921670 | disen 0.29553056-0.09851019 | temporal 0.62901849-0.20967283 | total 0.37739974\n",
      "Epoch 26, loss: 0.37739974-(best 0.37739974)\n",
      "\n",
      "Detailed Loss: recon 0.20636332-0.06878777 | disen 0.29563713-0.09854571 | temporal 0.62904859-0.20968286 | total 0.37701637\n",
      "Epoch 27, loss: 0.37701637-(best 0.37701637)\n",
      "\n",
      "Detailed Loss: recon 0.20771995-0.06923998 | disen 0.29544502-0.09848167 | temporal 0.62897831-0.20965944 | total 0.37738109\n",
      "Epoch 28, loss: 0.37738109-(best 0.37701637)\n",
      "\n",
      "Detailed Loss: recon 0.20462756-0.06820919 | disen 0.29580480-0.09860160 | temporal 0.62904704-0.20968235 | total 0.37649313\n",
      "Epoch 29, loss: 0.37649313-(best 0.37649313)\n",
      "\n",
      "Detailed Loss: recon 0.20420665-0.06806888 | disen 0.29433703-0.09811234 | temporal 0.62903184-0.20967728 | total 0.37585852\n",
      "Epoch 30, loss: 0.37585852-(best 0.37585852)\n",
      "\n",
      "Detailed Loss: recon 0.20537172-0.06845724 | disen 0.29297173-0.09765724 | temporal 0.62906235-0.20968745 | total 0.37580195\n",
      "Epoch 31, loss: 0.37580195-(best 0.37580195)\n",
      "\n",
      "Detailed Loss: recon 0.20690194-0.06896731 | disen 0.29153520-0.09717840 | temporal 0.62901992-0.20967331 | total 0.37581903\n",
      "Epoch 32, loss: 0.37581903-(best 0.37580195)\n",
      "\n",
      "Detailed Loss: recon 0.20444128-0.06814709 | disen 0.29319382-0.09773127 | temporal 0.62901509-0.20967170 | total 0.37555009\n",
      "Epoch 33, loss: 0.37555009-(best 0.37555009)\n",
      "\n",
      "Detailed Loss: recon 0.20566855-0.06855618 | disen 0.29060924-0.09686975 | temporal 0.62897718-0.20965906 | total 0.37508500\n",
      "Epoch 34, loss: 0.37508500-(best 0.37508500)\n",
      "\n",
      "Detailed Loss: recon 0.20537204-0.06845735 | disen 0.29042321-0.09680774 | temporal 0.62906677-0.20968892 | total 0.37495402\n",
      "Epoch 35, loss: 0.37495402-(best 0.37495402)\n",
      "\n",
      "Detailed Loss: recon 0.20645896-0.06881965 | disen 0.28993583-0.09664528 | temporal 0.62906229-0.20968743 | total 0.37515238\n",
      "Epoch 36, loss: 0.37515238-(best 0.37495402)\n",
      "\n",
      "Detailed Loss: recon 0.20607266-0.06869089 | disen 0.29056472-0.09685491 | temporal 0.62902534-0.20967511 | total 0.37522089\n",
      "Epoch 37, loss: 0.37522089-(best 0.37495402)\n",
      "\n",
      "Detailed Loss: recon 0.20590734-0.06863578 | disen 0.28812861-0.09604287 | temporal 0.62902409-0.20967470 | total 0.37435335\n",
      "Epoch 38, loss: 0.37435335-(best 0.37435335)\n",
      "\n",
      "Detailed Loss: recon 0.20750305-0.06916768 | disen 0.28981334-0.09660445 | temporal 0.62911338-0.20970446 | total 0.37547660\n",
      "Epoch 39, loss: 0.37547660-(best 0.37435335)\n",
      "\n",
      "Detailed Loss: recon 0.20476548-0.06825516 | disen 0.28962606-0.09654202 | temporal 0.62914246-0.20971415 | total 0.37451136\n",
      "Epoch 40, loss: 0.37451136-(best 0.37435335)\n",
      "\n",
      "Detailed Loss: recon 0.20498326-0.06832775 | disen 0.28797692-0.09599231 | temporal 0.62914181-0.20971394 | total 0.37403399\n",
      "Epoch 41, loss: 0.37403399-(best 0.37403399)\n",
      "\n",
      "Detailed Loss: recon 0.20469497-0.06823166 | disen 0.28794312-0.09598104 | temporal 0.62916720-0.20972240 | total 0.37393510\n",
      "Epoch 42, loss: 0.37393510-(best 0.37393510)\n",
      "\n",
      "Detailed Loss: recon 0.20379898-0.06793299 | disen 0.28849876-0.09616625 | temporal 0.62915033-0.20971678 | total 0.37381601\n",
      "Epoch 43, loss: 0.37381601-(best 0.37381601)\n",
      "\n",
      "Detailed Loss: recon 0.20377469-0.06792490 | disen 0.28507251-0.09502417 | temporal 0.62910056-0.20970019 | total 0.37264925\n",
      "Epoch 44, loss: 0.37264925-(best 0.37264925)\n",
      "\n",
      "Detailed Loss: recon 0.20468080-0.06822693 | disen 0.28602821-0.09534274 | temporal 0.62917483-0.20972494 | total 0.37329462\n",
      "Epoch 45, loss: 0.37329462-(best 0.37264925)\n",
      "\n",
      "Detailed Loss: recon 0.20539038-0.06846346 | disen 0.28552711-0.09517570 | temporal 0.62920129-0.20973376 | total 0.37337291\n",
      "Epoch 46, loss: 0.37337291-(best 0.37264925)\n",
      "\n",
      "Detailed Loss: recon 0.20447195-0.06815732 | disen 0.28579021-0.09526340 | temporal 0.62922478-0.20974159 | total 0.37316233\n",
      "Epoch 47, loss: 0.37316233-(best 0.37264925)\n",
      "\n",
      "Detailed Loss: recon 0.20374428-0.06791476 | disen 0.28652883-0.09550961 | temporal 0.62918454-0.20972818 | total 0.37315255\n",
      "Epoch 48, loss: 0.37315255-(best 0.37264925)\n",
      "\n",
      "Detailed Loss: recon 0.20309755-0.06769918 | disen 0.28703809-0.09567936 | temporal 0.62931490-0.20977163 | total 0.37315017\n",
      "Epoch 49, loss: 0.37315017-(best 0.37264925)\n",
      "\n",
      "Detailed Loss: recon 0.20385799-0.06795266 | disen 0.28495991-0.09498664 | temporal 0.62926000-0.20975333 | total 0.37269264\n",
      "Epoch 50, loss: 0.37269264-(best 0.37264925)\n",
      "\n",
      ">> Reached final epoch. Loading best model...\n",
      "TRAINING XGB PREDICTOR\n",
      ">> Ending training!\n",
      "\n",
      "-----------------------\n",
      "TIME COST: 6.440024375915527 s\n",
      "Best Val: REC 60.92 PRE 49.63 MF1 67.39 AUC 76.64 TP 608 FP 617 TN 2023 FN 390\n",
      "-----------------------\n",
      "Generating additional positive data from adversary...\n",
      "Generating additional negative data by duplicating random nodes...\n",
      "\n",
      "ASSIGNED GRAPH FOR EVALUATION: 14106 {1: 3078, 0: 11028} Nodes, 1487344 Edges\n",
      "\n",
      "-------------------\n",
      "STARTING EVALUATION\n",
      "-------------------\n",
      "\n",
      "Dataset - Overall: REC 52.92 PRE 49.45 MF1 68.44 AUC 79.60 TP 1629 FP 1665 TN 9363 FN 1449 | 14106 {1: 3078, 0: 11028}\n",
      "Dataset - Train: REC 63.13 PRE 57.73 MF1 72.16 AUC 82.65 TP 945 FP 692 TN 3268 FN 552 | 5457 {0: 3960, 1: 1497}\n",
      "Dataset - Val: REC 51.90 PRE 48.32 MF1 65.09 AUC 73.65 TP 518 FP 554 TN 2086 FN 480 | 3638 {1: 998, 0: 2640}\n",
      "Dataset - Test: REC 28.47 PRE 28.38 MF1 59.49 AUC 74.84 TP 166 FP 419 TN 4009 FN 417 | 5011 {0: 4428, 1: 583}\n",
      "PREDICTION STATUS - {'1-True': 2661, '0-True': 7019, '0-False': 4009, '1-False': 417}\n",
      "    >> 417 positive nodes left unpredicted...\n",
      "    >> 164 OG positive nodes left unpredicted...\n",
      "-----------------------\n",
      "PREDICTION RESULT - ROUNDS\n",
      "Dataset - Round 0: REC 59.60 PRE 49.70 MF1 70.15 AUC 81.50 TP 742 FP 751 TN 3883 FN 503 | 5879 {1: 1245, 0: 4634}\n",
      "Dataset - Round 1: REC 43.75 PRE 49.56 MF1 66.32 AUC 78.08 TP 56 FP 57 TN 402 FN 72 | 587 {1: 128, 0: 459}\n",
      "Dataset - Round 2: REC 39.84 PRE 42.50 MF1 62.68 AUC 72.98 TP 51 FP 69 TN 390 FN 77 | 587 {1: 128, 0: 459}\n",
      "Dataset - Round 3: REC 32.81 PRE 35.90 MF1 58.48 AUC 69.97 TP 42 FP 75 TN 384 FN 86 | 587 {1: 128, 0: 459}\n",
      "Dataset - Round 4: REC 33.59 PRE 32.58 MF1 57.02 AUC 64.98 TP 43 FP 89 TN 370 FN 85 | 587 {1: 128, 0: 459}\n",
      "-----------------------\n",
      "PREDICTION RESULT - SEEDS\n",
      "Seeds - Current: REC 28.12 PRE 31.58 MF1 55.76 AUC 63.49 TP 36 FP 78 TN 381 FN 92 | 587 {1: 128, 0: 459}\n",
      "Seeds - Prev: REC 0.00 PRE 0.00 MF1 39.48 AUC 63.49 TP 0 FP 76 TN 383 FN 128 | 587 {1: 128, 0: 459}\n",
      "Selecting data as budgeted ground truth for next round...\n",
      "The file ../checkpoint/working_model_file_1756868657.1658275_epoch.py does not exist\n",
      "The file ../checkpoint/working_model_file_1756868657.1658275_round-M.json does not exist\n",
      "The file ../checkpoint/working_model_file_1756868657.1658275_round-T.json does not exist\n",
      "The file ../checkpoint/working_model_file_1756868657.1658275_round.pt does not exist\n",
      "Experiment ended, experienced 0 failures\n",
      "Elapsed experiment time 39.99934053s\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import datetime\n",
    "import itertools\n",
    "import re\n",
    "import gc\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import dgl\n",
    "import pandas as pd\n",
    "\n",
    "from time import time\n",
    "from experiment.supervised_multi import MultiroundExperiment\n",
    "from utils.utils_const import DEFAULT_MAIN_CONFIG, DEFAULT_TRAIN_CONFIG, DEFAULT_ADVER_CONFIG, DEFAULT_MODEL_CONFIG,  DEFAULT_STRAT_CONFIG\n",
    "from utils.utils_const import LOSS_DICT, BACKBONE_DICT\n",
    "\n",
    "main_config = DEFAULT_MAIN_CONFIG.copy()\n",
    "train_config = DEFAULT_TRAIN_CONFIG.copy()\n",
    "model_config = DEFAULT_MODEL_CONFIG.copy()\n",
    "strat_config = DEFAULT_STRAT_CONFIG.copy()\n",
    "adver_config = DEFAULT_ADVER_CONFIG.copy()\n",
    "\n",
    "def main():\n",
    "    # HARD CONFIG\n",
    "    TRIAL_NUM = 1\n",
    "    FAILURE_LIMIT = 10\n",
    "\n",
    "    # EXPERIMENT PARAMETER GRID - Full List on utils/utils_const.py\n",
    "    EXPERIMENT_DESC = \"Example experiment - this description will be written to the log .txt file\"\n",
    "    LIST_DSET = ['tolokers_bid'] # Array of datasets\n",
    "    LIST_TRAIN_DSET = ['NONE'] # Array of datasets for training set on round #0; should be set to 'NONE' except for 'tcdataset_all' (which should be set to 'tcdataset_tr')\n",
    "    EXP_DICT = {\n",
    "      'device': ['cuda:0'],\n",
    "      'exp_type': ['ADVER'],\n",
    "      'round_num': [5],\n",
    "\n",
    "      'model_name': ['XGB-SP'],\n",
    "      'num_epoch': [100],\n",
    "      'num_round_epoch': [50],\n",
    "      'round_reset_model': [False],\n",
    "      \n",
    "      'h_feats': [64],\n",
    "      'num_layers': [2],\n",
    "      'round_window': [7],\n",
    "      'norm_name': ['layer'],\n",
    "      'temporal_agg': ['mean_final'],\n",
    "\n",
    "      'alpha': [0, 1],\n",
    "      'beta': [0, 1],\n",
    "\n",
    "      'augment_name': ['REAGE'],\n",
    "    }\n",
    "\n",
    "    outer_dfs = []\n",
    "    ts = datetime.datetime.now().strftime(\"%y%m%d%H%M%S\")\n",
    "\n",
    "    # WRITE LOG FIRST\n",
    "    if not os.path.exists(f'../result/{ts}'):\n",
    "        os.mkdir(f'../result/{ts}')\n",
    "\n",
    "    f = open(f'../result/{ts}/meta.txt', \"a\")\n",
    "    f.write(f'{EXPERIMENT_DESC}\\n\\n')\n",
    "    f.write(f'MAIN CONFIG\\n{str(main_config)}\\n\\n')\n",
    "    f.write(f'TRAIN CONFIG\\n{str(train_config)}\\n\\n')\n",
    "    f.write(f'MODEL CONFIG\\n{str(model_config)}\\n\\n')\n",
    "    f.write(f'STRAT CONFIG\\n{str(strat_config)}\\n\\n')\n",
    "    f.write(f'ADVER CONFIG\\n{str(adver_config)}\\n\\n')\n",
    "    f.write(f'EXP DICT\\n{str(EXP_DICT)}\\n\\n')\n",
    "  \n",
    "    f.close()\n",
    "\n",
    "    ### DATASET STUFF\n",
    "    for idx, dset in enumerate(LIST_DSET):\n",
    "      dataset, _ = dgl.load_graphs(f'../dataset/{dset}')\n",
    "      graph = dataset[0].long()\n",
    "    \n",
    "      if len(graph.ndata['label'].shape) > 1:\n",
    "          graph.ndata['label'] = graph.ndata['label'].argmax(1)\n",
    "          graph.ndata['label'] = graph.ndata['label'].long().squeeze(-1)\n",
    "          \n",
    "      graph.ndata['feature'] = graph.ndata['feature'].float()\n",
    "      train_config['dset_name'] = dset\n",
    "    \n",
    "      if LIST_TRAIN_DSET[idx] != 'NONE':\n",
    "        dataset, _ = dgl.load_graphs(f'../dataset/{LIST_TRAIN_DSET[idx]}')\n",
    "        train_graph = dataset[0].long()\n",
    "    \n",
    "        if len(train_graph.ndata['label'].shape) > 1:\n",
    "            train_graph.ndata['label'] = train_graph.ndata['label'].argmax(1)\n",
    "            train_graph.ndata['label'] = train_graph.ndata['label'].long().squeeze(-1)\n",
    "        train_graph.ndata['feature'] = train_graph.ndata['feature'].float()\n",
    "      else:\n",
    "        train_graph = None\n",
    "    \n",
    "      ### ADJUST BUDGETS\n",
    "      pos = (graph.ndata['label'] == 1).sum().item()\n",
    "      neg = (graph.ndata['label'] == 0).sum().item()\n",
    "    \n",
    "      main_config['round_new_pos'] = int(0.05 * pos)\n",
    "      main_config['round_new_neg'] = int(0.05 * neg)\n",
    "      main_config['round_budget_pos'] = 0\n",
    "      main_config['round_budget_neg'] = 0\n",
    "    \n",
    "      EXP_DICT['round_budget'] = [0]\n",
    "      EXP_DICT['augment_pos_ratio'] = [pos / (pos + neg) * 0.125]\n",
    "      EXP_DICT['augment_neg_ratio'] = [neg / (pos + neg) * 0.125]\n",
    "    \n",
    "      for key in ['num_epoch', 'num_round_epoch', 'early_stopping']:\n",
    "        model_config[key] = train_config[key]\n",
    "    \n",
    "      ### MAIN LOOP\n",
    "      keywords = EXP_DICT.keys()\n",
    "      combinations = list(itertools.product(*EXP_DICT.values()))\n",
    "    \n",
    "      for combi in combinations:\n",
    "        for key, val in (zip(keywords, combi)):\n",
    "          # Special cases\n",
    "          if key == 'round_budget':\n",
    "            main_config['round_budget_pos'] = int(val * pos)\n",
    "            main_config['round_budget_neg'] = int(val * neg)\n",
    "          if key == 'loss':\n",
    "            train_config['loss'] = LOSS_DICT[val]\n",
    "          if key == 'boost_agg_backbone_name':\n",
    "            model_config['boost_agg_backbone'] = BACKBONE_DICT[val]\n",
    "          if key == 'round_window':\n",
    "            model_config['round_window'] = val\n",
    "            strat_config['augment_round_split'] = val + 1\n",
    "          if key == 'dropedge_prob':\n",
    "            strat_config['dropedge_prob'] = val\n",
    "          \n",
    "          # Else just copy\n",
    "          else:\n",
    "            for cnfg in [main_config, strat_config, train_config, model_config, adver_config]:\n",
    "                if key in cnfg.keys():\n",
    "                    cnfg[key] = val\n",
    "    \n",
    "            # Other hardcoded settings\n",
    "          model_config['mlp_feats'] = model_config['h_feats']\n",
    "    \n",
    "        # Counter and container   \n",
    "        dfs, exp, dataset, graph = [], [], [], []\n",
    "        trial_counter, failure_counter = 0, 0\n",
    "    \n",
    "        start = time()\n",
    "        while trial_counter < TRIAL_NUM:\n",
    "          print(f\"================\")\n",
    "          print(f\"++++++++++++++++\")\n",
    "          print(f\"TRIAL NUMBER {trial_counter}\")\n",
    "          print(f\"++++++++++++++++\")\n",
    "          print(f\"================\")\n",
    "\n",
    "          del dataset \n",
    "          del graph\n",
    "          del train_graph\n",
    "          del exp\n",
    "          gc.collect()\n",
    "            \n",
    "          # REREAD GRAPH DATA\n",
    "          print(f\"  > Rereading graph data...\")\n",
    "          dataset, _ = dgl.load_graphs(f'../dataset/{dset}')\n",
    "          graph = dataset[0].long()\n",
    "    \n",
    "          if len(graph.ndata['label'].shape) > 1:\n",
    "              graph.ndata['label'] = graph.ndata['label'].argmax(1)\n",
    "              graph.ndata['label'] = graph.ndata['label'].long().squeeze(-1)\n",
    "          graph.ndata['feature'] = graph.ndata['feature'].float()\n",
    "          train_config['dset_name'] = dset\n",
    "    \n",
    "          if LIST_TRAIN_DSET[idx] != 'NONE':\n",
    "            dataset, _ = dgl.load_graphs(f'../dataset/{LIST_TRAIN_DSET[idx]}')\n",
    "            train_graph = dataset[0].long()\n",
    "    \n",
    "            if len(train_graph.ndata['label'].shape) > 1:\n",
    "                train_graph.ndata['label'] = train_graph.ndata['label'].argmax(1)\n",
    "                train_graph.ndata['label'] = train_graph.ndata['label'].long().squeeze(-1)\n",
    "            train_graph.ndata['feature'] = train_graph.ndata['feature'].float()\n",
    "          else:\n",
    "            train_graph = None\n",
    "    \n",
    "          print(f\"  > Initializing multiround object...\")\n",
    "          print(graph.num_nodes(), graph.num_edges())\n",
    "          \n",
    "          exp = MultiroundExperiment(\n",
    "            graph, train_graph=train_graph,\n",
    "            main_config=main_config, model_config=model_config, strat_config=strat_config, \n",
    "            adver_config=adver_config, train_config=train_config\n",
    "          )\n",
    "          \n",
    "          # Adversarial Round\n",
    "          round_counter = 0\n",
    "          round_flag = True\n",
    "          while (round_counter < main_config['round_num']) and (round_flag):\n",
    "            print(f\"  > Starting round {round_counter}...\")\n",
    "            round_flag = exp.one_round_node(round_counter)\n",
    "            round_counter = round_counter + 1\n",
    "    \n",
    "          # Check if round successful or need hard reset\n",
    "          if round_flag:\n",
    "            eval_df = pd.DataFrame(sum([r['log_single_eval'] for r in exp.rounds], []), columns=['round', 'eval_type', 'time', 'rec', 'prec', 'f1', 'auc', 'tp', 'fp', 'tn', 'fn']) # Evaluation log\n",
    "            trainlog_df = pd.DataFrame([r['log_round'] for r in exp.rounds]) # Round training log\n",
    "            log_df = pd.merge(left=eval_df, right=trainlog_df, on='round', how='left')\n",
    "            \n",
    "            # Other global log            \n",
    "            log_df['trial'] = trial_counter\n",
    "            log_df['num_nodes'] = graph.num_nodes()\n",
    "            log_df['num_edges'] = graph.num_edges()\n",
    "            \n",
    "            dfs.append(log_df)\n",
    "            trial_counter = trial_counter + 1\n",
    "          else:\n",
    "            failure_counter = failure_counter + 1\n",
    "    \n",
    "          if failure_counter > FAILURE_LIMIT:\n",
    "            raise Exception('Too many failed experiments!')\n",
    "          \n",
    "          # Remove checkpoints\n",
    "          exp.clean_temp_files()\n",
    "    \n",
    "        print(f'Experiment ended, experienced {failure_counter} failures')\n",
    "        print(f'Elapsed experiment time {time() - start:.8f}s')\n",
    "        \n",
    "        # Save artifacts per config setting\n",
    "        if main_config['save_df']:\n",
    "          final_df = pd.concat(dfs)\n",
    "          \n",
    "          for cnfg in [main_config, strat_config, model_config, train_config, adver_config]:\n",
    "            for key, value in cnfg.items():\n",
    "                if key not in ['verbose']:\n",
    "                    final_df[key] = str(value)\n",
    "    \n",
    "          final_df['timestamp'] = ts\n",
    "          \n",
    "          stripped = [re.sub(r\"\\W+\", \"\", str(val))[:6] for val in combi]\n",
    "          suffix = '-'.join(stripped)\n",
    "          final_df.to_csv(f'../result/{ts}/{dset}-{suffix}-E.csv')\n",
    "          outer_dfs.append(final_df)\n",
    "        \n",
    "        if main_config['save_embedding']:\n",
    "          exp.model.set_graph(exp.dset['graph'], round_num=(main_config['round_num']-1), device=main_config['device'])\n",
    "          embedding = exp.model.embed_nodes(exp.model.graph, exp.model.graph.ndata['feature'])\n",
    "          torch.save(torch.linalg.vector_norm(embedding[:,int(embedding.shape[1] / 2):], ord=2, dim=1), f'../result/{ts}/{dset}-{suffix}-TEMP.pt')\n",
    "          torch.save(torch.linalg.vector_norm(embedding[:,:int(embedding.shape[1] / 2)], ord=2, dim=1), f'../result/{ts}/{dset}-{suffix}-NONTEMP.pt')\n",
    "    \n",
    "    # Save overall artifacts\n",
    "    if main_config['save_df']:\n",
    "      final_outer_df = pd.concat(outer_dfs)\n",
    "      final_outer_df.to_csv(f'../result/{ts}/combined_result.csv')\n",
    "\n",
    "if __name__ == \"__main__\":    \n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dgl_pyg_cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
